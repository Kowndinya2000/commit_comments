    Update README.md
    Update README.md
    Trivial test change
    Test commit
    Test change to readme.
    Create README.md
    Adding Dominic's Serendipity affiliation.
    Adding Manuel as an author, with many thanks for starting the infer package. :)
    Adding new contributors to the AUTHORS file, with heartfelt thanks
    Added Yevgeniy to AUTHORS and copyright statement to test/pitt/search/semanticvectors/ThreadSafetyTest.java
    Added license to Andrew MacKinlay's CompareTermsBatch, and added Andrew to AUTHORS file.
    Adding AUTHORS file for managing copyright issues.
    Updating version number in ant build file (though it's not really maintained)
    Changes to test utilities to handle windows and unix pathnames.
    Updating ant build number
    Updating build numbers before release 5.4
    Updating build.xml version number
    Updating version number to 4.3 now that I hope 4.2 is being synced and distributed.
    Updating version numbers, trying to get a release out
    Updating build.xml
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Updating version number to 4.1 after release of 4.0.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Updating build version to 3.9, after shipping 3.8.
    (Last commit - should be working properly now.)
    
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Version update to 3.7 after shipping 3.6.
    Updating version number to 3.5 after releasing 3.4.
    Updated version number to 3.3 after releasing version 3.2.
    Incrementing version number to 3.1 after shipping 3.0. Woohoo\!
    Hacked away at our logging verbosity. I believe (I think) that normal operation is much easier to interpret using VerbatimLogger, and that we should use other loggers for debugging. I'd like to write / find a one-line logging message which is basically verbatim with a file and line number prefixed.
    Cleaned up PrincipalComponents and Plot2dVectors. Removed PrincipalComponents2, which has become PrincipalComponents anyway.
    Updated version number to 2.5 after shipping 2.4. And related cleanup.
    Incrementing version number to 2.3.
    Fixed trailing slash which makes thirdparty code actually get included.
    Updating version number to 2.1, new dev version after shipping 2.0.
    Updated version number to 2.0 (should update to 2.01 as soon as javadoc us updated.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Small buildfile changes to get thirdparty deps understood by javadoc target.
    Better dependency order for running the unit and integration tests.
    Making build file work with env classpath. Fixed some javadoc and indentation.
    Added classpath to compile-deps, compile-tests, and compile-ext.
    Set "includeantruntime" to false in build.xml. This stops ant from producing a warning every time javac is called.
    No code calls anything from ant so there is no existing reason to include ant's runtime
    Removed the echos for debugging.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Build file improvements from johann.petrak that enable dependencies to be configured by variables in the build file (read from environment variables), as an alternatvie to doing everything through CLASSPATH.
    Updating version number after shipping version 1.30.
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    Updated version number after shipping 1.26.
    Shipped 1.24. Updated version number to 1.25.
    Added a couple of javadocs.
    Removed some deprecated html javadocs. Updated version number to 1.21 (should have done this before).
    Build file configured to empty test/testdata/tmp dir before and after running.
    Trying to refactor ubild paths for cleaner / easier distinction between src and src extensions.
    Continued refactoring, wired command line flags into BuildIndex, made some changes to tests to remove Java internal deletion code that isn't reliable.
    Test data was not being cleaned up effectively by Java on Ubuntu ... ahve added ant support for deleting, and made testing for deletes during the testing process more explicit.
    Added RegressionTests class and some real improvements to RunTests to run both unit tests and regression tests and report results.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Updating build file to v1.19 after shipping v1.18
    Added a test for CompoundVectorBuilder and VectorStoreRAM, and utils for outputting principal component plots as tex source.
    Adding visualization utils. Added extra sophistication to build file to support this: now, utils will ship with the package but will not compile by default, to avoid extra dependencies for basic users.
    Buildfile updated to version 1.17.
    Added LICENSE file to build.xml file for distribution.
    Changind build.xml file so that distributions contain AUTHORS file.
    Updated version number, set one Javadoc property, change BuildPositionalIndex to only create docvectors in BASIC mode.
    Updating version number to 1.13
    Minor changes to usage messages while releasing version 1.12
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Remembering to update version number to 1.11 after shipping 1.10 ;-)
    Updated version number in build file and fixed some minor typos.
    Some cosmentic changes including removing legacy javadoc html files.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    Edited build file: dev version is now 1.7
    Committing javadoc for new classes, including all pitt.search.lucene package.
    Alterations to build file, to i. ship less and build more, and ii. compile and run tests if checked out of svn.
    Several changes for getting the bilingual version working, and
    associated functionality.
    i. BuildBilingualIndex now works! (According to tests on europarl english-french data.)
    ii. TermVectorsFromLucene edited to properly enable reuse of basicDocVectors.
    
    This isn't perfect yet, by any means: it's probably inefficient in how much state is kept around when BuildBilingualIndex reuses TermVectorsFromLucene objects, and the interface to TermVectorsFromLucene could probably be improved. Since neither of these is a really basic class, I haven't thought too deeply about this - just don't write code that depends on these without rethinking and probably refactoring.
    
    iii. Added a scripts directory, initially with the preprocessing script for the europarl chapter alignment,
    iv. Added READMEs to the test and scripts directories.
    v. Changed build.xml file so that tests aren't automatically compiled (this is so that casual users of the source won't have to depend on JUnit).
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Updated build to 1.5 after shipping 1.4.
    Changed one .out.println to a .err.println, and updated the build to 1.4, hopefully about to ship.
    Did some slight refactoring to make for more robust backwards compatibility with old indexes, with usage messages if things go wrong.
    Updated build.xml to version 1.3, after shipping version 1.2.
    Added index optimization before opening in TermVectorsFromLucene to make sure that docids are contiguous integers; avoids out-of-bounds error reported by Lionel.
    Added command line SearchTensorRelation interface. Results don't look too promising: where things work well, it's because tensor similarities just reduce to products of scalar products of the factors  (I think).
    
    Also adding product functions to VectorUtils and appropriate html javadoc files.
    Refactored to have search go through a VectorSearcher object that enables the use of different similarity scores while using common getNearestNeighbors implementation. Appears to be working fine, next step is to add some other similarity scores, e.g., using tensors.
    More tweaking of javadoc.
    Created CompareTerms command line interface, and CompoundVectorBuilder which now handles query building from several query terms for both Search and CompareTerms.
    Updated build.xml to include zip distribution, and so that tar and zip distributions both unzip files into a new directory with the semanticvectors- prefix.
    Configured jar and src (.tar.gz) build of project appropriately. Small change to Search usage string.
    Initial semantic vectors package
    Adding old download-counting script.
    Added VectorStoreReaderRAMCache and small test of this.
    Several changes for getting the bilingual version working, and
    associated functionality.
    i. BuildBilingualIndex now works! (According to tests on europarl english-french data.)
    ii. TermVectorsFromLucene edited to properly enable reuse of basicDocVectors.
    
    This isn't perfect yet, by any means: it's probably inefficient in how much state is kept around when BuildBilingualIndex reuses TermVectorsFromLucene objects, and the interface to TermVectorsFromLucene could probably be improved. Since neither of these is a really basic class, I haven't thought too deeply about this - just don't write code that depends on these without rethinking and probably refactoring.
    
    iii. Added a scripts directory, initially with the preprocessing script for the europarl chapter alignment,
    iv. Added READMEs to the test and scripts directories.
    v. Changed build.xml file so that tests aren't automatically compiled (this is so that casual users of the source won't have to depend on JUnit).
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    This class takes a tab-delimited text file of the form [subject] [predicate] [object], and transforms it into a Lucene index in which each document represents a single predication. This index will be used to build PSI models. "nationalfacts.txt" is an incomplete set of facts extracted from Wikipedia entries to do with countries, for testing, development and demonstration.
    Removed duplicate row
    Changes to make experimental results on tabular data more reproducible  / configurable.
    Presidents tabular datasets.
    Presidents tabular datasets.
    Some fixing to tests. Not good solutions but things are busy.
    Fixing test to keep up with changes to complex normalization and similarity measurement.
    Changes to make experimental results on tabular data more reproducible  / configurable.
    Introducing experiments using learned character vectors. Some refactoring and testing of orthography code to support learned character vectors.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    More CompareTerms tests that make sure that orthographic similarity is working for real and complex and complexflat numbers.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Changes to weighted superposition (with the purpose of supporting bookend vectors for continuous quantities).
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Refactored CompareTerms slightly to be able to run in tests. Coded up a couple of tests for orthographic usage.
    
    This should help to figure out why the complex version is giving NaNs at the moment.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Changed negation term from NOT to ~NOT, and fixed tests accordingly. Minor refactoring and javadoc synchronizing.
    Getting much closer to version 3.0. Loads and loads of changes. All (current) tests pass, but I think some binary positional indexing is still broken.
    
    Main changes include:
    
    - Making dimension and vector type part of the interface of all vector stores. So less reliance on flags. Nonetheless flags are still important, and the challenge of figuring out who can set dimension where and when remains unsolved and difficult.
    
    - Lots of improvements to tests, including making unit tests out of some formerly regression-style tests.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    Committing a compiling version that is broken at runtime. This is normally a bad thing to do but I nearly lost my hard drive the other day and I've got the fear about losing this work! If this breaks your build in the branch then please revert.
    Lots of work on the typed vectors branch. Ready to start integrating use of Vectors package into the semanticvectors main operations I think. Comments welcome.
    (Recommit after eclipse mismatch.) Added dimension setting and checking to VectorStoreRAM.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Added flag configuration for lucene contents fields and docid field. this should be released soon.
    Fixed a lowercasing-misses-negation bug, and added unit test for negation including new flag.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Added a test for CompoundVectorBuilder and VectorStoreRAM, and utils for outputting principal component plots as tex source.
    Refactored DocVectors.java to use VectorStoreRAM, and added addVector method and test to VectorStoreRAM.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Some cosmentic changes including removing legacy javadoc html files.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Fixed a couple of binary vector tests.
    Checked in a simpler method of generating random vectors for the special case in which dimension==seedlength, and updated one of the unit tests to reflect the new determinism.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Added unit test for VectorStoreDeterministic
    Adding simulations / samples of real vectors and their similarities, and associated trivial stat utils.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Added unit test for VectorStoreDeterministic
    Fixing the build in trunk. Looks good for now.
    Getting much closer to version 3.0. Loads and loads of changes. All (current) tests pass, but I think some binary positional indexing is still broken.
    
    Main changes include:
    
    - Making dimension and vector type part of the interface of all vector stores. So less reliance on flags. Nonetheless flags are still important, and the challenge of figuring out who can set dimension where and when remains unsolved and difficult.
    
    - Lots of improvements to tests, including making unit tests out of some formerly regression-style tests.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Corrected an error in PermutationUtils, and added the test to AllTests, my bad for not having it there before which masked the bug.
    Started to wrap complex vectors with a couple of unit tests. These ones break at the end but I hope this is a good way of getting common understanding of what should be happening.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    Added Lucene I/O and some tests to BinaryVector (and some interfering whitespace / style cleanup, hopt this isn't annoying).
    Lots of work on the typed vectors branch. Ready to start integrating use of Vectors package into the semanticvectors main operations I think. Comments welcome.
    New test suite.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Removing test for deprecated flags metadata routines. Thought I'd committed this already.
    Cleaning up expectations in tests.
    Turned searchtype into an enum, apparently successfully.
    
    Renamed LOG_ENTROPY to LOGENTROPY, realizing that this too will be passed normally as -termweight logentropy.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Corrected an error in PermutationUtils, and added the test to AllTests, my bad for not having it there before which masked the bug.
    Put the binary dimension and seedlength constraints into flag parsing. I think this is the right thing to do for now.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    Lots of work on the typed vectors branch. Ready to start integrating use of Vectors package into the semanticvectors main operations I think. Comments welcome.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Added flag configuration for lucene contents fields and docid field. this should be released soon.
    Added RegressionTests class and some real improvements to RunTests to run both unit tests and regression tests and report results.
    Moving forward with flag descriptions and value enumerations.
    Flags library now uses reflection, using ClusterResults as test case.
    Initial commit of new flags library.
    Fixed VectorStoreReader and Writer closing operations by using FSDirectory instead of MMapDirectory. This seems to be working, tests fixed accordingly.
    Small change in close() for VectorStoreReader, but with consequences in catching undeclared NullPointerExceptions, in main Search code as well as in tests. Trying to track down some read - write problems, see http://groups.google.com/group/semanticvectors/browse_thread/thread/48198c639c9b6d72/814e034b34f5af62#814e034b34f5af62 for details.
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Some cosmentic changes including removing legacy javadoc html files.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Getting much closer to version 3.0. Loads and loads of changes. All (current) tests pass, but I think some binary positional indexing is still broken.
    
    Main changes include:
    
    - Making dimension and vector type part of the interface of all vector stores. So less reliance on flags. Nonetheless flags are still important, and the challenge of figuring out who can set dimension where and when remains unsolved and difficult.
    
    - Lots of improvements to tests, including making unit tests out of some formerly regression-style tests.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    Committing a compiling version that is broken at runtime. This is normally a bad thing to do but I nearly lost my hard drive the other day and I've got the fear about losing this work! If this breaks your build in the branch then please revert.
    Lots of work on the typed vectors branch. Ready to start integrating use of Vectors package into the semanticvectors main operations I think. Comments welcome.
    (Recommit after eclipse mismatch.) Added dimension setting and checking to VectorStoreRAM.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Added a test for CompoundVectorBuilder and VectorStoreRAM, and utils for outputting principal component plots as tex source.
    Refactored DocVectors.java to use VectorStoreRAM, and added addVector method and test to VectorStoreRAM.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Some cosmentic changes including removing legacy javadoc html files.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    Committing a compiling version that is broken at runtime. This is normally a bad thing to do but I nearly lost my hard drive the other day and I've got the fear about losing this work! If this breaks your build in the branch then please revert.
    Lots of work on the typed vectors branch. Ready to start integrating use of Vectors package into the semanticvectors main operations I think. Comments welcome.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Some cosmentic changes including removing legacy javadoc html files.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Corrected test so that it doesn't get confused by binary dimension setting to be a multiple of 64.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Getting much closer to version 3.0. Loads and loads of changes. All (current) tests pass, but I think some binary positional indexing is still broken.
    
    Main changes include:
    
    - Making dimension and vector type part of the interface of all vector stores. So less reliance on flags. Nonetheless flags are still important, and the challenge of figuring out who can set dimension where and when remains unsolved and difficult.
    
    - Lots of improvements to tests, including making unit tests out of some formerly regression-style tests.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Trying to make tests run with the Eclipse (or hopefully any other) JUnit test runner. Not quite there yet but working on it.
    Small addition of factory class VectorStoreReader.
    Fixed VectorStoreReader and Writer closing operations by using FSDirectory instead of MMapDirectory. This seems to be working, tests fixed accordingly.
    Small change in close() for VectorStoreReader, but with consequences in catching undeclared NullPointerExceptions, in main Search code as well as in tests. Trying to track down some read - write problems, see http://groups.google.com/group/semanticvectors/browse_thread/thread/48198c639c9b6d72/814e034b34f5af62#814e034b34f5af62 for details.
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Some cosmentic changes including removing legacy javadoc html files.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Changes to make experimental results on tabular data more reproducible  / configurable.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Getting much closer to version 3.0. Loads and loads of changes. All (current) tests pass, but I think some binary positional indexing is still broken.
    
    Main changes include:
    
    - Making dimension and vector type part of the interface of all vector stores. So less reliance on flags. Nonetheless flags are still important, and the challenge of figuring out who can set dimension where and when remains unsolved and difficult.
    
    - Lots of improvements to tests, including making unit tests out of some formerly regression-style tests.
    Fixes to text vector store. Integration tests now all pass. (These are the old integration tests, don't test the complex or binary paths). More problems with dimension being either a local or a global variable crop up, fixed for now but it's not a pretty thing.
    Committing a compiling version that is broken at runtime. This is normally a bad thing to do but I nearly lost my hard drive the other day and I've got the fear about losing this work! If this breaks your build in the branch then please revert.
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Closed some vector stores so that VectorStoreReaderLuceneTest passes on XP, cleans up lost file handles on all platforms.
    Added closeIndexInput() method to VectorStoreReaderLucene to allow a thread to close its own index input without closing the underlying directory, so that the other threads can continue to use the underlying directory.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    I think this fixes the worst of issue 21. Unit tests in place, though getting failures from the inside of threads seems to be a problem.
    working on tracking down threading issue.
    Adding unit test for thread safety, mainly contributed by Yevgeniy. Works when synchronized, breaks when synchronized is commented out.
    Trying to make tests run with the Eclipse (or hopefully any other) JUnit test runner. Not quite there yet but working on it.
    Small addition of factory class VectorStoreReader.
    Fixed VectorStoreReader and Writer closing operations by using FSDirectory instead of MMapDirectory. This seems to be working, tests fixed accordingly.
    Small change in close() for VectorStoreReader, but with consequences in catching undeclared NullPointerExceptions, in main Search code as well as in tests. Trying to track down some read - write problems, see http://groups.google.com/group/semanticvectors/browse_thread/thread/48198c639c9b6d72/814e034b34f5af62#814e034b34f5af62 for details.
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Added a test for CompoundVectorBuilder and VectorStoreRAM, and utils for outputting principal component plots as tex source.
    Refactored DocVectors.java to use VectorStoreRAM, and added addVector method and test to VectorStoreRAM.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Some cosmentic changes including removing legacy javadoc html files.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Fixed test, caveat emptor comments, some formatting.
    updates to tests to reflect number of vectors increased by alpha an omega vectors.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    New test showing that binary orthographic vectors are (rather unfortunately) the same as their nearer demarcator vector, because of the majority voting rules.
    This is a test written to demonstrate a drawback in the linear generation system explained by Trevor.
    Trying out a linear version of number-vector allocation rather than the recursive one. So far results look the same and it looks like a nice simplification.
    
    Removed the constraint that start index must be zero, with appropriate test to check that this is working and some code-fixes accordingly.
    Initial tests for NumberRepresentation (again, this partly helps to document my understanding of this class).
    Changes to make experimental results on tabular data more reproducible  / configurable.
    Introducing experiments using learned character vectors. Some refactoring and testing of orthography code to support learned character vectors.
    Continuous table values work nicely now for all vectortypes.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Initial work on making semantic vectors from rows in tables. Number representations not wired in yet.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Getting much closer to version 3.0. Loads and loads of changes. All (current) tests pass, but I think some binary positional indexing is still broken.
    
    Main changes include:
    
    - Making dimension and vector type part of the interface of all vector stores. So less reliance on flags. Nonetheless flags are still important, and the challenge of figuring out who can set dimension where and when remains unsolved and difficult.
    
    - Lots of improvements to tests, including making unit tests out of some formerly regression-style tests.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    Committing a compiling version that is broken at runtime. This is normally a bad thing to do but I nearly lost my hard drive the other day and I've got the fear about losing this work! If this breaks your build in the branch then please revert.
    Lots of work on the typed vectors branch. Ready to start integrating use of Vectors package into the semanticvectors main operations I think. Comments welcome.
    (Recommit after eclipse mismatch.) Added dimension setting and checking to VectorStoreRAM.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Added a test for CompoundVectorBuilder and VectorStoreRAM, and utils for outputting principal component plots as tex source.
    Refactored DocVectors.java to use VectorStoreRAM, and added addVector method and test to VectorStoreRAM.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Some cosmentic changes including removing legacy javadoc html files.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Build indexes in tmp rather than root directory, and ensure dense
    vectors for real and complex ESP models
    Tests for ESP. Complex vector tests are failing at this point. This may
    have to do with the shift from Dense Polar to Hermitian representations
    as the default for complex vectors.
    Fixed some test numbers to be more lenient.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    RunSearch -> runSearch for more normal Java style.
    Fixed real PSI (w00t!). Turns out it was to do with the inexact release / bind inverse relationship, so we shouldn't use this in training.
    Enabled maven javadoc
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Tests for real permutation PSI, and some refactoring.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Improved testing: some integration tests work in eclipse if you set the directory in the run configurations menu. I'm not sure how to check in this eclipse config change yet.
    Removed extra flag settings from tests that were the legacy of bleedover. A small pleasure :)
    Adapting PSITest to use -luceneindexpath, to be like other main entry points.
    Reordered tests to avoid flag bleedover (an increasingly bad problem).
    This contains an attempt at a binary equivalent of the Gram-Schmidt orthogonalization process for pseudo-subspace search and perhaps also negation. Said attempt is found in BinaryVectorUtils, which is called from VectorSearcher/CompoundVectorBuilder instead of the usual orthogonalizevectors method when binary vectors are used. There's probably a more elegant way to integrate these sorts of special cases, but I thought it would be worth including to attempt to bring the different vector representations closer to functional equivalence.
    Added unit tests for binary and complex editions of PSI
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Refactored integration tests slightly to always use positional_index. This means that all indexes for testing are created using our own IndexFilePositions, and thus ignore dot files (.svn ... etc.). LSA test now passes reliably.
    Document vector writing fix and regression tests for LSA.
    RunSearch -> runSearch for more normal Java style.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Improved testing: some integration tests work in eclipse if you set the directory in the run configurations menu. I'm not sure how to check in this eclipse config change yet.
    Removed extra flag settings from tests that were the legacy of bleedover. A small pleasure :)
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Refactored integration tests slightly to always use positional_index. This means that all indexes for testing are created using our own IndexFilePositions, and thus ignore dot files (.svn ... etc.). LSA test now passes reliably.
    Document vector writing fix and regression tests for LSA.
    Suggested alterations needed to get complex vector unit tests green again.
    Fixed some test numbers to be more lenient.
    Fixing tests and some orthogonalization behavior.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Changes to test utilities to handle windows and unix pathnames.
    Making doc vectors use external IDs (path or filenames) rather than Lucene's internal integer IDs throughout.
    RunSearch -> runSearch for more normal Java style.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Minor tweaking of regression test expected results.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Small improvement to asserts. Tests sill fail under ant, and large tests in eclipse. Single tests work under eclipse most of the time. Confused.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Improved testing: some integration tests work in eclipse if you set the directory in the run configurations menu. I'm not sure how to check in this eclipse config change yet.
    Turned -positionalmethod into an enum. This concludes the effort of turning flag values into enums.
    Removed extra flag settings from tests that were the legacy of bleedover. A small pleasure :)
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Changed negation term from NOT to ~NOT, and fixed tests accordingly. Minor refactoring and javadoc synchronizing.
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Minor edits and improvements to test harness; no solution to remaining last weird test (see comments).
    Hooray - complex directional indexing works very well after all, just needs term weighting. Good to know!
    
    Fixed regression tests accordingly, and made minor improvement to test runner.
    Reordered tests to avoid flag bleedover (an increasingly bad problem).
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Changing Vector from abstract class to interface.
    Refactored integration tests slightly to always use positional_index. This means that all indexes for testing are created using our own IndexFilePositions, and thus ignore dot files (.svn ... etc.). LSA test now passes reliably.
    Document vector writing fix and regression tests for LSA.
    Got binary directional search working. Complex convolution still needs fixed.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Regression tests all pass now. We might Code complete for v3.0.
    Added a test for getMaximumSharedWeight and addition and normalization.
    
    I think getMaxiumumSharedWeight may be broken, or I don't understand it properly. It looks like it is most likely to return zero unless there is an entire row of 1s in the voting record, which seems highly unlikely.
    
    Also uncovered a "last bit gets turned to 1" behavior which I'm not sure is intended.
    
    Some name changing and commenting, let me know if there are any objections or misunderstandings here.
    Very grubby hacking of testBuildAndSearchBinaryPermutationIndex to isolate test breakage - BuildPositionalIndex is fine, Search is broken. More generally we need to improve the test runner to make this more obvious.
    Need to remove the Sys.exit call from BuildPositionalIndex so that remaining tests actually run.
    A few small changes, all tests seem to run successfully now. Firstly, increased the dimensionality of the binary vectors in the tests to even the playing field some, now meets the assertTrue(peterRank < 5) tests. Secondly, and more mysteriously,  BuildPositionalIndex hangs at the end of the Main method unless explicitly killed. I've included a "System.exit(0)" at the end, which allows all the tests to complete, but is hardly an elegant solution. I'd be interested to hear if this fixes things elsewhere, and will attempt to seek out the arcane Java mechanism or trivial oversight responsible for the persistence of BuildPositionalIndex.
    Some changes trying to get binary permutation search to work.
    Fixing the build in trunk. Looks good for now.
    More regression tests check out. The one that files is binary permutation indexing, but I'm checking this in any way because it looks like the test is right and the implementation needs fixing.
    All tests working with wrappers making complex and binary regresstion testing easy.
    Getting much closer to version 3.0. Loads and loads of changes. All (current) tests pass, but I think some binary positional indexing is still broken.
    
    Main changes include:
    
    - Making dimension and vector type part of the interface of all vector stores. So less reliance on flags. Nonetheless flags are still important, and the challenge of figuring out who can set dimension where and when remains unsolved and difficult.
    
    - Lots of improvements to tests, including making unit tests out of some formerly regression-style tests.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Fixes to text vector store. Integration tests now all pass. (These are the old integration tests, don't test the complex or binary paths). More problems with dimension being either a local or a global variable crop up, fixed for now but it's not a pretty thing.
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    New TestUtils method for starting a child process and redirecting I/O to prevent deadlock. Child I/O can be easily redirected to parent's I/O or another source/sink. Child process is started with the same arguments and environment as the parent, facilitating debugging and ensuring proper classpaths. Child process is identified by a Class object which forces the compiler to verify that the child class is in the classpath at compile time rather than runtime. Added helper to start a process with a Scanner, and closed a few abandoned Scanners in the test suite.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Really am changing the RegressionTests.
    Fixed one of the tests just by allocating extra memory with a hack in TermTermVectorsFromLucene ... need to figure out what the array index bound actually is.
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    Removed some deprecated html javadocs. Updated version number to 1.21 (should have done this before).
    Added small extra test for balanced_permutation.
    Made some improvements and fixed bugs in test suite.
    Trying to make tests run with the Eclipse (or hopefully any other) JUnit test runner. Not quite there yet but working on it.
    Small addition of factory class VectorStoreReader.
    Refactoring and testing of permutation indexes in TermTermVectorsFromLucene.
    Flags refactor and regression testing of BuildPositionalIndex.
    Continued refactoring, wired command line flags into BuildIndex, made some changes to tests to remove Java internal deletion code that isn't reliable.
    Added RegressionTests class and some real improvements to RunTests to run both unit tests and regression tests and report results.
    Fixed some test numbers to be more lenient.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    RunSearch -> runSearch for more normal Java style.
    Fixed real PSI (w00t!). Turns out it was to do with the inexact release / bind inverse relationship, so we shouldn't use this in training.
    Enabled maven javadoc
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Tests for real permutation PSI, and some refactoring.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Improved testing: some integration tests work in eclipse if you set the directory in the run configurations menu. I'm not sure how to check in this eclipse config change yet.
    Removed extra flag settings from tests that were the legacy of bleedover. A small pleasure :)
    Adapting PSITest to use -luceneindexpath, to be like other main entry points.
    Reordered tests to avoid flag bleedover (an increasingly bad problem).
    This contains an attempt at a binary equivalent of the Gram-Schmidt orthogonalization process for pseudo-subspace search and perhaps also negation. Said attempt is found in BinaryVectorUtils, which is called from VectorSearcher/CompoundVectorBuilder instead of the usual orthogonalizevectors method when binary vectors are used. There's probably a more elegant way to integrate these sorts of special cases, but I thought it would be worth including to attempt to bring the different vector representations closer to functional equivalence.
    Added unit tests for binary and complex editions of PSI
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Refactored integration tests slightly to always use positional_index. This means that all indexes for testing are created using our own IndexFilePositions, and thus ignore dot files (.svn ... etc.). LSA test now passes reliably.
    Document vector writing fix and regression tests for LSA.
    Build indexes in tmp rather than root directory, and ensure dense
    vectors for real and complex ESP models
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Improved testing: some integration tests work in eclipse if you set the directory in the run configurations menu. I'm not sure how to check in this eclipse config change yet.
    Build Lucene indexes in tests using calls to main() functions, not forking a subprocess.
    
    (More generally, I'd like to get the tests to run under Eclipse, but this shouldn't be a priority.)
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Cleanup revision. Have removed stray javadocs instead of fixing them - this is interim but good enough for now.
    Hooray - complex directional indexing works very well after all, just needs term weighting. Good to know!
    
    Fixed regression tests accordingly, and made minor improvement to test runner.
    Reordered tests to avoid flag bleedover (an increasingly bad problem).
    Added unit tests for binary and complex editions of PSI
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Removed unnecessary import.
    Refactored integration tests slightly to always use positional_index. This means that all indexes for testing are created using our own IndexFilePositions, and thus ignore dot files (.svn ... etc.). LSA test now passes reliably.
    Document vector writing fix and regression tests for LSA.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Reinstated and fixed ThreadSafetyTest.
    Need to remove the Sys.exit call from BuildPositionalIndex so that remaining tests actually run.
    Some changes trying to get binary permutation search to work.
    Getting much closer to version 3.0. Loads and loads of changes. All (current) tests pass, but I think some binary positional indexing is still broken.
    
    Main changes include:
    
    - Making dimension and vector type part of the interface of all vector stores. So less reliance on flags. Nonetheless flags are still important, and the challenge of figuring out who can set dimension where and when remains unsolved and difficult.
    
    - Lots of improvements to tests, including making unit tests out of some formerly regression-style tests.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Fixes to text vector store. Integration tests now all pass. (These are the old integration tests, don't test the complex or binary paths). More problems with dimension being either a local or a global variable crop up, fixed for now but it's not a pretty thing.
    Fixed compile erros in 2d plotter (not tested yet).
    Lots of work on the typed vectors branch. Ready to start integrating use of Vectors package into the semanticvectors main operations I think. Comments welcome.
    New TestUtils method for starting a child process and redirecting I/O to prevent deadlock. Child I/O can be easily redirected to parent's I/O or another source/sink. Child process is started with the same arguments and environment as the parent, facilitating debugging and ensuring proper classpaths. Child process is identified by a Class object which forces the compiler to verify that the child class is in the classpath at compile time rather than runtime. Added helper to start a process with a Scanner, and closed a few abandoned Scanners in the test suite.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    working on tracking down threading issue.
    Adding unit test for thread safety, mainly contributed by Yevgeniy. Works when synchronized, breaks when synchronized is commented out.
    Added flag configuration for lucene contents fields and docid field. this should be released soon.
    Made some improvements and fixed bugs in test suite.
    Trying to make tests run with the Eclipse (or hopefully any other) JUnit test runner. Not quite there yet but working on it.
    Flags refactor and regression testing of BuildPositionalIndex.
    Continued refactoring, wired command line flags into BuildIndex, made some changes to tests to remove Java internal deletion code that isn't reliable.
    Test data was not being cleaned up effectively by Java on Ubuntu ... ahve added ant support for deleting, and made testing for deletes during the testing process more explicit.
    Added RegressionTests class and some real improvements to RunTests to run both unit tests and regression tests and report results.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Flags library now uses reflection, using ClusterResults as test case.
    Small change in close() for VectorStoreReader, but with consequences in catching undeclared NullPointerExceptions, in main Search code as well as in tests. Trying to track down some read - write problems, see http://groups.google.com/group/semanticvectors/browse_thread/thread/48198c639c9b6d72/814e034b34f5af62#814e034b34f5af62 for details.
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Added a test for CompoundVectorBuilder and VectorStoreRAM, and utils for outputting principal component plots as tex source.
    Refactored DocVectors.java to use VectorStoreRAM, and added addVector method and test to VectorStoreRAM.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Getting much closer to version 3.0. Loads and loads of changes. All (current) tests pass, but I think some binary positional indexing is still broken.
    
    Main changes include:
    
    - Making dimension and vector type part of the interface of all vector stores. So less reliance on flags. Nonetheless flags are still important, and the challenge of figuring out who can set dimension where and when remains unsolved and difficult.
    
    - Lots of improvements to tests, including making unit tests out of some formerly regression-style tests.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    Fixed indentation, no substantive changes.
    Changed scanner delimiter for platform independence.
    Makes testBuildAndSearchBasicIndex work on Windows XP.
    New TestUtils method for starting a child process and redirecting I/O to prevent deadlock. Child I/O can be easily redirected to parent's I/O or another source/sink. Child process is started with the same arguments and environment as the parent, facilitating debugging and ensuring proper classpaths. Child process is identified by a Class object which forces the compiler to verify that the child class is in the classpath at compile time rather than runtime. Added helper to start a process with a Scanner, and closed a few abandoned Scanners in the test suite.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Adding TestUtils
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    RunSearch -> runSearch for more normal Java style.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Improved testing: some integration tests work in eclipse if you set the directory in the run configurations menu. I'm not sure how to check in this eclipse config change yet.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Refactored integration tests slightly to always use positional_index. This means that all indexes for testing are created using our own IndexFilePositions, and thus ignore dot files (.svn ... etc.). LSA test now passes reliably.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Reinstated and fixed ThreadSafetyTest.
    Getting much closer to version 3.0. Loads and loads of changes. All (current) tests pass, but I think some binary positional indexing is still broken.
    
    Main changes include:
    
    - Making dimension and vector type part of the interface of all vector stores. So less reliance on flags. Nonetheless flags are still important, and the challenge of figuring out who can set dimension where and when remains unsolved and difficult.
    
    - Lots of improvements to tests, including making unit tests out of some formerly regression-style tests.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Added Yevgeniy to AUTHORS and copyright statement to test/pitt/search/semanticvectors/ThreadSafetyTest.java
    I think this fixes the worst of issue 21. Unit tests in place, though getting failures from the inside of threads seems to be a problem.
    working on tracking down threading issue.
    Adding unit test for thread safety, mainly contributed by Yevgeniy. Works when synchronized, breaks when synchronized is commented out.
    More work on DyadicIndexer
    Changes to enable redistributing coordinates to make their distributions approximately uniform.
    Test for sigmoid function
    Adding simulations / samples of real vectors and their similarities, and associated trivial stat utils.
    Changes to enable redistributing coordinates to make their distributions approximately uniform.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Checking to make sure calls to get a particular field return something or give a meaningful error.
    Factoring out results-printing methods to simplify main flow of Search.java.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Initial work on making semantic vectors from rows in tables. Number representations not wired in yet.
    Test fix earlier mistake.
    Experiments in progress
    Adding simulations / samples of real vectors and their similarities, and associated trivial stat utils.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Changed RealVector.bind to be expected orthogonal to both factors, and for release to retrieve (sort of) one of the factors, including under superposition.
    
    The test for this passes, and should perhaps be used similarly elsewhere. But it's not very convincing really - for three pairs of vectors, the recovery of the original under probing is not very effective. Still, it's a baseline.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Copyright statements for several of Dom's experimental files, a new logger, and removal of reference to a separate CircRep.
    Lots of work on the typed vectors branch. Ready to start integrating use of Vectors package into the semanticvectors main operations I think. Comments welcome.
    First attempts to share some of the ideas for supporting vectors over different ground fields.
    
    Not integrated at all yet, to begin with just figuring out if the svn branching worked.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Some cosmentic changes including removing legacy javadoc html files.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Fixing tests and some orthogonalization behavior.
    Better wiring for redistributing coordinates
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Committing a compiling version that is broken at runtime. This is normally a bad thing to do but I nearly lost my hard drive the other day and I've got the fear about losing this work! If this breaks your build in the branch then please revert.
    Lots of work on the typed vectors branch. Ready to start integrating use of Vectors package into the semanticvectors main operations I think. Comments welcome.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Made some improvements and fixed bugs in test suite.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Added RegressionTests class and some real improvements to RunTests to run both unit tests and regression tests and report results.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Some cosmentic changes including removing legacy javadoc html files.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Removed some test printing and set createZeroVector to be sparse for complex vectors
    Fixed one more test from same reason as last.
    Fixed test, caveat emptor comments, some formatting.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    (1) Checked in the Hermitian edition of ComplexVector - currently this is switched on by changing a boolean variable at the very end of the code, but as discussed a better thought-out (or at least somewhat thought-out) way of integrating the current cornucopia of complex vector capabilities is in our sights.
    (2) Restored the "renderPairwiseOrthogonal" code in ComplexVectorUtils, for posterity but mostly for the purpose of passing one of the unit tests.
    (3) Altered said unit test so it uses this type of orthogonalization.
    Little bits of work from the plane a few days ago.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Changes to complex measure polar overlap so that sparse elemental vectors are self-similar.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Modified complex unit test to be inline with phase angle based scalar product.
    
    Updated some comments to make this clear.
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Got binary directional search working. Complex convolution still needs fixed.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Mainly work on complex vectors to catch convolution up with angular representation with 0. Also realized that 2^14 should be used as phase resolution.
    
    Minor changes to string array logging.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Committing tests as well ...
    Working on complex vector tests. Will describe more fully in code review.
    Started to wrap complex vectors with a couple of unit tests. These ones break at the end but I hope this is a good way of getting common understanding of what should be happening.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Some changes trying to get binary permutation search to work.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Did some slight refactoring to make for more robust backwards compatibility with old indexes, with usage messages if things go wrong.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    Initial semantic vectors package
    Introducing experiments using learned character vectors. Some refactoring and testing of orthography code to support learned character vectors.
    Corrected an error that crept in when attempting to speed up
    normalization - the error resulted in 50/50 splits  being set to 1 with
    100% rather than 50% probability....
    Fixed a couple of binary vector tests.
    Some fixing to tests. Not good solutions but things are busy.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Continuous table values work nicely now for all vectortypes.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    I think this test change is necessary for probabilistic normalization not to be seen as an error.
    Implemented binding using XOR, with and without permutation (to allow for nesting). Also, included in Vectors a "release" method as binding and releasing are the same operation with binary vectors, but this is not the case with HRR.
    
    Currently, superposition (bundling) is not addressed here - binding produces a new vector that is dissimilar from its two component vectors.
    Made elementalToSemantic initialize the voting record and added a test.
    
    Lots of little cleanups including renaming exact to setTempSetToExactMatch and making it void (side effect is writing to tempSet).
    Mainly work on complex vectors to catch convolution up with angular representation with 0. Also realized that 2^14 should be used as phase resolution.
    
    Minor changes to string array logging.
    Regression tests all pass now. We might Code complete for v3.0.
    Added a test for getMaximumSharedWeight and addition and normalization.
    
    I think getMaxiumumSharedWeight may be broken, or I don't understand it properly. It looks like it is most likely to return zero unless there is an entire row of 1s in the voting record, which seems highly unlikely.
    
    Also uncovered a "last bit gets turned to 1" behavior which I'm not sure is intended.
    
    Some name changing and commenting, let me know if there are any objections or misunderstandings here.
    Some changes trying to get binary permutation search to work.
    Put the binary dimension and seedlength constraints into flag parsing. I think this is the right thing to do for now.
    A few new permutation-related tests added, and old tests updated to conform to constraint that dimensionality must be a multiple of 64 (so 64 is now the minimum dimensionality).
    
    Also adapted the I/O part of binary vectors to accommodate this constraint. The unit tests pass, though I've yet to try this on a corpus, will do shortly.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    More tests. Haven't tracked anything down.
    Added some extra logging and tests for BinaryVector, and some comments and suggestions for comments. Getting my head round this slowly.
    Making binary string representation shorter, the pipe delimiters are a bit of a waste of space here.
    Added Lucene I/O and some tests to BinaryVector (and some interfering whitespace / style cleanup, hopt this isn't annoying).
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Committing a test that got away.
    README for src directory.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    These are the source files for Adrian Kuhn and David Erni's adaptation (well, direct translation) of Doug Rhodes' SVDLIBC, a stripped down and approachable implementation of the las2 algorithm derived from SVDPACK (Michael Berry, Theresa Do, Gavin O'Brien, Vijay Krishna and Sowmini Varadhan).
    
    There are also some unit tests related data files in this package that I haven't included for now, but we may want to think about finding a home for moving forward.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    These are the source files for Adrian Kuhn and David Erni's adaptation (well, direct translation) of Doug Rhodes' SVDLIBC, a stripped down and approachable implementation of the las2 algorithm derived from SVDPACK (Michael Berry, Theresa Do, Gavin O'Brien, Vijay Krishna and Sowmini Varadhan).
    
    There are also some unit tests related data files in this package that I haven't included for now, but we may want to think about finding a home for moving forward.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Quick and dirty attempt at supporting 2D visualization without JAMA, involving one new Class that is identical to the original aside from an attempt to replace JAMA with the tedlab library we use for LSA. For some reason this produces a reflection about the Y axis of the result with the original Class. It probably wouldn't be too hard to change this, though the symmetry is quite appealing. Using the tedlab library required making one method in it public,  public static SMat svdConvertDtoS(DMat D), as the tedlab SVD implementation requires a sparse matrix as input.
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    These are the source files for Adrian Kuhn and David Erni's adaptation (well, direct translation) of Doug Rhodes' SVDLIBC, a stripped down and approachable implementation of the las2 algorithm derived from SVDPACK (Michael Berry, Theresa Do, Gavin O'Brien, Vijay Krishna and Sowmini Varadhan).
    
    There are also some unit tests related data files in this package that I haven't included for now, but we may want to think about finding a home for moving forward.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    These are the source files for Adrian Kuhn and David Erni's adaptation (well, direct translation) of Doug Rhodes' SVDLIBC, a stripped down and approachable implementation of the las2 algorithm derived from SVDPACK (Michael Berry, Theresa Do, Gavin O'Brien, Vijay Krishna and Sowmini Varadhan).
    
    There are also some unit tests related data files in this package that I haven't included for now, but we may want to think about finding a home for moving forward.
    Initial commit of permutation-based embedding code (ahead of reviewing
    issues with permutation/directional based unit tests).
    
    There are some fairly significant structural changes here including:
    (1) using -embeddingmethod (instead of positionalmethod) to distinguish
    between RI and SGNS
    (2) proximity-based permutations
    (3) facilities for storing, generating and searching with random
    permutations
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Added small else clause to make sure term vectors are still written if doc vectors aren't wanted. Fixes bug 26.
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    i. Checking in some slight edits to Positional indexing code form Trevor (just occasional bracketing conventions, javadoc, minor tweaks to usage messages); ii. Removed typo class BuildIPositionalndex; iii. Added new class for clustering entire vectors stores (this isn't optimized at all, run from text not lucene-based vector indexes, but the clustering doesn't scale for large vector stores anyway.
    This version of Semantic Vectors 1.7 (dev) contains four additional classes which implement a within-document sliding context window to generate term-term co-occurrence vectors. The classes are:
    (1) Classes that build a Lucene index that includes per-document term frequency and position vectors
    pitt.search.lucene.FilePositionDoc.java
    pitt.search.lucene.IndexFilePositions.java
    (2) Classes that derive from this Lucene index a set of term vectors based on term-term cooccurence within a sliding window
    pitt.search.semanticvectors.BuildPositionalIndex.java
    pitt.search.semanticvectors.TermTermVectorsFromLucene.java
    Added WhitespaceAnalyzer as an option at indexing time.
    Eliminate StandardAnalyzer's diminutive stopword list (we provide the
    option to use a stopword list after the Lucene index is created)
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    A few minor changes:
    (1) added -indexRootDirectory flag, to allow for the specification of a directory in which to place the resulting lucene index. This requires a trailing slash at the end currently.
    (2) added the "incremental_" to a CreateIncrementalDocVectors call in BuildIndex, for the sake of consistency
    Refactored integration tests slightly to always use positional_index. This means that all indexes for testing are created using our own IndexFilePositions, and thus ignore dot files (.svn ... etc.). LSA test now passes reliably.
    Added stemming as an option to IndexFilePositions, using Lucene's PorterStemFilter.
    Moved the stoplist machinery out of IndexFilePositions and into LuceneUtils. There's a new Flag -stoplistfile which when used will cause any instantiated LuceneUtils object to load the file that follows, and return false for any term on the stoplist. Consequently the stoplist applies to BuildIndex, BuildPositionalIndex and LSA without actually requiring any change to these classes.  This was really easy to do on account of the package-wide Flags class.
    The default indexer now no longer eliminates the standard Lucene StandardAnalyzer stopword list. This seems to be necessary for positional indexing to work with 3.0, so it would probably be a good idea to eliminate the stop word option from this class entirely.
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    Fixed some long to int type casting errors given when tring to compile Beagle stuff.
    Flags refactor and regression testing of BuildPositionalIndex.
    Small changes to index path, plus indentation and whitespace (I clearly have problems between emacs on linux and aquamacs on mac).
    Adding visualization utils. Added extra sophistication to build file to support this: now, utils will ship with the package but will not compile by default, to avoid extra dependencies for basic users.
    Just tidied up some javadoc before realease 1.10.
    i. Checking in some slight edits to Positional indexing code form Trevor (just occasional bracketing conventions, javadoc, minor tweaks to usage messages); ii. Removed typo class BuildIPositionalndex; iii. Added new class for clustering entire vectors stores (this isn't optimized at all, run from text not lucene-based vector indexes, but the clustering doesn't scale for large vector stores anyway.
    This version of Semantic Vectors 1.7 (dev) contains four additional classes which implement a within-document sliding context window to generate term-term co-occurrence vectors. The classes are:
    (1) Classes that build a Lucene index that includes per-document term frequency and position vectors
    pitt.search.lucene.FilePositionDoc.java
    pitt.search.lucene.IndexFilePositions.java
    (2) Classes that derive from this Lucene index a set of term vectors based on term-term cooccurence within a sliding window
    pitt.search.semanticvectors.BuildPositionalIndex.java
    pitt.search.semanticvectors.TermTermVectorsFromLucene.java
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Fixed a weird bug where the absolute value of Integer.MIN_VALUE returns a negative number.
    Fixed a null pointer dereference.
    (both bugs found by FindBugs static analyzer).
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    minor fix to IndexBilingualFiles.
    Several changes for getting the bilingual version working, and
    associated functionality.
    i. BuildBilingualIndex now works! (According to tests on europarl english-french data.)
    ii. TermVectorsFromLucene edited to properly enable reuse of basicDocVectors.
    
    This isn't perfect yet, by any means: it's probably inefficient in how much state is kept around when BuildBilingualIndex reuses TermVectorsFromLucene objects, and the interface to TermVectorsFromLucene could probably be improved. Since neither of these is a really basic class, I haven't thought too deeply about this - just don't write code that depends on these without rethinking and probably refactoring.
    
    iii. Added a scripts directory, initially with the preprocessing script for the europarl chapter alignment,
    iv. Added READMEs to the test and scripts directories.
    v. Changed build.xml file so that tests aren't automatically compiled (this is so that casual users of the source won't have to depend on JUnit).
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    added LuceneTokenizer.java class that helps in tokenizing just as Lucene does
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    More documentation twiddles to get package info to show up.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Some work trying to get bilingual indexing to work on recursive directories.
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    minor fix to IndexBilingualFiles.
    Classes for transforming between Lucene binary and plain text versions of indexes.
    Minor copyright change: IndexBilingualFiles was written on Google 20% time.
    Better description for IndexBilingualFiles util.
    Several changes for getting the bilingual version working, and
    associated functionality.
    i. BuildBilingualIndex now works! (According to tests on europarl english-french data.)
    ii. TermVectorsFromLucene edited to properly enable reuse of basicDocVectors.
    
    This isn't perfect yet, by any means: it's probably inefficient in how much state is kept around when BuildBilingualIndex reuses TermVectorsFromLucene objects, and the interface to TermVectorsFromLucene could probably be improved. Since neither of these is a really basic class, I haven't thought too deeply about this - just don't write code that depends on these without rethinking and probably refactoring.
    
    iii. Added a scripts directory, initially with the preprocessing script for the europarl chapter alignment,
    iv. Added READMEs to the test and scripts directories.
    v. Changed build.xml file so that tests aren't automatically compiled (this is so that casual users of the source won't have to depend on JUnit).
    Added WhitespaceAnalyzer as an option at indexing time.
    Initial commit of permutation-based embedding code (ahead of reviewing
    issues with permutation/directional based unit tests).
    
    There are some fairly significant structural changes here including:
    (1) using -embeddingmethod (instead of positionalmethod) to distinguish
    between RI and SGNS
    (2) proximity-based permutations
    (3) facilities for storing, generating and searching with random
    permutations
    Eliminate StandardAnalyzer's diminutive stopword list (we provide the
    option to use a stopword list after the Lucene index is created)
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    A few minor changes:
    (1) added -indexRootDirectory flag, to allow for the specification of a directory in which to place the resulting lucene index. This requires a trailing slash at the end currently.
    (2) added the "incremental_" to a CreateIncrementalDocVectors call in BuildIndex, for the sake of consistency
    Refactored integration tests slightly to always use positional_index. This means that all indexes for testing are created using our own IndexFilePositions, and thus ignore dot files (.svn ... etc.). LSA test now passes reliably.
    Added stemming as an option to IndexFilePositions, using Lucene's PorterStemFilter.
    Moved the stoplist machinery out of IndexFilePositions and into LuceneUtils. There's a new Flag -stoplistfile which when used will cause any instantiated LuceneUtils object to load the file that follows, and return false for any term on the stoplist. Consequently the stoplist applies to BuildIndex, BuildPositionalIndex and LSA without actually requiring any change to these classes.  This was really easy to do on account of the package-wide Flags class.
    The default indexer now no longer eliminates the standard Lucene StandardAnalyzer stopword list. This seems to be necessary for positional indexing to work with 3.0, so it would probably be a good idea to eliminate the stop word option from this class entirely.
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    Fixed some long to int type casting errors given when tring to compile Beagle stuff.
    Flags refactor and regression testing of BuildPositionalIndex.
    Small changes to index path, plus indentation and whitespace (I clearly have problems between emacs on linux and aquamacs on mac).
    Adding visualization utils. Added extra sophistication to build file to support this: now, utils will ship with the package but will not compile by default, to avoid extra dependencies for basic users.
    Just tidied up some javadoc before realease 1.10.
    i. Checking in some slight edits to Positional indexing code form Trevor (just occasional bracketing conventions, javadoc, minor tweaks to usage messages); ii. Removed typo class BuildIPositionalndex; iii. Added new class for clustering entire vectors stores (this isn't optimized at all, run from text not lucene-based vector indexes, but the clustering doesn't scale for large vector stores anyway.
    This version of Semantic Vectors 1.7 (dev) contains four additional classes which implement a within-document sliding context window to generate term-term co-occurrence vectors. The classes are:
    (1) Classes that build a Lucene index that includes per-document term frequency and position vectors
    pitt.search.lucene.FilePositionDoc.java
    pitt.search.lucene.IndexFilePositions.java
    (2) Classes that derive from this Lucene index a set of term vectors based on term-term cooccurence within a sliding window
    pitt.search.semanticvectors.BuildPositionalIndex.java
    pitt.search.semanticvectors.TermTermVectorsFromLucene.java
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Reinstating real PSI, capturing first predicate line, supporting complex flat serialization (which is the same as complex).
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Added / changed some error messages while using PSI.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Slight change made to weighting metrics, for consistency with previous experiments (now = global weight of predication + global weight of other concept; previously these two weights were multiplied)
     Index structure and indexing approach altered such that this now uses one step for each unique predication (as oppose to one step for each predication). This makes it easier to use log(global predication frequency) as a weighting metric, which has improved performance in our experiments to date.
    This class takes a tab-delimited text file of the form [subject] [predicate] [object], and transforms it into a Lucene index in which each document represents a single predication. This index will be used to build PSI models. "nationalfacts.txt" is an incomplete set of facts extracted from Wikipedia entries to do with countries, for testing, development and demonstration.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Updated version number to 2.5 after shipping 2.4. And related cleanup.
    Added convenience methods to Porter stem queries, or an entire text file from the main method if required. These may ultimately be better placed elsewhere in the package, happy to reposition them as required.
    Added stemming as an option to IndexFilePositions, using Lucene's PorterStemFilter.
    Utility class to create Lucene index with some additional fields for use
    with the SemMedDB database.
    Restored permutation-based Random Indexing to working order after errors
    introduced during the course of developing permutation-based neural
    embeddings
    Initial commit of permutation-based embedding code (ahead of reviewing
    issues with permutation/directional based unit tests).
    
    There are some fairly significant structural changes here including:
    (1) using -embeddingmethod (instead of positionalmethod) to distinguish
    between RI and SGNS
    (2) proximity-based permutations
    (3) facilities for storing, generating and searching with random
    permutations
    More testing still required, but strongly suspect that the issue alluded
    to in the last commit had less to do with the granularity of the
    subsampling procedure, and more to do with the alignment of the sliding
    window and a bug that resulted in confusion between the actual position
    within the window, and the position within the sequence of occupied
    window positions (where unoccupied means subsampled or termfiltered).
    A few changes to the SGNS implementation, for consistency with other
    implementations: (1) less aggressive subsampling; (2) a lower minimum
    learning rate. In addition, the pairwise correlation class should now
    permit comparison between compound terms, using simple vector addition.
    Issue-106: Update to Lucene 6.6.0
    
    Updated per review. Use specific classes in imports.
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Added:
    (1) Experimental feature permitting retraining using embedding weights
    derived from another corpus (reverting to random weights for terms
    present in the current corpus only).
    (2) Generation of document vectors, in a manner approximating the
    "distributed bag of words" model (Le and Mikolov), but using negative
    sampling. This is also experimental. It produces meaningful document
    vectors, but there's a lot yet to work out e.g. how many training
    iterations are advisable, and whether to train word vectors ahead of
    time or not. A side-effect of generating document vectors this way is
    that word vectors learn something about document-level co-occurrence, in
    addition to neighboring terms within a sliding window.
    Added randomization of the document queue (in chunks to reduce seek
    time), and moved normalization of the term vectors to
    BuildPositionalIndex (to facilitate document vector representation down
    the line).
    Corrected an error in the calculation of the learning rate, and
    simplified the queueing process (todo: find an efficient way to
    randomize the order in which documents are presented, and utilize an
    independent thread to handle the IO).
    Cleaned up a few things, and added pre-calculation of the sigmoid
    function.
    Regressed the previous change, and added a check for null Terms objects,
    which seems to solve the problem (so randomization is restored).
    I have removed, for the moment, my attempt at randomizing the queue on
    account of its horrible and unforeseen consequences. However, this issue
    will be revisited in the future.
    Exception handling for a null pointer exception that may be generated
    when attempting to add an empty set to the queue - at least this seems
    the most likely explanation currently. Further investigation required.
    
    Also - corrected the term counting process so excluded terms are
    included in the count, for accurate estimation of subsampling
    thresholds.
    Added a little more in the way of thread safety.
    Some simplification of the code, removal of redundancy and probable
    bugfixes to TermTermVectorsFromLucene.
    
    Also, a check in Binary Vectors such that tallying of votes occurs only
    if changes have happened, and an attempt at making binary vectors play
    nicely with the new TermTermVectorsFromLucene additions.
    Removed some unused code, and fixed a bug such that the negative
    sampling machinery excludes the context term in question as a candidate
    for negative sampling.
    Refactored initialization of complex vectors, and made a few other
    changes to enable initialization of hermitian/cartesian (rather than
    circular) random vectors.
    More cleanup
    Dominic doing some picky style normalization.
    First pass at an implementation of skipgram with negative sampling
    (Mikolov 2013). The main changes are as follows:
    
    (1) The negative sampling algorithm itself, with the scalar product and
    vector updates currently implemented within VectorUtils using
    netlib-java's blas routines (already a dependency), and sampling of
    out-of-context terms in accordance with their frequency^.75.
    
    (2) Subsampling of frequent terms, currently in need of reassessment and
    review, as although it does (vastly) speed up training, it does not
    improve performance as assorted papers suggest it should. Without
    subsampling results on pairwise correlation sets (e.g. wordsim353, MEN)
    and Mikolov's analogy set (n=~19,000) match reported results - in the
    region of 0.7 correlation and 0.6 accuracy for similarity and analogies
    respectively. But with subsampling as implemented currently performance
    deteriorates, more so as the threshold is reduced, which should not be
    the case according to the literature.
    
    (3) Multiple processing threads (with some initial movements in the
    direction of thread safety, though this doesn't yet extend to the
    document queue)
    Cleaned up the PROXIMITY sliding window code, such that permutation affects the demarcator vectors only, and the permuted demarcator vectors are output to facilitate queries along the lines of:
    
     "E(king)*P(-1)"
    Opening elemental query vector store from file: deterministic
    Opening semantic query vector store from file: proxtermvectors.bin
    Opening predicate query vector store from file: numbervectors.bin
    Opening search vector store from file: proxtermvectors.bin
    Searching term vectors, searchtype BOUNDPRODUCT
    Found vector for '-1'
    Search output follows ...
    12.726996:minos
    12.423948:aegeus
    11.939073:midas
    10.666273:lear
    8.423721:cheng
    
    Interestingly, this allows for a (very simple) sort of analogy query:
    "E(king)*S(minos)*E(duke)"
    Search output follows ...
    5.511599:ellington
    
    "E(king)*S(minos)*E(queen)"...
    6.596088:sheba
    5.246219:victoria
    4.796262:anne's
    4.410585:elizabeth
    4.282026:finches
    Updated the "PROXIMITY" method, so that the same set of demarcator vectors are used for the left and right side of the sliding window - as permutation is used to encode directionality, there's no need to have more than (windowradius+2) demarcator vectors (including alpha and omega, which we'd rather not include as this would make the beginning of the left window orthogonal to the end of it).
    
    This also means that the distances on the left are consistent with those on the right, which would not have been the case previously in the complex and real vector representations.
    Avoid calling termFilter when processing individual documents (filtered terms will not have semantic vectors) to speed up processing.
    Two small changes:
    (1) it is now possible to do other searches aside from PSI searches again (apologies - broke the stack again).
    (2) for the proximity-based sliding window model, I've avoided attempting to restore elemental vectors to their natural state using "release" (by generating copies for binding instead), as this doesn't seem to work very well with real vectors. I'm not sure if this is a cumulative consequence of an approximate inverse or something more sinister, I'll poke around and see what I can find out.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Removing public status of ComplexVector.setDominantMode, and catching all setters. Callers should now use VectorType COMPLEXFLAT for all of this, and set it once in flag config.
    Suppress unnecessary logging, some minor changes.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Inserted check for absent term vectors.
    
    PS - it looks as though Lucene 4 will read Lucene indexes produced by Lucene 3.6
    More minor cleanup.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Change to the PROXIMITY method: this now also uses permutation to encode direction (i.e. left vs. right), so it falls midway between the standard permutation approach (exact position information) and the directional approach (no position information beyond direction). Also the "number prefix" for demarcator vectors was removed, as we're no longer using these.
    Renaming "infer" package to "orthography". This should help readers connect with literature on "orthographic similarity", of which there seems to be plenty.
    Added a variant of the sliding window model based on the edit distance idea (i.e. less constrained than permtermvectors.bin), and a new sort of vector store based on this idea also.
    Turned -positionalmethod into an enum. This concludes the effort of turning flag values into enums.
    Refactored -termweight values to be an enum, {NONE, IDF, LOGENTROPY}. On public call to LuceneUtils.getGlobalTermWeight, flag-based switching is now all inside LuceneUtils.
    
    This should make it much easier to make sure that termweighting enhancements are properly incorporated into LSA, BuildIndex, BuildPositionalIndex, etc., without having to maintain multiple codepaths.
    
    All tests pass, but this demonstrates the need for more reliable testing at different parts of the stack.
    Refactored vector building interfaces in TermTermVectorsFromLucene, IncrementalTermVectors, and IncrementalDocVectors, to use flagConfigs much more. This simplifies interfaces considerably.
    
    However, I chickened out of completely removing args[i] parameters from Incremental*Vectors classes, since I don't use them regularly and don't have proper tests.
    Made LuceneUtils.TermFilter use flagConfig in all cases except PSI. This is good because callers will get new features (e.g., length filtering) automatically.
    
    PSI is different, it has to set some overrides for which the old more direct interfaces are necessary. So this is an improvement, but the story isn't over yet.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added -deterministicvectors Flag. Seems to be working, though further testing is required. Will create a unit test in the near future.
    Changing constructor for LuceneUtils to use only FlagConfig.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Update to be compatible with Lucene 3.6.2. Exactly as recommended in Issue 60 (https://code.google.com/p/semanticvectors/issues/detail?id=60).
    Introduced Flags.filternumbers parameter, that, when set to true, causes terms that represent numbers to be filtered out.
    * overloaded the termFilter method with an additional boolean to reflect this new parameter and updated all calls of the termFilter method except for in the PSI code, where also the maxnonalphabet parameter is not used
    * by default filternumbers is set to false so the default behavior of the SV package does not change
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Added the option to apply global weighting to terms within sliding window, such that more informative (i.e. less frequently occurring) terms will be selectively emphasized.
    Quick fix to the error I introduced by changing "bind": TermTermVectors now uses permutations exclusively, although we may wish to introduce the option of other binding operators once these are implemented.
    Got binary directional search working. Complex convolution still needs fixed.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Hacked away at our logging verbosity. I believe (I think) that normal operation is much easier to interpret using VerbatimLogger, and that we should use other loggers for debugging. I'd like to write / find a one-line logging message which is basically verbatim with a file and line number prefixed.
    Some changes trying to get binary permutation search to work.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    start list feature added
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    Fixes issue 36, I believe. Refactored VectorStoreWriter to switch properly between text and lucene format so that text vector stores can be created by BuildIndex as well.
    (Recommit after eclipse mismatch.) Added dimension setting and checking to VectorStoreRAM.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Making build file work with env classpath. Fixed some javadoc and indentation.
    Added types to generic enumerations as discussed in issue 23.
    tiny typo
    Add a constructor that takes the additional parameters dimension and positionalmethod, making the constructor completely independent from Flags.
    Changed two bitwise OR operators (ie '|') into conditional OR operators (ie '||'). This allows short-circuiting the test when the first condition is true and it also reads easier because there is no expectation that the operands have bitwise meaning.
    Prevent IndexOutOfBoundsException in TermTermVectorsFromLucene by deriving numpositions from the vector term positions. This allows BuildPositionalIndex to operate on documents that skip stopwords.
    (1) Included exception handling such that rather than crashing, TermTermVectorsFromLucene now outputs the name of any document it could not handle.
    
    (2) Added the ability to generate combined 'content and order' based vectors, such that the permutation-based model and general sliding window model are combined.
    This change is something of a follow-up to the refactoring of the stoplist - it seems sensible to have the TermFilter in LuceneUtils use the "Flags" parameters, so I've refactored it this way. I've also added a -maxfrequency flag to complement -minfrequency, as Magnus has documented performance improvements with an upper limit on term frequency.
    Fixed TermTermVectorsFromLucene to iterate over fieldsToIndex instead of using 'contents'. Broke out indexing of each TermPositionVector into a private method which seems to work well and makes the constructor smaller and less nested.
    Changed regression test, seems like martha does't tend to find mary and more but I don't think this is a bug.
    Some refactoring, haven't fixed indexing bug yet.
    Fixed one of the tests just by allocating extra memory with a hack in TermTermVectorsFromLucene ... need to figure out what the array index bound actually is.
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    Refactoring and testing of permutation indexes in TermTermVectorsFromLucene.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Flags library now uses reflection, using ClusterResults as test case.
    Trimmed down the code a little, as I had a number of redundancies in there previously - particularly the sliding window implementation
    Added cyclical retraining, and the ability to start with a set of trained term vectors (command line option -pt) to allow for combining vector stores in interesting ways.
    Minor style tweaks to Vidya's useful refactoring of TermFilter.
    Bug fix courtesy of Vidya Vasuki - terms in fields outside FieldsToIndex were also being included in vector stores. To fix this, an additional check has been added to TermFilter. Also, TermFilter has been refactored and is now part of LuceneUtils.
    
    One new command line option "-n" to allow a number of non-alphabet characters (useful for biomedical text) has also been included.
    Minor changes to comments and style.
    Housecleaning - usage for ClusterVectorStore, better exiting for PRINTQUERY.
    bug fix in "directional indexing" - previous version permuted vectors by rotating to the left regardless of the direction of the term
    Package now throws exceptions always instead of hard coding System.exit
    Some refactoring for permutations stuff: in particular, changed IndexType to Enumeration and made TermTermVectors use VectorStoreRAM and VectorStoreSparseRAM to make writing vectors simpler. Also wrote some checks for permutation searcher.
    no longer requires separate class to write random index vectors, uses modified VectorStoreWriter class
    The committed version should contain an implementation of the Sahlgren (2008) permutation-based encoding of word order. This includes:
    (1) a modification to BuildPositionalIndex such that it can take an argument indextype [basic (default), directional (the HAL model), or permutation (Sahlgren '08)].
    (2) modifications to TermTermVectorsFromLucene such that term vectors are permuted appropriately
    (3) additions to Search, CompoundVectorBuilder and VectorSearcher to facilitate index-based retrieval such that "president ?" retrieves terms occurring frequently after president
    (4) an additional class IndexTermVectorsFromRandomIndex that generates a vectors store on disk from a hashtable, used to preserve the random vectors as these are the basis for generating order-based queries
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    i. Checking in some slight edits to Positional indexing code form Trevor (just occasional bracketing conventions, javadoc, minor tweaks to usage messages); ii. Removed typo class BuildIPositionalndex; iii. Added new class for clustering entire vectors stores (this isn't optimized at all, run from text not lucene-based vector indexes, but the clustering doesn't scale for large vector stores anyway.
    This version of Semantic Vectors 1.7 (dev) contains four additional classes which implement a within-document sliding context window to generate term-term co-occurrence vectors. The classes are:
    (1) Classes that build a Lucene index that includes per-document term frequency and position vectors
    pitt.search.lucene.FilePositionDoc.java
    pitt.search.lucene.IndexFilePositions.java
    (2) Classes that derive from this Lucene index a set of term vectors based on term-term cooccurence within a sliding window
    pitt.search.semanticvectors.BuildPositionalIndex.java
    pitt.search.semanticvectors.TermTermVectorsFromLucene.java
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Removed some redundancies, and included local term weighting - but still
    haven't worked out why results from the TASA corpus are looking odd.
    Shall keep digging.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Make LSA iterate through terms enum explicitly instead of relying on "size". See if this helps.
    Improved logging messages to include contentsField.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Refactored -termweight values to be an enum, {NONE, IDF, LOGENTROPY}. On public call to LuceneUtils.getGlobalTermWeight, flag-based switching is now all inside LuceneUtils.
    
    This should make it much easier to make sure that termweighting enhancements are properly incorporated into LSA, BuildIndex, BuildPositionalIndex, etc., without having to maintain multiple codepaths.
    
    All tests pass, but this demonstrates the need for more reliable testing at different parts of the stack.
    Made LuceneUtils.TermFilter use flagConfig in all cases except PSI. This is good because callers will get new features (e.g., length filtering) automatically.
    
    PSI is different, it has to set some overrides for which the old more direct interfaces are necessary. So this is an improvement, but the story isn't over yet.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Changing constructor for LuceneUtils to use only FlagConfig.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Introduced Flags.filternumbers parameter, that, when set to true, causes terms that represent numbers to be filtered out.
    * overloaded the termFilter method with an additional boolean to reflect this new parameter and updated all calls of the termFilter method except for in the PSI code, where also the maxnonalphabet parameter is not used
    * by default filternumbers is set to false so the default behavior of the SV package does not change
    Adding support for idf weighting in LSA, and modified class to get rid of internal variable and depend on flags. (Pending introduction of some better flag config strategy.)
    
    Some extra comments and cleanup.
    Added normalization of document vectors
    Change to make file extensions automatic.
    
    That is, internal vector store names are just "termvectors", not "termvectors.bin", etc. The ".bin" or ".txt" suffix is applied automatically.
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Changed LSA to automatically cut the number of dimensions to number of documents if configured to be greater. This prevents a correct but confusion out of bounds exception for small document collections.
    
    Also introduced a constructor to check some of these invariants early on without having to open and reopen filesystem resources.
    Added -docidfield option, as previously LSA insisted on the "path" field.
    Document vector writing fix and regression tests for LSA.
    Small fix to LSA.java to make index writing work. This is still a hack and we might expect regressions to persist until LSA uses the same vector store writers as other methods.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Probable solution to degenerate results with LSA: the SVD implementation prefers matrices with more rows than columns. So it was working very well on small corpora such as KJB or TASA with more individual words than documents.... and falling flat on anything larger. Committing so user can test on large corpus, for the moment I've confirmed that transposing the matrix fixes the problem for TASA when terms are restricted to < number documents.
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    I'm not sure what happened here - I committed some code awhile back that I thought I'd tested extensively that attempted to normalize document vectors before performing SVD. This was probably misguided at best, and definitely has a detrimental effect on the quality of the vectors produced. It is now eliminated.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Removed the "seedlength" flag
    Two minor changes:
    (1) In Flags, changed the default term weighting option from "logentropy" to none
    (2) In LSA, added normalization of the document-by-term vectors prior to SVD, to conform to LSA traditions.
    Fixed poor documentation for maxnonalphabetchars.
    This change is something of a follow-up to the refactoring of the stoplist - it seems sensible to have the TermFilter in LuceneUtils use the "Flags" parameters, so I've refactored it this way. I've also added a -maxfrequency flag to complement -minfrequency, as Magnus has documented performance improvements with an upper limit on term frequency.
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    Fixed LSA.java to be up to Lucene 2.9.1 compatibility.
    Removed some superfluous console output
    This class provides an interface to the Kuhn and Erni native Java edition of SVDLIB. I've included it in pitt.search.semanticvectors rather than the "ext" package, as it requires access to some variables that are invisible outside the package.
    First pass at an implementation of skipgram with negative sampling
    (Mikolov 2013). The main changes are as follows:
    
    (1) The negative sampling algorithm itself, with the scalar product and
    vector updates currently implemented within VectorUtils using
    netlib-java's blas routines (already a dependency), and sampling of
    out-of-context terms in accordance with their frequency^.75.
    
    (2) Subsampling of frequent terms, currently in need of reassessment and
    review, as although it does (vastly) speed up training, it does not
    improve performance as assorted papers suggest it should. Without
    subsampling results on pairwise correlation sets (e.g. wordsim353, MEN)
    and Mikolov's analogy set (n=~19,000) match reported results - in the
    region of 0.7 correlation and 0.6 accuracy for similarity and analogies
    respectively. But with subsampling as implemented currently performance
    deteriorates, more so as the threshold is reduced, which should not be
    the case according to the literature.
    
    (3) Multiple processing threads (with some initial movements in the
    direction of thread safety, though this doesn't yet extend to the
    document queue)
    Changes to enable redistributing coordinates to make their distributions approximately uniform.
    Indexing experiment for Russian folk tales relations.
    Informative exceptions here and there.
    Adding a new experiment for getting "best" semantic relations on PSI vectors for predicting types.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Found a conflict between PSI and the new elemental vector store implementation: PSI was calling getVector before checking whether a corresponding semantic vector existed, creating new elemental vectors for many terms.
    
    I've committed one possible fix - replacing the "getVector(x) == null" check with an explicit "containsVector(x) == false" check. This seems a safer practice, though there are a fair number of checks we would need to change scattered throughout the codebase still.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Moved json generation to pathfinder class
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Added method to VectorStoreRAM to remove a vector.
    In initFromFile(), use VectorStoreReader to get the reader for the file type set in Flags.java (currently index or text)
    Hacked away at our logging verbosity. I believe (I think) that normal operation is much easier to interpret using VerbatimLogger, and that we should use other loggers for debugging. I'd like to write / find a one-line logging message which is basically verbatim with a file and line number prefixed.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    (Recommit after eclipse mismatch.) Added dimension setting and checking to VectorStoreRAM.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Fixes premature close of VectorStoreReaderLucene.
    Unlike VectorStoreReaderText.getAllVectors(), VectorStoreReaderLucene.getAllVectors() does not create a new file reader. So I need to wait until the enumeration is carried out before closing the index input.
    For the sake of consistency, both VectorStoreReader types should behave the same, which would require creating a new index input thread for the enumeration which automatically closes when the enumeration is done. But I don't want to be creating threads too often. So I think the best practice is to close the reader after going through the enumeration.
    Added types to generic enumerations as discussed in issue 23.
    Close the disk-based vector store reader when initializing a VectorStoreRAM.
    This fixes the VectorStoreWriterTest integration test problem on Windows XP, and cleans up a lost file handle on all platforms.
    Before:
         [java] Running tests for VectorStoreWriter
         [java] java.io.IOException: Cannot overwrite: C:\cygwin\home\Thomas Richmond\SemanticVectors\trunk\test\testdata\tmp\testtermvectors.bin
    After:
         [java] Running tests for VectorStoreWriter
         [java] About to write vectors to file testtermvectors.bin
         [java] 0 ... Finished writing vectors.
    (note that there are other problems with the integration tests under XP, this doesn't fix everything but it is a step in the right direction).
    Use logger instead of direct output to System.err or
    System.out.
    Just cleaned up some indentation since I was looking at this file.
    Renaming VectorStoreReader to VectorStoreReaderLucene.
    Removed 'clone()' command from getVector in VectorStoreRAM, this was breaking the building of DocVectors. Clearly I don't know what I'm doing with Java memory management yet.
    Added a test for CompoundVectorBuilder and VectorStoreRAM, and utils for outputting principal component plots as tex source.
    Some refactoring for permutations stuff: in particular, changed IndexType to Enumeration and made TermTermVectors use VectorStoreRAM and VectorStoreSparseRAM to make writing vectors simpler. Also wrote some checks for permutation searcher.
    Refactored DocVectors.java to use VectorStoreRAM, and added addVector method and test to VectorStoreRAM.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Added VectorStoreReaderRAMCache and small test of this.
    RunSearch -> runSearch for more normal Java style.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Clean up and commenting to make clustering of vector stores more reliable. The overlap measure should be properly generalized, this change simply disables it with a compile-time boolean.
    Refactored vector building interfaces in TermTermVectorsFromLucene, IncrementalTermVectors, and IncrementalDocVectors, to use flagConfigs much more. This simplifies interfaces considerably.
    
    However, I chickened out of completely removing args[i] parameters from Incremental*Vectors classes, since I don't use them regularly and don't have proper tests.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Some cleanup of clustering.
    Cleanup revision. Have removed stray javadocs instead of fixing them - this is interim but good enough for now.
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Added struct for storing more expressive cluster results including centroids in output, and outputting vectors to file.
    
    Use at your own risk, this is not well factored or encapsulated code!
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Fixed a weird bug where the absolute value of Integer.MIN_VALUE returns a negative number.
    Fixed a null pointer dereference.
    (both bugs found by FindBugs static analyzer).
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Retrofitted Clustering classes to use Flags.
    Minor synchronization of my client with Trevor's revision for balanced_permutation.
    Flags library now uses reflection, using ClusterResults as test case.
    Exception catching for ZeroVectorExceptions (next time I should run 'ant clean; ant'.)
    Changed ClusterResults to throw exceptions rather than Syste.exit() when arguments are ill-formed - should be done for other classes as well.
    Small changes: i. Changed IncrementalDocVectors and calling code to put VectorFile argument last (since it's a write argument); ii. Much Javadoc for Search.SearchType options; iii. Changed ClusterResults to do random rather than round robin assignment (request from Pitt).
    Corrected omission of ClusterResults.java
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Found a conflict between PSI and the new elemental vector store implementation: PSI was calling getVector before checking whether a corresponding semantic vector existed, creating new elemental vectors for many terms.
    
    I've committed one possible fix - replacing the "getVector(x) == null" check with an explicit "containsVector(x) == false" check. This seems a safer practice, though there are a fair number of checks we would need to change scattered throughout the codebase still.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Fixes premature close of VectorStoreReaderLucene.
    Unlike VectorStoreReaderText.getAllVectors(), VectorStoreReaderLucene.getAllVectors() does not create a new file reader. So I need to wait until the enumeration is carried out before closing the index input.
    For the sake of consistency, both VectorStoreReader types should behave the same, which would require creating a new index input thread for the enumeration which automatically closes when the enumeration is done. But I don't want to be creating threads too often. So I think the best practice is to close the reader after going through the enumeration.
    Added types to generic enumerations as discussed in issue 23.
    Closed some vector stores so that VectorStoreReaderLuceneTest passes on XP, cleans up lost file handles on all platforms.
    Added closeIndexInput() method to VectorStoreReaderLucene to allow a thread to close its own index input without closing the underlying directory, so that the other threads can continue to use the underlying directory.
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    Uses Yevgeniy's fix for thread safety to VectorStoreReaderLucene, using ThreadLocal variables for the IndexInputs.
    I think this fixes the worst of issue 21. Unit tests in place, though getting failures from the inside of threads seems to be a problem.
    working on tracking down threading issue.
    Renaming VectorStoreReader to VectorStoreReaderLucene.
    Trying to refactor ubild paths for cleaner / easier distinction between src and src extensions.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Fixed VectorStoreReader and Writer closing operations by using FSDirectory instead of MMapDirectory. This seems to be working, tests fixed accordingly.
    Small change in close() for VectorStoreReader, but with consequences in catching undeclared NullPointerExceptions, in main Search code as well as in tests. Trying to track down some read - write problems, see http://groups.google.com/group/semanticvectors/browse_thread/thread/48198c639c9b6d72/814e034b34f5af62#814e034b34f5af62 for details.
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Added VectorStoreReaderRAMCache and small test of this.
    Fixed bugs with command line parsing in Search.java and number of clusters in ClusterResults.java
    Did some slight refactoring to make for more robust backwards compatibility with old indexes, with usage messages if things go wrong.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    Refactored to have search go through a VectorSearcher object that enables the use of different similarity scores while using common getNearestNeighbors implementation. Appears to be working fine, next step is to add some other similarity scores, e.g., using tensors.
    Minor changes to whitespace and comments.
    Initial semantic vectors package
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Found a conflict between PSI and the new elemental vector store implementation: PSI was calling getVector before checking whether a corresponding semantic vector existed, creating new elemental vectors for many terms.
    
    I've committed one possible fix - replacing the "getVector(x) == null" check with an explicit "containsVector(x) == false" check. This seems a safer practice, though there are a fair number of checks we would need to change scattered throughout the codebase still.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Closed some more abandoned VectorStoreReaders.
    Modified VectorStoreReaderText so that getting the vector enum creates a new filereader, so that (1) the new filereader can be closed when done enumerating the vectors, and (2) a call to getVector or another function that resets the filereader doesn't also reset the enumeration.
    Added types to generic enumerations as discussed in issue 23.
    I think this fixes the worst of issue 21. Unit tests in place, though getting failures from the inside of threads seems to be a problem.
    Renaming VectorStoreReader to VectorStoreReaderLucene.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Edited build file: dev version is now 1.7
    Improved javadoc for translater. More importantly, refactored VectorSearcher to take VectorStore interface instead of VectorStoreReader, so that both Lucene index and plain text index VectorStoreReaders work for search interface.
    Classes for transforming between Lucene binary and plain text versions of indexes.
    It turns out Lucene Terms are unreliable keys for HashMaps, so I have
    used Strings instead
    Added support for a couple more term weighting options. Logfreq looks pretty good in wikipedia term search.
    Changed name of usetermweightsintermsearch for consistency.
    Issue-106: Update to Lucene 6.6.0
    
    Updated per review. Use specific classes in imports.
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Dominic doing some picky style normalization.
    Additional changes for the embeddings implementation, including a change
    to the "analogy"search in accordance with the y-x+z idea
    The dreaded "dividing by an integer produces an integer" error.
    Small change to print contents fields when initializing LuceneUtils.
    Catch NullPointer as well as IOException when looking up docidfield
    Added option to avoid looking up external document ID if so desired (to
    save time, or if no suitable field exists)
    
    -docidfield luceneID
    Moving sigmoid function to statutils
    Added method to calculate the sigmoid function.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Logging message when startwords are loaded to make it clear that only these are indexed.
    Startlist logic restored to what was originally intended by Sid - i.e. only terms on the startlist are included.
    Make startwords properly supported in LuceneUtils term filter.
    Making doc vectors use external IDs (path or filenames) rather than Lucene's internal integer IDs throughout.
    Reassembled the "startlistfile" code - the pieces were in there for the most part. However, using a small startlistfile within an iterative process (other than at the very end of one) is liable to lead to inefficiency downstream with -incrementaldocvectors, as currently we depend upon there being perfect alignment between the generate docvectors.bin and the documents in the Lucene index - i.e. we will end up with many zero vectors on disk.
    Bits ot cleanup to make sure that term weights are working
    Follow FlagConfig for local (and global) weighting of predications - localweight(occurrences of predication), globalweight(other concept). Added a SQRT option as a weighting metric (local = SQRT(occurrences of predication), global = 1
    Checking to make sure calls to get a particular field return something or give a meaningful error.
    Added a command line flag, -mintermlength, that allows for the specification of a minimum term length so as to exclude single characters and such if need be.
    This may have been the root of the issue with "GetGlobalTermWeightFromString" I ran into awhile back - changed the variable that is returned from an int to a float.
    Comments and appropriate version warnings.
    maven and lucene upgrades to make the SV jar work standalone
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Removed temporary main method left in on last commit.
    Changed getGlobalTermFreq to take advantage of newly-exposed Lucene method, totalTermFreq. This improves performance, hopefully it stays exposed.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Building a hash table for global term frequencies of all terms, including those that do not pass the frequency threshold test and are therefore never seen again, turns out to be a monumentally bad idea.
    Added local weighting to IncrementalDocVectors, and caching of global frequencies to LuceneUtils.
    Added local weighting (binary vs. TF vs. log(1+TF))
    Avoid dividing by zero, which it would appear can occur when DocVectors processes documents with multiple fields (as it checks for the occurrences of each term in each field)
    Turned searchtype into an enum, apparently successfully.
    
    Renamed LOG_ENTROPY to LOGENTROPY, realizing that this too will be passed normally as -termweight logentropy.
    Refactored -termweight values to be an enum, {NONE, IDF, LOGENTROPY}. On public call to LuceneUtils.getGlobalTermWeight, flag-based switching is now all inside LuceneUtils.
    
    This should make it much easier to make sure that termweighting enhancements are properly incorporated into LSA, BuildIndex, BuildPositionalIndex, etc., without having to maintain multiple codepaths.
    
    All tests pass, but this demonstrates the need for more reliable testing at different parts of the stack.
    Made LuceneUtils.TermFilter use flagConfig in all cases except PSI. This is good because callers will get new features (e.g., length filtering) automatically.
    
    PSI is different, it has to set some overrides for which the old more direct interfaces are necessary. So this is an improvement, but the story isn't over yet.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Changing constructor for LuceneUtils to use only FlagConfig.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Introduced Flags.filternumbers parameter, that, when set to true, causes terms that represent numbers to be filtered out.
    * overloaded the termFilter method with an additional boolean to reflect this new parameter and updated all calls of the termFilter method except for in the PSI code, where also the maxnonalphabet parameter is not used
    * by default filternumbers is set to false so the default behavior of the SV package does not change
    Adding support for idf weighting in LSA, and modified class to get rid of internal variable and depend on flags. (Pending introduction of some better flag config strategy.)
    
    Some extra comments and cleanup.
    Code added to calculate IDF as alternative weighting metric.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Setting a min term weight to be 1 rather than 0.
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    added private TreeSet startwords = null;
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Making build file work with env classpath. Fixed some javadoc and indentation.
    Added types to generic enumerations as discussed in issue 23.
    This change is something of a follow-up to the refactoring of the stoplist - it seems sensible to have the TermFilter in LuceneUtils use the "Flags" parameters, so I've refactored it this way. I've also added a -maxfrequency flag to complement -minfrequency, as Magnus has documented performance improvements with an upper limit on term frequency.
    Changes to get rid of hard-coded 'contents' fields.
    Moved the stoplist machinery out of IndexFilePositions and into LuceneUtils. There's a new Flag -stoplistfile which when used will cause any instantiated LuceneUtils object to load the file that follows, and return false for any term on the stoplist. Consequently the stoplist applies to BuildIndex, BuildPositionalIndex and LSA without actually requiring any change to these classes.  This was really easy to do on account of the package-wide Flags class.
    Some refactoring, haven't fixed indexing bug yet.
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    working on tracking down threading issue.
    In Lucene 2.9, TermDocs.freq() is invalid until next() is called for the first time.
    OK, I prefer the explicit check for null rather than the exception catch.
    Workaround fix for issue 19 that throws NPE in getGlobalTermWeight.
    Implemented TRRI as per the RRI paper. This required adding another constructor to TermVectorsFromLucene that generates elemental term vectors for future recycling. Also entropy weighting has been added to LuceneUtils, to facilitate log-entropy weighting in DocVectors and IncrementalDocVectors. TRRI can now be performed by calling BuildIndex with the command line flags:
    -initialtermvectors random
    -trainingcycles 2
    
    This implementation doesn't scale as well as the one used in the RRI paper, as this one had an opposite number to IncrementalDocVectors named IncrementalTermVectors - combining these two allows one to reflect without ever keeping the document vectors in RAM, which is critical for large data sets on small machines. Hope to add this in the near future.
    Minor style tweaks to Vidya's useful refactoring of TermFilter.
    Bug fix courtesy of Vidya Vasuki - terms in fields outside FieldsToIndex were also being included in vector stores. To fix this, an additional check has been added to TermFilter. Also, TermFilter has been refactored and is now part of LuceneUtils.
    
    One new command line option "-n" to allow a number of non-alphabet characters (useful for biomedical text) has also been included.
    i. Checking in some slight edits to Positional indexing code form Trevor (just occasional bracketing conventions, javadoc, minor tweaks to usage messages); ii. Removed typo class BuildIPositionalndex; iii. Added new class for clustering entire vectors stores (this isn't optimized at all, run from text not lucene-based vector indexes, but the clustering doesn't scale for large vector stores anyway.
    Initial semantic vectors package
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Turned vectorstorefomat into enum.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    And another.
    Minor bug fix.
    Change to make file extensions automatic.
    
    That is, internal vector store names are just "termvectors", not "termvectors.bin", etc. The ".bin" or ".txt" suffix is applied automatically.
    Copying across from branch to trunk.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Did some slight refactoring to make for more robust backwards compatibility with old indexes, with usage messages if things go wrong.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    Initial semantic vectors package
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Tests for real permutation PSI, and some refactoring.
    Found a conflict between PSI and the new elemental vector store implementation: PSI was calling getVector before checking whether a corresponding semantic vector existed, creating new elemental vectors for many terms.
    
    I've committed one possible fix - replacing the "getVector(x) == null" check with an explicit "containsVector(x) == false" check. This seems a safer practice, though there are a fair number of checks we would need to change scattered throughout the codebase still.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    And again.
    Javadoc cleanup.
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Copying across from branch to trunk.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Did some slight refactoring to make for more robust backwards compatibility with old indexes, with usage messages if things go wrong.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    Initial semantic vectors package
    Reinstate writing of first set of vectors.
    Better wiring for redistributing coordinates
    Moving term vector writing to happen before docindexing starts so that term vectors always get written out.
    Change trainingcyles implementation to respect user-provided termvectorsfile and docvectorsfile.
    Making doc vectors use external IDs (path or filenames) rather than Lucene's internal integer IDs throughout.
    This issue was raised by Ken a little while back, sorry to be so slow in responding. It does seem as though there was a conditional statement preventing the generation of elemental term vectors when the "-initialtermvectors random" flag is specified. Removing this condition in BuildIndex results in TRRI functioning as intended.
    Minor cleanup, mainly recommended by intellij.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Changed the conditional statement for "-initialtermvectors random" as this seemed to be launching into an in-memory indexing process.
    Changed an inputStream name to be docVectorsInputStream.
    A couple of minor fixes to get incremental TRRI working again.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    I suspect the "break" belongs outside the loop here, as this eliminates some odd behavior also (unnecessary builds, peculiar filenames and such).
    Fixing typo in usage message.
    Turned -docindexing into an enum.
    Refactored vector building interfaces in TermTermVectorsFromLucene, IncrementalTermVectors, and IncrementalDocVectors, to use flagConfigs much more. This simplifies interfaces considerably.
    
    However, I chickened out of completely removing args[i] parameters from Incremental*Vectors classes, since I don't use them regularly and don't have proper tests.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Introduced Flags.filternumbers parameter, that, when set to true, causes terms that represent numbers to be filtered out.
    * overloaded the termFilter method with an additional boolean to reflect this new parameter and updated all calls of the termFilter method except for in the PSI code, where also the maxnonalphabet parameter is not used
    * by default filternumbers is set to false so the default behavior of the SV package does not change
    Second attempt at a fix for the ".bin" bug.
    Added "incremental_" to filename during training cycle loop, such that "incremental_docvectors" is sought when -docindexing incremental is true
    A few minor changes:
    (1) added -indexRootDirectory flag, to allow for the specification of a directory in which to place the resulting lucene index. This requires a trailing slash at the end currently.
    (2) added the "incremental_" to a CreateIncrementalDocVectors call in BuildIndex, for the sake of consistency
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Whitespace and line formatting only.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Shortening line length of logging as in BuildPositionalIndex.
    Hacked away at our logging verbosity. I believe (I think) that normal operation is much easier to interpret using VerbatimLogger, and that we should use other loggers for debugging. I'd like to write / find a one-line logging message which is basically verbatim with a file and line number prefixed.
    Passed on some additional parameters to get TRRI working
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    "Dimensions" passed to TRRI implementation
    Fixes issue 36, I believe. Refactored VectorStoreWriter to switch properly between text and lucene format so that text vector stores can be created by BuildIndex as well.
    Passed the dimensionality to createTermBasedRRIVectorsImpl
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Trivial whitespace cleanup.
    Now integrated with IncrementalTermVectors such that RRI can take place without maintaining document vectors in memory (other than elemental vectors on the first iteration, if "-initialtermvectors random" is not used). Useful for corpora with many documents and comparatively few unique terms to be indexed.
    Use the new, non-deprecated constructors which do not silently depend on parameters from Flags.
    Added small else clause to make sure term vectors are still written if doc vectors aren't wanted. Fixes bug 26.
    Fixed poor documentation for maxnonalphabetchars.
    This change is something of a follow-up to the refactoring of the stoplist - it seems sensible to have the TermFilter in LuceneUtils use the "Flags" parameters, so I've refactored it this way. I've also added a -maxfrequency flag to complement -minfrequency, as Magnus has documented performance improvements with an upper limit on term frequency.
    Small changes made to allow for starting with a set of pre-existing initial term vectors (rather than random term vectors or some variant of document vector)
    Added flag configuration for lucene contents fields and docid field. this should be released soon.
    Fix for flag documentation error http://code.google.com/p/semanticvectors/issues/detail?id=17
    Implemented TRRI as per the RRI paper. This required adding another constructor to TermVectorsFromLucene that generates elemental term vectors for future recycling. Also entropy weighting has been added to LuceneUtils, to facilitate log-entropy weighting in DocVectors and IncrementalDocVectors. TRRI can now be performed by calling BuildIndex with the command line flags:
    -initialtermvectors random
    -trainingcycles 2
    
    This implementation doesn't scale as well as the one used in the RRI paper, as this one had an opposite number to IncrementalDocVectors named IncrementalTermVectors - combining these two allows one to reflect without ever keeping the document vectors in RAM, which is critical for large data sets on small machines. Hope to add this in the near future.
    Fix for training cycles to behave as advertized.
    Continued refactoring, wired command line flags into BuildIndex, made some changes to tests to remove Java internal deletion code that isn't reliable.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Minor style tweaks to Vidya's useful refactoring of TermFilter.
    Bug fix courtesy of Vidya Vasuki - terms in fields outside FieldsToIndex were also being included in vector stores. To fix this, an additional check has been added to TermFilter. Also, TermFilter has been refactored and is now part of LuceneUtils.
    
    One new command line option "-n" to allow a number of non-alphabet characters (useful for biomedical text) has also been included.
    Removed 'clone()' command from getVector in VectorStoreRAM, this was breaking the building of DocVectors. Clearly I don't know what I'm doing with Java memory management yet.
    Package now throws exceptions always instead of hard coding System.exit
    Minor changes to usage messages while releasing version 1.12
    Small changes: i. Changed IncrementalDocVectors and calling code to put VectorFile argument last (since it's a write argument); ii. Much Javadoc for Search.SearchType options; iii. Changed ClusterResults to do random rather than round robin assignment (request from Pitt).
    Two methods were removed from BuildIndex:
    (1) GetLuceneIndex
    (2) GetFieldsToIndex
    
    These were required for the previous implementation of IncrementalDocVectors, but the revised implementation does not require these, as the Lucene Index and FieldsToIndex String[] are passed to the constructor instead.
    
    The constructor for IncrementalDocVectors now requires these additional parameters, but it is able to take anything implementing the VectorStore interface rather than being restricted to a TermVectorsFromLucene object.
    
    BuildPositionalIndex now produces (incremental) docvectors.
    Added new command line configuration of incremental doc vectors in BuildIndex. Added copyright to IncrementalDocVectors.
    Slight edits to IncrementalDocVectors, comments and javadoc. Adding javadoc to html pages.
    An new class, IncrementalDocVectors.java is included. This builds document vectors using per-document statistics without requiring all the document vectors in RAM at once. On testing, it is about 3x faster than the present implementation, but this may have a lot to do with it not performing a second run through the document vectors to include their file/pathname (these are included on the first run in this implementation).
    
    BuildIndex has a line of code, presently commented out, to instantiate this class.
    Refactored DocVectors.java to use VectorStoreRAM, and added addVector method and test to VectorStoreRAM.
    Changed one line to write docvectors with the correct variable file name.
    Changed a couple of 0s to 1s to make training cycles behave appropriately.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Submitting much cleaned up version of Bilingual code, including slightly improved DocVector builder. Hopefully nearly ready to ship version 1.6.
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Minor tinkering with command line messages.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    More tweaking of javadoc.
    Cleaned up usage on BuildIndex and set mime types on new html docs.
    Initial semantic vectors package
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Checking in the following:
    (1) small change to DocVectors.bin such that normalization occurs without the need to look up document IDs, which seems to speed this up some (subjectively, no formal testing occurred)
    (2) an update to NumberRepresentation, such that the alpha and omega vectors are included as "alpha" and "omega" when graded vectors are generated (everything else stays the same, so they would also be included as numbered demarcator vectors)
    (3) an update to SentenceVectors such that it no longer requires the field to be indexed to be "stored''
    (4) an experimental version of proximity search *INCOMPLETE* - this needs to be updated to incorporate (2), currently it will only work if the indexed corpus happens to contain a document 100 words in length (as it must guess where to find an alpha and omega vector at query time and SentenceVectors currently outputs a vector store of the graded vectors for each document length in the corpus)
    Minor formatting and logging changes.
    Making doc vectors use external IDs (path or filenames) rather than Lucene's internal integer IDs throughout.
    Removed defunct "-deterministicvectors" flag, and added a check for nonexistent term:field combinations when constructing a space from indexes with multiple fields.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Found a conflict between PSI and the new elemental vector store implementation: PSI was calling getVector before checking whether a corresponding semantic vector existed, creating new elemental vectors for many terms.
    
    I've committed one possible fix - replacing the "getVector(x) == null" check with an explicit "containsVector(x) == false" check. This seems a safer practice, though there are a fair number of checks we would need to change scattered throughout the codebase still.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Suppress unnecessary logging, some minor changes.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Added local weighting (binary vs. TF vs. log(1+TF))
    Turned -docindexing into an enum.
    Redundant weighting, removed
    Refactored -termweight values to be an enum, {NONE, IDF, LOGENTROPY}. On public call to LuceneUtils.getGlobalTermWeight, flag-based switching is now all inside LuceneUtils.
    
    This should make it much easier to make sure that termweighting enhancements are properly incorporated into LSA, BuildIndex, BuildPositionalIndex, etc., without having to maintain multiple codepaths.
    
    All tests pass, but this demonstrates the need for more reliable testing at different parts of the stack.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Changing constructor for LuceneUtils to use only FlagConfig.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Added check to ensure that division by 0 doesn't occur with terms that occur in one of the index fields only.
    idf weighting added to DocVectors and IncrementalDocVectors. Seems a good idea to add this to sliding window models too, will look into this. Didn't seem worth factoring this out into LuceneUtils as it is a one-liner, but happy to do this if it is preferred.
    Added the option to weight document vectors (-fieldweight) when constructed, such that terms from shorter fields count more  - the weighting is 1/sqrt(number of terms in field).
    Hacked away at our logging verbosity. I believe (I think) that normal operation is much easier to interpret using VerbatimLogger, and that we should use other loggers for debugging. I'd like to write / find a one-line logging message which is basically verbatim with a file and line number prefixed.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    (Recommit after eclipse mismatch.) Added dimension setting and checking to VectorStoreRAM.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Fix for issue 34. This was obviously caused by
    IndexReader.directory().toString() returning the lockFactory information
    too. The old replace code to find the actual directory path did not
    work correctly any more. I have replaced this with code that works, but
    this whole code relies on Lucene giving us a nice toString String and
    might break again if they decide to change the format of that string.
    Added types to generic enumerations as discussed in issue 23.
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    Added flag configuration for lucene contents fields and docid field. this should be released soon.
    Added initialization for Flags.termweight; seems to fix a bug in DocVectors constructer. Illustrates the need for Flags checking at program initialization, and possibly more modular constructor / initializer functions (the DocVectors constructor is quite long). On the bright side, the regression tests helped us to diagnose this problem early on.
    Implemented TRRI as per the RRI paper. This required adding another constructor to TermVectorsFromLucene that generates elemental term vectors for future recycling. Also entropy weighting has been added to LuceneUtils, to facilitate log-entropy weighting in DocVectors and IncrementalDocVectors. TRRI can now be performed by calling BuildIndex with the command line flags:
    -initialtermvectors random
    -trainingcycles 2
    
    This implementation doesn't scale as well as the one used in the RRI paper, as this one had an opposite number to IncrementalDocVectors named IncrementalTermVectors - combining these two allows one to reflect without ever keeping the document vectors in RAM, which is critical for large data sets on small machines. Hope to add this in the near future.
    Cleaned up some of the new Beagle model implementation due to some movement in the SV codebase, e.g., ObjectVector.vecLength -> Flags.dimension.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Removed 'clone()' command from getVector in VectorStoreRAM, this was breaking the building of DocVectors. Clearly I don't know what I'm doing with Java memory management yet.
    Some refactoring for permutations stuff: in particular, changed IndexType to Enumeration and made TermTermVectors use VectorStoreRAM and VectorStoreSparseRAM to make writing vectors simpler. Also wrote some checks for permutation searcher.
    Added basic accessor methods back to TermVectorsFromLucene so that DocVectors will build.
    Refactored DocVectors.java to use VectorStoreRAM, and added addVector method and test to VectorStoreRAM.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Refactored Search.java to make parsing arguments and searching much cleaner. Refactors VectorSearcher accordingly - VectorSearchers now have to look after building the appropriate queries from the query arguments.
    Submitting much cleaned up version of Bilingual code, including slightly improved DocVector builder. Hopefully nearly ready to ship version 1.6.
    Added command line SearchTensorRelation interface. Results don't look too promising: where things work well, it's because tensor similarities just reduce to products of scalar products of the factors  (I think).
    
    Also adding product functions to VectorUtils and appropriate html javadoc files.
    Initial semantic vectors package
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Changes to enable redistributing coordinates to make their distributions approximately uniform.
    Making doc vectors use external IDs (path or filenames) rather than Lucene's internal integer IDs throughout.
    Checking to make sure calls to get a particular field return something or give a meaningful error.
    Adding dependency generation to main semanticvectors jar.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    ....forgot this on the last commit.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Moved json generation to pathfinder class
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Fixes to BuildBilingualIndex, which hadn't worked since the flagConfig update.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    I think this test change is necessary for probabilistic normalization not to be seen as an error.
    Made LuceneUtils.TermFilter use flagConfig in all cases except PSI. This is good because callers will get new features (e.g., length filtering) automatically.
    
    PSI is different, it has to set some overrides for which the old more direct interfaces are necessary. So this is an improvement, but the story isn't over yet.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added -deterministicvectors Flag. Seems to be working, though further testing is required. Will create a unit test in the near future.
    Changed back to the old "-initialtermvectors random" configuration for TRRI.
    
    As this was written previously, Flags.getInitialtermvectors() would need to be non-empty to enter createTermBasedRRIVectorsImpl, but would have to be empty for elemental vectors for terms to be generated, so this could not occur.
    Changing constructor for LuceneUtils to use only FlagConfig.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Introduced Flags.filternumbers parameter, that, when set to true, causes terms that represent numbers to be filtered out.
    * overloaded the termFilter method with an additional boolean to reflect this new parameter and updated all calls of the termFilter method except for in the PSI code, where also the maxnonalphabet parameter is not used
    * by default filternumbers is set to false so the default behavior of the SV package does not change
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Hacked away at our logging verbosity. I believe (I think) that normal operation is much easier to interpret using VerbatimLogger, and that we should use other loggers for debugging. I'd like to write / find a one-line logging message which is basically verbatim with a file and line number prefixed.
    Passed on some additional parameters to get TRRI working
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    "Dimensions" passed to TRRI implementation
    Fixes issue 36, I believe. Refactored VectorStoreWriter to switch properly between text and lucene format so that text vector stores can be created by BuildIndex as well.
    Passed the dimensionality to createTermBasedRRIVectorsImpl
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Fixes premature close of VectorStoreReaderLucene.
    Unlike VectorStoreReaderText.getAllVectors(), VectorStoreReaderLucene.getAllVectors() does not create a new file reader. So I need to wait until the enumeration is carried out before closing the index input.
    For the sake of consistency, both VectorStoreReader types should behave the same, which would require creating a new index input thread for the enumeration which automatically closes when the enumeration is done. But I don't want to be creating threads too often. So I think the best practice is to close the reader after going through the enumeration.
    Added types to generic enumerations as discussed in issue 23.
    Closed some more vectorstorereaderlucene file handles. This is done for general cleanliness and isn't necessary to complete any integration tests.
    Deprecate the constructors that depend on Flags.dimensions and Flags.initialtermvectors and replace with versions that allow to pass these parameters directly.
    This is a first attempt at plugging what appears to be a GC-related memory leak. It does involve runtime class checking, but seems to do the trick. It has been tested with the basic model, as well as with -trainingcycles 2 and is producing meaningful results. However, the acid test will be Danica's ridiculously large corpus.
    
    
            if (this.basicDocVectors.getClass().equals(VectorStoreSparseRAM.class)) //random docvectors
            {
                    termVector = VectorUtils.addVectors(termVector, ((VectorStoreSparseRAM) this.basicDocVectors).getSparseVector(docID), freq);
            }
            else  //pretrained docvectors
            {
            termVector = VectorUtils.addVectors(termVector, this.basicDocVectors.getVector(docID),freq);
            }
    Fixed poor documentation for maxnonalphabetchars.
    This change is something of a follow-up to the refactoring of the stoplist - it seems sensible to have the TermFilter in LuceneUtils use the "Flags" parameters, so I've refactored it this way. I've also added a -maxfrequency flag to complement -minfrequency, as Magnus has documented performance improvements with an upper limit on term frequency.
    Changed regression test, seems like martha does't tend to find mary and more but I don't think this is a bug.
    Some refactoring, haven't fixed indexing bug yet.
    Fixed one of the tests just by allocating extra memory with a hack in TermTermVectorsFromLucene ... need to figure out what the array index bound actually is.
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    Small changes made to allow for starting with a set of pre-existing initial term vectors (rather than random term vectors or some variant of document vector)
    Removed stray @return tag.
    Implemented TRRI as per the RRI paper. This required adding another constructor to TermVectorsFromLucene that generates elemental term vectors for future recycling. Also entropy weighting has been added to LuceneUtils, to facilitate log-entropy weighting in DocVectors and IncrementalDocVectors. TRRI can now be performed by calling BuildIndex with the command line flags:
    -initialtermvectors random
    -trainingcycles 2
    
    This implementation doesn't scale as well as the one used in the RRI paper, as this one had an opposite number to IncrementalDocVectors named IncrementalTermVectors - combining these two allows one to reflect without ever keeping the document vectors in RAM, which is critical for large data sets on small machines. Hope to add this in the near future.
    Removed some deprecated html javadocs. Updated version number to 1.21 (should have done this before).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Minor style tweaks to Vidya's useful refactoring of TermFilter.
    Bug fix courtesy of Vidya Vasuki - terms in fields outside FieldsToIndex were also being included in vector stores. To fix this, an additional check has been added to TermFilter. Also, TermFilter has been refactored and is now part of LuceneUtils.
    
    One new command line option "-n" to allow a number of non-alphabet characters (useful for biomedical text) has also been included.
    Minor changes to comments and style.
    Added basic accessor methods back to TermVectorsFromLucene so that DocVectors will build.
    2 methods removed: GetFieldsToIndex and GetLuceneIndex, as these are no longer required for IncrementalDocVectors to run.
    Refactored DocVectors.java to use VectorStoreRAM, and added addVector method and test to VectorStoreRAM.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Added VectorStoreReaderRAMCache and small test of this.
    i. Checking in some slight edits to Positional indexing code form Trevor (just occasional bracketing conventions, javadoc, minor tweaks to usage messages); ii. Removed typo class BuildIPositionalndex; iii. Added new class for clustering entire vectors stores (this isn't optimized at all, run from text not lucene-based vector indexes, but the clustering doesn't scale for large vector stores anyway.
    Classes for transforming between Lucene binary and plain text versions of indexes.
    Submitting much cleaned up version of Bilingual code, including slightly improved DocVector builder. Hopefully nearly ready to ship version 1.6.
    Several changes for getting the bilingual version working, and
    associated functionality.
    i. BuildBilingualIndex now works! (According to tests on europarl english-french data.)
    ii. TermVectorsFromLucene edited to properly enable reuse of basicDocVectors.
    
    This isn't perfect yet, by any means: it's probably inefficient in how much state is kept around when BuildBilingualIndex reuses TermVectorsFromLucene objects, and the interface to TermVectorsFromLucene could probably be improved. Since neither of these is a really basic class, I haven't thought too deeply about this - just don't write code that depends on these without rethinking and probably refactoring.
    
    iii. Added a scripts directory, initially with the preprocessing script for the europarl chapter alignment,
    iv. Added READMEs to the test and scripts directories.
    v. Changed build.xml file so that tests aren't automatically compiled (this is so that casual users of the source won't have to depend on JUnit).
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Slight change to TermVectorsFromLucene, such that the initial matrix of randomvectors is of size [no documents] by [seedlength] (rather than Veclength), saving memory.
    Slight refactoring of generateRandomVector in TermVectorsFromLucene, to use only class-level fileds vecLength and seedLength (so method takes no additional arguments).
    Regenerated javadoc, slight javadoc copyediting to TermVectorsFromLucene.
    Added some extra comments after Trevor's improvements to TermVectorsFromLucene, partly to explain the +/-1 offsetting.
    Underlying implementation of (random) document vectors changed to array of short[] of size seedLength, which should reduce memory footprint and increase processing speed.
    Added index optimization before opening in TermVectorsFromLucene to make sure that docids are contiguous integers; avoids out-of-bounds error reported by Lionel.
    Added command line SearchTensorRelation interface. Results don't look too promising: where things work well, it's because tensor similarities just reduce to products of scalar products of the factors  (I think).
    
    Also adding product functions to VectorUtils and appropriate html javadoc files.
    Initial semantic vectors package
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Added new classes and html docs to source, and created better usage documentation for Search and CompareTerms.
    Initial semantic vectors package
    Missed this also.
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Further limits for the -trainingcyles 0 condition
    (1) Iterative PSI will only occur if -trainingcycles > 0 (it slows
    things down).
    (2) Restored global weighting of predicates, which seems to result in
    more interpretable reasoning pathways
    More progress tracking in PSI
    Added permutation of either the subject or object elemental vector
    before it is added to the emerging predicate vector, such that P2(X) !=
    P2(X-INV)
    Support in PSI for training predicate vectors (as well as just using elemental predicate vectors).
    
    Still very unsure of what inverse predicates should look like in this model. (Which basically comes down to the question "should there be inverse vetors under binding / release, and if so what's the mutiplicative identity vector?"
    Retract the previous submission, missed the "getLocalTermWeight" bit.
    Added local weighting of predications (i.e. based on predication count)
    Follow FlagConfig for local (and global) weighting of predications - localweight(occurrences of predication), globalweight(other concept). Added a SQRT option as a weighting metric (local = SQRT(occurrences of predication), global = 1
    Informative exceptions here and there.
    Fixed what I believe I broke with the last commit (a call to TermFilter that didn't include the new parameter)
    Fixed real PSI (w00t!). Turns out it was to do with the inexact release / bind inverse relationship, so we shouldn't use this in training.
    Copying vectors we're supposed to modify in-place was a terrible idea.
    Removing an over-repeated logging statement.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Tests for real permutation PSI, and some refactoring.
    Found a conflict between PSI and the new elemental vector store implementation: PSI was calling getVector before checking whether a corresponding semantic vector existed, creating new elemental vectors for many terms.
    
    I've committed one possible fix - replacing the "getVector(x) == null" check with an explicit "containsVector(x) == false" check. This seems a safer practice, though there are a fair number of checks we would need to change scattered throughout the codebase still.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Reinstating real PSI, capturing first predicate line, supporting complex flat serialization (which is the same as complex).
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Added / changed some error messages while using PSI.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Refactored -termweight values to be an enum, {NONE, IDF, LOGENTROPY}. On public call to LuceneUtils.getGlobalTermWeight, flag-based switching is now all inside LuceneUtils.
    
    This should make it much easier to make sure that termweighting enhancements are properly incorporated into LSA, BuildIndex, BuildPositionalIndex, etc., without having to maintain multiple codepaths.
    
    All tests pass, but this demonstrates the need for more reliable testing at different parts of the stack.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added -deterministicvectors Flag. Seems to be working, though further testing is required. Will create a unit test in the near future.
    Changing constructor for LuceneUtils to use only FlagConfig.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    Adapting PSI to use -luceneindexpath, to be like other main entry points.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Changed the weighting metric to something more reasonable, and moved the "real vector" check to after the point at which the command line parameters are read (I'll make the same changes to the branch shortly).
    Slight change made to weighting metrics, for consistency with previous experiments (now = global weight of predication + global weight of other concept; previously these two weights were multiplied)
    Added a warning that PSI is not yet implemented for real vectors (we'd need a way to attach permutations to predicates, e.g. a permutation store that functions much like a vector store), and a switch to complex vectors. Both will fire if PSI with real vectors is requested.
     Index structure and indexing approach altered such that this now uses one step for each unique predication (as oppose to one step for each predication). This makes it easier to use log(global predication frequency) as a weighting metric, which has improved performance in our experiments to date.
    Cleanup revision. Have removed stray javadocs instead of fixing them - this is interim but good enough for now.
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Fixed a bug in closing bound / product vector store.
    Added some javadoc and changed name "queryvectorfile2" to "boundvectorfile". (Not a great name, but ...)
    Some test fixes to come.
    Working prototype of a PSI implementation with complex vectors. This didn't take much doing, as all the hard work was already done thanks to Lance and Dominic. Of note, I did remove the constraint that complex vectors must be sparse in the "many zeros" sense, as things seem to work much better this way. Here are some example queries on the national facts data:
    
    PSI
    -vectortype complex
    -dimension 1000
    -seedlength 1000
    predication_index
    
    SEARCHES
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    lion HAS_NATIONAL_ANIMAL-INV
    
    Found vector for 'lion'
    Found vector for 'HAS_NATIONAL_ANIMAL-INV'
    Search output follows ...
    0.3619873304097799:sweden
    0.3384664837853647:liberia
    0.3282750592424923:belgium
    0.31246171572126347:luxembourg
    0.31236739306024536:united_kingdom
    0.3070540803456166:bulgaria
    0.30435112416653176:macedonia
    0.3016290787884374:netherlands
    0.07023105799656197:managua
    0.06497062922105992:belarus
    
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 elementalvectors.bin
    -searchvectorfile predicatevectors.bin
    -matchcase
    united_states united_states_dollar
    
    Opening query vector store from file: semanticvectors.bin
    Opening second query vector store from file: elementalvectors.bin
    Opening search vector store from file: predicatevectors.bin
    Searching term vectors, searchtype boundproduct
    Found vector for 'united_states'
    Found vector for 'united_states_dollar'
    0.529266436624566:HAS_CURRENCY
    0.0057567168349519264:HAS_NATIONAL_ANIMAL
    
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    mexico HAS_CURRENCY
    
    Opening query vector store from file: semanticvectors.bin
    Opening second query vector store from file: predicatevectors.bin
    Opening search vector store from file: elementalvectors.bin
    Searching term vectors, searchtype boundproduct
    Found vector for 'mexico'
    Found vector for 'HAS_CURRENCY'
    Search output follows ...
    0.5408946657776929:mexican_peso
    0.07199221477091654:san_salvador
    Fixed bug in handling of directed relationships. The Search example posted previously would now take command line arguments:
    
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase lion HAS_NATIONAL_ANIMAL-INV
    Basic implementation of PSI added. Unit tests to follow. This will run on the nationalfacts.txt file in testdata - first produce a Lucene edition of this file using LuceneIndexFromTriples, then run PSI on it.
    
    Concepts will be encoded as lowercase terms, with spaces replaced by underscores. Predicates will be encoded as uppercase terms, with spaces replaced by underscores.
    
    To search the resulting files with a bound product, use the command line parameters:
    -searchtype boundproduct
    -queryvectorfile elementalvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile semanticvectors.bin
    -matchcase
    concept predicate
    e.g.
    lion HAS_NATIONAL_ANIMAL-INV
    
    0.7587579617834395:liberia
    0.755374203821656:united_kingdom
    0.7548765923566879:sweden
    0.7532842356687898:bulgaria
    0.7501990445859873:macedonia
    0.7482085987261147:belgium
    0.6879976114649682:netherlands
    0.6230095541401274:luxembourg
    Support in PSI for training predicate vectors (as well as just using elemental predicate vectors).
    
    Still very unsure of what inverse predicates should look like in this model. (Which basically comes down to the question "should there be inverse vetors under binding / release, and if so what's the mutiplicative identity vector?"
    Command line access to intersection search (binary vectors only)
    Added comparisons between PSI expressions (using the elementalvectorfile, semanticvectorfile and predicatvectorfile flags), to accommodate e.g.
    S(prozac)*P(ISA)|E(fluoxetine)
    Score = 0.459500. Terms: S(prozac)*P(ISA)|E(fluoxetine)
    0.4595
    Small fix to make CompareTerms work with default file extensions.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Refactored CompareTerms slightly to be able to run in tests. Coded up a couple of tests for orthographic usage.
    
    This should help to figure out why the complex version is giving NaNs at the moment.
    Cleaning up logging and exact calling pathways to improve output readability.
    Minor fix to prevent CompareTerms from crashing over an empty -luceneindexpath which isn't necessary.
    Fixed bug I introduced into StringEdit (and in the process eliminated the "iEnd" prefix from the vectors returned), and added option for specification of orthographic vector store at query time (so OOV words can be used as a cue)
    -queryvectorfile orthographic has a nicer ring I think
    Force binary vectors if real vectors are selected (though dense complex would be fine too), and add the orthographic option for CompareTerms (-queryvectorfile orthographic)
    Refactored vector building interfaces in TermTermVectorsFromLucene, IncrementalTermVectors, and IncrementalDocVectors, to use flagConfigs much more. This simplifies interfaces considerably.
    
    However, I chickened out of completely removing args[i] parameters from Incremental*Vectors classes, since I don't use them regularly and don't have proper tests.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Changing constructor for LuceneUtils to use only FlagConfig.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Fixed regression bug with VectorStore filenames in CompareTerms. Brought some logging and usage into line with other classes.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Updated version number to 2.5 after shipping 2.4. And related cleanup.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Closed some more vectorstorereaderlucene file handles. This is done for general cleanliness and isn't necessary to complete any integration tests.
    Updating version number after shipping version 1.30.
    Renaming VectorStoreReader to VectorStoreReaderLucene.
    Updating Flags.java (to add new flags) and associated documentation files.
    Flags library now uses reflection, using ClusterResults as test case.
    Added a test for CompoundVectorBuilder and VectorStoreRAM, and utils for outputting principal component plots as tex source.
    Cleaned up regex comparison in CompareTerms - working well now (have discovered the synoptic gospels).
    Incremental commit - trying to get regular expression search working to build summary vectors from lots of documents at once.
    Package now throws exceptions always instead of hard coding System.exit
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    Overhauled documentation (usages and javadoc) to include reference to NOT query syntax.
    Completed initial implementation of orthogonal negation, including an orthogonalize routine in VectorUtils and query parsing in CompoundVectorBuilder.
    More tweaking of javadoc.
    Added new classes and html docs to source, and created better usage documentation for Search and CompareTerms.
    Created CompareTerms command line interface, and CompoundVectorBuilder which now handles query building from several query terms for both Search and CompareTerms.
    Minor changes to whitespace and comments.
    Configured jar and src (.tar.gz) build of project appropriately. Small change to Search usage string.
    Initial semantic vectors package
    Replace pipes in String (term) so these can serve as delimiters
    Quick fix for the problem that the java.nio component causes SV to crash
    at the point of writing out vectors if another file of the same name
    exists. While it may be better to warn of this and create a new name,
    this one liner reverts to the previous SV behavior of writing over
    vector stores.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Turned vectorstorefomat into enum.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Change to make file extensions automatic.
    
    That is, internal vector store names are just "termvectors", not "termvectors.bin", etc. The ".bin" or ".txt" suffix is applied automatically.
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Hacked away at our logging verbosity. I believe (I think) that normal operation is much easier to interpret using VerbatimLogger, and that we should use other loggers for debugging. I'd like to write / find a one-line logging message which is basically verbatim with a file and line number prefixed.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Just trailing whitespace from working with eclipse and then emacs.
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    Fixes issue 36, I believe. Refactored VectorStoreWriter to switch properly between text and lucene format so that text vector stores can be created by BuildIndex as well.
    I'm in pursuit of a malevolent bug which is causing sets of zero vectors to be recorded during TRRI with incremental indexing. These changes appear to fix it, but I haven't quite got to the root of why this works:
    
    (1) normalize the document vectors before adding them to term vectors by default
    - this is a reasonable thing to do, and if we keep them as is on disk we retain the possibility of incremental indexing
    
    However.... what fixed the bug was:
    (2) change the number of dimensions written to disk from the "dimension" parameter to the length of the vector from the vector store
    
    Mysteriously, I've yet to find evidence of a case where the vector-to-be-written and the dimension variable didn't match one another.  However, the bug is reproducible by changing back "i < tmpVector.length" to "i < dimension", and disappears upon reversing this change.
    
    More from me as new clues are uncovered, but I thought it would be better to have a working implementation checked in for now.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    Changed MMapDirectory to FSDirectory in IncrementalDocVectors, this should solve the Lucene 2.9 compatibility bug.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Fixed VectorStoreReader and Writer closing operations by using FSDirectory instead of MMapDirectory. This seems to be working, tests fixed accordingly.
    Some refactoring for permutations stuff: in particular, changed IndexType to Enumeration and made TermTermVectors use VectorStoreRAM and VectorStoreSparseRAM to make writing vectors simpler. Also wrote some checks for permutation searcher.
    overloaded WriteVectors method so it also takes (String, Hashtable<String,short[]>) for preserving random index vectors for order-based querying of a permuted index
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Classes for transforming between Lucene binary and plain text versions of indexes.
    Adding simple vector strore to text writer, about to work more on this.
    Did some slight refactoring to make for more robust backwards compatibility with old indexes, with usage messages if things go wrong.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    Regenerated javadoc, slight javadoc copyediting to TermVectorsFromLucene.
    Initial semantic vectors package
    Added  java.nio.file.Files.deleteIfExists(vectorFile.toPath()); so we
    don't crash at the point of writing out a VectorStore when a filename is
    already occupied.
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Small change to print contents fields when initializing LuceneUtils.
    Comment on incremental doc vectors not doing text output.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Minor formatting and logging changes.
    Making doc vectors use external IDs (path or filenames) rather than Lucene's internal integer IDs throughout.
    A couple of changes and bug fixes:
    (1) In IncrementalTermVectors, updated an if statement to (docVectorsInputStream.getFilePointer() < docVectorsInputStream.length() - 1) to comply with the current Lucene conventions. Without this, crashing occurs.
    (2) In IncrementalDocVectors, changed such that if a zero document vector is encountered this is nonetheless written to disk. Without this the document vector store may fall out of sequence with the Lucene Index, wreaking havoc on the next iteration.
    (3) Eliminated an else statement that may result in IncrementalTermVectors creating a random document vector if the incoming document vector store runs out. I'm not sure if this alternative was ever invoked, but it seems as though keeping the document vector store in synch with the Lucene index should make it unnecessary.
    Adding required argument to format string.
    Making empty fields just a fine warning, whereas empty documents are severe warnings.
    Make IncrementalDocVectors give info messages when individual fields are empty, and only give warning messages when all fields in a document are empty.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Added local weighting to IncrementalDocVectors, and caching of global frequencies to LuceneUtils.
    Minor enhancement: calculate global weights of terms to-be-indexed only.
    GC appears more inclined to collect last iteration's document vector this way.
    Improved description of IncrementalDocVectors.
    Apologies - broke the stack on my last commit by including a call to an experimental weighted superposition method that does not exist here. This should fix it.
    Quick fix: getGlobalTermWeightFromString was returning 0 much of the time with log entropy weighting (for terms that exist in the corpus).
    
    I haven't yet got to the bottom of this, but it makes more sense to use getGlobalTermWeight in this context anyhow (as this occurs within a loop across fields).
    
    This fix is likely to improve results when using incremental indexing and weighting together.
    Refactored -termweight values to be an enum, {NONE, IDF, LOGENTROPY}. On public call to LuceneUtils.getGlobalTermWeight, flag-based switching is now all inside LuceneUtils.
    
    This should make it much easier to make sure that termweighting enhancements are properly incorporated into LSA, BuildIndex, BuildPositionalIndex, etc., without having to maintain multiple codepaths.
    
    All tests pass, but this demonstrates the need for more reliable testing at different parts of the stack.
    Refactored vector building interfaces in TermTermVectorsFromLucene, IncrementalTermVectors, and IncrementalDocVectors, to use flagConfigs much more. This simplifies interfaces considerably.
    
    However, I chickened out of completely removing args[i] parameters from Incremental*Vectors classes, since I don't use them regularly and don't have proper tests.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Changing constructor for LuceneUtils to use only FlagConfig.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Use lUtils.idf (with caching) to calculate IDF, instead of reimplementing this locally.
    Change to make file extensions automatic.
    
    That is, internal vector store names are just "termvectors", not "termvectors.bin", etc. The ".bin" or ".txt" suffix is applied automatically.
    Added check to ensure that division by 0 doesn't occur with terms that occur in one of the index fields only (applies to IDF weighting).
    idf weighting added to DocVectors and IncrementalDocVectors. Seems a good idea to add this to sliding window models too, will look into this. Didn't seem worth factoring this out into LuceneUtils as it is a one-liner, but happy to do this if it is preferred.
    Added the option to weight document vectors (-fieldweight) when constructed, such that terms from shorter fields count more  - the weighting is 1/sqrt(number of terms in field).
    Added "toUpperCase" to VectorType.getValueOf(Flags.vectortype) to facilitate flag recognition.
    Problem when indexing multiple fields - writing of the document vector was inside the "fieldsToIndex" loop, so we ended up with two document vectors, one per field.
    Hacked away at our logging verbosity. I believe (I think) that normal operation is much easier to interpret using VerbatimLogger, and that we should use other loggers for debugging. I'd like to write / find a one-line logging message which is basically verbatim with a file and line number prefixed.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Apologies - patch to previous commit
    Included parsing of command line flags with IncrementalDocVectors, so docvectors can be built this way from the command line.
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    (Recommit after eclipse mismatch.) Added dimension setting and checking to VectorStoreRAM.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    found superfluous "dc++" (coding gremlins at work), causing the IndexReader to look for nonexistent documents and crash. this has been removed.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Further cleanup of docid field.
    Changes to get rid of hard-coded 'contents' fields.
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    Changed MMapDirectory to FSDirectory in IncrementalDocVectors, this should solve the Lucene 2.9 compatibility bug.
    Implemented TRRI as per the RRI paper. This required adding another constructor to TermVectorsFromLucene that generates elemental term vectors for future recycling. Also entropy weighting has been added to LuceneUtils, to facilitate log-entropy weighting in DocVectors and IncrementalDocVectors. TRRI can now be performed by calling BuildIndex with the command line flags:
    -initialtermvectors random
    -trainingcycles 2
    
    This implementation doesn't scale as well as the one used in the RRI paper, as this one had an opposite number to IncrementalDocVectors named IncrementalTermVectors - combining these two allows one to reflect without ever keeping the document vectors in RAM, which is critical for large data sets on small machines. Hope to add this in the near future.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Relaxed the constraint that TermPositionVectors (rather than TermFreqVectors) are required, which must have crept across from the sliding-window based indexing code.
    Putting all of term vector logic inside try/catch clause: this should fix null pointer exception reported in issue 9.
    bug fix: previous version looked for size of null vectors when term was not found in index
    Package now throws exceptions always instead of hard coding System.exit
    Small changes: i. Changed IncrementalDocVectors and calling code to put VectorFile argument last (since it's a write argument); ii. Much Javadoc for Search.SearchType options; iii. Changed ClusterResults to do random rather than round robin assignment (request from Pitt).
    The constructor for this class now takes two additional elements:
    
    *String indexDir: The directory of the Lucene Index used to generate termVectorData
    
    *String[] fieldsToIndex  containing fields indexed when generating termVectorData
    Added new command line configuration of incremental doc vectors in BuildIndex. Added copyright to IncrementalDocVectors.
    Slight edits to IncrementalDocVectors, comments and javadoc. Adding javadoc to html pages.
    A new class, IncrementalDocVectors.java is included. This builds document vectors using per-document statistics without requiring all the document vectors in RAM at once. On testing, it is about 3x faster than the present implementation, but this may have a lot to do with it not performing a second run through the document vectors to include their file/pathname (these are included on the first run in this implementation).
    
    BuildIndex has a line of code, presently commented out, to instantiate this class.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Copying across from branch to trunk.
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Committing Beagle
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Fixed bugs with command line parsing in Search.java and number of clusters in ClusterResults.java
    Slight refactoring of generateRandomVector in TermVectorsFromLucene, to use only class-level fileds vecLength and seedLength (so method takes no additional arguments).
    Initial semantic vectors package
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Fixes to BuildBilingualIndex, which hadn't worked since the flagConfig update.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Introduced Flags.filternumbers parameter, that, when set to true, causes terms that represent numbers to be filtered out.
    * overloaded the termFilter method with an additional boolean to reflect this new parameter and updated all calls of the termFilter method except for in the PSI code, where also the maxnonalphabet parameter is not used
    * by default filternumbers is set to false so the default behavior of the SV package does not change
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Fixes issue 36, I believe. Refactored VectorStoreWriter to switch properly between text and lucene format so that text vector stores can be created by BuildIndex as well.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Use the new, non-deprecated constructors which do not silently depend on parameters from Flags.
    Added flag configuration for lucene contents fields and docid field. this should be released soon.
    This fixes some strange bit-drift in BuildBilingualIndex.
    Continued refactoring, wired command line flags into BuildIndex, made some changes to tests to remove Java internal deletion code that isn't reliable.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Minor style tweaks to Vidya's useful refactoring of TermFilter.
    Bug fix courtesy of Vidya Vasuki - terms in fields outside FieldsToIndex were also being included in vector stores. To fix this, an additional check has been added to TermFilter. Also, TermFilter has been refactored and is now part of LuceneUtils.
    
    One new command line option "-n" to allow a number of non-alphabet characters (useful for biomedical text) has also been included.
    Package now throws exceptions always instead of hard coding System.exit
    Refactored DocVectors.java to use VectorStoreRAM, and added addVector method and test to VectorStoreRAM.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Classes for transforming between Lucene binary and plain text versions of indexes.
    Submitting much cleaned up version of Bilingual code, including slightly improved DocVector builder. Hopefully nearly ready to ship version 1.6.
    Several changes for getting the bilingual version working, and
    associated functionality.
    i. BuildBilingualIndex now works! (According to tests on europarl english-french data.)
    ii. TermVectorsFromLucene edited to properly enable reuse of basicDocVectors.
    
    This isn't perfect yet, by any means: it's probably inefficient in how much state is kept around when BuildBilingualIndex reuses TermVectorsFromLucene objects, and the interface to TermVectorsFromLucene could probably be improved. Since neither of these is a really basic class, I haven't thought too deeply about this - just don't write code that depends on these without rethinking and probably refactoring.
    
    iii. Added a scripts directory, initially with the preprocessing script for the europarl chapter alignment,
    iv. Added READMEs to the test and scripts directories.
    v. Changed build.xml file so that tests aren't automatically compiled (this is so that casual users of the source won't have to depend on JUnit).
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Initial commit of permutation-based embedding code (ahead of reviewing
    issues with permutation/directional based unit tests).
    
    There are some fairly significant structural changes here including:
    (1) using -embeddingmethod (instead of positionalmethod) to distinguish
    between RI and SGNS
    (2) proximity-based permutations
    (3) facilities for storing, generating and searching with random
    permutations
    Cleanup / indents and imports and such
    Checking in the following:
    (1) small change to DocVectors.bin such that normalization occurs without the need to look up document IDs, which seems to speed this up some (subjectively, no formal testing occurred)
    (2) an update to NumberRepresentation, such that the alpha and omega vectors are included as "alpha" and "omega" when graded vectors are generated (everything else stays the same, so they would also be included as numbered demarcator vectors)
    (3) an update to SentenceVectors such that it no longer requires the field to be indexed to be "stored''
    (4) an experimental version of proximity search *INCOMPLETE* - this needs to be updated to incorporate (2), currently it will only work if the indexed corpus happens to contain a document 100 words in length (as it must guess where to find an alpha and omega vector at query time and SentenceVectors currently outputs a vector store of the graded vectors for each document length in the corpus)
    Added a "PRINTPSIQUERY" search, to reveal the entire search vector generated by a -boundproduct type query for debugging, or downstream re-use
    Bits ot cleanup to make sure that term weights are working
    Command line access to intersection search (binary vectors only)
    Cleaned up the PROXIMITY sliding window code, such that permutation affects the demarcator vectors only, and the permuted demarcator vectors are output to facilitate queries along the lines of:
    
     "E(king)*P(-1)"
    Opening elemental query vector store from file: deterministic
    Opening semantic query vector store from file: proxtermvectors.bin
    Opening predicate query vector store from file: numbervectors.bin
    Opening search vector store from file: proxtermvectors.bin
    Searching term vectors, searchtype BOUNDPRODUCT
    Found vector for '-1'
    Search output follows ...
    12.726996:minos
    12.423948:aegeus
    11.939073:midas
    10.666273:lear
    8.423721:cheng
    
    Interestingly, this allows for a (very simple) sort of analogy query:
    "E(king)*S(minos)*E(duke)"
    Search output follows ...
    5.511599:ellington
    
    "E(king)*S(minos)*E(queen)"...
    6.596088:sheba
    5.246219:victoria
    4.796262:anne's
    4.410585:elizabeth
    4.282026:finches
    Factoring out results-printing methods to simplify main flow of Search.java.
    Added interface to Lucene document search
    Numerous small modifications to accommodate more flexible command line PSI queries with QI in mind. One of these is definitely temporary - "-bindnotreleasehack", which will ultimately be replaced by an expanded query syntax with a different symbol for bind and release.
    
    Nonetheless, it seemed worth checking in the status quo.
    Attempt at rendering the generation of determinsitic vectors at Search time consistent with the "-elementalmethod" flag use elsewhere.
    
    So we'd now configure this using -queryvectorfile orthographic and and -elementalmethod,  for example:
    
    java -cp semanticvectors-5.5.jar pitt.search.semanticvectors.Search -queryvectorfile deterministic -vectortype binary -dimension 8000 -searchvectorfile elementalvectors.bin -elementalmethod contenthash prozac
    Two small changes:
    (1) it is now possible to do other searches aside from PSI searches again (apologies - broke the stack again).
    (2) for the proximity-based sliding window model, I've avoided attempting to restore elemental vectors to their natural state using "release" (by generating copies for binding instead), as this doesn't seem to work very well with real vectors. I'm not sure if this is a cumulative consequence of an approximate inverse or something more sinister, I'll poke around and see what I can find out.
    A first attempt at incorporating the S(X), E(X), P(predicate) notation into the search interface. It should now be possible to specify queries this way for boundproduct, boundproductsubspace and boundminimum, if -elementalvectorfile -semanticvectorfile -predicatevectorfile and -searchvectorfile are specified.
    Trivial warning whacking
    RunSearch -> runSearch for more normal Java style.
    Closing files in Search.java - should have been doing this all along. This makes several hitherto-failing tests pass on Windows.
    Removing SPARSESUM and updating a comment.
    ....probably best not to load semantic vectors, elemental vectors and predicate vectors all from the same file.
    A few experimental Search approaches have been added:
    (1) minimum score across sets of vectors with and without binding (to find middle terms)
    (2) a first attempt at producing .json files for predication searches
    
    I've been a little lazy about defining new flags here, specifically:
    - I've repurposed the "CompoundVectorBuilder" code that creates disjoint subspaces, such that it will not normalize the subspace if the searchtype is "BoundMinimum"
    - The "semanticvectorfile", "predicatevectorfile" and "elementalvectorfile" are used to tell the code building the graph where to find these vectors, so the graph construction is not tied to the search type
    - If this flags are not set to their default values and the .jsonfile flag is set, this triggers construction of a predication graph
    
    There are most likely clearer ways to trigger this behavior, but this seems adequate for the moment.
    Removing SPARSESUM as a search option.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Moved json generation to pathfinder class
    Moving PathFinder to viz package, and some breaking out of methods and catching exceptions in RunSearch that I should have caught earlier.
    Output search results as the graph resulting from Pathfinder network scaling of the pairwise connectivity matrix in .json format compatible with the D3 visualization library. The new command line parameters are:
    
    -jsonfile  (e.g. jsonfile graph.json - will output to this if nonempty)
    -pathfinderQ, pathfinderR (the Pathfinder q and r parameters)
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Removed unnecessary (ObjectVector) cast and cleaned up results formatting.
    Fixed bug I introduced into StringEdit (and in the process eliminated the "iEnd" prefix from the vectors returned), and added option for specification of orthographic vector store at query time (so OOV words can be used as a cue)
    Tiny correction to usage message (that should be updated more thoroughly).
    Turned searchtype into an enum, apparently successfully.
    
    Renamed LOG_ENTROPY to LOGENTROPY, realizing that this too will be passed normally as -termweight logentropy.
    Refactored -termweight values to be an enum, {NONE, IDF, LOGENTROPY}. On public call to LuceneUtils.getGlobalTermWeight, flag-based switching is now all inside LuceneUtils.
    
    This should make it much easier to make sure that termweighting enhancements are properly incorporated into LSA, BuildIndex, BuildPositionalIndex, etc., without having to maintain multiple codepaths.
    
    All tests pass, but this demonstrates the need for more reliable testing at different parts of the stack.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Changing constructor for LuceneUtils to use only FlagConfig.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Improved Javadoc formatting in Search.
    Changed negation term from NOT to ~NOT, and fixed tests accordingly. Minor refactoring and javadoc synchronizing.
    Corrected an error in the previous submission, wrong VectorSearcher.
    Command line "Search" interface added for VectorSearcher changes in previous commit
    This code aims to facilitate searching across dual-predicate paths, as per the "schizophrenia's prozac" paper. This is accomplished by expanding n predicate vectors by adding n-squared dual predicate paths (binary), or 2xn-squared dual predicate paths (complex) by combining the individual predicates using the "release" operator. I've also switched around the order in certain of the boundproduct complexqueryvector methods, as this is important in the complex vector version.
    Implemented bind and release for real and complex vectors.
    
    Working "correctly" for real case, not producing good results yet.
    
    I wonder how useful it is to have four total bind and release methods?
    Experimenting with a new AnalogySearcher, which doesn't work yet but is a useful test for bind and release (which doesn't work reliably yet except for with binary vectors!)
    
    I've cleaned up some not-so-useful verbiage in Search.java - it's much easier to read the big switch-list on searchtype now. This violates the "catch exceptions as tightly as possible" principle but it's worth it to get so much more code together.
    Added option for -boundproduct search to accept definitions of the form:
    
    S(united_states)*S(mexico)*E(dollar)
    
    It might be simpler to have this as the only option eventually.
    Added methods to CompoundVectorBuilder to facilitate:
    
    (1) parsing expressions that combine bundling and binding into a single superposed query vector
    (2) parsing expressions with multiple bound products into a subspace for search across multiple predicate paths
    
    (1) facilitates the "dollar of mexico" example when searching on the national facts data with the following parameters:
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -boundvectorfile elementalvectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    united_states*mexico united_states_dollar
    
    0.25148809523809523:mexican_peso
    0.0580357142857143:senegal
    
    (2) takes queries of the form concept predicate_path+predicate_path. The idea is to facilitate search from a cue across multiple paths simultaneously, using the quantum-inspired OR operator. An example with the national facts data might be:
    -searchtype boundproductsubspace
    -queryvectorfile semanticvectors.bin
    -boundvectorfile predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    united_kingdom HAS_NATIONAL_ANIMAL+HAS_CURRENCY
    
    0.504960298538208:british_pound
    0.4712301790714264:lion
    0.0625:ascension_pound
    0.0545634925365448:hanoi
    Fixed a bug in closing bound / product vector store.
    Added some javadoc and changed name "queryvectorfile2" to "boundvectorfile". (Not a great name, but ...)
    Some test fixes to come.
    Fixed bug induced by missing parentheses previously.
    Basic implementation of PSI added. Unit tests to follow. This will run on the nationalfacts.txt file in testdata - first produce a Lucene edition of this file using LuceneIndexFromTriples, then run PSI on it.
    
    Concepts will be encoded as lowercase terms, with spaces replaced by underscores. Predicates will be encoded as uppercase terms, with spaces replaced by underscores.
    
    To search the resulting files with a bound product, use the command line parameters:
    -searchtype boundproduct
    -queryvectorfile elementalvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile semanticvectors.bin
    -matchcase
    concept predicate
    e.g.
    lion HAS_NATIONAL_ANIMAL-INV
    
    0.7587579617834395:liberia
    0.755374203821656:united_kingdom
    0.7548765923566879:sweden
    0.7532842356687898:bulgaria
    0.7501990445859873:macedonia
    0.7482085987261147:belgium
    0.6879976114649682:netherlands
    0.6230095541401274:luxembourg
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Remove the import from java.lang as this package is imported by default.
    I think this fixes the worst of issue 21. Unit tests in place, though getting failures from the inside of threads seems to be a problem.
    Cleaned up some indentation.
    Fixed "-numsearchresults" argument which was being ignored by Search
    Cleaned up some javadoc a bit for Search.java.
    Removed some deprecated html javadocs. Updated version number to 1.21 (should have done this before).
    minor fix to IndexBilingualFiles.
    Retrofitted Clustering classes to use Flags.
    Minor synchronization of my client with Trevor's revision for balanced_permutation.
    Added "balanced_permutation" as a search type, and changed the usage() method to correspond to the new flags (-queryvectorfile, -searchvectorfile)
    Small addition of factory class VectorStoreReader.
    Renaming VectorStoreReader to VectorStoreReaderLucene.
    Some cleanup before QI2009, errors of mine that I found when reproducing demos.
    Fixed problem with setting Search.numResults, in response to issue 12.
    Fixed VectorStoreReader and Writer closing operations by using FSDirectory instead of MMapDirectory. This seems to be working, tests fixed accordingly.
    Small change in close() for VectorStoreReader, but with consequences in catching undeclared NullPointerExceptions, in main Search code as well as in tests. Trying to track down some read - write problems, see http://groups.google.com/group/semanticvectors/browse_thread/thread/48198c639c9b6d72/814e034b34f5af62#814e034b34f5af62 for details.
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Adding visualization utils. Added extra sophistication to build file to support this: now, utils will ship with the package but will not compile by default, to avoid extra dependencies for basic users.
    Housecleaning - usage for ClusterVectorStore, better exiting for PRINTQUERY.
    Added command line support for -lowercase flag.
    Added exceptions to disjunction vector searcher classes; removed exception clauses from search wrapping functions since ZeroVectorExceptions are all caught in Search.RunSearch now.
    Catches ZeroVectorExceptions which may by thrown in different constructors of VectorSearcher.
    If a ZeroVectorException occurs, the message is brought to STDERR and results are set to an empty LinkedList.
    
    Dominik Jednoralski - twelve02
    Exception catching for ZeroVectorExceptions (next time I should run 'ant clean; ant'.)
    Package now throws exceptions always instead of hard coding System.exit
    Some refactoring for permutations stuff: in particular, changed IndexType to Enumeration and made TermTermVectors use VectorStoreRAM and VectorStoreSparseRAM to make writing vectors simpler. Also wrote some checks for permutation searcher.
    The committed version should contain an implementation of the Sahlgren (2008) permutation-based encoding of word order. This includes:
    (1) a modification to BuildPositionalIndex such that it can take an argument indextype [basic (default), directional (the HAL model), or permutation (Sahlgren '08)].
    (2) modifications to TermTermVectorsFromLucene such that term vectors are permuted appropriately
    (3) additions to Search, CompoundVectorBuilder and VectorSearcher to facilitate index-based retrieval such that "president ?" retrieves terms occurring frequently after president
    (4) an additional class IndexTermVectorsFromRandomIndex that generates a vectors store on disk from a hashtable, used to preserve the random vectors as these are the basis for generating order-based queries
    Changed lowercasing logic to lowercase all arguments and give warnings if this changes arguments.
    Added support for -results command line flag; behavior consistent with similar usage in ClusterResults.
    Fixed tiny bug in command line searchtype option parsing.
    Updated javadoc for searchtypes and vector searchers.
    Added new command line configuration of incremental doc vectors in BuildIndex. Added copyright to IncrementalDocVectors.
    Changed one line to write docvectors with the correct variable file name.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    i. Checking in some slight edits to Positional indexing code form Trevor (just occasional bracketing conventions, javadoc, minor tweaks to usage messages); ii. Removed typo class BuildIPositionalndex; iii. Added new class for clustering entire vectors stores (this isn't optimized at all, run from text not lucene-based vector indexes, but the clustering doesn't scale for large vector stores anyway.
    Fixed bugs with command line parsing in Search.java and number of clusters in ClusterResults.java
    Fixed null results for upper cased words, issue 4.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    Refactored Search.java to make parsing arguments and searching much cleaner. Refactors VectorSearcher accordingly - VectorSearchers now have to look after building the appropriate queries from the query arguments.
    Minor refactor to make Search.java use (explicitly) the VectorStore interface, rather than insisting on the Lucene VectorStoreReader implementation.
    Improved javadoc for translater. More importantly, refactored VectorSearcher to take VectorStore interface instead of VectorStoreReader, so that both Lucene index and plain text index VectorStoreReaders work for search interface.
    Changed one .out.println to a .err.println, and updated the build to 1.4, hopefully about to ship.
    Continued updating documentation.
    Overhauled documentation (usages and javadoc) to include reference to NOT query syntax.
    Completed initial implementation of orthogonal negation, including an orthogonalize routine in VectorUtils and query parsing in CompoundVectorBuilder.
    Refactored to have search go through a VectorSearcher object that enables the use of different similarity scores while using common getNearestNeighbors implementation. Appears to be working fine, next step is to add some other similarity scores, e.g., using tensors.
    More tweaking of javadoc.
    Added new classes and html docs to source, and created better usage documentation for Search and CompareTerms.
    Created CompareTerms command line interface, and CompoundVectorBuilder which now handles query building from several query terms for both Search and CompareTerms.
    Minor changes to whitespace and comments.
    Configured jar and src (.tar.gz) build of project appropriately. Small change to Search usage string.
    Initial semantic vectors package
    Hash positive examples to speed up processing with exhaustive negatives
    Updated option to train exhaustively over negative examples such that
    positive inverse predicates are protected cases too.
    Updated label smoothing (div K)
    Added option for exhaustive negative sampling, activated if -negsamples
    > number represented concepts
    First attempt at negative sampling
    First attempt at label smoothing
    Ensure Hermetian mode is maintained throughout the binding operation,
    and also that this is the dominant mode for ESP.
    Fix to subsampling procedure, courtesy Hannah
    Reduce number of negsamples to negsamples (instead of negsamples +1)
    Keep track of Math.abs(loss)
    Added output of loss on each epoch to ESPperm and ESP, and logging of
    command line flags to ESPperm
    Added logging of command line variables (probably something we should
    consider implementing across the package at a later date)
     Reimplemented the mutable predicate vectors bit, starting with a "known
    to work" prototype.
    Work in progress - correcting errors/inefficiencies for mutable
    predicate vectors.
    Found a bug in the mutable predicate vectors code, which involved
    measuring error in relation to an already-updated vector.
    Replaced ArrayList with HashSet as .contains() in ArrayList is not a
    particularly efficient way to keep track of things (thanks to Will
    Kearns for this observation).
    Added support for mutable predicate vectors, and eliminated another
    "crash after building as the file exists already" instance.
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Retain pre-trained vectors if they are present.
    Added capacity for pre-training as an experimental feature.
    Changed the "PSI" references to "ESP".
    The ESP model, which provides a neural-probabilistic counterpart to PSI.
    The binary variable "semtypesAndCUIs" can be set to true if the
    predication index used was derived from SemMedDB using
    LuceneIndexFromSemrepTriples
    Added option to output to word2vec binary format.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Change to make file extensions automatic.
    
    That is, internal vector store names are just "termvectors", not "termvectors.bin", etc. The ".bin" or ".txt" suffix is applied automatically.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Fixes issue 36, I believe. Refactored VectorStoreWriter to switch properly between text and lucene format so that text vector stores can be created by BuildIndex as well.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Closed some open file handles.
    
    This problem looks innocuous because when main exits the files will be closed anyway, but this main method is also called from RunTests which leaves the files open even after exiting main.
    Just cleaned up some indentation since I was looking at this file.
    Renaming VectorStoreReader to VectorStoreReaderLucene.
    Some cleanup before QI2009, errors of mine that I found when reproducing demos.
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Package now throws exceptions always instead of hard coding System.exit
    Improved javadoc for translater. More importantly, refactored VectorSearcher to take VectorStore interface instead of VectorStoreReader, so that both Lucene index and plain text index VectorStoreReaders work for search interface.
    Forgot to commit the Translater itself.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    Copying across from branch to trunk.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Did some slight refactoring to make for more robust backwards compatibility with old indexes, with usage messages if things go wrong.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    Initial semantic vectors package
    Restored Lucene parsing of incoming query string (without this, each
    line is treated as a term without tokenization)
    Dominic doing some picky style normalization.
    First pass at an implementation of skipgram with negative sampling
    (Mikolov 2013). The main changes are as follows:
    
    (1) The negative sampling algorithm itself, with the scalar product and
    vector updates currently implemented within VectorUtils using
    netlib-java's blas routines (already a dependency), and sampling of
    out-of-context terms in accordance with their frequency^.75.
    
    (2) Subsampling of frequent terms, currently in need of reassessment and
    review, as although it does (vastly) speed up training, it does not
    improve performance as assorted papers suggest it should. Without
    subsampling results on pairwise correlation sets (e.g. wordsim353, MEN)
    and Mikolov's analogy set (n=~19,000) match reported results - in the
    region of 0.7 correlation and 0.6 accuracy for similarity and analogies
    respectively. But with subsampling as implemented currently performance
    deteriorates, more so as the threshold is reduced, which should not be
    the case according to the literature.
    
    (3) Multiple processing threads (with some initial movements in the
    direction of thread safety, though this doesn't yet extend to the
    document queue)
    Cleanup / indents and imports and such
    Recovered -boundproduct search functionality
    (a) (somewhat tangential) don't require provision of a Lucene index
    (b) don't use StandardAnalzyer to parse queries if "-matchcase" flag
    active (as this lowercases terms and tokenizes on parentheses)
    Avoid hardcoding Lucene version number, which appears unnecessary and also causes issues on some Maven installations, but not others (a phenomenon that remains unexplained)
    ....this edition compiles with Maven, attaining this involved removing some vestigial import statements.
    Added code such that Lucene parses the incoming queries with a StandardAnalyzer (for the sake of consistency)
    First attempt at batch search, i.e. hold vectors in RAM and work through a list of queries in a text file (one per line).
    Factoring out results-printing methods to simplify main flow of Search.java.
    Added interface to Lucene document search
    Numerous small modifications to accommodate more flexible command line PSI queries with QI in mind. One of these is definitely temporary - "-bindnotreleasehack", which will ultimately be replaced by an expanded query syntax with a different symbol for bind and release.
    
    Nonetheless, it seemed worth checking in the status quo.
    Attempt at rendering the generation of determinsitic vectors at Search time consistent with the "-elementalmethod" flag use elsewhere.
    
    So we'd now configure this using -queryvectorfile orthographic and and -elementalmethod,  for example:
    
    java -cp semanticvectors-5.5.jar pitt.search.semanticvectors.Search -queryvectorfile deterministic -vectortype binary -dimension 8000 -searchvectorfile elementalvectors.bin -elementalmethod contenthash prozac
    Two small changes:
    (1) it is now possible to do other searches aside from PSI searches again (apologies - broke the stack again).
    (2) for the proximity-based sliding window model, I've avoided attempting to restore elemental vectors to their natural state using "release" (by generating copies for binding instead), as this doesn't seem to work very well with real vectors. I'm not sure if this is a cumulative consequence of an approximate inverse or something more sinister, I'll poke around and see what I can find out.
    A first attempt at incorporating the S(X), E(X), P(predicate) notation into the search interface. It should now be possible to specify queries this way for boundproduct, boundproductsubspace and boundminimum, if -elementalvectorfile -semanticvectorfile -predicatevectorfile and -searchvectorfile are specified.
    Trivial warning whacking
    RunSearch -> runSearch for more normal Java style.
    Closing files in Search.java - should have been doing this all along. This makes several hitherto-failing tests pass on Windows.
    Removing SPARSESUM and updating a comment.
    ....probably best not to load semantic vectors, elemental vectors and predicate vectors all from the same file.
    A few experimental Search approaches have been added:
    (1) minimum score across sets of vectors with and without binding (to find middle terms)
    (2) a first attempt at producing .json files for predication searches
    
    I've been a little lazy about defining new flags here, specifically:
    - I've repurposed the "CompoundVectorBuilder" code that creates disjoint subspaces, such that it will not normalize the subspace if the searchtype is "BoundMinimum"
    - The "semanticvectorfile", "predicatevectorfile" and "elementalvectorfile" are used to tell the code building the graph where to find these vectors, so the graph construction is not tied to the search type
    - If this flags are not set to their default values and the .jsonfile flag is set, this triggers construction of a predication graph
    
    There are most likely clearer ways to trigger this behavior, but this seems adequate for the moment.
    Removing SPARSESUM as a search option.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Moved json generation to pathfinder class
    Moving PathFinder to viz package, and some breaking out of methods and catching exceptions in RunSearch that I should have caught earlier.
    Output search results as the graph resulting from Pathfinder network scaling of the pairwise connectivity matrix in .json format compatible with the D3 visualization library. The new command line parameters are:
    
    -jsonfile  (e.g. jsonfile graph.json - will output to this if nonempty)
    -pathfinderQ, pathfinderR (the Pathfinder q and r parameters)
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Removed unnecessary (ObjectVector) cast and cleaned up results formatting.
    Fixed bug I introduced into StringEdit (and in the process eliminated the "iEnd" prefix from the vectors returned), and added option for specification of orthographic vector store at query time (so OOV words can be used as a cue)
    Tiny correction to usage message (that should be updated more thoroughly).
    Turned searchtype into an enum, apparently successfully.
    
    Renamed LOG_ENTROPY to LOGENTROPY, realizing that this too will be passed normally as -termweight logentropy.
    Refactored -termweight values to be an enum, {NONE, IDF, LOGENTROPY}. On public call to LuceneUtils.getGlobalTermWeight, flag-based switching is now all inside LuceneUtils.
    
    This should make it much easier to make sure that termweighting enhancements are properly incorporated into LSA, BuildIndex, BuildPositionalIndex, etc., without having to maintain multiple codepaths.
    
    All tests pass, but this demonstrates the need for more reliable testing at different parts of the stack.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Changing constructor for LuceneUtils to use only FlagConfig.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Improved Javadoc formatting in Search.
    Changed negation term from NOT to ~NOT, and fixed tests accordingly. Minor refactoring and javadoc synchronizing.
    Corrected an error in the previous submission, wrong VectorSearcher.
    Command line "Search" interface added for VectorSearcher changes in previous commit
    This code aims to facilitate searching across dual-predicate paths, as per the "schizophrenia's prozac" paper. This is accomplished by expanding n predicate vectors by adding n-squared dual predicate paths (binary), or 2xn-squared dual predicate paths (complex) by combining the individual predicates using the "release" operator. I've also switched around the order in certain of the boundproduct complexqueryvector methods, as this is important in the complex vector version.
    Implemented bind and release for real and complex vectors.
    
    Working "correctly" for real case, not producing good results yet.
    
    I wonder how useful it is to have four total bind and release methods?
    Experimenting with a new AnalogySearcher, which doesn't work yet but is a useful test for bind and release (which doesn't work reliably yet except for with binary vectors!)
    
    I've cleaned up some not-so-useful verbiage in Search.java - it's much easier to read the big switch-list on searchtype now. This violates the "catch exceptions as tightly as possible" principle but it's worth it to get so much more code together.
    Added option for -boundproduct search to accept definitions of the form:
    
    S(united_states)*S(mexico)*E(dollar)
    
    It might be simpler to have this as the only option eventually.
    Added methods to CompoundVectorBuilder to facilitate:
    
    (1) parsing expressions that combine bundling and binding into a single superposed query vector
    (2) parsing expressions with multiple bound products into a subspace for search across multiple predicate paths
    
    (1) facilitates the "dollar of mexico" example when searching on the national facts data with the following parameters:
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -boundvectorfile elementalvectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    united_states*mexico united_states_dollar
    
    0.25148809523809523:mexican_peso
    0.0580357142857143:senegal
    
    (2) takes queries of the form concept predicate_path+predicate_path. The idea is to facilitate search from a cue across multiple paths simultaneously, using the quantum-inspired OR operator. An example with the national facts data might be:
    -searchtype boundproductsubspace
    -queryvectorfile semanticvectors.bin
    -boundvectorfile predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    united_kingdom HAS_NATIONAL_ANIMAL+HAS_CURRENCY
    
    0.504960298538208:british_pound
    0.4712301790714264:lion
    0.0625:ascension_pound
    0.0545634925365448:hanoi
    Fixed a bug in closing bound / product vector store.
    Added some javadoc and changed name "queryvectorfile2" to "boundvectorfile". (Not a great name, but ...)
    Some test fixes to come.
    Fixed bug induced by missing parentheses previously.
    Basic implementation of PSI added. Unit tests to follow. This will run on the nationalfacts.txt file in testdata - first produce a Lucene edition of this file using LuceneIndexFromTriples, then run PSI on it.
    
    Concepts will be encoded as lowercase terms, with spaces replaced by underscores. Predicates will be encoded as uppercase terms, with spaces replaced by underscores.
    
    To search the resulting files with a bound product, use the command line parameters:
    -searchtype boundproduct
    -queryvectorfile elementalvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile semanticvectors.bin
    -matchcase
    concept predicate
    e.g.
    lion HAS_NATIONAL_ANIMAL-INV
    
    0.7587579617834395:liberia
    0.755374203821656:united_kingdom
    0.7548765923566879:sweden
    0.7532842356687898:bulgaria
    0.7501990445859873:macedonia
    0.7482085987261147:belgium
    0.6879976114649682:netherlands
    0.6230095541401274:luxembourg
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Remove the import from java.lang as this package is imported by default.
    I think this fixes the worst of issue 21. Unit tests in place, though getting failures from the inside of threads seems to be a problem.
    Cleaned up some indentation.
    Fixed "-numsearchresults" argument which was being ignored by Search
    Cleaned up some javadoc a bit for Search.java.
    Removed some deprecated html javadocs. Updated version number to 1.21 (should have done this before).
    minor fix to IndexBilingualFiles.
    Retrofitted Clustering classes to use Flags.
    Minor synchronization of my client with Trevor's revision for balanced_permutation.
    Added "balanced_permutation" as a search type, and changed the usage() method to correspond to the new flags (-queryvectorfile, -searchvectorfile)
    Small addition of factory class VectorStoreReader.
    Renaming VectorStoreReader to VectorStoreReaderLucene.
    Some cleanup before QI2009, errors of mine that I found when reproducing demos.
    Fixed problem with setting Search.numResults, in response to issue 12.
    Fixed VectorStoreReader and Writer closing operations by using FSDirectory instead of MMapDirectory. This seems to be working, tests fixed accordingly.
    Small change in close() for VectorStoreReader, but with consequences in catching undeclared NullPointerExceptions, in main Search code as well as in tests. Trying to track down some read - write problems, see http://groups.google.com/group/semanticvectors/browse_thread/thread/48198c639c9b6d72/814e034b34f5af62#814e034b34f5af62 for details.
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Adding visualization utils. Added extra sophistication to build file to support this: now, utils will ship with the package but will not compile by default, to avoid extra dependencies for basic users.
    Housecleaning - usage for ClusterVectorStore, better exiting for PRINTQUERY.
    Added command line support for -lowercase flag.
    Added exceptions to disjunction vector searcher classes; removed exception clauses from search wrapping functions since ZeroVectorExceptions are all caught in Search.RunSearch now.
    Catches ZeroVectorExceptions which may by thrown in different constructors of VectorSearcher.
    If a ZeroVectorException occurs, the message is brought to STDERR and results are set to an empty LinkedList.
    
    Dominik Jednoralski - twelve02
    Exception catching for ZeroVectorExceptions (next time I should run 'ant clean; ant'.)
    Package now throws exceptions always instead of hard coding System.exit
    Some refactoring for permutations stuff: in particular, changed IndexType to Enumeration and made TermTermVectors use VectorStoreRAM and VectorStoreSparseRAM to make writing vectors simpler. Also wrote some checks for permutation searcher.
    The committed version should contain an implementation of the Sahlgren (2008) permutation-based encoding of word order. This includes:
    (1) a modification to BuildPositionalIndex such that it can take an argument indextype [basic (default), directional (the HAL model), or permutation (Sahlgren '08)].
    (2) modifications to TermTermVectorsFromLucene such that term vectors are permuted appropriately
    (3) additions to Search, CompoundVectorBuilder and VectorSearcher to facilitate index-based retrieval such that "president ?" retrieves terms occurring frequently after president
    (4) an additional class IndexTermVectorsFromRandomIndex that generates a vectors store on disk from a hashtable, used to preserve the random vectors as these are the basis for generating order-based queries
    Changed lowercasing logic to lowercase all arguments and give warnings if this changes arguments.
    Added support for -results command line flag; behavior consistent with similar usage in ClusterResults.
    Fixed tiny bug in command line searchtype option parsing.
    Updated javadoc for searchtypes and vector searchers.
    Added new command line configuration of incremental doc vectors in BuildIndex. Added copyright to IncrementalDocVectors.
    Changed one line to write docvectors with the correct variable file name.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    i. Checking in some slight edits to Positional indexing code form Trevor (just occasional bracketing conventions, javadoc, minor tweaks to usage messages); ii. Removed typo class BuildIPositionalndex; iii. Added new class for clustering entire vectors stores (this isn't optimized at all, run from text not lucene-based vector indexes, but the clustering doesn't scale for large vector stores anyway.
    Fixed bugs with command line parsing in Search.java and number of clusters in ClusterResults.java
    Fixed null results for upper cased words, issue 4.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    Refactored Search.java to make parsing arguments and searching much cleaner. Refactors VectorSearcher accordingly - VectorSearchers now have to look after building the appropriate queries from the query arguments.
    Minor refactor to make Search.java use (explicitly) the VectorStore interface, rather than insisting on the Lucene VectorStoreReader implementation.
    Improved javadoc for translater. More importantly, refactored VectorSearcher to take VectorStore interface instead of VectorStoreReader, so that both Lucene index and plain text index VectorStoreReaders work for search interface.
    Changed one .out.println to a .err.println, and updated the build to 1.4, hopefully about to ship.
    Continued updating documentation.
    Overhauled documentation (usages and javadoc) to include reference to NOT query syntax.
    Completed initial implementation of orthogonal negation, including an orthogonalize routine in VectorUtils and query parsing in CompoundVectorBuilder.
    Refactored to have search go through a VectorSearcher object that enables the use of different similarity scores while using common getNearestNeighbors implementation. Appears to be working fine, next step is to add some other similarity scores, e.g., using tensors.
    More tweaking of javadoc.
    Added new classes and html docs to source, and created better usage documentation for Search and CompareTerms.
    Created CompareTerms command line interface, and CompoundVectorBuilder which now handles query building from several query terms for both Search and CompareTerms.
    Minor changes to whitespace and comments.
    Configured jar and src (.tar.gz) build of project appropriately. Small change to Search usage string.
    Initial semantic vectors package
    Fix to subsampling procedure, courtesy Hannah
    Reduce number of negsamples to negsamples (instead of negsamples +1)
    Keep track of Math.abs(loss)
    Added output of loss on each epoch to ESPperm and ESP, and logging of
    command line flags to ESPperm
    Removed code to incorporate text from source sentences into model from
    main branch
    ESP meets EARP
     Reimplemented the mutable predicate vectors bit, starting with a "known
    to work" prototype.
    Work in progress - correcting errors/inefficiencies for mutable
    predicate vectors.
    Found a bug in the mutable predicate vectors code, which involved
    measuring error in relation to an already-updated vector.
    Replaced ArrayList with HashSet as .contains() in ArrayList is not a
    particularly efficient way to keep track of things (thanks to Will
    Kearns for this observation).
    Added support for mutable predicate vectors, and eliminated another
    "crash after building as the file exists already" instance.
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Retain pre-trained vectors if they are present.
    Added capacity for pre-training as an experimental feature.
    Changed the "PSI" references to "ESP".
    The ESP model, which provides a neural-probabilistic counterpart to PSI.
    The binary variable "semtypesAndCUIs" can be set to true if the
    predication index used was derived from SemMedDB using
    LuceneIndexFromSemrepTriples
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Clean up and commenting to make clustering of vector stores more reliable. The overlap measure should be properly generalized, this change simply disables it with a compile-time boolean.
    Refactored vector building interfaces in TermTermVectorsFromLucene, IncrementalTermVectors, and IncrementalDocVectors, to use flagConfigs much more. This simplifies interfaces considerably.
    
    However, I chickened out of completely removing args[i] parameters from Incremental*Vectors classes, since I don't use them regularly and don't have proper tests.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Some cleanup of clustering.
    Added struct for storing more expressive cluster results including centroids in output, and outputting vectors to file.
    
    Use at your own risk, this is not well factored or encapsulated code!
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Changes made to fix Issue 41 : http://code.google.com/p/semanticvectors/issues/detail?id=41
    Added a flag called -clustersize that will control total size of clusters to avoid cases when too many results are returned for all clusters and it is hard to visualize.
    This will help visualization as well as manually sorting through clustered data.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Closed some more abandoned VectorStoreReaders.
    Modified VectorStoreReaderText so that getting the vector enum creates a new filereader, so that (1) the new filereader can be closed when done enumerating the vectors, and (2) a call to getVector or another function that resets the filereader doesn't also reset the enumeration.
    Fix for issue 20.
    Retrofitted Clustering classes to use Flags.
    Minor synchronization of my client with Trevor's revision for balanced_permutation.
    Some cleanup before QI2009, errors of mine that I found when reproducing demos.
    Adding visualization utils. Added extra sophistication to build file to support this: now, utils will ship with the package but will not compile by default, to avoid extra dependencies for basic users.
    Housecleaning - usage for ClusterVectorStore, better exiting for PRINTQUERY.
    Just tidied up some javadoc before realease 1.10.
    i. Checking in some slight edits to Positional indexing code form Trevor (just occasional bracketing conventions, javadoc, minor tweaks to usage messages); ii. Removed typo class BuildIPositionalndex; iii. Added new class for clustering entire vectors stores (this isn't optimized at all, run from text not lucene-based vector indexes, but the clustering doesn't scale for large vector stores anyway.
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Restored the capacity to generate term vectors from random document
    vectors directly, which can now be invoked using "-docvectorsfile
    random" (or "-docvectorsfile henrietta", for that matter - provided
    these files don't exist).
    
    One thing worth taking note of here is that java.nio.NoSuchFileException
    is now what should be caught here, not java.io.FileNotFoundException -
    this may be an endemic issue on account of the Lucene 5 upgrade.
    Changes to enable redistributing coordinates to make their distributions approximately uniform.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    A couple of changes and bug fixes:
    (1) In IncrementalTermVectors, updated an if statement to (docVectorsInputStream.getFilePointer() < docVectorsInputStream.length() - 1) to comply with the current Lucene conventions. Without this, crashing occurs.
    (2) In IncrementalDocVectors, changed such that if a zero document vector is encountered this is nonetheless written to disk. Without this the document vector store may fall out of sequence with the Lucene Index, wreaking havoc on the next iteration.
    (3) Eliminated an else statement that may result in IncrementalTermVectors creating a random document vector if the incoming document vector store runs out. I'm not sure if this alternative was ever invoked, but it seems as though keeping the document vector store in synch with the Lucene index should make it unnecessary.
    Make IncrementalTermVectors work with random as well as pregenerated document vectors.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Found a conflict between PSI and the new elemental vector store implementation: PSI was calling getVector before checking whether a corresponding semantic vector existed, creating new elemental vectors for many terms.
    
    I've committed one possible fix - replacing the "getVector(x) == null" check with an explicit "containsVector(x) == false" check. This seems a safer practice, though there are a fair number of checks we would need to change scattered throughout the codebase still.
    Changed an inputStream name to be docVectorsInputStream.
    A couple of minor fixes to get incremental TRRI working again.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Improved description of IncrementalTermVectors.
    Refactored vector building interfaces in TermTermVectorsFromLucene, IncrementalTermVectors, and IncrementalDocVectors, to use flagConfigs much more. This simplifies interfaces considerably.
    
    However, I chickened out of completely removing args[i] parameters from Incremental*Vectors classes, since I don't use them regularly and don't have proper tests.
    Made LuceneUtils.TermFilter use flagConfig in all cases except PSI. This is good because callers will get new features (e.g., length filtering) automatically.
    
    PSI is different, it has to set some overrides for which the old more direct interfaces are necessary. So this is an improvement, but the story isn't over yet.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Ensure that the correct document file name is passed across training cycles when incremental indexing is used.
    Changing constructor for LuceneUtils to use only FlagConfig.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Introduced Flags.filternumbers parameter, that, when set to true, causes terms that represent numbers to be filtered out.
    * overloaded the termFilter method with an additional boolean to reflect this new parameter and updated all calls of the termFilter method except for in the PSI code, where also the maxnonalphabet parameter is not used
    * by default filternumbers is set to false so the default behavior of the SV package does not change
    Added "toUpperCase" to VectorType.getValueOf(Flags.vectortype) to facilitate flag recognition.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Gremlins at it again. For some reason once again not reading document ID's and hence producing zero vectors (i.e. bug from r468 recurs). Fixed now.
    Fixes issue 36, I believe. Refactored VectorStoreWriter to switch properly between text and lucene format so that text vector stores can be created by BuildIndex as well.
    Found the bug. Pernicious little thing. IncrementalTermVectors forgot to read in the String representing the document ID, and so was interpreting document identifiers as floats.
    I'm in pursuit of a malevolent bug which is causing sets of zero vectors to be recorded during TRRI with incremental indexing. These changes appear to fix it, but I haven't quite got to the root of why this works:
    
    (1) normalize the document vectors before adding them to term vectors by default
    - this is a reasonable thing to do, and if we keep them as is on disk we retain the possibility of incremental indexing
    
    However.... what fixed the bug was:
    (2) change the number of dimensions written to disk from the "dimension" parameter to the length of the vector from the vector store
    
    Mysteriously, I've yet to find evidence of a case where the vector-to-be-written and the dimension variable didn't match one another.  However, the bug is reproducible by changing back "i < tmpVector.length" to "i < dimension", and disappears upon reversing this change.
    
    More from me as new clues are uncovered, but I thought it would be better to have a working implementation checked in for now.
    (Recommit after eclipse mismatch.) Added dimension setting and checking to VectorStoreRAM.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Corrected error in normalization of vectors, moved writing of vectors to main method by implementing the VectorStore interface. Also constructor no longer depends on Flags.dimension. Still need to integrate call to this Class into BuildIndex such that incremental RRI is possible, will do in near future.
    Making build file work with env classpath. Fixed some javadoc and indentation.
    IncrementalDocVectors' opposite number: allows for generating term vectors from a set of document vectors without requiring the document vectors be retained in RAM (a few students have required this for their class projects). Planning to clean this up a little still, but it appears to work.
    Introducing experiments using learned character vectors. Some refactoring and testing of orthography code to support learned character vectors.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Found a conflict between PSI and the new elemental vector store implementation: PSI was calling getVector before checking whether a corresponding semantic vector existed, creating new elemental vectors for many terms.
    
    I've committed one possible fix - replacing the "getVector(x) == null" check with an explicit "containsVector(x) == false" check. This seems a safer practice, though there are a fair number of checks we would need to change scattered throughout the codebase still.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Rather than generating n number vectors for a term of length n, generate n+2 so E(alpha) and E(omega) are not used to generate bound products. This allows for some similarity between a term that starts with a letter, and another term that ends with that letter.
    End-correction changes that appear to have unbroken CompareTerms (at least). Decent looking results for real and binary vectors, NaN for complex.
    Cleaning up logging and exact calling pathways to improve output readability.
    Renaming "infer" package to "orthography". This should help readers connect with literature on "orthographic similarity", of which there seems to be plenty.
    Minor cleanup of VectorStoreOrthographical, no changes to interfaces.
    Fixed bug I introduced into StringEdit (and in the process eliminated the "iEnd" prefix from the vectors returned), and added option for specification of orthographic vector store at query time (so OOV words can be used as a cue)
    Force binary vectors if real vectors are selected (though dense complex would be fine too), and add the orthographic option for CompareTerms (-queryvectorfile orthographic)
    Added a variant of the sliding window model based on the edit distance idea (i.e. less constrained than permtermvectors.bin), and a new sort of vector store based on this idea also.
    Made LuceneUtils.TermFilter use flagConfig in all cases except PSI. This is good because callers will get new features (e.g., length filtering) automatically.
    
    PSI is different, it has to set some overrides for which the old more direct interfaces are necessary. So this is an improvement, but the story isn't over yet.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    New class VectorStoreDeterministic. Utilizing the Bobcat hash, it deterministically generates term vectors, eliminating the need to retain a set of elemental term vectors. A built-in cache is enabled by default but can be disabled and cleared. Independent of vector type and dimension.
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Remove old javadoc
    A (GIT repo test and) working first commit of Symmetric Random Indexing,
    a RI variant we developed awhile back that produces a
    reduced-dimensional approximation of the term-document matrix multiplied
    by its transpose.
    
    SRI was first described in:
    Cohen, Trevor, and Roger W. Schvaneveldt. "The trajectory of scientific
    discovery: concept co-occurrence and converging semantic distance." Stud
    Health Technol Inform 160 (2010): 661-665.
    
    Despite generating a reduced-dimensional approximation of the results of
    a matrix multiplication (which is conducted by calculating components of
    the scalar product between row vectors individually on a
    document-by-document basis), the implementation is straightforward, and
    amounts to adding elemental vectors for other terms in a document to the
    semantic vector for each term it contains.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Turned vectorstorefomat into enum.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Change to make file extensions automatic.
    
    That is, internal vector store names are just "termvectors", not "termvectors.bin", etc. The ".bin" or ".txt" suffix is applied automatically.
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Copying across from branch to trunk.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Did some slight refactoring to make for more robust backwards compatibility with old indexes, with usage messages if things go wrong.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    Initial semantic vectors package
    First attempt at label smoothing
    Added WhitespaceAnalyzer as an option at indexing time.
    Added support for mutable predicate vectors, and eliminated another
    "crash after building as the file exists already" instance.
    Initial commit of permutation-based embedding code (ahead of reviewing
    issues with permutation/directional based unit tests).
    
    There are some fairly significant structural changes here including:
    (1) using -embeddingmethod (instead of positionalmethod) to distinguish
    between RI and SGNS
    (2) proximity-based permutations
    (3) facilities for storing, generating and searching with random
    permutations
    Changed name of usetermweightsintermsearch for consistency.
    Two small changes:
    (1
    
    Signed-off-by: tcohen1 <tcohen1@129.106.137.189>
    Renamed a command line flag - "-negsamples" now denotes the number of
    negative samples.
    Dominic doing some picky style normalization.
    First pass at an implementation of skipgram with negative sampling
    (Mikolov 2013). The main changes are as follows:
    
    (1) The negative sampling algorithm itself, with the scalar product and
    vector updates currently implemented within VectorUtils using
    netlib-java's blas routines (already a dependency), and sampling of
    out-of-context terms in accordance with their frequency^.75.
    
    (2) Subsampling of frequent terms, currently in need of reassessment and
    review, as although it does (vastly) speed up training, it does not
    improve performance as assorted papers suggest it should. Without
    subsampling results on pairwise correlation sets (e.g. wordsim353, MEN)
    and Mikolov's analogy set (n=~19,000) match reported results - in the
    region of 0.7 correlation and 0.6 accuracy for similarity and analogies
    respectively. But with subsampling as implemented currently performance
    deteriorates, more so as the threshold is reduced, which should not be
    the case according to the literature.
    
    (3) Multiple processing threads (with some initial movements in the
    direction of thread safety, though this doesn't yet extend to the
    document queue)
    Support in PSI for training predicate vectors (as well as just using elemental predicate vectors).
    
    Still very unsure of what inverse predicates should look like in this model. (Which basically comes down to the question "should there be inverse vetors under binding / release, and if so what's the mutiplicative identity vector?"
    Restored the majority rule (as per the spatter code) as the default
    normalization method for binary vectors, while providing probabilistic
    normalization as a command line option (following the procedure used for
    RealBindMethod).
    
    Though I haven't examined in great detail, the majority rule is clearly
    more efficient, and the accuracy improvements I found with probabilistic
    normalization in IR experiments don't necessarily occur in PSI reasoning
    experiments. So it seems as though users should be able to choose a
    speed vs. (possible) accuracy tradeoff.
    Correction to termweight doc
    Better wiring for redistributing coordinates
    Changes to enable redistributing coordinates to make their distributions approximately uniform.
    Moving term vector writing to happen before docindexing starts so that term vectors always get written out.
    Improvements to memory copying for NarrativeRelationsIndexer. Seems to work for complex and binary now.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Reassembled the "startlistfile" code - the pieces were in there for the most part. However, using a small startlistfile within an iterative process (other than at the very end of one) is liable to lead to inefficiency downstream with -incrementaldocvectors, as currently we depend upon there being perfect alignment between the generate docvectors.bin and the documents in the Lucene index - i.e. we will end up with many zero vectors on disk.
    Bits ot cleanup to make sure that term weights are working
    Updates to flags
    Utility method added to allow other classes to change the ExpandSearchSpace option in a FlagConfig without reconfiguring it entirely
    Changes to make experimental results on tabular data more reproducible  / configurable.
    Added a command line flag, -mintermlength, that allows for the specification of a minimum term length so as to exclude single characters and such if need be.
    Numerous small modifications to accommodate more flexible command line PSI queries with QI in mind. One of these is definitely temporary - "-bindnotreleasehack", which will ultimately be replaced by an expanded query syntax with a different symbol for bind and release.
    
    Nonetheless, it seemed worth checking in the status quo.
    Removed defunct "-deterministicvectors" flag, and added a check for nonexistent term:field combinations when constructing a space from indexes with multiple fields.
    Javadoc improvements
    Javadoc improvements
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Removing deterministic flag.
    Renaming hybridvectors flag to hybrid orthographic vectors.
    Making seedlength field private and making accessors use the getter.
    Suppress unnecessary logging, some minor changes.
    Output search results as the graph resulting from Pathfinder network scaling of the pairwise connectivity matrix in .json format compatible with the D3 visualization library. The new command line parameters are:
    
    -jsonfile  (e.g. jsonfile graph.json - will output to this if nonempty)
    -pathfinderQ, pathfinderR (the Pathfinder q and r parameters)
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Clean up and commenting to make clustering of vector stores more reliable. The overlap measure should be properly generalized, this change simply disables it with a compile-time boolean.
    Added / changed some error messages while using PSI.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Added "hybridvectors" command line flag, for those inclined toward conceptual/orthographic blends.
    Found that ClusterResults was ignoring flag arguments, which led to a bug-fix and fixing of lots of test configs into the bargain.
    
    2 complex positional index tests are still failing run under ant, but equivalent tests pass manually I believe .. TODO fix these.
    Cleaning up logging and exact calling pathways to improve output readability.
    Added a variant of the sliding window model based on the edit distance idea (i.e. less constrained than permtermvectors.bin), and a new sort of vector store based on this idea also.
    Removed old enums-as-strings parsing code.
    Turned -positionalmethod into an enum. This concludes the effort of turning flag values into enums.
    Turned vectorstorefomat into enum.
    Simple improvement to log possible enum flag values to the console before throwing an exception.
    Turned -docindexing into an enum.
    Cleaned up CompareTermsBatch and removed -vectorstorelocation flag, since this is the only place it's used and I can't imagine that users want to deliberately use vectors from disk every time for a large batch. (If the vector store is too big to fit in main memory, you're not going to be a happy camper anyway.)
    
    Would be nice to write a procedural test for CompareTerms and CompareTermsBatch ...
    Turn -vectorlookupsyntax into an enum. Never has been really tested.
    Turned searchtype into an enum, apparently successfully.
    
    Renamed LOG_ENTROPY to LOGENTROPY, realizing that this too will be passed normally as -termweight logentropy.
    Refactored -termweight values to be an enum, {NONE, IDF, LOGENTROPY}. On public call to LuceneUtils.getGlobalTermWeight, flag-based switching is now all inside LuceneUtils.
    
    This should make it much easier to make sure that termweighting enhancements are properly incorporated into LSA, BuildIndex, BuildPositionalIndex, etc., without having to maintain multiple codepaths.
    
    All tests pass, but this demonstrates the need for more reliable testing at different parts of the stack.
    Made LuceneUtils.TermFilter use flagConfig in all cases except PSI. This is good because callers will get new features (e.g., length filtering) automatically.
    
    PSI is different, it has to set some overrides for which the old more direct interfaces are necessary. So this is an improvement, but the story isn't over yet.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added -deterministicvectors Flag. Seems to be working, though further testing is required. Will create a unit test in the near future.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Introduced Flags.filternumbers parameter, that, when set to true, causes terms that represent numbers to be filtered out.
    * overloaded the termFilter method with an additional boolean to reflect this new parameter and updated all calls of the termFilter method except for in the PSI code, where also the maxnonalphabet parameter is not used
    * by default filternumbers is set to false so the default behavior of the SV package does not change
    Liberated elementalvectorfile, boundvectorfile and predicatevectorfile. These were previously designated as "Final".
    I've submitted an experimental approach to limiting the self-inverse property of XOR with binary vector binding, which can be activated at search/build time with the -binarybindingwithpermute flag. When binding, the implementation first XOR's and then shift-permutes the result +1. Upon release, the implementation first shift-permutes -1, and then XOR's.
    This code aims to facilitate searching across dual-predicate paths, as per the "schizophrenia's prozac" paper. This is accomplished by expanding n predicate vectors by adding n-squared dual predicate paths (binary), or 2xn-squared dual predicate paths (complex) by combining the individual predicates using the "release" operator. I've also switched around the order in certain of the boundproduct complexqueryvector methods, as this is important in the complex vector version.
    Experimenting with a new AnalogySearcher, which doesn't work yet but is a useful test for bind and release (which doesn't work reliably yet except for with binary vectors!)
    
    I've cleaned up some not-so-useful verbiage in Search.java - it's much easier to read the big switch-list on searchtype now. This violates the "catch exceptions as tightly as possible" principle but it's worth it to get so much more code together.
    A few minor changes:
    (1) added -indexRootDirectory flag, to allow for the specification of a directory in which to place the resulting lucene index. This requires a trailing slash at the end currently.
    (2) added the "incremental_" to a CreateIncrementalDocVectors call in BuildIndex, for the sake of consistency
    Change to make file extensions automatic.
    
    That is, internal vector store names are just "termvectors", not "termvectors.bin", etc. The ".bin" or ".txt" suffix is applied automatically.
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Added methods to CompoundVectorBuilder to facilitate:
    
    (1) parsing expressions that combine bundling and binding into a single superposed query vector
    (2) parsing expressions with multiple bound products into a subspace for search across multiple predicate paths
    
    (1) facilitates the "dollar of mexico" example when searching on the national facts data with the following parameters:
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -boundvectorfile elementalvectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    united_states*mexico united_states_dollar
    
    0.25148809523809523:mexican_peso
    0.0580357142857143:senegal
    
    (2) takes queries of the form concept predicate_path+predicate_path. The idea is to facilitate search from a cue across multiple paths simultaneously, using the quantum-inspired OR operator. An example with the national facts data might be:
    -searchtype boundproductsubspace
    -queryvectorfile semanticvectors.bin
    -boundvectorfile predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    united_kingdom HAS_NATIONAL_ANIMAL+HAS_CURRENCY
    
    0.504960298538208:british_pound
    0.4712301790714264:lion
    0.0625:ascension_pound
    0.0545634925365448:hanoi
    Fixed a bug in closing bound / product vector store.
    Added some javadoc and changed name "queryvectorfile2" to "boundvectorfile". (Not a great name, but ...)
    Some test fixes to come.
    Basic implementation of PSI added. Unit tests to follow. This will run on the nationalfacts.txt file in testdata - first produce a Lucene edition of this file using LuceneIndexFromTriples, then run PSI on it.
    
    Concepts will be encoded as lowercase terms, with spaces replaced by underscores. Predicates will be encoded as uppercase terms, with spaces replaced by underscores.
    
    To search the resulting files with a bound product, use the command line parameters:
    -searchtype boundproduct
    -queryvectorfile elementalvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile semanticvectors.bin
    -matchcase
    concept predicate
    e.g.
    lion HAS_NATIONAL_ANIMAL-INV
    
    0.7587579617834395:liberia
    0.755374203821656:united_kingdom
    0.7548765923566879:sweden
    0.7532842356687898:bulgaria
    0.7501990445859873:macedonia
    0.7482085987261147:belgium
    0.6879976114649682:netherlands
    0.6230095541401274:luxembourg
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Included a Flags.binaryvectordecimalplaces parameter for those in need of more precise binary vectors
    idf weighting added to DocVectors and IncrementalDocVectors. Seems a good idea to add this to sliding window models too, will look into this. Didn't seem worth factoring this out into LuceneUtils as it is a one-liner, but happy to do this if it is preferred.
    Added the option to weight document vectors (-fieldweight) when constructed, such that terms from shorter fields count more  - the weighting is 1/sqrt(number of terms in field).
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Restoring stdev and searchresultsminscore functionality that was accidentally deleted in the merge.
    Mainly work on complex vectors to catch convolution up with angular representation with 0. Also realized that 2^14 should be used as phase resolution.
    
    Minor changes to string array logging.
    Hacked away at our logging verbosity. I believe (I think) that normal operation is much easier to interpret using VerbatimLogger, and that we should use other loggers for debugging. I'd like to write / find a one-line logging message which is basically verbatim with a file and line number prefixed.
    Fixing the build in trunk. Looks good for now.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Cosmetic changes: changed Flags.standard_deviations to Flags.stdev (it seemed a common enough acronym, and standarddeviations was hard to tolerate), also switch from | to || where indicated earlier.
    Implemented "standard_deviations" flag for searching, which uses a simple and approximate one-pass algorithm to estimate the standard deviations above the mean association strength across all search results for the top results returned (in response to the "thresholding" thread awhile back). This can be used in conjunction with the new "searchresultsminscore" flag.
    Changes made to fix Issue 41 : http://code.google.com/p/semanticvectors/issues/detail?id=41
    Added a flag called -clustersize that will control total size of clusters to avoid cases when too many results are returned for all clusters and it is hard to visualize.
    This will help visualization as well as manually sorting through clustered data.
    Added support for searchresultsminscore flag. (And hence double flags in general.)
    Adding new vectorsearchfilterregex flag and supporting functionality.
    Added stemming as an option to IndexFilePositions, using Lucene's PorterStemFilter.
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Stopped lowercasing flag values, this is bad for pathnames.
    Two minor changes:
    (1) In Flags, changed the default term weighting option from "logentropy" to none
    (2) In LSA, added normalization of the document-by-term vectors prior to SVD, to conform to LSA traditions.
    (1) Included exception handling such that rather than crashing, TermTermVectorsFromLucene now outputs the name of any document it could not handle.
    
    (2) Added the ability to generate combined 'content and order' based vectors, such that the permutation-based model and general sliding window model are combined.
    This change is something of a follow-up to the refactoring of the stoplist - it seems sensible to have the TermFilter in LuceneUtils use the "Flags" parameters, so I've refactored it this way. I've also added a -maxfrequency flag to complement -minfrequency, as Magnus has documented performance improvements with an upper limit on term frequency.
    Moved the stoplist machinery out of IndexFilePositions and into LuceneUtils. There's a new Flag -stoplistfile which when used will cause any instantiated LuceneUtils object to load the file that follows, and return false for any term on the stoplist. Consequently the stoplist applies to BuildIndex, BuildPositionalIndex and LSA without actually requiring any change to these classes.  This was really easy to do on account of the package-wide Flags class.
    working on tracking down threading issue.
    Set sensible defaults for Flags numsearchresults and numclusters.
    Added flag configuration for lucene contents fields and docid field. this should be released soon.
    Added initialization for Flags.termweight; seems to fix a bug in DocVectors constructer. Illustrates the need for Flags checking at program initialization, and possibly more modular constructor / initializer functions (the DocVectors constructor is quite long). On the bright side, the regression tests helped us to diagnose this problem early on.
    Implemented TRRI as per the RRI paper. This required adding another constructor to TermVectorsFromLucene that generates elemental term vectors for future recycling. Also entropy weighting has been added to LuceneUtils, to facilitate log-entropy weighting in DocVectors and IncrementalDocVectors. TRRI can now be performed by calling BuildIndex with the command line flags:
    -initialtermvectors random
    -trainingcycles 2
    
    This implementation doesn't scale as well as the one used in the RRI paper, as this one had an opposite number to IncrementalDocVectors named IncrementalTermVectors - combining these two allows one to reflect without ever keeping the document vectors in RAM, which is critical for large data sets on small machines. Hope to add this in the near future.
    added flag to suppress query negation and check for it when building the query vector
    Added CompareTermsBatch and related modifications to Flags, and the convention that normalizing the zero vector returns zero. Thanks to Andrew Mackinlay.
    Removed some unnecessary logging.
    Retrofitted Clustering classes to use Flags.
    Minor synchronization of my client with Trevor's revision for balanced_permutation.
    Added flag for "balanced_permutation" as a search type
    Small addition of factory class VectorStoreReader.
    Trying to refactor ubild paths for cleaner / easier distinction between src and src extensions.
    Flags refactor and regression testing of BuildPositionalIndex.
    Continued refactoring, wired command line flags into BuildIndex, made some changes to tests to remove Java internal deletion code that isn't reliable.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Refactoring test runner to make regression test framework. Committing some static test data files.
    Moving forward with flag descriptions and value enumerations.
    Updating Flags.java (to add new flags) and associated documentation files.
    Flags library now uses reflection, using ClusterResults as test case.
    Initial commit of new flags library.
    Table experiments changes (remaining changes from completing vector reasoning paper draft)
    Factoring out results-printing methods to simplify main flow of Search.java.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    More minor cleanup.
    Dramatically speeding up VectorSearcher.getNearestNeighbors() for large number of results.
    Added sorting to the GetAllAboveThreshold method, such that results are returned in order. Also some minor alterations to CompoundVectorBuilder for the purpose of bound product searches.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Initial semantic vectors package
    Initial commit of permutation-based embedding code (ahead of reviewing
    issues with permutation/directional based unit tests).
    
    There are some fairly significant structural changes here including:
    (1) using -embeddingmethod (instead of positionalmethod) to distinguish
    between RI and SGNS
    (2) proximity-based permutations
    (3) facilities for storing, generating and searching with random
    permutations
    Changed the flow of document vector generation for word embeddings, such
    with the flag "-docindexing inmemory", document vectors will be
    generated at the same time as word vectors, as per the "distributed bag
    of words" approach of Le and Mikolov.
    Slight change in the document indexing strategy sequence for word
    embeddings, to allow for generation of document vectors and word vectors
    simultaneously using the "continuous bag of words" approach described by
    Mikolov. This will occur when using -docindexing inmemory. With
    -docindexing incremental, the usual vector superposition approach occurs
    (for the moment - it would be worth developing a neural-probabilistic
    alternative here, as retaining document vectors in memory limits
    scalability).
    Added randomization of the document queue (in chunks to reduce seek
    time), and moved normalization of the term vectors to
    BuildPositionalIndex (to facilitate document vector representation down
    the line).
    First pass at an implementation of skipgram with negative sampling
    (Mikolov 2013). The main changes are as follows:
    
    (1) The negative sampling algorithm itself, with the scalar product and
    vector updates currently implemented within VectorUtils using
    netlib-java's blas routines (already a dependency), and sampling of
    out-of-context terms in accordance with their frequency^.75.
    
    (2) Subsampling of frequent terms, currently in need of reassessment and
    review, as although it does (vastly) speed up training, it does not
    improve performance as assorted papers suggest it should. Without
    subsampling results on pairwise correlation sets (e.g. wordsim353, MEN)
    and Mikolov's analogy set (n=~19,000) match reported results - in the
    region of 0.7 correlation and 0.6 accuracy for similarity and analogies
    respectively. But with subsampling as implemented currently performance
    deteriorates, more so as the threshold is reduced, which should not be
    the case according to the literature.
    
    (3) Multiple processing threads (with some initial movements in the
    direction of thread safety, though this doesn't yet extend to the
    document queue)
    BuildPositionalIndex should write out vectors after, not before repeating trainingcycles!
    Change trainingcyles implementation to respect user-provided termvectorsfile and docvectorsfile.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Added a variant of the sliding window model based on the edit distance idea (i.e. less constrained than permtermvectors.bin), and a new sort of vector store based on this idea also.
    Turned -positionalmethod into an enum. This concludes the effort of turning flag values into enums.
    Turned -docindexing into an enum.
    Refactored vector building interfaces in TermTermVectorsFromLucene, IncrementalTermVectors, and IncrementalDocVectors, to use flagConfigs much more. This simplifies interfaces considerably.
    
    However, I chickened out of completely removing args[i] parameters from Incremental*Vectors classes, since I don't use them regularly and don't have proper tests.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Fixing a typo in usage and comments.
    Introduced Flags.filternumbers parameter, that, when set to true, causes terms that represent numbers to be filtered out.
    * overloaded the termFilter method with an additional boolean to reflect this new parameter and updated all calls of the termFilter method except for in the PSI code, where also the maxnonalphabet parameter is not used
    * by default filternumbers is set to false so the default behavior of the SV package does not change
    Parametrised all hard-coded pathnames with flags, with the exception of the "termvectors_n.bin" type files that are programmatically created in training cycles.
    
    Merged elemental and random vectors names.
    
    Removed incremental_docvectors as a name and changes back to docvectors - I think this is an implementation detail. Same with svd_term and doc vectors.
    Mainly work on complex vectors to catch convolution up with angular representation with 0. Also realized that 2^14 should be used as phase resolution.
    
    Minor changes to string array logging.
    Hacked away at our logging verbosity. I believe (I think) that normal operation is much easier to interpret using VerbatimLogger, and that we should use other loggers for debugging. I'd like to write / find a one-line logging message which is basically verbatim with a file and line number prefixed.
    Need to remove the Sys.exit call from BuildPositionalIndex so that remaining tests actually run.
    A few small changes, all tests seem to run successfully now. Firstly, increased the dimensionality of the binary vectors in the tests to even the playing field some, now meets the assertTrue(peterRank < 5) tests. Secondly, and more mysteriously,  BuildPositionalIndex hangs at the end of the Main method unless explicitly killed. I've included a "System.exit(0)" at the end, which allows all the tests to complete, but is hardly an elegant solution. I'd be interested to hear if this fixes things elsewhere, and will attempt to seek out the arcane Java mechanism or trivial oversight responsible for the persistence of BuildPositionalIndex.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    Fixes issue 36, I believe. Refactored VectorStoreWriter to switch properly between text and lucene format so that text vector stores can be created by BuildIndex as well.
    (Recommit after eclipse mismatch.) Added dimension setting and checking to VectorStoreRAM.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Use the new, non-deprecated constructor which do not silently depend on parameters from Flags.
    Added "-docindexing none" option to BuildPositionalIndex
    (1) Included exception handling such that rather than crashing, TermTermVectorsFromLucene now outputs the name of any document it could not handle.
    
    (2) Added the ability to generate combined 'content and order' based vectors, such that the permutation-based model and general sliding window model are combined.
    This change is something of a follow-up to the refactoring of the stoplist - it seems sensible to have the TermFilter in LuceneUtils use the "Flags" parameters, so I've refactored it this way. I've also added a -maxfrequency flag to complement -minfrequency, as Magnus has documented performance improvements with an upper limit on term frequency.
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    Fix for flag documentation error http://code.google.com/p/semanticvectors/issues/detail?id=17
    Removed some unnecessary logging.
    Refactoring and testing of permutation indexes in TermTermVectorsFromLucene.
    Flags refactor and regression testing of BuildPositionalIndex.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Flags library now uses reflection, using ClusterResults as test case.
    Some cleanup before QI2009, errors of mine that I found when reproducing demos.
    I seem to remember fixing the error referred to on the mailing list some time back. In any event, I can't reproduce it in this edition, and it is now possible to generate document vectors from all three types of positional index. The value of these vectors has yet to be established.
    Added cyclical retraining, and the ability to start with a set of trained term vectors (command line option -pt) to allow for combining vector stores in interesting ways.
    Minor style tweaks to Vidya's useful refactoring of TermFilter.
    Bug fix courtesy of Vidya Vasuki - terms in fields outside FieldsToIndex were also being included in vector stores. To fix this, an additional check has been added to TermFilter. Also, TermFilter has been refactored and is now part of LuceneUtils.
    
    One new command line option "-n" to allow a number of non-alphabet characters (useful for biomedical text) has also been included.
    Housecleaning - usage for ClusterVectorStore, better exiting for PRINTQUERY.
    2 small changes: linked by "else" statements different indextype options in command line parsing, previous version accepted "permutation only". Also, "directional" indexes now output "drxntermvectors.bin", for consistency with convention of naming termvectors according to index type.
    Updated version number, set one Javadoc property, change BuildPositionalIndex to only create docvectors in BASIC mode.
    Some refactoring for permutations stuff: in particular, changed IndexType to Enumeration and made TermTermVectors use VectorStoreRAM and VectorStoreSparseRAM to make writing vectors simpler. Also wrote some checks for permutation searcher.
    The committed version should contain an implementation of the Sahlgren (2008) permutation-based encoding of word order. This includes:
    (1) a modification to BuildPositionalIndex such that it can take an argument indextype [basic (default), directional (the HAL model), or permutation (Sahlgren '08)].
    (2) modifications to TermTermVectorsFromLucene such that term vectors are permuted appropriately
    (3) additions to Search, CompoundVectorBuilder and VectorSearcher to facilitate index-based retrieval such that "president ?" retrieves terms occurring frequently after president
    (4) an additional class IndexTermVectorsFromRandomIndex that generates a vectors store on disk from a hashtable, used to preserve the random vectors as these are the basis for generating order-based queries
    Small changes: i. Changed IncrementalDocVectors and calling code to put VectorFile argument last (since it's a write argument); ii. Much Javadoc for Search.SearchType options; iii. Changed ClusterResults to do random rather than round robin assignment (request from Pitt).
    This Class now builds docvectors (incrementally).
    Added new command line configuration of incremental doc vectors in BuildIndex. Added copyright to IncrementalDocVectors.
    Just tidied up some javadoc before realease 1.10.
    i. Checking in some slight edits to Positional indexing code form Trevor (just occasional bracketing conventions, javadoc, minor tweaks to usage messages); ii. Removed typo class BuildIPositionalndex; iii. Added new class for clustering entire vectors stores (this isn't optimized at all, run from text not lucene-based vector indexes, but the clustering doesn't scale for large vector stores anyway.
    Updated version number in build file and fixed some minor typos.
    Repaired name of class "BuildPositionalIndex"
    Repaired the name of the class "BuildPositionalIndex"
    This version of Semantic Vectors 1.7 (dev) contains four additional classes which implement a within-document sliding context window to generate term-term co-occurrence vectors. The classes are:
    (1) Classes that build a Lucene index that includes per-document term frequency and position vectors
    pitt.search.lucene.FilePositionDoc.java
    pitt.search.lucene.IndexFilePositions.java
    (2) Classes that derive from this Lucene index a set of term vectors based on term-term cooccurence within a sliding window
    pitt.search.semanticvectors.BuildPositionalIndex.java
    pitt.search.semanticvectors.TermTermVectorsFromLucene.java
    Submitting much cleaned up version of Bilingual code, including slightly improved DocVector builder. Hopefully nearly ready to ship version 1.6.
    In the middle of work on trying to adapt semanticvectors package to index bilingual corpora. All previous monolingual functions still work; bilingual functions don't seem to work yet.
    Minor tinkering with command line messages.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    More tweaking of javadoc.
    Cleaned up usage on BuildIndex and set mime types on new html docs.
    Initial semantic vectors package
    Altered getAdditiveQueryVector such that it doesn't try to calculate weights for terms without vectors, and catches errors if attempts are made to calculate weights
    Bits ot cleanup to make sure that term weights are working
    Changed -searchtype boundminimum, such that it finds the best pairwise minimum across a sequence of vector pairs
    Added -searchtype intersection, currently experimental and works with binary vectors only
    Added term weighting for PSI query vector generation - but only for the single query string edition (e.g. P(X)+E(Y)*S(Z))
    Numerous small modifications to accommodate more flexible command line PSI queries with QI in mind. One of these is definitely temporary - "-bindnotreleasehack", which will ultimately be replaced by an expanded query syntax with a different symbol for bind and release.
    
    Nonetheless, it seemed worth checking in the status quo.
    Exception handling, such that one missing concept in the search expression does not derail the whole procedure
    A first attempt at incorporating the S(X), E(X), P(predicate) notation into the search interface. It should now be possible to specify queries this way for boundproduct, boundproductsubspace and boundminimum, if -elementalvectorfile -semanticvectorfile -predicatevectorfile and -searchvectorfile are specified.
    A few experimental Search approaches have been added:
    (1) minimum score across sets of vectors with and without binding (to find middle terms)
    (2) a first attempt at producing .json files for predication searches
    
    I've been a little lazy about defining new flags here, specifically:
    - I've repurposed the "CompoundVectorBuilder" code that creates disjoint subspaces, such that it will not normalize the subspace if the searchtype is "BoundMinimum"
    - The "semanticvectorfile", "predicatevectorfile" and "elementalvectorfile" are used to tell the code building the graph where to find these vectors, so the graph construction is not tied to the search type
    - If this flags are not set to their default values and the .jsonfile flag is set, this triggers construction of a predication graph
    
    There are most likely clearer ways to trigger this behavior, but this seems adequate for the moment.
    Little bits of work from the plane a few days ago.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Removing public status of ComplexVector.setDominantMode, and catching all setters. Callers should now use VectorType COMPLEXFLAT for all of this, and set it once in flag config.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    Turn -vectorlookupsyntax into an enum. Never has been really tested.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Changed negation term from NOT to ~NOT, and fixed tests accordingly. Minor refactoring and javadoc synchronizing.
    I've submitted an experimental approach to limiting the self-inverse property of XOR with binary vector binding, which can be activated at search/build time with the -binarybindingwithpermute flag. When binding, the implementation first XOR's and then shift-permutes the result +1. Upon release, the implementation first shift-permutes -1, and then XOR's.
    Added sorting to the GetAllAboveThreshold method, such that results are returned in order. Also some minor alterations to CompoundVectorBuilder for the purpose of bound product searches.
    Allow for the specification of a subspace using the E(C1)*E(C2)+S(C3)*E(C4) notation. In this case, "+" separates the components of the subspace.
    This code aims to facilitate searching across dual-predicate paths, as per the "schizophrenia's prozac" paper. This is accomplished by expanding n predicate vectors by adding n-squared dual predicate paths (binary), or 2xn-squared dual predicate paths (complex) by combining the individual predicates using the "release" operator. I've also switched around the order in certain of the boundproduct complexqueryvector methods, as this is important in the complex vector version.
    Added option for -boundproduct search to accept definitions of the form:
    
    S(united_states)*S(mexico)*E(dollar)
    
    It might be simpler to have this as the only option eventually.
    Cleanup revision. Have removed stray javadocs instead of fixing them - this is interim but good enough for now.
    Added methods to CompoundVectorBuilder to facilitate:
    
    (1) parsing expressions that combine bundling and binding into a single superposed query vector
    (2) parsing expressions with multiple bound products into a subspace for search across multiple predicate paths
    
    (1) facilitates the "dollar of mexico" example when searching on the national facts data with the following parameters:
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -boundvectorfile elementalvectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    united_states*mexico united_states_dollar
    
    0.25148809523809523:mexican_peso
    0.0580357142857143:senegal
    
    (2) takes queries of the form concept predicate_path+predicate_path. The idea is to facilitate search from a cue across multiple paths simultaneously, using the quantum-inspired OR operator. An example with the national facts data might be:
    -searchtype boundproductsubspace
    -queryvectorfile semanticvectors.bin
    -boundvectorfile predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    united_kingdom HAS_NATIONAL_ANIMAL+HAS_CURRENCY
    
    0.504960298538208:british_pound
    0.4712301790714264:lion
    0.0625:ascension_pound
    0.0545634925365448:hanoi
    Removed redundant OrthogonalizeVectors
    This contains an attempt at a binary equivalent of the Gram-Schmidt orthogonalization process for pseudo-subspace search and perhaps also negation. Said attempt is found in BinaryVectorUtils, which is called from VectorSearcher/CompoundVectorBuilder instead of the usual orthogonalizevectors method when binary vectors are used. There's probably a more elegant way to integrate these sorts of special cases, but I thought it would be worth including to attempt to bring the different vector representations closer to functional equivalence.
    Some changes trying to get binary permutation search to work.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Setting a min term weight to be 1 rather than 0.
    Adding new vectorsearchfilterregex flag and supporting functionality.
    CompoundVectorBuilder now gets dimension from its VectorStore.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    Use logger instead of direct output to System.err or
    System.out.
    Changes to get rid of hard-coded 'contents' fields.
    Workaround fix for issue 19 that throws NPE in getGlobalTermWeight.
    Added flag configuration for lucene contents fields and docid field. this should be released soon.
    Fixed a lowercasing-misses-negation bug, and added unit test for negation including new flag.
    added flag to suppress query negation and check for it when building the query vector
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Updating Flags.java (to add new flags) and associated documentation files.
    Added a test for CompoundVectorBuilder and VectorStoreRAM, and utils for outputting principal component plots as tex source.
    Cleaned up regex comparison in CompareTerms - working well now (have discovered the synoptic gospels).
    Incremental commit - trying to get regular expression search working to build summary vectors from lots of documents at once.
    Removed extra console prints from CompoundVectorBuilder.
    Some refactoring for permutations stuff: in particular, changed IndexType to Enumeration and made TermTermVectors use VectorStoreRAM and VectorStoreSparseRAM to make writing vectors simpler. Also wrote some checks for permutation searcher.
    The committed version should contain an implementation of the Sahlgren (2008) permutation-based encoding of word order. This includes:
    (1) a modification to BuildPositionalIndex such that it can take an argument indextype [basic (default), directional (the HAL model), or permutation (Sahlgren '08)].
    (2) modifications to TermTermVectorsFromLucene such that term vectors are permuted appropriately
    (3) additions to Search, CompoundVectorBuilder and VectorSearcher to facilitate index-based retrieval such that "president ?" retrieves terms occurring frequently after president
    (4) an additional class IndexTermVectorsFromRandomIndex that generates a vectors store on disk from a hashtable, used to preserve the random vectors as these are the basis for generating order-based queries
    Added VectorStoreReaderRAMCache and small test of this.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    Refactored Search.java to make parsing arguments and searching much cleaner. Refactors VectorSearcher accordingly - VectorSearchers now have to look after building the appropriate queries from the query arguments.
    Changed one .out.println to a .err.println, and updated the build to 1.4, hopefully about to ship.
    Overhauled documentation (usages and javadoc) to include reference to NOT query syntax.
    Completed initial implementation of orthogonal negation, including an orthogonalize routine in VectorUtils and query parsing in CompoundVectorBuilder.
    Fixed small error in CompoundVectorBuilder.java, which was failing to multiply by Lucene term weight.
    Added command line SearchTensorRelation interface. Results don't look too promising: where things work well, it's because tensor similarities just reduce to products of scalar products of the factors  (I think).
    
    Also adding product functions to VectorUtils and appropriate html javadoc files.
    Added new classes and html docs to source, and created better usage documentation for Search and CompareTerms.
    Replaced the Hashtable with a ConcurrentHashMap.
    Attempt at rendering the generation of determinsitic vectors at Search time consistent with the "-elementalmethod" flag use elsewhere.
    
    So we'd now configure this using -queryvectorfile orthographic and and -elementalmethod,  for example:
    
    java -cp semanticvectors-5.5.jar pitt.search.semanticvectors.Search -queryvectorfile deterministic -vectortype binary -dimension 8000 -searchvectorfile elementalvectors.bin -elementalmethod contenthash prozac
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Found a conflict between PSI and the new elemental vector store implementation: PSI was calling getVector before checking whether a corresponding semantic vector existed, creating new elemental vectors for many terms.
    
    I've committed one possible fix - replacing the "getVector(x) == null" check with an explicit "containsVector(x) == false" check. This seems a safer practice, though there are a fair number of checks we would need to change scattered throughout the codebase still.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    I think this test change is necessary for probabilistic normalization not to be seen as an error.
    Made LuceneUtils.TermFilter use flagConfig in all cases except PSI. This is good because callers will get new features (e.g., length filtering) automatically.
    
    PSI is different, it has to set some overrides for which the old more direct interfaces are necessary. So this is an improvement, but the story isn't over yet.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    New class VectorStoreDeterministic. Utilizing the Bobcat hash, it deterministically generates term vectors, eliminating the need to retain a set of elemental term vectors. A built-in cache is enabled by default but can be disabled and cleared. Independent of vector type and dimension.
    Moving term vector writing to happen before docindexing starts so that term vectors always get written out.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Found a conflict between PSI and the new elemental vector store implementation: PSI was calling getVector before checking whether a corresponding semantic vector existed, creating new elemental vectors for many terms.
    
    I've committed one possible fix - replacing the "getVector(x) == null" check with an explicit "containsVector(x) == false" check. This seems a safer practice, though there are a fair number of checks we would need to change scattered throughout the codebase still.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    Initial commit of permutation-based embedding code (ahead of reviewing
    issues with permutation/directional based unit tests).
    
    There are some fairly significant structural changes here including:
    (1) using -embeddingmethod (instead of positionalmethod) to distinguish
    between RI and SGNS
    (2) proximity-based permutations
    (3) facilities for storing, generating and searching with random
    permutations
    Added support for a couple more term weighting options. Logfreq looks pretty good in wikipedia term search.
    Changed name of usetermweightsintermsearch for consistency.
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Two small changes:
    (1
    
    Signed-off-by: tcohen1 <tcohen1@129.106.137.189>
    Additional changes for the embeddings implementation, including a change
    to the "analogy"search in accordance with the y-x+z idea
    Euclidean distance appears to work better for proximity search when
    small differences are involved.Results have moved from nonsensical to
    merely impractical.
    Some changes to facilitate experiments with proximity search on the
    presidential corpus (including some that are so task-specific as to
    warrant deletion in the near future) not working particularly well at
    present.
    Checking in the following:
    (1) small change to DocVectors.bin such that normalization occurs without the need to look up document IDs, which seems to speed this up some (subjectively, no formal testing occurred)
    (2) an update to NumberRepresentation, such that the alpha and omega vectors are included as "alpha" and "omega" when graded vectors are generated (everything else stays the same, so they would also be included as numbered demarcator vectors)
    (3) an update to SentenceVectors such that it no longer requires the field to be indexed to be "stored''
    (4) an experimental version of proximity search *INCOMPLETE* - this needs to be updated to incorporate (2), currently it will only work if the indexed corpus happens to contain a document 100 words in length (as it must guess where to find an alpha and omega vector at query time and SentenceVectors currently outputs a vector store of the graded vectors for each document length in the corpus)
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Changed -searchtype boundminimum, such that it finds the best pairwise minimum across a sequence of vector pairs
    Added -searchtype intersection, currently experimental and works with binary vectors only
    Fixed a bug in the Subspace search, to do with mismatched vector types.
    Added weighting for PSI searches
    Added interface to Lucene document search
    Numerous small modifications to accommodate more flexible command line PSI queries with QI in mind. One of these is definitely temporary - "-bindnotreleasehack", which will ultimately be replaced by an expanded query syntax with a different symbol for bind and release.
    
    Nonetheless, it seemed worth checking in the status quo.
    A first attempt at incorporating the S(X), E(X), P(predicate) notation into the search interface. It should now be possible to specify queries this way for boundproduct, boundproductsubspace and boundminimum, if -elementalvectorfile -semanticvectorfile -predicatevectorfile and -searchvectorfile are specified.
    Continuous table values work nicely now for all vectortypes.
    A few experimental Search approaches have been added:
    (1) minimum score across sets of vectors with and without binding (to find middle terms)
    (2) a first attempt at producing .json files for predication searches
    
    I've been a little lazy about defining new flags here, specifically:
    - I've repurposed the "CompoundVectorBuilder" code that creates disjoint subspaces, such that it will not normalize the subspace if the searchtype is "BoundMinimum"
    - The "semanticvectorfile", "predicatevectorfile" and "elementalvectorfile" are used to tell the code building the graph where to find these vectors, so the graph construction is not tied to the search type
    - If this flags are not set to their default values and the .jsonfile flag is set, this triggers construction of a predication graph
    
    There are most likely clearer ways to trigger this behavior, but this seems adequate for the moment.
    fixed the .stdev calculation by moving the call to TransformToStats down a little way
    added a search based on the minimum score across a set of vector cues, e.g. for finding middle terms
    Adding a new experiment for getting "best" semantic relations on PSI vectors for predicting types.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Dramatically speeding up VectorSearcher.getNearestNeighbors() for large number of results.
    PS: added two lines that should have been committed with the previous change.
    Some minor alterations:
    (1) added an additional constructor to the BoundProduct, BoundProductSubspace and MaxSim searchers, to allow for the specification of a set of query vectors as an ArrayList<Vector>
    (2) getAllAboveThreshold will now return all results if Float.MINIMUM_VALUE is specified as the threshold (i.e. in this case it returns all >= threshold, rather than all > threshold). On occasion I would get incomplete result sets without this change.
    Changed negation term from NOT to ~NOT, and fixed tests accordingly. Minor refactoring and javadoc synchronizing.
    Added sorting to the GetAllAboveThreshold method, such that results are returned in order. Also some minor alterations to CompoundVectorBuilder for the purpose of bound product searches.
    Allow for the specification of a subspace using the E(C1)*E(C2)+S(C3)*E(C4) notation. In this case, "+" separates the components of the subspace (previous commit appears on the Google Code site, but for some reason is not accessible when retrieving code via svn).
    Corrected bug introduced in previous submit (set local vector store to the expanded version rather than global one)
    Changed expandSearchSpace, to allow for passing a vector store as a parameter and receiving the expanded vector store as the product of the method, so this can be accessed from outside a particular VectorSearcher instance.
    This code aims to facilitate searching across dual-predicate paths, as per the "schizophrenia's prozac" paper. This is accomplished by expanding n predicate vectors by adding n-squared dual predicate paths (binary), or 2xn-squared dual predicate paths (complex) by combining the individual predicates using the "release" operator. I've also switched around the order in certain of the boundproduct complexqueryvector methods, as this is important in the complex vector version.
    Experimenting with a new AnalogySearcher, which doesn't work yet but is a useful test for bind and release (which doesn't work reliably yet except for with binary vectors!)
    
    I've cleaned up some not-so-useful verbiage in Search.java - it's much easier to read the big switch-list on searchtype now. This violates the "catch exceptions as tightly as possible" principle but it's worth it to get so much more code together.
    Added option for -boundproduct search to accept definitions of the form:
    
    S(united_states)*S(mexico)*E(dollar)
    
    It might be simpler to have this as the only option eventually.
    Cleanup revision. Have removed stray javadocs instead of fixing them - this is interim but good enough for now.
    Minor edits and improvements to test harness; no solution to remaining last weird test (see comments).
    Added methods to CompoundVectorBuilder to facilitate:
    
    (1) parsing expressions that combine bundling and binding into a single superposed query vector
    (2) parsing expressions with multiple bound products into a subspace for search across multiple predicate paths
    
    (1) facilitates the "dollar of mexico" example when searching on the national facts data with the following parameters:
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -boundvectorfile elementalvectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    united_states*mexico united_states_dollar
    
    0.25148809523809523:mexican_peso
    0.0580357142857143:senegal
    
    (2) takes queries of the form concept predicate_path+predicate_path. The idea is to facilitate search from a cue across multiple paths simultaneously, using the quantum-inspired OR operator. An example with the national facts data might be:
    -searchtype boundproductsubspace
    -queryvectorfile semanticvectors.bin
    -boundvectorfile predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    united_kingdom HAS_NATIONAL_ANIMAL+HAS_CURRENCY
    
    0.504960298538208:british_pound
    0.4712301790714264:lion
    0.0625:ascension_pound
    0.0545634925365448:hanoi
    This contains an attempt at a binary equivalent of the Gram-Schmidt orthogonalization process for pseudo-subspace search and perhaps also negation. Said attempt is found in BinaryVectorUtils, which is called from VectorSearcher/CompoundVectorBuilder instead of the usual orthogonalizevectors method when binary vectors are used. There's probably a more elegant way to integrate these sorts of special cases, but I thought it would be worth including to attempt to bring the different vector representations closer to functional equivalence.
    Slight change to MeasureOverlap for binary vectors.
    OLD: score = 1 - normalized hamming distance
    NEW: score = 2 * (0.5 - normalized hamming distance)
    
    The idea being that a normalized hamming distance of 0.5 is equivalent to orthogonality, so it's misleading to include this anticipated random overlap in the similarity metric.
    
    Also have requested that the VectorSearcherSubspaceSim not attempt to orthogonalize binary vectors, at least until such time as we include something analogous to orthogonalization for these vectors.
    
    The combination of these two changes produces reasonable results for subspace-like searches with binary vectors, assuming the component vectors of the subspace are dissimilar (normHD +- 0.5) to start out with.
    Working prototype of a PSI implementation with complex vectors. This didn't take much doing, as all the hard work was already done thanks to Lance and Dominic. Of note, I did remove the constraint that complex vectors must be sparse in the "many zeros" sense, as things seem to work much better this way. Here are some example queries on the national facts data:
    
    PSI
    -vectortype complex
    -dimension 1000
    -seedlength 1000
    predication_index
    
    SEARCHES
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    lion HAS_NATIONAL_ANIMAL-INV
    
    Found vector for 'lion'
    Found vector for 'HAS_NATIONAL_ANIMAL-INV'
    Search output follows ...
    0.3619873304097799:sweden
    0.3384664837853647:liberia
    0.3282750592424923:belgium
    0.31246171572126347:luxembourg
    0.31236739306024536:united_kingdom
    0.3070540803456166:bulgaria
    0.30435112416653176:macedonia
    0.3016290787884374:netherlands
    0.07023105799656197:managua
    0.06497062922105992:belarus
    
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 elementalvectors.bin
    -searchvectorfile predicatevectors.bin
    -matchcase
    united_states united_states_dollar
    
    Opening query vector store from file: semanticvectors.bin
    Opening second query vector store from file: elementalvectors.bin
    Opening search vector store from file: predicatevectors.bin
    Searching term vectors, searchtype boundproduct
    Found vector for 'united_states'
    Found vector for 'united_states_dollar'
    0.529266436624566:HAS_CURRENCY
    0.0057567168349519264:HAS_NATIONAL_ANIMAL
    
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    mexico HAS_CURRENCY
    
    Opening query vector store from file: semanticvectors.bin
    Opening second query vector store from file: predicatevectors.bin
    Opening search vector store from file: elementalvectors.bin
    Searching term vectors, searchtype boundproduct
    Found vector for 'mexico'
    Found vector for 'HAS_CURRENCY'
    Search output follows ...
    0.5408946657776929:mexican_peso
    0.07199221477091654:san_salvador
    Basic implementation of PSI added. Unit tests to follow. This will run on the nationalfacts.txt file in testdata - first produce a Lucene edition of this file using LuceneIndexFromTriples, then run PSI on it.
    
    Concepts will be encoded as lowercase terms, with spaces replaced by underscores. Predicates will be encoded as uppercase terms, with spaces replaced by underscores.
    
    To search the resulting files with a bound product, use the command line parameters:
    -searchtype boundproduct
    -queryvectorfile elementalvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile semanticvectors.bin
    -matchcase
    concept predicate
    e.g.
    lion HAS_NATIONAL_ANIMAL-INV
    
    0.7587579617834395:liberia
    0.755374203821656:united_kingdom
    0.7548765923566879:sweden
    0.7532842356687898:bulgaria
    0.7501990445859873:macedonia
    0.7482085987261147:belgium
    0.6879976114649682:netherlands
    0.6230095541401274:luxembourg
    Added alternative constructor for VectorSearcherCosine, which allows one to pass a pregenerated vector rather than a set of terms. This will be useful for the generation of query vectors that combine elemental and semantic vectors, e.g. to infer relationship types in PSI.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Restoring stdev and searchresultsminscore functionality that was accidentally deleted in the merge.
    Some changes trying to get binary permutation search to work.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Revised -stdev implementation in response to reported bug: reinstated threshold (of lowest score so far) at search time, impose -stdev related threshold later after statistics calculated
    Cosmetic changes: changed Flags.standard_deviations to Flags.stdev (it seemed a common enough acronym, and standarddeviations was hard to tolerate), also switch from | to || where indicated earlier.
    Implemented "standard_deviations" flag for searching, which uses a simple and approximate one-pass algorithm to estimate the standard deviations above the mean association strength across all search results for the top results returned (in response to the "thresholding" thread awhile back). This can be used in conjunction with the new "searchresultsminscore" flag.
    Added support for searchresultsminscore flag. (And hence double flags in general.)
    Adding new vectorsearchfilterregex flag and supporting functionality.
    More cleanup and following through on better encapsulation of Flags.dimension and Flags.seedlength.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Added types to generic enumerations as discussed in issue 23.
    Started to cleanup some warnings. Mainly refactored tests so that the unit tests that don't touch the filesystem can be run in eclipse (not working yet but working from ant).
    altered SubspaceSim such that it no longer calls getSumScalarProduct, rather it calls a new method VectorUtils.compareWithProjection which measures the proportion of the length of a vector that is retained by its projection into a subspace
    working on tracking down threading issue.
    Changed BalancedPermutationVectorSearcher to use maximum score from two possible directions (random index as cue for permuted vectors or vice versa) of search. When combined with term weighting (as occurs when -indexFilePath is used with Search) this seems to produce reasonable results with reciprocity.
    Retrofitted Clustering classes to use Flags.
    Minor synchronization of my client with Trevor's revision for balanced_permutation.
    Added a new VectorSearcher BalancedVectorSearcherPerm which takes the mean of the results obtained when searching through permuted vectors for the closest vector to a random index vector, and the reverse of this approach.
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Updated version number, set one Javadoc property, change BuildPositionalIndex to only create docvectors in BASIC mode.
    Removed some deprecated html docs.
    Added exceptions to disjunction vector searcher classes; removed exception clauses from search wrapping functions since ZeroVectorExceptions are all caught in Search.RunSearch now.
    Package now throws exceptions always instead of hard coding System.exit
    Some refactoring for permutations stuff: in particular, changed IndexType to Enumeration and made TermTermVectors use VectorStoreRAM and VectorStoreSparseRAM to make writing vectors simpler. Also wrote some checks for permutation searcher.
    VectorSearcher.VectorSearcherPerm added to support index-based retrieval.
    Small changes: i. Changed IncrementalDocVectors and calling code to put VectorFile argument last (since it's a write argument); ii. Much Javadoc for Search.SearchType options; iii. Changed ClusterResults to do random rather than round robin assignment (request from Pitt).
    Updated javadoc for searchtypes and vector searchers.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Added VectorStoreReaderRAMCache and small test of this.
    i. Checking in some slight edits to Positional indexing code form Trevor (just occasional bracketing conventions, javadoc, minor tweaks to usage messages); ii. Removed typo class BuildIPositionalndex; iii. Added new class for clustering entire vectors stores (this isn't optimized at all, run from text not lucene-based vector indexes, but the clustering doesn't scale for large vector stores anyway.
    Some cosmentic changes including removing legacy javadoc html files.
    Finished refactoring of Search.java, several different search options now work pretty smoothly. Subspace disjunctions still seem to need debugging, wrote one test for this so far. Added first blush at kMeansCLustering routine; not happy with results.
    Refactored Search.java to make parsing arguments and searching much cleaner. Refactors VectorSearcher accordingly - VectorSearchers now have to look after building the appropriate queries from the query arguments.
    Improved javadoc for translater. More importantly, refactored VectorSearcher to take VectorStore interface instead of VectorStoreReader, so that both Lucene index and plain text index VectorStoreReaders work for search interface.
    Improved usage and javadoc for VectorSearcher and SearchTensorRelation
    Added command line SearchTensorRelation interface. Results don't look too promising: where things work well, it's because tensor similarities just reduce to products of scalar products of the factors  (I think).
    
    Also adding product functions to VectorUtils and appropriate html javadoc files.
    Refactored to have search go through a VectorSearcher object that enables the use of different similarity scores while using common getNearestNeighbors implementation. Appears to be working fine, next step is to add some other similarity scores, e.g., using tensors.
    Initial semantic vectors package
    Support in PSI for training predicate vectors (as well as just using elemental predicate vectors).
    
    Still very unsure of what inverse predicates should look like in this model. (Which basically comes down to the question "should there be inverse vetors under binding / release, and if so what's the mutiplicative identity vector?"
    Added comparisons between PSI expressions (using the elementalvectorfile, semanticvectorfile and predicatvectorfile flags), to accommodate e.g.
    S(prozac)*P(ISA)|E(fluoxetine)
    Score = 0.459500. Terms: S(prozac)*P(ISA)|E(fluoxetine)
    0.4595
    Small fix to make CompareTerms work with default file extensions.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Turned -docindexing into an enum.
    Cleaned up CompareTermsBatch and removed -vectorstorelocation flag, since this is the only place it's used and I can't imagine that users want to deliberately use vectors from disk every time for a large batch. (If the vector store is too big to fit in main memory, you're not going to be a happy camper anyway.)
    
    Would be nice to write a procedural test for CompareTerms and CompareTermsBatch ...
    Refactored vector building interfaces in TermTermVectorsFromLucene, IncrementalTermVectors, and IncrementalDocVectors, to use flagConfigs much more. This simplifies interfaces considerably.
    
    However, I chickened out of completely removing args[i] parameters from Incremental*Vectors classes, since I don't use them regularly and don't have proper tests.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Changing constructor for LuceneUtils to use only FlagConfig.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Copying across from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    (Recommit after eclipse mismatch.) Added dimension setting and checking to VectorStoreRAM.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Closed some more vectorstorereaderlucene file handles. This is done for general cleanliness and isn't necessary to complete any integration tests.
    fixed checking of vectorstorelacation argument (python habits...)
    Added license to Andrew MacKinlay's CompareTermsBatch, and added Andrew to AUTHORS file.
    Added CompareTermsBatch and related modifications to Flags, and the convention that normalizing the zero vector returns zero. Thanks to Andrew Mackinlay.
    Adds an experiment to analyze structural similarities between xml documents.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    Issue-106: Update to Lucene 6.6.0
    
    Updated per review. Use specific classes in imports.
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    ....forgot to normalize the experimental proximity based term vectors on the last commit
    Checking in the following:
    (1) small change to DocVectors.bin such that normalization occurs without the need to look up document IDs, which seems to speed this up some (subjectively, no formal testing occurred)
    (2) an update to NumberRepresentation, such that the alpha and omega vectors are included as "alpha" and "omega" when graded vectors are generated (everything else stays the same, so they would also be included as numbered demarcator vectors)
    (3) an update to SentenceVectors such that it no longer requires the field to be indexed to be "stored''
    (4) an experimental version of proximity search *INCOMPLETE* - this needs to be updated to incorporate (2), currently it will only work if the indexed corpus happens to contain a document 100 words in length (as it must guess where to find an alpha and omega vector at query time and SentenceVectors currently outputs a vector store of the graded vectors for each document length in the corpus)
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Removing deterministic flag.
    Renaming hybridvectors flag to hybrid orthographic vectors.
    Making seedlength field private and making accessors use the getter.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Removing public status of ComplexVector.setDominantMode, and catching all setters. Callers should now use VectorType COMPLEXFLAT for all of this, and set it once in flag config.
    Added "release" so the term vectors don't get distorted beyond recognition during training
    Removed some hardcoded file names, and changed NumberRepresentation to allow for the specification of seeds at instantiation, which will hopefully facilitate creating sets of demarcator vectors that do not conflict with one another.
    Renaming "infer" package to "orthography". This should help readers connect with literature on "orthographic similarity", of which there seems to be plenty.
    First commit of "infer" packge with character-based string-edit similarity metric.
    
    Thanks to Manuel Wahle and Trevor Cohen.
    Some fixing to tests. Not good solutions but things are busy.
    Introducing experiments using learned character vectors. Some refactoring and testing of orthography code to support learned character vectors.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Removing deterministic flag.
    Renaming hybridvectors flag to hybrid orthographic vectors.
    Making seedlength field private and making accessors use the getter.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Removing public status of ComplexVector.setDominantMode, and catching all setters. Callers should now use VectorType COMPLEXFLAT for all of this, and set it once in flag config.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    More minor cleanup.
    Added "hybridvectors" command line flag, for those inclined toward conceptual/orthographic blends.
    Quick fix to last commit - removed a half-baked attempt at characer weighting, eliminating the annoying output that accompanied it.
    Changed the "null" character to an underscore ("_"), as "*" sends the command line shell into regular expression land. We may not want to include this ultimately, but does provide the means to evaluate the distance between individual character*demarcator_vector pairs in the context of a set of demarcator vectors of a certain size (e.g. c__ vs. _c_; or c_ vs _c).
    
    In VSO, changed the number of demarcator vectors to 2+termlength.
    Rather than generating n number vectors for a term of length n, generate n+2 so E(alpha) and E(omega) are not used to generate bound products. This allows for some similarity between a term that starts with a letter, and another term that ends with that letter.
    permutation-based orthographic real vectors
    End-correction changes that appear to have unbroken CompareTerms (at least). Decent looking results for real and binary vectors, NaN for complex.
    Renaming "infer" package to "orthography". This should help readers connect with literature on "orthographic similarity", of which there seems to be plenty.
    Fixed bug I introduced into StringEdit (and in the process eliminated the "iEnd" prefix from the vectors returned), and added option for specification of orthographic vector store at query time (so OOV words can be used as a cue)
    eliminated annoying print statements
    First commit of "infer" packge with character-based string-edit similarity metric.
    
    Thanks to Manuel Wahle and Trevor Cohen.
    Introducing experiments using learned character vectors. Some refactoring and testing of orthography code to support learned character vectors.
    Enforce probabilistic normalization when binary vectors are used.
    Checking in the following:
    (1) small change to DocVectors.bin such that normalization occurs without the need to look up document IDs, which seems to speed this up some (subjectively, no formal testing occurred)
    (2) an update to NumberRepresentation, such that the alpha and omega vectors are included as "alpha" and "omega" when graded vectors are generated (everything else stays the same, so they would also be included as numbered demarcator vectors)
    (3) an update to SentenceVectors such that it no longer requires the field to be indexed to be "stored''
    (4) an experimental version of proximity search *INCOMPLETE* - this needs to be updated to incorporate (2), currently it will only work if the indexed corpus happens to contain a document 100 words in length (as it must guess where to find an alpha and omega vector at query time and SentenceVectors currently outputs a vector store of the graded vectors for each document length in the corpus)
    Introducing experiments using learned character vectors. Some refactoring and testing of orthography code to support learned character vectors.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Removing public status of ComplexVector.setDominantMode, and catching all setters. Callers should now use VectorType COMPLEXFLAT for all of this, and set it once in flag config.
    Initial work on making semantic vectors from rows in tables. Number representations not wired in yet.
    Changes to weighted superposition (with the purpose of supporting bookend vectors for continuous quantities).
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    Removed some hardcoded file names, and changed NumberRepresentation to allow for the specification of seeds at instantiation, which will hopefully facilitate creating sets of demarcator vectors that do not conflict with one another.
    Binary demarcator vectors are now working. The issue had to do with the need to use the same random seed across all of the weighted superposition operations, so the probability of a particular bit exclusive to v1 being preserved at weight v1*0.25 + v2*0.75 is complementary to the probability of this bit being preserved at v1*0.75 + v2*0.25. I've used Bobcat to set a seed based on the String representation of v1 currently, which may preserve the consistency of our measurements across demarcator sets of different sizes (provided v1 stays constant), though I've not yet tested this.
    Linear version of demarcator vectors, appears to work for the real and complex case (since orthogonalization update to complex vectors)
    Replaced the "padding" approach with orthogonalization, such that sim(vL,vR) approx 0.
    added a weighted superposition for binary vectors, so the binary vector and complex vector number representations have the same (slightly wonky) behavior
    Trying out a linear version of number-vector allocation rather than the recursive one. So far results look the same and it looks like a nice simplification.
    
    Removed the constraint that start index must be zero, with appropriate test to check that this is working and some code-fixes accordingly.
    Modest improvements in commenting to code in NumberRepresentation, and large improvements to my understanding of it. (Good news: the analogy with orbital angular momentum is not nuts, though it is extremely simple.)
    For real and complex vectors, it seems as though some other distance metric than the cosine metric may be more appropriate if we are to have this metric reflect the proportional distance between demarcator vectors. One possibility might be to use the angle between these vectors directly.
    Renaming "infer" package to "orthography". This should help readers connect with literature on "orthographic similarity", of which there seems to be plenty.
    Running the main class now outputs an nxn matrix of similarities.
    Fixed bug I introduced into StringEdit (and in the process eliminated the "iEnd" prefix from the vectors returned), and added option for specification of orthographic vector store at query time (so OOV words can be used as a cue)
    eliminated annoying print statements
    Added caching of generated VectorStores
    First commit of "infer" packge with character-based string-edit similarity metric.
    
    Thanks to Manuel Wahle and Trevor Cohen.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    Some changes to facilitate experiments with proximity search on the
    presidential corpus (including some that are so task-specific as to
    warrant deletion in the near future) not working particularly well at
    present.
    Changes to make experimental results on tabular data more reproducible  / configurable.
    Continuous table values work nicely now for all vectortypes.
    Improved table implementation to cover continuous (double) values.
    Enabled maven javadoc
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Initial work on making semantic vectors from rows in tables. Number representations not wired in yet.
    Issue-106: Update to Lucene 6.6.0
    
    Updated to lucene 6.6.0
    Switched the following:
    
    FROM
    columnTypes[i].addMinMaxVectors(flagConfig,"standard_demarcator");
    
    TO
    columnTypes[i].addMinMaxVectors(flagConfig,columnHeaders[i].getObject().toString());
    
    This permits separate reference ranges for columns of the same type
    (which was deliberately disabled for some of the "graded vectors"
    experiments)
    Fixing tests and some orthogonalization behavior.
    Some changes to facilitate experiments with proximity search on the
    presidential corpus (including some that are so task-specific as to
    warrant deletion in the near future) not working particularly well at
    present.
    Table experiments changes (remaining changes from completing vector reasoning paper draft)
    Changes to make experimental results on tabular data more reproducible  / configurable.
    Continuous table values work nicely now for all vectortypes.
    Improved table implementation to cover continuous (double) values.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Removing deterministic flag.
    Renaming hybridvectors flag to hybrid orthographic vectors.
    Making seedlength field private and making accessors use the getter.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Initial work on making semantic vectors from rows in tables. Number representations not wired in yet.
    Added test of the sort via alphaVector idea
    Some changes to facilitate experiments with proximity search on the
    presidential corpus (including some that are so task-specific as to
    warrant deletion in the near future) not working particularly well at
    present.
    Table experiments changes (remaining changes from completing vector reasoning paper draft)
    Changes to make experimental results on tabular data more reproducible  / configurable.
    Continuous table values work nicely now for all vectortypes.
    Improved table implementation to cover continuous (double) values.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Initial work on making semantic vectors from rows in tables. Number representations not wired in yet.
    More work on DyadicIndexer
    Some stat utils for comparing norms / distances of vectors.
    Adding DyadicIndexer experiment
    Restored important line of code that somehow went AWOL on the last
    commit.
    First attempt at implementing an approach to retrofitting vectors to
    accommodate lexical relationships.
    
    As described in: Faruqui, Manaal, et al. "Retrofitting word vectors to
    semantic lexicons." arXiv preprint arXiv:1411.4166 (2014).
    
    Original python implementation https://github.com/mfaruqui/retrofitting
    
    We have found this to be useful for medical concepts: Retrofitting
    Concept Vector Representations of Medical Concepts to Improve Estimates
    of Semantic Similarity and Relatedness Zhiguo Yu, Byron Wallace, Todd
    Johnson, Trevor Cohen (Medinfo 2017)
    Table experiments changes (remaining changes from completing vector reasoning paper draft)
    Informative exceptions here and there.
    Improved table implementation to cover continuous (double) values.
    Comments and appropriate version warnings.
    Little bits of work from the plane a few days ago.
    Fixed real PSI (w00t!). Turns out it was to do with the inexact release / bind inverse relationship, so we shouldn't use this in training.
    Setting java version to 1.7 in maven.
    
    And some extensions to PSI Type Lister to print types explicitly.
    VectorStoreTruncater produces a truncated edition of a vector store -
    e.g. VectorStoreTruncater incomingvectors.bin 200 produces a
    200-dimensional edition of this vector store, incomingvectors_200.bin.
    
    For binary vectors and real/complex vectors with seedlength==dimension
    this should allow one to estimate the effect of dimensionality without
    regenerating the vector stores concerned.
    Ensure that only examples in which both elements are represented are
    considered (with binary vectors too)
    A few changes to the SGNS implementation, for consistency with other
    implementations: (1) less aggressive subsampling; (2) a lower minimum
    learning rate. In addition, the pairwise correlation class should now
    permit comparison between compound terms, using simple vector addition.
    Correct for identical values (using the average rank)
    This class will output the correlation between pairwise similarities
    from a vector store, and a set of reference similarity scores provided
    in a file of the format:
    
    term1 , term2 , score
    
    (tab is also an acceptable delimiter)
    Hopefully helpful code to generate input for Pathfinder software
    packages
    Patch to exclude examples where the "d" term is not present.
    Initial commit of permutation-based embedding code (ahead of reviewing
    issues with permutation/directional based unit tests).
    
    There are some fairly significant structural changes here including:
    (1) using -embeddingmethod (instead of positionalmethod) to distinguish
    between RI and SGNS
    (2) proximity-based permutations
    (3) facilities for storing, generating and searching with random
    permutations
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Checking to make sure calls to get a particular field return something or give a meaningful error.
    Factoring out results-printing methods to simplify main flow of Search.java.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Initial work on making semantic vectors from rows in tables. Number representations not wired in yet.
    Factoring out results-printing methods to simplify main flow of Search.java.
    Experiments in progress
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    New package and class: pitt.search.semanticvectors.hashing.Bobcat. Bobcat is a hash algorithm that computes a 48 bit long hash code from a string (not cryptographically safe but reasonably collision free).
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    Changes to enable redistributing coordinates to make their distributions approximately uniform.
    Some stat utils for comparing norms / distances of vectors.
    Some stat utils for comparing norms / distances of vectors.
    Logger counter utility
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Copying across from branch to trunk.
    Pre-calculate the sigmoid function, within a permitted range of values.
    Some stat utils for comparing norms / distances of vectors.
    Moving sigmoid function to statutils
    Adding simulations / samples of real vectors and their similarities, and associated trivial stat utils.
    Somewhat hastily constructed but apparently functional utility class to
    extract a subset, or a set of vectors derived as combinations of a
    subset, of a vector store using an input file with one 'query' per line.
    
    If -elementalvectorfile, -semanticvectofile and -predicatevectorfile are
    specified, the queryies will be parsed as PSI/ESP queries, e.g.
    
    S(docetaxel)*P(TREATS)
    S(aspirin)*P(TREATS)
    
    If not, they will be parsed as free text and queries will be constructed
    from flagConfig.queryvectorfile(), e.g.
    
    president
    precedent
    president precedent
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Mainly work on complex vectors to catch convolution up with angular representation with 0. Also realized that 2^14 should be used as phase resolution.
    
    Minor changes to string array logging.
    Copying across from branch to trunk.
    Two modifications:
    (1) Faster initialization of binary vectors
    (2) Code permitting deterministic initialization of permutation vectors
    (this is not yet used)
    Initial commit of permutation-based embedding code (ahead of reviewing
    issues with permutation/directional based unit tests).
    
    There are some fairly significant structural changes here including:
    (1) using -embeddingmethod (instead of positionalmethod) to distinguish
    between RI and SGNS
    (2) proximity-based permutations
    (3) facilities for storing, generating and searching with random
    permutations
    Avoid breaking up unit circle components during shift permutations by enforcing (shift%2==0)
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Some changes trying to get binary permutation search to work.
    Copying across from branch to trunk.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Did some slight refactoring to make for more robust backwards compatibility with old indexes, with usage messages if things go wrong.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    Initial semantic vectors package
    Two modifications:
    (1) Faster initialization of binary vectors
    (2) Code permitting deterministic initialization of permutation vectors
    (this is not yet used)
    Apologies - failed to include this in the last commit
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Trying out a linear version of number-vector allocation rather than the recursive one. So far results look the same and it looks like a nice simplification.
    
    Removed the constraint that start index must be zero, with appropriate test to check that this is working and some code-fixes accordingly.
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Copying across from branch to trunk.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Did some slight refactoring to make for more robust backwards compatibility with old indexes, with usage messages if things go wrong.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    Initial semantic vectors package
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Removing public status of ComplexVector.setDominantMode, and catching all setters. Callers should now use VectorType COMPLEXFLAT for all of this, and set it once in flag config.
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    Altered behavior of binary vector implementation, such that "normalize()" resets the voting record to register a single vote for the vector resulting from the vote tally. So binary vectors should behave more like other vectors in this regard (this difference was causing problems with the hybrid vector implementation).
    
    Also added a tallyVotes() method, which will count up the votes without resetting the voting record. This is currently only used in
    SemanticVectorCollider.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    Added support for Enums to FlagConfig. No more VectorType.valueOf() - w00t!
    
    As a consequence, header strings in vector stores will say (e.g.) BINARY instead of binary, but reading code is robust to case so I decided to allow this as a (I think) backward-compatible change.
    
    This paves the way for term weighting, searchtype, and any other enums to be parsed on the way in. This is good - error checking is early, raw strings aren't passed around and parsed later.
    
    Tests all pass.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Version 2.0 based on Roger Schvaneveldt's Matlab edition: compare superposition:origin vs. superposition:random (previous edition compared superposition:origin vs. random:origin). This appears to eliminate the bias toward unbalanced binary vectors, solving a part of The Mystery of the Twisted Vectors.
    The SemanticVectorCollider class measures the number of elemental vector superpositions occur before the superposed product is closer to some random vector than to the first vector superposed. So it gives a measure of the capacity of the configuration of vector type, seedlength and dimensionality used.
    
    This experiment is repeated over 100 iterations, each using up to 15,000 superpositions (though we seldom get this far). These parameters can be altered by changing the code as required.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Changed -searchtype boundminimum, such that it finds the best pairwise minimum across a sequence of vector pairs
    Added -searchtype intersection, currently experimental and works with binary vectors only
    pom and javadoc cleanup
    Little bits of work from the plane a few days ago.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Changes to weighted superposition (with the purpose of supporting bookend vectors for continuous quantities).
    Binary demarcator vectors are now working. The issue had to do with the need to use the same random seed across all of the weighted superposition operations, so the probability of a particular bit exclusive to v1 being preserved at weight v1*0.25 + v2*0.75 is complementary to the probability of this bit being preserved at v1*0.75 + v2*0.25. I've used Bobcat to set a seed based on the String representation of v1 currently, which may preserve the consistency of our measurements across demarcator sets of different sizes (provided v1 stays constant), though I've not yet tested this.
    added a weighted superposition for binary vectors, so the binary vector and complex vector number representations have the same (slightly wonky) behavior
    I've submitted an experimental approach to limiting the self-inverse property of XOR with binary vector binding, which can be activated at search/build time with the -binarybindingwithpermute flag. When binding, the implementation first XOR's and then shift-permutes the result +1. Upon release, the implementation first shift-permutes -1, and then XOR's.
    Apologies - left some debugging code in the previous commit, gone now.
    Slight change to the binary approximation of OrthogonalizeVectors, such that vectors with a normalized Hamming distance (NHD) of less than 0.5 are also altered, rendering the NHD to 0.5 exactly (MeasureOverlap = 0). Previously only vectors with NHD > 0.5 were altered.
    Cleanup revision. Have removed stray javadocs instead of fixing them - this is interim but good enough for now.
    This contains an attempt at a binary equivalent of the Gram-Schmidt orthogonalization process for pseudo-subspace search and perhaps also negation. Said attempt is found in BinaryVectorUtils, which is called from VectorSearcher/CompoundVectorBuilder instead of the usual orthogonalizevectors method when binary vectors are used. There's probably a more elegant way to integrate these sorts of special cases, but I thought it would be worth including to attempt to bring the different vector representations closer to functional equivalence.
    Replace the voting record on binding - without this tallying votes rolls
    back the bind
    Update BinaryVector.java
    
    fix typo
    fix typo
    fix indentation
    change totalNumberOfVotes and minimum from int to long data type to prevent overflow; fix some indentation.
    Improved the copy method (thanks to Hannah Burkhardt), corrected output
    of number of votes, included code to check for overflow of this value as
    we fear may occur with many iterations of training. If this turns out to
    be a problem we'll need to find an alternative to AtomicInteger here....
    Corrected an error that crept in when attempting to speed up
    normalization - the error resulted in 50/50 splits  being set to 1 with
    100% rather than 50% probability....
    Two modifications:
    (1) Faster initialization of binary vectors
    (2) Code permitting deterministic initialization of permutation vectors
    (this is not yet used)
    Change to facilitate superposition atop a randomly generated vector.
    Formerly this would erase pre-existing information. Now this is included
    in the voting record as an initial vote before new vectors are added.
    Correction (again). Setting the total number of votes to more than one
    and initializing a single row of the voting record is an excellent way
    to reproduce the bug in question, but an abysmal way to address it (I
    blame the altitude).
    
    One FixedBitSet, one vote.
    Corrected a bug on the last commit, which underestimated the initial
    vote.
    Found the source of the rogue zero vectors, which occur when converting
    elemental to semantic vectors without augmenting the number of votes to
    reflect a vote for the current voting record.
    I appear to have found the root of the recently recurring binary vector
    error. The issue was that zero vectors were not being recognized as
    such, so their votes were counted for the purpose of keeping track of
    the total number of votes, but they don't register on the voting record,
    which counts ones. Fixing this stops the Exceptions... now to track down
    the source of the superposed zero vectors.
    Further attempts to add thread safety to the BinaryVector
    implementation. It looks as though a recurring and unpredictable error
    has to do with trying to tally votes in the case where the number where
    looking for exceeds the capacity of the voting record. I'm not sure why
    such a circumstance would arise (as we're usually looking for only half
    the total number of votes), but I've added a check for it and tried to
    render some of the counts thread safe.
    Some simplification of the code, removal of redundancy and probable
    bugfixes to TermTermVectorsFromLucene.
    
    Also, a check in Binary Vectors such that tallying of votes occurs only
    if changes have happened, and an attempt at making binary vectors play
    nicely with the new TermTermVectorsFromLucene additions.
    (1) Leveraged FixedBitSet's "next positive bit" method to improve the
    election process (this is used when ties occur, and the old process was
    to iterate across all of a sparsely populated bit vector that indicated
    the position of the ties).
    (2) Fixed a bug in ComplexVector that caused an error on integration -
    though there are some failed tests on account of the Hermitian mode
    being the new dominant mode for these vectors that I have yet to
    address.
    Bugfix: flip all of the bits, instead of (dimension / 64) bits.
    Synchronized the methods that access the voting record, which will
    hopefully avoid the occasional null pointer exception that seems to
    emerge from time to time with multiple threads running.
    Added a method to write out a truncated form of a vector, to support a
    utility class to follow. For dense vectors, this should allow one to
    assess the influence of dimensionality without the need to regenerate
    the vector space (i.e. start large and shrink).
    Restored the majority rule (as per the spatter code) as the default
    normalization method for binary vectors, while providing probabilistic
    normalization as a command line option (following the procedure used for
    RealBindMethod).
    
    Though I haven't examined in great detail, the majority rule is clearly
    more efficient, and the accuracy improvements I found with probabilistic
    normalization in IR experiments don't necessarily occur in PSI reasoning
    experiments. So it seems as though users should be able to choose a
    speed vs. (possible) accuracy tradeoff.
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Added a "PRINTPSIQUERY" search, to reveal the entire search vector generated by a -boundproduct type query for debugging, or downstream re-use
    Changed -searchtype boundminimum, such that it finds the best pairwise minimum across a sequence of vector pairs
    Added -searchtype intersection, currently experimental and works with binary vectors only
    Switched the output of the toString() method, such that the count of the voting record is output before normalization occurs.
    
    Also, added getCoordinates() method, to provide access to the bitset directly.
    maven and lucene upgrades to make the SV jar work standalone
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Javadoc warnings cleanup.
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    Moved some machinery outside of the inner loop, speeding things up a little further.
    Removed some unnecessary import statements
    Removed some vestigial code that made it in on the last commit
    Faster probabilistic normalization (though still slower than the BSC majority rule)
    Slight modification to previous commit, such that each superposition is seeded with a unique signature derived from the voting record. This way, the same set of vectors should always result in the same superposed product (if weighted the same way), but the same (very biased) set of per-dimension probabilities will not be used in every superposition.
    ....another possibility for probabilistic normalization would be to render this deterministic such that superposing the same set of vectors will produce the same superposition each time by seeding the normalization pseudorandom number generator. It seems as though this would be desirable in some circumstances, though further experimentation is required to determine the ups and downs of this approach.
    The "majority rule" is replaced by probabilistic normalization as a normalization method of choice. The performance seems tolerable (mostly as n(superposition) > n(normalization)), though I have yet to do any comparative evaluations of normalization speed. However, this doesn't seem prohibitive, and the advantage of having (x+x+y) = 2x + y rather then (x+x+y) = x seems worth pursuing.
    Small change to previous commit - removed resetting of the voting record parameters (total number of votes and minimum) upon calling elementalToSemantic(), as this produces peculiar and as yet unexplained side effects at search time. As this weren't broke, I am reevaluating the merits of my attempt to fix it.
    Altered behavior of binary vector implementation, such that "normalize()" resets the voting record to register a single vote for the vector resulting from the vote tally. So binary vectors should behave more like other vectors in this regard (this difference was causing problems with the hybrid vector implementation).
    
    Also added a tallyVotes() method, which will count up the votes without resetting the voting record. This is currently only used in
    SemanticVectorCollider.
    Avoid attempting to count the votes in an empty record (I'm running into "zero vector" issues on account of some work I'm doing)
    Added check to avoid trying to superpose zero vectors (while erroneously augmenting the total vote count thereby throwing everything out of whack)
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Added binding with permutation as per Kanerva 2009, such that:
    A*B =  perm+1(A) XOR B = C
    left inverse(A,C) = A*C = perm+1(A) XOR C = B  (same as the binding operation)
    right inverse(B,C) = perm-1(B XOR C) = perm-1(perm+1(A)) = A
    
    This makes things quite complicated, but we'll have to incorporate something like this if we're to accommodate nested relationships with binary vectors (this is much simpler with HRR, but at the cost of storage capacity).
    I've submitted an experimental approach to limiting the self-inverse property of XOR with binary vector binding, which can be activated at search/build time with the -binarybindingwithpermute flag. When binding, the implementation first XOR's and then shift-permutes the result +1. Upon release, the implementation first shift-permutes -1, and then XOR's.
    Implemented bind and release for real and complex vectors.
    
    Working "correctly" for real case, not producing good results yet.
    
    I wonder how useful it is to have four total bind and release methods?
    Implemented recommended switch to OpenBitSet.xorcount()
    This contains an attempt at a binary equivalent of the Gram-Schmidt orthogonalization process for pseudo-subspace search and perhaps also negation. Said attempt is found in BinaryVectorUtils, which is called from VectorSearcher/CompoundVectorBuilder instead of the usual orthogonalizevectors method when binary vectors are used. There's probably a more elegant way to integrate these sorts of special cases, but I thought it would be worth including to attempt to bring the different vector representations closer to functional equivalence.
    Slight change to MeasureOverlap for binary vectors.
    OLD: score = 1 - normalized hamming distance
    NEW: score = 2 * (0.5 - normalized hamming distance)
    
    The idea being that a normalized hamming distance of 0.5 is equivalent to orthogonality, so it's misleading to include this anticipated random overlap in the similarity metric.
    
    Also have requested that the VectorSearcherSubspaceSim not attempt to orthogonalize binary vectors, at least until such time as we include something analogous to orthogonalization for these vectors.
    
    The combination of these two changes produces reasonable results for subspace-like searches with binary vectors, assuming the component vectors of the subspace are dissimilar (normHD +- 0.5) to start out with.
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Included a Flags.binaryvectordecimalplaces parameter for those in need of more precise binary vectors
    Changing Vector from abstract class to interface.
    Do not superpose zero-weighted vectors.
    Reversing change submitted yesterday, apologies. The issue was with my vectors, and the ZeroVectorException was entirely justified.
    Ran into some aberrant behavior this morning, where what I'd anticipated being (non-zero) elemental vectors were throwing zeroVectorExceptions on account of an empty voting record. I'm not sure what the root cause of this is, but will keep looking.
    
    For the moment, I've made a slight modification to isZeroVector() - insist that in the case of a semantic vector both the BitSet and the voting record should be zero before the exception is thrown.
    Implemented binding using XOR, with and without permutation (to allow for nesting). Also, included in Vectors a "release" method as binding and releasing are the same operation with binary vectors, but this is not the case with HRR.
    
    Currently, superposition (bundling) is not addressed here - binding produces a new vector that is dissimilar from its two component vectors.
    Got binary directional search working. Complex convolution still needs fixed.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Made elementalToSemantic initialize the voting record and added a test.
    
    Lots of little cleanups including renaming exact to setTempSetToExactMatch and making it void (side effect is writing to tempSet).
    Updated header comments for complex and binary vectors. Ready to go out for testing I think.
    Fixed the bug that resulted in random vectors being written out as zero vectors. Previously calling normalize() on an elemental vector would result in the vector being replaced with the result of tallying the (empty) voting record. Now calling normalize() on an elemental vector has no effect.
    Another minor change: the toString() method now outputs the voting record for easier debugging
    Fixed the "trailing 1" problem. This had to do with initializing temporary vectors, which I had been doing using e.g. set(0, dimension-1). It turns out that the OpenBitSet.set method takes (startindex, lastindex to set +1) as parameters, not (startindex, lastindex to set), so the -1 was the problem.
    Small change to BinaryVector, enforcing "normalization" of the vector prior to writing out to a string. This forces a tally of the voting record, so that the resulting binary vector superposition is output after "NORMALIZED:". Without this, a zero vector is output, as the votes have yet to be tallied.
    Added a test for getMaximumSharedWeight and addition and normalization.
    
    I think getMaxiumumSharedWeight may be broken, or I don't understand it properly. It looks like it is most likely to return zero unless there is an entire row of 1s in the voting record, which seems highly unlikely.
    
    Also uncovered a "last bit gets turned to 1" behavior which I'm not sure is intended.
    
    Some name changing and commenting, let me know if there are any objections or misunderstandings here.
    Some changes trying to get binary permutation search to work.
    Copying across from branch to trunk.
    Initial commit of permutation-based embedding code (ahead of reviewing
    issues with permutation/directional based unit tests).
    
    There are some fairly significant structural changes here including:
    (1) using -embeddingmethod (instead of positionalmethod) to distinguish
    between RI and SGNS
    (2) proximity-based permutations
    (3) facilities for storing, generating and searching with random
    permutations
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Copying across from branch to trunk.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Did some slight refactoring to make for more robust backwards compatibility with old indexes, with usage messages if things go wrong.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    Initial semantic vectors package
    Removed some test printing and set createZeroVector to be sparse for complex vectors
    Fixed test, caveat emptor comments, some formatting.
    Initial commit of permutation-based embedding code (ahead of reviewing
    issues with permutation/directional based unit tests).
    
    There are some fairly significant structural changes here including:
    (1) using -embeddingmethod (instead of positionalmethod) to distinguish
    between RI and SGNS
    (2) proximity-based permutations
    (3) facilities for storing, generating and searching with random
    permutations
    Suggested alterations needed to get complex vector unit tests green again.
    Refactored initialization of complex vectors, and made a few other
    changes to enable initialization of hermitian/cartesian (rather than
    circular) random vectors.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Reinstating real PSI, capturing first predicate line, supporting complex flat serialization (which is the same as complex).
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Checked in an option to create dense random vectors, with each dimension initialized to a real value between -1 and 1. This option is invoked automatically if seedlength==dimension in a real vector.
    Working prototype of a PSI implementation with complex vectors. This didn't take much doing, as all the hard work was already done thanks to Lance and Dominic. Of note, I did remove the constraint that complex vectors must be sparse in the "many zeros" sense, as things seem to work much better this way. Here are some example queries on the national facts data:
    
    PSI
    -vectortype complex
    -dimension 1000
    -seedlength 1000
    predication_index
    
    SEARCHES
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    lion HAS_NATIONAL_ANIMAL-INV
    
    Found vector for 'lion'
    Found vector for 'HAS_NATIONAL_ANIMAL-INV'
    Search output follows ...
    0.3619873304097799:sweden
    0.3384664837853647:liberia
    0.3282750592424923:belgium
    0.31246171572126347:luxembourg
    0.31236739306024536:united_kingdom
    0.3070540803456166:bulgaria
    0.30435112416653176:macedonia
    0.3016290787884374:netherlands
    0.07023105799656197:managua
    0.06497062922105992:belarus
    
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 elementalvectors.bin
    -searchvectorfile predicatevectors.bin
    -matchcase
    united_states united_states_dollar
    
    Opening query vector store from file: semanticvectors.bin
    Opening second query vector store from file: elementalvectors.bin
    Opening search vector store from file: predicatevectors.bin
    Searching term vectors, searchtype boundproduct
    Found vector for 'united_states'
    Found vector for 'united_states_dollar'
    0.529266436624566:HAS_CURRENCY
    0.0057567168349519264:HAS_NATIONAL_ANIMAL
    
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    mexico HAS_CURRENCY
    
    Opening query vector store from file: semanticvectors.bin
    Opening second query vector store from file: predicatevectors.bin
    Opening search vector store from file: elementalvectors.bin
    Searching term vectors, searchtype boundproduct
    Found vector for 'mexico'
    Found vector for 'HAS_CURRENCY'
    Search output follows ...
    0.5408946657776929:mexican_peso
    0.07199221477091654:san_salvador
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Incrementing version number to 3.1 after shipping 3.0. Woohoo\!
    Copying across from branch to trunk.
    Ensure Hermetian mode is maintained throughout the binding operation,
    and also that this is the dominant mode for ESP.
    Removed some test printing and set createZeroVector to be sparse for complex vectors
    Fixed test, caveat emptor comments, some formatting.
    Suggested alterations needed to get complex vector unit tests green again.
    (1) Leveraged FixedBitSet's "next positive bit" method to improve the
    election process (this is used when ties occur, and the old process was
    to iterate across all of a sparsely populated bit vector that indicated
    the position of the ties).
    (2) Fixed a bug in ComplexVector that caused an error on integration -
    though there are some failed tests on account of the Hermitian mode
    being the new dominant mode for these vectors that I have yet to
    address.
    Refactored initialization of complex vectors, and made a few other
    changes to enable initialization of hermitian/cartesian (rather than
    circular) random vectors.
    More cleanup
    Added HERMITIAN dominant mode
    First pass at an implementation of skipgram with negative sampling
    (Mikolov 2013). The main changes are as follows:
    
    (1) The negative sampling algorithm itself, with the scalar product and
    vector updates currently implemented within VectorUtils using
    netlib-java's blas routines (already a dependency), and sampling of
    out-of-context terms in accordance with their frequency^.75.
    
    (2) Subsampling of frequent terms, currently in need of reassessment and
    review, as although it does (vastly) speed up training, it does not
    improve performance as assorted papers suggest it should. Without
    subsampling results on pairwise correlation sets (e.g. wordsim353, MEN)
    and Mikolov's analogy set (n=~19,000) match reported results - in the
    region of 0.7 correlation and 0.6 accuracy for similarity and analogies
    respectively. But with subsampling as implemented currently performance
    deteriorates, more so as the threshold is reduced, which should not be
    the case according to the literature.
    
    (3) Multiple processing threads (with some initial movements in the
    direction of thread safety, though this doesn't yet extend to the
    document queue)
    Added a method to write out a truncated form of a vector, to support a
    utility class to follow. For dense vectors, this should allow one to
    assess the influence of dimensionality without the need to regenerate
    the vector space (i.e. start large and shrink).
    Checked in a simpler method of generating random vectors for the special case in which dimension==seedlength, and updated one of the unit tests to reflect the new determinism.
    Checked in a more general edition of the convolveCartesian method, such that the radius of each circular vector element after convolution is equal to the product of the radii of this element of the component vectors
    (1) Checked in the Hermitian edition of ComplexVector - currently this is switched on by changing a boolean variable at the very end of the code, but as discussed a better thought-out (or at least somewhat thought-out) way of integrating the current cornucopia of complex vector capabilities is in our sights.
    (2) Restored the "renderPairwiseOrthogonal" code in ComplexVectorUtils, for posterity but mostly for the purpose of passing one of the unit tests.
    (3) Altered said unit test so it uses this type of orthogonalization.
    Introducing experiments using learned character vectors. Some refactoring and testing of orthography code to support learned character vectors.
    Not a definitive change, but it does seem worth being consistent with respect to the Hermitian vs. Angular debate - prior to this change we had angular normalization with hermitian overlap. I'll look into making these consistent in accordance with the flag I seem to recall you created awhile back, but thought I'd check this in as a record of what was used for the QI submission.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Changes to complex measure polar overlap so that sparse elemental vectors are self-similar.
    Removing public status of ComplexVector.setDominantMode, and catching all setters. Callers should now use VectorType COMPLEXFLAT for all of this, and set it once in flag config.
    Suppress unnecessary logging, some minor changes.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Javadoc warnings cleanup.
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    Avoid dividing by zero for "0" complex vector components. Disclaimer: arguably these "0" complex vector components should not exist in the first place, as the components of complex circular vectors are unit vectors by definition.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Modified complex unit test to be inline with phase angle based scalar product.
    
    Updated some comments to make this clear.
    Two small changes to the Cartesian complex vector implementation:
    (1) normalize such that the vector composed of a real + imaginary pair falls within the unit circle
    (2) calculate overlap on this basis also, using pairwise norms (rather than normalizing according to the length of the entire vector, as wouldbe the case with the standard implementation of cosine comparison)
    Implemented bind and release for real and complex vectors.
    
    Working "correctly" for real case, not producing good results yet.
    
    I wonder how useful it is to have four total bind and release methods?
    Dominant mode needs to be set to dense polar for directional indexing to work well.
    Working prototype of a PSI implementation with complex vectors. This didn't take much doing, as all the hard work was already done thanks to Lance and Dominic. Of note, I did remove the constraint that complex vectors must be sparse in the "many zeros" sense, as things seem to work much better this way. Here are some example queries on the national facts data:
    
    PSI
    -vectortype complex
    -dimension 1000
    -seedlength 1000
    predication_index
    
    SEARCHES
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    lion HAS_NATIONAL_ANIMAL-INV
    
    Found vector for 'lion'
    Found vector for 'HAS_NATIONAL_ANIMAL-INV'
    Search output follows ...
    0.3619873304097799:sweden
    0.3384664837853647:liberia
    0.3282750592424923:belgium
    0.31246171572126347:luxembourg
    0.31236739306024536:united_kingdom
    0.3070540803456166:bulgaria
    0.30435112416653176:macedonia
    0.3016290787884374:netherlands
    0.07023105799656197:managua
    0.06497062922105992:belarus
    
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 elementalvectors.bin
    -searchvectorfile predicatevectors.bin
    -matchcase
    united_states united_states_dollar
    
    Opening query vector store from file: semanticvectors.bin
    Opening second query vector store from file: elementalvectors.bin
    Opening search vector store from file: predicatevectors.bin
    Searching term vectors, searchtype boundproduct
    Found vector for 'united_states'
    Found vector for 'united_states_dollar'
    0.529266436624566:HAS_CURRENCY
    0.0057567168349519264:HAS_NATIONAL_ANIMAL
    
    -searchtype boundproduct
    -queryvectorfile semanticvectors.bin
    -queryvectorfile2 predicatevectors.bin
    -searchvectorfile elementalvectors.bin
    -matchcase
    mexico HAS_CURRENCY
    
    Opening query vector store from file: semanticvectors.bin
    Opening second query vector store from file: predicatevectors.bin
    Opening search vector store from file: elementalvectors.bin
    Searching term vectors, searchtype boundproduct
    Found vector for 'mexico'
    Found vector for 'HAS_CURRENCY'
    Search output follows ...
    0.5408946657776929:mexican_peso
    0.07199221477091654:san_salvador
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Changing Vector from abstract class to interface.
    Added placeholders for alternative bind and release methods. We may end up removing these, but this will allow the head revision to compile for now.
    Got binary directional search working. Complex convolution still needs fixed.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Updated header comments for complex and binary vectors. Ready to go out for testing I think.
    Mainly work on complex vectors to catch convolution up with angular representation with 0. Also realized that 2^14 should be used as phase resolution.
    
    Minor changes to string array logging.
    Copying across from branch to trunk.
    Added NORMALIZEDCONVOLUTION as a new default bind method, which makes
    "release" behave as expected. Also changed the implementation of this
    slightly, so the initial vectors are not disturbed.
    First pass at an implementation of skipgram with negative sampling
    (Mikolov 2013). The main changes are as follows:
    
    (1) The negative sampling algorithm itself, with the scalar product and
    vector updates currently implemented within VectorUtils using
    netlib-java's blas routines (already a dependency), and sampling of
    out-of-context terms in accordance with their frequency^.75.
    
    (2) Subsampling of frequent terms, currently in need of reassessment and
    review, as although it does (vastly) speed up training, it does not
    improve performance as assorted papers suggest it should. Without
    subsampling results on pairwise correlation sets (e.g. wordsim353, MEN)
    and Mikolov's analogy set (n=~19,000) match reported results - in the
    region of 0.7 correlation and 0.6 accuracy for similarity and analogies
    respectively. But with subsampling as implemented currently performance
    deteriorates, more so as the threshold is reduced, which should not be
    the case according to the literature.
    
    (3) Multiple processing threads (with some initial movements in the
    direction of thread safety, though this doesn't yet extend to the
    document queue)
    Added a method to write out a truncated form of a vector, to support a
    utility class to follow. For dense vectors, this should allow one to
    assess the influence of dimensionality without the need to regenerate
    the vector space (i.e. start large and shrink).
    Big scary upgrade to Lucene 5.0. All tests pass, not without some tweaking (some of the integration tests show random variations in IDEA), but generally good. Have made IDF the default -termweight.
    Make IncrementalDocVectors give info messages when individual fields are empty, and only give warning messages when all fields in a document are empty.
    Should make real coordinates output the right thing without changing sparse vector in memory.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Suppress unnecessary logging, some minor changes.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Javadoc warnings cleanup.
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    Changed RealVector.bind to be expected orthogonal to both factors, and for release to retrieve (sort of) one of the factors, including under superposition.
    
    The test for this passes, and should perhaps be used similarly elsewhere. But it's not very convincing really - for three pairs of vectors, the recovery of the original under probing is not very effective. Still, it's a baseline.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Checked in an option to create dense random vectors, with each dimension initialized to a real value between -1 and 1. This option is invoked automatically if seedlength==dimension in a real vector.
    Implemented bind and release for real and complex vectors.
    
    Working "correctly" for real case, not producing good results yet.
    
    I wonder how useful it is to have four total bind and release methods?
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Changing Vector from abstract class to interface.
    Added placeholders for alternative bind and release methods. We may end up removing these, but this will allow the head revision to compile for now.
    Got binary directional search working. Complex convolution still needs fixed.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Copying across from branch to trunk.
    Added a method to write out a truncated form of a vector, to support a
    utility class to follow. For dense vectors, this should allow one to
    assess the influence of dimensionality without the need to regenerate
    the vector space (i.e. start large and shrink).
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Changing Vector from abstract class to interface.
    Implemented binding using XOR, with and without permutation (to allow for nesting). Also, included in Vectors a "release" method as binding and releasing are the same operation with binary vectors, but this is not the case with HRR.
    
    Currently, superposition (bundling) is not addressed here - binding produces a new vector that is dissimilar from its two component vectors.
    Got binary directional search working. Complex convolution still needs fixed.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Incrementing version number to 3.1 after shipping 3.0. Woohoo\!
    Copying across from branch to trunk.
    Fixed test, caveat emptor comments, some formatting.
    Refactored initialization of complex vectors, and made a few other
    changes to enable initialization of hermitian/cartesian (rather than
    circular) random vectors.
    Changed the implementation Gram-Schmidt procedure such that
    re-normalization occurs only after a component vector has been rendered
    orthogonal to the others in the set.
    (1) Checked in the Hermitian edition of ComplexVector - currently this is switched on by changing a boolean variable at the very end of the code, but as discussed a better thought-out (or at least somewhat thought-out) way of integrating the current cornucopia of complex vector capabilities is in our sights.
    (2) Restored the "renderPairwiseOrthogonal" code in ComplexVectorUtils, for posterity but mostly for the purpose of passing one of the unit tests.
    (3) Altered said unit test so it uses this type of orthogonalization.
    renderOrthogonal method altered, such that orthogonality is defined by a hermitian scalar product between the vectors = 0, rather than a mean pairwise cosine between circular components of zero (code for the latter option has been retained but is currently commented out, and could perhaps be deleted entirely)
    pom and javadoc cleanup
    Little bits of work from the plane a few days ago.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    Added a complex vector orthogonalization procedure (for the cartesian case only)
    Cleanup to make integration tests work better on Windows. Java on Windows will not delete if a filehandle is open somewhere else, even if the file on the filesystem has already been deleted and the reference is out of scope - calling System.gc() is a kludge to make this get garbage collected.
    
    There are still some testing problems, mainly (I think) due to state being maintained in Flags when it shouldn't be. I've tracked down most of this but some remains.
    
    Also changed some names like dwiddows -> Dominic Widdows, etc., just for tidyness.
    Copying across from branch to trunk.
    Initial commit of permutation-based embedding code (ahead of reviewing
    issues with permutation/directional based unit tests).
    
    There are some fairly significant structural changes here including:
    (1) using -embeddingmethod (instead of positionalmethod) to distinguish
    between RI and SGNS
    (2) proximity-based permutations
    (3) facilities for storing, generating and searching with random
    permutations
    Added a method to check for NaNs (it looks as though
    CompoundVectorBuilder tries to normalize zero vectors at some point -
    todo: look into the root cause)
    Bugfix - blas methods were only using half the components of the complex
    vectors.
    Some simplification of the code, removal of redundancy and probable
    bugfixes to TermTermVectorsFromLucene.
    
    Also, a check in Binary Vectors such that tallying of votes occurs only
    if changes have happened, and an attempt at making binary vectors play
    nicely with the new TermTermVectorsFromLucene additions.
    Dominic doing some picky style normalization.
    First pass at an implementation of skipgram with negative sampling
    (Mikolov 2013). The main changes are as follows:
    
    (1) The negative sampling algorithm itself, with the scalar product and
    vector updates currently implemented within VectorUtils using
    netlib-java's blas routines (already a dependency), and sampling of
    out-of-context terms in accordance with their frequency^.75.
    
    (2) Subsampling of frequent terms, currently in need of reassessment and
    review, as although it does (vastly) speed up training, it does not
    improve performance as assorted papers suggest it should. Without
    subsampling results on pairwise correlation sets (e.g. wordsim353, MEN)
    and Mikolov's analogy set (n=~19,000) match reported results - in the
    region of 0.7 correlation and 0.6 accuracy for similarity and analogies
    respectively. But with subsampling as implemented currently performance
    deteriorates, more so as the threshold is reduced, which should not be
    the case according to the literature.
    
    (3) Multiple processing threads (with some initial movements in the
    direction of thread safety, though this doesn't yet extend to the
    document queue)
    Corrected an erroneous call to VectorUtils.compareToProjection for binary vectors. One option would have been to call BinaryVectorUtils.compareToProjection(), but it seems as though we could standardize this process across implementations as the BinaryVectorUtils was the sum(overlap), while the standard version is sqrt(sum(overlap^2)).
    Little bits of work from the plane a few days ago.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Corrected for gremlins at work. Infinite  loop eliminated.
    Lots of useful cleanup, mainly:
    - Term[Term]VectorsFromLucene no longer implements VectorStore.
    - VectorStore is cleaned up to remove methods that are now dealt with through flagConfig.
    Changes to weighted superposition (with the purpose of supporting bookend vectors for continuous quantities).
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    Cleanup revision. Have removed stray javadocs instead of fixing them - this is interim but good enough for now.
    Copying across from branch to trunk.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Copying across from branch to trunk.
    Significant changes on complex vector implementation. Moved to enum of modes including POLAR_SPARSE. Tested many of the codepaths, and everything seems to be working. Normalize and measure overlap behave genuinely differently between cartesian and polar paradigms. This is interesting but a bit challenging to configure. TBD. For now, a hearty w00t since everything including positional and permutation indexing seems to be working. Removed ComplexVectorTestMain, didn't want to maintain parallel tests any more once changes grew.
    Basic search works! testBuildAndSearchPositionalIndex still hangs. Refactored dimension to be just flag-based, the claims that we'd encapsulated it weren't really that honest.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Changed BuildBilingualIndex to use new Flags library, in response to a bug report that dimension parsing was breaking the program. Have not refactored interfaces to TermVectorsFromLucene, I think I should do this.
    Did some slight refactoring to make for more robust backwards compatibility with old indexes, with usage messages if things go wrong.
    Continued updating documentation.
    Command line options added for vector length, seed length and minimum term frequency, -d for dimensions,
    -s for seed length and -m for minimum term frequency. Changes were made to VectorStoreReader and VectorStoreWriter such
    that a file header consisting of a string "dimensions" and a float containing the number of dimensions are written
    at the head of the file. If this header is present, ObjectVector.vecLength is modified upon reading the header, but
    it should still be possible to read files produced by older versions without the header.... if the dimensions match.
    The "final" modifier was removed where necessary.
    Initial semantic vectors package
    Added NORMALIZEDCONVOLUTION as a new default bind method, which makes
    "release" behave as expected. Also changed the implementation of this
    slightly, so the initial vectors are not disturbed.
    Fixing tests and some orthogonalization behavior.
    Better wiring for redistributing coordinates
    Changes to enable redistributing coordinates to make their distributions approximately uniform.
    Changed the implementation Gram-Schmidt procedure such that
    re-normalization occurs only after a component vector has been rendered
    orthogonal to the others in the set.
    Little bits of work from the plane a few days ago.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Some factoring improvements to make calls to vector orthogonalize more systematic.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Committing ElementalVectorStore and wiring it in so that we can easily switch between random, contenthash, and orthographic vectors.
    
    All tests pass, but load tests should be done before release.
    RunSearch -> runSearch for more normal Java style.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Fixed compile erros in 2d plotter (not tested yet).
    Cleaned up PrincipalComponents and Plot2dVectors. Removed PrincipalComponents2, which has become PrincipalComponents anyway.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Popping extensions up a level.
    Trying to refactor ubild paths for cleaner / easier distinction between src and src extensions.
    Added a test for CompoundVectorBuilder and VectorStoreRAM, and utils for outputting principal component plots as tex source.
    Adding visualization utils. Added extra sophistication to build file to support this: now, utils will ship with the package but will not compile by default, to avoid extra dependencies for basic users.
    Cleanup / indents and imports and such
    Removed a flawed attempt at finding the middle terms while constructing the predication-based PFNET
    A small detail, to get the direction of the arrows right for "INV" predications.
    Some improvements to the Pathfinder implementation, courtesy of Roger Schvaneveldt
    Factoring out results-printing methods to simplify main flow of Search.java.
    Getting rid of javadoc warnings
    Eliminated repeated getVector() calls.
    A few experimental Search approaches have been added:
    (1) minimum score across sets of vectors with and without binding (to find middle terms)
    (2) a first attempt at producing .json files for predication searches
    
    I've been a little lazy about defining new flags here, specifically:
    - I've repurposed the "CompoundVectorBuilder" code that creates disjoint subspaces, such that it will not normalize the subspace if the searchtype is "BoundMinimum"
    - The "semanticvectorfile", "predicatevectorfile" and "elementalvectorfile" are used to tell the code building the graph where to find these vectors, so the graph construction is not tied to the search type
    - If this flags are not set to their default values and the .jsonfile flag is set, this triggers construction of a predication graph
    
    There are most likely clearer ways to trigger this behavior, but this seems adequate for the moment.
    Enabled maven javadoc
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Moved json generation to pathfinder class
    Moving PathFinder to viz package, and some breaking out of methods and catching exceptions in RunSearch that I should have caught earlier.
    Setting up project for Maven.
    
    This includes moving source directories to the src/main/java and src/test/java patterns.
    Lots of changes on the plane ... main improvement is getting the real VSA working properly with convolutions. Other refactorings including promoting plotting utils to main src.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Fixed compile erros in 2d plotter (not tested yet).
    Cleaned up PrincipalComponents and Plot2dVectors. Removed PrincipalComponents2, which has become PrincipalComponents anyway.
    Test commit
    Popping extensions up a level.
    Trying to refactor ubild paths for cleaner / easier distinction between src and src extensions.
    Added a test for CompoundVectorBuilder and VectorStoreRAM, and utils for outputting principal component plots as tex source.
    Adding visualization utils. Added extra sophistication to build file to support this: now, utils will ship with the package but will not compile by default, to avoid extra dependencies for basic users.
    Some stat utils for comparing norms / distances of vectors.
    Updating exampleclient to new release.
    Updating exampleclient dependency version number.
    pom indentation
    Example search client that uses VectorSearcher
    Example search client that uses VectorSearcher
    Indexing experiment for Russian folk tales relations.
    Improvements to memory copying for NarrativeRelationsIndexer. Seems to work for complex and binary now.
    Indexing experiment for Russian folk tales relations.
    Changed query-reading scanner to read a line - never knew that Java's scanner read to next whitespace.
    Logging message when startwords are loaded to make it clear that only these are indexed.
    Updated with link to example page on Wiki
    Example search client that uses VectorSearcher
    Adds an experiment to analyze structural similarities between xml documents.
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    More explicit README including ParallelColt dependency.
    Popping extensions up a level.
    Trying to refactor ubild paths for cleaner / easier distinction between src and src extensions.
    Adding visualization utils. Added extra sophistication to build file to support this: now, utils will ship with the package but will not compile by default, to avoid extra dependencies for basic users.
    Demo form QI 2013 - dog looking for food.
    
    No mutual dependencies with the rest of the semantic vectors package.
    Demo form QI 2013 - dog looking for food.
    
    No mutual dependencies with the rest of the semantic vectors package.
    Demo form QI 2013 - dog looking for food.
    
    No mutual dependencies with the rest of the semantic vectors package.
    Demo form QI 2013 - dog looking for food.
    
    No mutual dependencies with the rest of the semantic vectors package.
    Demo form QI 2013 - dog looking for food.
    
    No mutual dependencies with the rest of the semantic vectors package.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Beagle code compiles ... pending testing.
    Committing a compiling version that is broken at runtime. This is normally a bad thing to do but I nearly lost my hard drive the other day and I've got the fear about losing this work! If this breaks your build in the branch then please revert.
    Committing Beagle
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Committing Beagle
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Fixed bugs with command line parsing in Search.java and number of clusters in ClusterResults.java
    Slight refactoring of generateRandomVector in TermVectorsFromLucene, to use only class-level fileds vecLength and seedLength (so method takes no additional arguments).
    Initial semantic vectors package
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Beagle code compiles ... pending testing.
    Committing a compiling version that is broken at runtime. This is normally a bad thing to do but I nearly lost my hard drive the other day and I've got the fear about losing this work! If this breaks your build in the branch then please revert.
    Committing Beagle
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Committing Beagle
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Fixed bugs with command line parsing in Search.java and number of clusters in ClusterResults.java
    Slight refactoring of generateRandomVector in TermVectorsFromLucene, to use only class-level fileds vecLength and seedLength (so method takes no additional arguments).
    Initial semantic vectors package
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Cleaned up some of the new Beagle model implementation due to some movement in the SV codebase, e.g., ObjectVector.vecLength -> Flags.dimension.
    Committing Beagle
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Committing Beagle
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Fixed bugs with command line parsing in Search.java and number of clusters in ClusterResults.java
    Slight refactoring of generateRandomVector in TermVectorsFromLucene, to use only class-level fileds vecLength and seedLength (so method takes no additional arguments).
    Initial semantic vectors package
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Working towards a good implementation of Vector.bind() for use in directional indexing. Real case works, complex and binary don't yet. Checking in for sharing purposes, if there are problems please notify me and I'll fix or revert.
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Beagle code compiles ... pending testing.
    Several minor refactorings done on the plane - the main one being that IncrementalDocVectors now looks more like the other building classes, building with a static factory method rather than a constructor.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    - Changed all prints to log statements in the main package. I've shortened some of the process counter printing to avoid console verbiage here. - Refactored some of the big classes (TermVectorsFromLucene, TermTermVectorsFromLucene) to make the internal state more explicit and the constructor process a bit easier to follow (I hope).
    Closed some more abandoned VectorStoreReaders.
    Modified VectorStoreReaderText so that getting the vector enum creates a new filereader, so that (1) the new filereader can be closed when done enumerating the vectors, and (2) a call to getVector or another function that resets the filereader doesn't also reset the enumeration.
    Cleaned up some of the new Beagle model implementation due to some movement in the SV codebase, e.g., ObjectVector.vecLength -> Flags.dimension.
    Committing Beagle
    Committing Beagle
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Committing Beagle
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Fixed bugs with command line parsing in Search.java and number of clusters in ClusterResults.java
    Slight refactoring of generateRandomVector in TermVectorsFromLucene, to use only class-level fileds vecLength and seedLength (so method takes no additional arguments).
    Initial semantic vectors package
    Integrating Lucene43 branch into trunk. We will test this a bit more and then release.
    Make the FlagConfig getter methods to be just (e.g.) "dimension()" instead of
    "getDimension()".
    - Instead of a dimensionDescription field, we just use the
    (programmatically accessible) JavaDoc of the public dimension()
    method.
      - This JavaDoc can list the default values used.
    
    We could add a test to the FlagConfigTest class to make sure that
    there is programmatically accessible JavaDoc for all public methods.
    
    I think all of these combined will lead to an easy way to document
    more clearly and generally "how flags work", and would lead to the
    FlagConfig.html page being a very useful "how to" document for what
    flags are available.
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Update to be compatible with Lucene 3.6.2. Exactly as recommended in Issue 60 (https://code.google.com/p/semanticvectors/issues/detail?id=60).
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Getting much closer to version 3.0. Loads and loads of changes. All (current) tests pass, but I think some binary positional indexing is still broken.
    
    Main changes include:
    
    - Making dimension and vector type part of the interface of all vector stores. So less reliance on flags. Nonetheless flags are still important, and the challenge of figuring out who can set dimension where and when remains unsolved and difficult.
    
    - Lots of improvements to tests, including making unit tests out of some formerly regression-style tests.
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Beagle code compiles ... pending testing.
    (Recommit after eclipse mismatch.) Added dimension setting and checking to VectorStoreRAM.
    Encapsulated dimension for VectorStore classes. This is starting to look pretty reasonable.
    Added type to enumartion. This is the last modification for issue 23.
    Updated syntactically to Lucene 3.0 compatibility, all compiles but getting runtime errors in TermTermVectorsFromLucene.
    Cleaned up some of the new Beagle model implementation due to some movement in the SV codebase, e.g., ObjectVector.vecLength -> Flags.dimension.
    Committing Beagle
    Committing Beagle
    This is the main reintegration of flaginstance branch. (Explicit SVN reintegrate in Eclipse failed.) All tests pass.
    
    Also modified build.xml to clean up testtmpdir in "ant clean".
    
    Committing in batches to avoid Eclipse / SVN non-specific claims of conflict.
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Moving everything to 'dimension' not 'dimensions'. Hopefully for the last time\!
    Cleanup of auxiliary packages and tests for dimension -> dimensions consistency.
    Fixed some long to int type casting errors given when tring to compile Beagle stuff.
    Cleaned up some of the new Beagle model implementation due to some movement in the SV codebase, e.g., ObjectVector.vecLength -> Flags.dimension.
    Committing Beagle
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Committing Beagle
    Copying accrose src-ext from branch to trunk.
    Deleting branches in trunk in preparation for forced merge :-(
    Committing Beagle
    Attempted to improve VectorStore implementations and tests by creating a CloseableVectorStore interface to be implemented by VectorStores that need to give back resources. This is supposed to solve the problem that Search followed by BuildIndex in the same application locks ... but according to the tests I've written, this might not be the problem.
    Several changes to enable retraining of termvectors from learned docvectors. basicTermVectors is now a VectorStore so that it can be either sparse vectors or learned float vectors. Also added to VectorUtils to support necessary vector transformations. These functions and the new VectorStoreSparseRAM are tested reasonably well.
    Fixed bugs with command line parsing in Search.java and number of clusters in ClusterResults.java
    Slight refactoring of generateRandomVector in TermVectorsFromLucene, to use only class-level fileds vecLength and seedLength (so method takes no additional arguments).
    Initial semantic vectors package
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    These are the source files for Adrian Kuhn and David Erni's adaptation (well, direct translation) of Doug Rhodes' SVDLIBC, a stripped down and approachable implementation of the las2 algorithm derived from SVDPACK (Michael Berry, Theresa Do, Gavin O'Brien, Vijay Krishna and Sowmini Varadhan).
    
    There are also some unit tests related data files in this package that I haven't included for now, but we may want to think about finding a home for moving forward.
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    These are the source files for Adrian Kuhn and David Erni's adaptation (well, direct translation) of Doug Rhodes' SVDLIBC, a stripped down and approachable implementation of the las2 algorithm derived from SVDPACK (Michael Berry, Theresa Do, Gavin O'Brien, Vijay Krishna and Sowmini Varadhan).
    
    There are also some unit tests related data files in this package that I haven't included for now, but we may want to think about finding a home for moving forward.
    Quick and dirty attempt at supporting 2D visualization without JAMA, involving one new Class that is identical to the original aside from an attempt to replace JAMA with the tedlab library we use for LSA. For some reason this produces a reflection about the Y axis of the result with the original Class. It probably wouldn't be too hard to change this, though the symmetry is quite appealing. Using the tedlab library required making one method in it public,  public static SMat svdConvertDtoS(DMat D), as the tedlab SVD implementation requires a sparse matrix as input.
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    These are the source files for Adrian Kuhn and David Erni's adaptation (well, direct translation) of Doug Rhodes' SVDLIBC, a stripped down and approachable implementation of the las2 algorithm derived from SVDPACK (Michael Berry, Theresa Do, Gavin O'Brien, Vijay Krishna and Sowmini Varadhan).
    
    There are also some unit tests related data files in this package that I haven't included for now, but we may want to think about finding a home for moving forward.
    Created dependencies in thirdparty after checking with legal folks about licensing. Also fixed some flag problems in answer to issue 22.
    These are the source files for Adrian Kuhn and David Erni's adaptation (well, direct translation) of Doug Rhodes' SVDLIBC, a stripped down and approachable implementation of the las2 algorithm derived from SVDPACK (Michael Berry, Theresa Do, Gavin O'Brien, Vijay Krishna and Sowmini Varadhan).
    
    There are also some unit tests related data files in this package that I haven't included for now, but we may want to think about finding a home for moving forward.
