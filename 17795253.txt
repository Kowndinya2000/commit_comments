    Hide C++ symbols from dmlc-core (#6188)
    Option for generating device debug info. (#6168)
    
    * Supply `-G;-src-in-ptx` when `USE_DEVICE_DEBUG` is set and debug mode is selected.
    * Refactor CMake script to gather all CUDA configuration.
    * Use CMAKE_CUDA_ARCHITECTURES.  Close #6029.
    * Add compute 80.  Close #5999
    [CI] Test C API demo (#6159)
    
    * Fix CMake install config to use dependencies
    
    * [CI] Test C API demo
    
    * Explicitly cast num_feature, to avoid warning in Linux
    Fall back to CUB allocator if RMM memory pool is not set up (#6150)
    
    * Fall back to CUB allocator if RMM memory pool is not set up
    
    * Fix build
    
    * Prevent memory leak
    
    * Add note about lack of memory initialisation
    
    * Add check for other fast allocators
    
    * Set use_cub_allocator_ to true when RMM is not enabled
    
    * Fix clang-tidy
    
    * Do not demangle symbol; add check to ensure Linux+Clang/GCC combo
    Remove unused RABIT targets. (#6110)
    
    
    * Remove rabit mock.
    * Remove rabit base.
    Enable building rabit on Windows (#6105)
    Refactor rabit tests (#6096)
    
    
    * Merge rabit tests into XGBoost.
    * Run them On CI.
    * Simplification for CMake scripts.
    Fix CMake build with BUILD_STATIC_LIB option (#6090)
    
    * Fix CMake build with BUILD_STATIC_LIB option
    
    * Disable BUILD_STATIC_LIB option when R/JVM pkg is enabled
    
    * Add objxgboost to install target only when BUILD_STATIC_LIB=ON
    Update GPUTreeShap (#6064)
    
    * Update GPUTreeShap
    
    * Update src/CMakeLists.txt
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Bump version to 1.3.0 snapshot in master (#6052)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Introducing DPC++-based plugin (predictor, objective function) supporting oneAPI programming model (#5825)
    
    * Added plugin with DPC++-based predictor and objective function
    
    * Update CMakeLists.txt
    
    * Update regression_obj_oneapi.cc
    
    * Added README.md for OneAPI plugin
    
    * Added OneAPI predictor support to gbtree
    
    * Update README.md
    
    * Merged kernels in gradient computation. Enabled multiple loss functions with DPC++ backend
    
    * Aligned plugin CMake files with latest master changes. Fixed whitespace typos
    
    * Removed debug output
    
    * [CI] Make oneapi_plugin a CMake target
    
    * Added tests for OneAPI plugin for predictor and obj. functions
    
    * Temporarily switched to default selector for device dispacthing in OneAPI plugin to enable execution in environments without gpus
    
    * Updated readme file.
    
    * Fixed USM usage in predictor
    
    * Removed workaround with explicit templated names for DPC++ kernels
    
    * Fixed warnings in plugin tests
    
    * Fix CMake build of gtest
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add CMake flag to log C API invocations, to aid debugging (#5925)
    
    * Add CMake flag to log C API invocations, to aid debugging
    
    * Remove unnecessary parentheses
    Force colored output for ninja build. (#5959)
    Add option to enable all compiler warnings in GCC/Clang (#5897)
    
    * Add option to enable all compiler warnings in GCC/Clang
    
    * Fix -Wall for CUDA sources
    
    * Make -Wall private req for xgboost-r
    [CI] Simplify CMake build with modern CMake techniques (#5871)
    
    * [CI] Simplify CMake build
    
    * Make sure that plugins can be built
    
    * [CI] Install lz4 on Mac
    [R] Remove dependency on gendef for Visual Studio builds (fixes #5608) (#5764)
    
    * [R-package] Remove dependency on gendef for Visual Studio builds (fixes #5608)
    
    * clarify docs
    
    * removed debugging print statement
    
    * Make R CMake install more robust
    
    * Fix doc format; add ToC
    
    * Update build.rst
    
    * Fix AppVeyor
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add pkgconfig to cmake (#5744)
    
    * Add pkgconfig to cmake
    
    * Move xgboost.pc.in to cmake/
    
    Co-authored-by: Peter Jung <peter.jung@heureka.cz>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Bump version to 1.2.0 snapshot in master (#5733)
    Require CUDA 10.0+ in CMake build (#5718)
    Define _CRT_SECURE_NO_WARNINGS to remove unneeded warnings in MSVC (#5434)
    C++14 for xgboost (#5664)
    Enhance nvtx support. (#5636)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Fix non-openmp build. (#5566)
    
    
    * Add test to Jenkins.
    * Fix threading utils tests.
    * Require thread library.
    Ensure that configured dmlc/build_config.h is picked up by Rabit and XGBoost (#5514)
    
    * Ensure that configured header (build_config.h) from dmlc-core is picked up by Rabit and XGBoost
    
    * Check which Rabit target is being used
    
    * Use CMake 3.13 in all Jenkins tests
    
    * Upgrade CMake in Travis CI
    
    * Install CMake using Kitware installer
    
    * Remove existing CMake (3.12.4)
    Adding static library option (#5397)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Enable OpenMP with Apple Clang (Mac default compiler) (#5146)
    
    * Add OpenMP as CMake target
    
    * Require CMake 3.12, to allow linking OpenMP target to objxgboost
    
    * Specify OpenMP compiler flag for CUDA host compiler
    
    * Require CMake 3.16+ if the OS is Mac OSX
    
    * Use AppleClang in Mac tests.
    
    * Update dmlc-core
    Implement training observer. (#5088)
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Fix cmake variable. (#126)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    Copy CMake parameter from dmlc-core. (#4948)
    exit when allreduce/broadcast error cause timeout (#112)
    
    * keep async timeout task
    
    * add missing pthread to cmake
    
    * add tests
    
    * Add a sleep period to avoid flushing the tracker.
    Use bundled gtest (#4900)
    
    * Suggest to use gtest bundled with dmlc
    
    * Use dmlc bundled gtest in all CI scripts
    
    * Make clang-tidy to use dmlc embedded gtest
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Clean up cmake script and code includes (#106)
    
    * Clean up CMake scripts and related include paths.
    * Add unittests.
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    remove is_bootstrap parameter (#102)
    
    * apply openmp simd
    
    * clean __buildin detection, moving windows build check from xgboost project, add openmp support for vectorize reduce
    
    * apply openmp only to rabit
    
    * orgnize rabit signature
    
    * remove is_bootstrap, use load_checkpoint as implict flag
    
    * visual studio don't support latest openmp
    
    * orgnize omp declarations
    
    * replace memory copy with vector cast
    
    * Revert "replace memory copy with vector cast"
    
    This reverts commit 28de4792dcdff40d83d458510d23b7ef0b191d79.
    
    * Revert "orgnize omp declarations"
    
    This reverts commit 31341233d31ce93ccf34d700262b1f3f6690bbfe.
    
    * remove openmp settings, merge into a upcoming pr
    
    * mis
    
    * per feedback, update comments
    support bootstrap allreduce/broadcast (#98)
    
    * support run rabit tests as xgboost subproject using xgboost/dmlc-core
    
    * support tracker config set/get
    
    * remove redudant printf
    
    * remove redudant printf
    
    * add c++0x declaration
    
    * log allreduce/broadcast caller, engine should track caller stack for
    investigation
    
    * tracker support binary config format
    
    * Revert "tracker support binary config format"
    
    This reverts commit 2a28e5e2b55c200cb621af8d19f17ab1bc62503b.
    
    * remove caller, prototype fetch allreduce/broadcast results from resbuf
    
    * store cached allreduce/broadcast seq_no to tracker
    
    * allow restore all caches from other nodes
    
    * try new rabit collective cache, todo: recv_link seems down
    
    * link up cache restore with main recovery
    
    * cleanup load cache state
    
    * update cache api
    
    * pass test.mk
    
    * have a working tests
    
    * try to unify check into actionsummary
    
    * more logging to debug distributed hist three method issue
    
    * update rabit interface to support caller signature matching
    
    * splite seq_counter from cur_cache_seq to different variables
    
    * still see issue with inf loop
    
    * support debug print caller as well as allreduce op
    
    * cleanup
    
    * remove get/set cache from model_recover, adding recover in
    loadcheckpoint
    
    * clarify rabit cache strategy, cache is set only by successful collective
    call involving all nodes with unique cache key. if all nodes call
    getcache at same time, we keep rabit run collective call. If some nodes
    call getcache while others not, we backfill cache from those nodes with
    most entries
    
    * revert caller logs
    
    * fix lint error
    
    * fix engine mpi signature
    
    * support getcache by ref
    
    * allow result buffer presiet to filestream
    
    * add loging
    
    * try fix checkpoint failure recovery case
    
    * use int64_t to avoid overflow caused seq fault
    
    * try avoid int overflow
    
    * try fix checkpoint failure recovery case
    
    * try avoid seqno overflow to negative by offseting specifial flag value
    adding cache seq no to checkpoint/load checkpoint/check point ack to avoid
    confusion from cache recovery
    
    * fix cache seq assert error
    
    * remove loging, handle edge case
    
    * add extensive log to checkpoint state  with different seq no
    
    * fix lint errors
    
    * clean up comments before merge back to master
    
    * add logs to allreduce/broadcast/checkpoint
    
    * use unsinged int 32 and give seq no larger range
    
    * address remove allreduce dropseq code segment
    
    * using caller signature to filter bootstrapallreduces
    
    * remove get/set cache from empty
    
    * apply signature to reducer
    
    * apply signature to broadcast
    
    * add key to broadcat log
    
    * fix broadcast signature
    
    * fix default _line value for non linux system
    
    * adding comments, remove sleep(1)
    
    * fix osx build issue
    
    * try fix mpi
    
    * fix doc
    
    * fix engine_empty api
    
    * logging, adding more logs, restore immutable assertion
    
    * print unsinged int with ud
    
    * fix lint
    
    * rename seqtype to kSeq and KCache indicating it's usage
    apply kDiffSeq check to load_cache routine
    
    * comment allreduce/broadcast log
    
    * allow tests run on arm
    
    * enable flag to turn on / off cache
    
    * add log info alert if user choose to enable rabit bootstrap cache
    
    * add rabit_debug setting so user can use config to turn on
    
    * log flags when user turn on rabit_debug
    
    * force rabit restart if tracker assign -1 rank
    
    * use OPENMP to vecotrize reducer
    
    * address comment
    
    * Revert "address comment"
    
    This reverts commit 1dc61f33e7357dad8fa65528abeb81db92c5f9ed.
    
    * fix checkpoint size print 0
    
    * per feedback, remove DISABLEOPEMP, address race condition
    
    * - remove openmp from this pr
    - update name from cache to boostrapcache
    
    * add default value of signature macros
    
    * remove openmp from cmake file
    
    * Update src/allreduce_robust.cc
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Update src/allreduce_robust.cc
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * run test with cmake
    
    * remove openmp
    
    * fix cmake based tests
    
    * use cmake test fix darwin .dylib issue
    
    * move around rabit_signature definition due to windows build
    
    * misc, add c++ check in CMakeFile
    
    * per feedback
    
    * resolve CMake file
    
    * update rabit version
    Remove plugin, cuda related code in automake & autoconf files (#4789)
    
    * Build plugin example with CMake.
    
    * Remove plugin, cuda related code in automake & autoconf files.
    
    * Fix typo in GPU doc.
    Specify version macro in CMake. (#4730)
    
    * Specify version macro in CMake.
    
    * Use `XGBOOST_DEFINITIONS` instead.
    upgrade version num (#4670)
    
    * upgrade version num
    
    * missign changes
    
    * fix version script
    
    * change versions
    
    * rm files
    
    * Update CMakeLists.txt
    Deprecate single node multi-gpu mode (#4579)
    
    * deprecate multi-gpu training
    
    * add single node
    
    * add warning
    Ensure gcc is at least 5.x (#4538)
    
    * make sure that xgboost has gcc 5.x at the very least to build on gcc tool chain
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    [CI] Add Python and C++ tests for Windows GPU target (#4469)
    
    * Add CMake option to use bundled gtest from dmlc-core, so that it is easy to build XGBoost with gtest on Windows
    
    * Consistently apply OpenMP flag to all targets. Force enable OpenMP when USE_CUDA is turned on.
    
    * Insert vcomp140.dll into Windows wheels
    
    * Add C++ and Python tests for CPU and GPU targets (CUDA 9.0, 10.0, 10.1)
    
    * Prevent spurious msbuild failure
    
    * Add GPU tests
    
    * Upgrade dmlc-core
    [CI] Add Windows GPU to Jenkins CI pipeline (#4463)
    
    * Fix #4462: Use /MT flag consistently for MSVC target
    
    * First attempt at Windows CI
    
    * Distinguish stages in Linux and Windows pipelines
    
    * Try running CMake in Windows pipeline
    
    * Add build step
    Enable building with shared NCCL. (#4447)
    
    * Add `BUILD_WITH_SHARED_NCCL` to CMake.
    Make CMakeLists.txt compatible with CMake 3.3 (#4420)
    
    * Make CMakeLists.txt compatible with CMake 3.3; require CMake 3.11 for MSVC
    
    * Use CMake 3.12 when sanitizer is enabled
    
    * Disable funroll-loops for MSVC
    
    * Use cmake version in container name
    
    * Add missing arg
    
    * Fix egrep use in ci_build.sh
    
    * Display CMake version
    
    * Do not set OpenMP_CXX_LIBRARIES for MSVC
    
    * Use cmake_minimum_required()
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Allow using external dmlc-core (#91)
    
    * Set `RABIT_BUILD_DMLC=1` if use dmlc-core in rabit
    
    * remove dmlc-core
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    add OpenMP option in CMakeLists.txt (#4339)
    Improve HostDeviceVector exception safety (#4301)
    
    
    * make the assignments of HostDeviceVector exception safe.
    * storing a dummy GPUDistribution instance in HDV for CPU based code.
    * change testxgboost binary location to build directory.
    [rabit harden] replace hardcopy dmlc-core headers with submodule links (#86)
    
    * backport dmlc header changes to rabit
    
    * use gitmodule to reference latest dmlc header files
    
    * include ref to dmlc-core
    fix cmake
    
    * update cmake file, add cmake build traivs task
    
    * try force using g++-4.8
    
    * per feedback, update cmake
    Remove various synchronisations from cuda API calls, instrument monitor (#4205)
    
    * Remove various synchronisations from cuda API calls, instrument monitor
    with nvtx profiler ranges.
    Remove dmlc logging. (#78)
    
    * Remove dmlc logging header.
    
    * Fix lint.
    Perform clang-tidy on both cpp and cuda source. (#4034)
    
    * Basic script for using compilation database.
    
    * Add `GENERATE_COMPILATION_DATABASE' to CMake.
    * Rearrange CMakeLists.txt.
    * Add basic python clang-tidy script.
    * Remove modernize-use-auto.
    * Add clang-tidy to Jenkins
    * Refine logic for correct path detection
    
    In Jenkins, the project root is of form /home/ubuntu/workspace/xgboost_PR-XXXX
    
    * Run clang-tidy in CUDA 9.2 container
    * Use clang_tidy container
    Correct JVM CMake GPU flag. (#4071)
    Performance optimizations for Intel CPUs (#3957)
    
    * Initial performance optimizations for xgboost
    
    * remove includes
    
    * revert float->double
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * Check existence of _mm_prefetch and __builtin_prefetch
    
    * Fix lint
    Produce xgboost.so for XGBoost-R on Mac OSX, so that `make install` works (#3767)
    
    * Produce xgboost.so for XGBoost-R on Mac OSX, so that `make install` works
    
    * Modernize R build instructions
    
    * Fix crossref
    Allow plug-ins to be built by cmake (#3752)
    
    * Remove references to AVX code.
    
    * Allow plugins to be built by cmake
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Remove redundant FindGTest.cmake. (#3533)
    
    During removal of FindGTest.cmake, also
    
    * Fix gtest include dirs.
    * Remove some blanks and use PWD for gtest dir.
    Enable building with sanitizers. (#3525)
    Fix building dmlc-core from xgboost. (#3522)
    
    Move building dmlc-core before adding DMLC_LOG_CUSTOMIZE.
    
    Fix #3520.
    Upgrading to NCCL2 (#3404)
    
    * Upgrading to NCCL2
    
    * Part - II of NCCL2 upgradation
    
     - Doc updates to build with nccl2
     - Dockerfile.gpu update for a correct CI build with nccl2
     - Updated FindNccl package to have env-var NCCL_ROOT to take precedence
    
    * Upgrading to v9.2 for CI workflow, since it has the nccl2 binaries available
    
    * Added NCCL2 license + copy the nccl binaries into /usr location for the FindNccl module to find
    
    * Set LD_LIBRARY_PATH variable to pick nccl2 binary at runtime
    
    * Need the nccl2 library download instructions inside Dockerfile.release as well
    
    * Use NCCL2 as a static library
    Add callback interface to re-direct console output (#3438)
    
    * Add callback interface to re-direct console output
    
    * Exempt TrackerLogger from custom logging
    
    * Fix lint
    fixed MinGW missed dll (#3430)
    Fix building shared library. (#58)
    Add cuda forwards compatibility (#3316)
    Update dmlc-core submodule (#3221)
    
    * Update dmlc-core submodule
    
    * Fix dense_parser to work with the latest dmlc-core
    
    * Specify location of Google Test
    
    * Add more source files in dmlc-minimum to get latest dmlc-core working
    
    * Update dmlc-core submodule
    add cmake w/ relocatable pkgconfig installation (#53)
    AVX gradients (#2878)
    
    * AVX gradients
    
    * Add google test for AVX
    
    * Create fallback implementation, remove fma instruction
    
    * Improved accuracy of AVX exp function
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    Only set OpenMP_CXX_FLAGS when OpenMP is found (#2613)
    
    * Only set OpenMP_CXX_FLAGS when OpenMP is found
    
    I found this trying to get the Mac build working without OpenMP. Tips in
    issue #2596 helped to point in the right direction.
    
    * Revise check
    
    * Trigger codecov
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    [GPU-Plugin] Major refactor (#2644)
    
    * Removal of redundant code/files.
    * Removal of exact namespace in GPU plugin
    * Revert double precision histograms to single precision for performance on Maxwell/Kepler
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    [WIP] Extract prediction into separate interface (#2531)
    
    * [WIP] Extract prediction into separate interface
    
    * Add copyright, fix linter errors
    
    * Add predictor to amalgamation
    
    * Fix documentation
    
    * Move prediction cache into predictor, add GBTreeModel
    
    * Updated predictor doc comments
    MinGW: shared library prefix and appveyor CI (#2539)
    
    * for MinGW, drop the 'lib' prefix from shared library name
    
    * fix defines for 'g++ 4.8 or higher' to include g++ >= 5
    
    * fix compile warnings
    
    * [Appveyor] add MinGW with python; remove redundant jobs
    
    * [Appveyor] also do python build for one of msvc jobs
    Update nccl (#2494)
    Cmake improvements (#2487)
    
    * Cmake improvements
    * Add google test to cmake
    [GPU-Plugin] Resolve double compilation issue (#2479)
    [GPU-Plugin] Multi-GPU gpu_id bug fixes for grow_gpu_hist and grow_gpu methods, and additional documentation for the gpu plugin. (#2463)
    [GPU-Plugin] Change GPU plugin to use tree_method parameter, bump cmake version to 3.5 for GPU plugin, add compute architecture 3.5, remove unused cmake files (#2455)
    [jvm-packages] Another pack of build/CI improvements (#2422)
    
    * [jvm-packages] Fixed compilation on Windows
    
    * [jvm-packages] Build the JNI bindings on Appveyor
    
    * [jvm-packages] Build & test on OS X
    
    * [jvm-packages] Re-applied the CMake build changes reverted by #2395
    
    * Fixed Appveyor JVM build
    
    * Muted Maven on Travis
    
    * Don't link with libawt
    
    * "linux2"->"linux"
    
    Python2.x and 3.X use slightly different values for ``sys.platform``.
    [GPU-Plugin] Support for building to specific GPU architectures (#2390)
    
    * Support for builing gpu-plugins to specific GPU architectures
    1. Option GPU_COMPUTE_VER exposed from both Makefile and CMakeLists.txt
    2. updater_gpu documentation updated accordingly
    
    * Re-introduced GPU_COMPUTE_VER option in the cmake flow.
    This seems to fix the compile-time, rdc=true and copy-constructor related
    errors seen and discussed in PR #2390.
    [GPU-Plugin] Multi-GPU for grow_gpu_hist histogram method using NVIDIA NCCL. (#2395)
    [jvm-packages] Minor improvements to the CMake build (#2379)
    
    * [jvm-packages] Fixed JNI_OnLoad overload
    
    It does not compile on Windows without proper export flags.
    
    * [jvm-packages] Use JNI types directly where appropriate
    
    * Removed lib hack from CMake build
    
    Prior to this commit the CMake build use hardcoded lib prefix for
    libxgboost and libxgboost4j. Unfortunatelly this did not play well with
    Windows, which does not use the lib- prefix.
    [GPU-Plugin] Integration of a faster version of grow_gpu plugin into mainstream (#2360)
    
    * Integrating a faster version of grow_gpu plugin
    1. Removed the older files to reduce duplication
    2. Moved all of the grow_gpu files under 'exact' folder
    3. All of them are inside 'exact' namespace to avoid any conflicts
    4. Fixed a bug in benchmark.py while running only 'grow_gpu' plugin
    5. Added cub and googletest submodules to ease integration and unit-testing
    6. Updates to CMakeLists.txt to directly build cuda objects into libxgboost
    
    * Added support for building gpu plugins through make flow
    1. updated makefile and config.mk to add right targets
    2. added unit-tests for gpu exact plugin code
    
    * 1. Added support for building gpu plugin using 'make' flow as well
    2. Updated instructions for building and testing gpu plugin
    
    * Fix travis-ci errors for PR#2360
    1. lint errors on unit-tests
    2. removed googletest, instead depended upon dmlc-core provide gtest cache
    
    * Some more fixes to travis-ci lint failures PR#2360
    
    * Added Rory's copyrights to the files containing code from both.
    
    * updated copyright statement as per Rory's request
    
    * moved the static datasets into a script to generate them at runtime
    
    * 1. memory usage print when silent=0
    2. tests/ and test/ folder organization
    3. removal of the dependency of googletest for just building xgboost
    4. coding style updates for .cuh as well
    
    * Fixes for compilation warnings
    
    * add cuda object files as well when JVM_BINDINGS=ON
    libxgboost4j is now part of the CMake build (#2373)
    
    * [jvm-packages] Added libxgboost4j to CMake build
    
    * [jvm-packages] Wired CMake build into create_jni.sh
    
    * User newer CMake version on Travis
    
    * Lowered CMake version constraints
    
    * Fixed various quirks in the new CMake build
    [GPU Plugin] Fast histogram speed improvements. Updated benchmarks. (#2258)
    [GPU-Plugin] (#2227)
    
    * Add fast histogram algorithm
    * Fix Linux build
    * Add 'gpu_id' parameter
    Fix cmake build for linux. Update GPU benchmarks. (#1904)
    Add benchmarks, fix GCC build (#1848)
    GPU plug-in improvements + basic Windows continuous integration (#1752)
    
    * GPU Plugin: Reduce memory, improve performance, fix gcc compiler bug, add
    out of memory exceptions
    
    * Add basic Windows continuous integration for cmake VS2013, VS2015
    Add GPU accelerated tree construction plugin (#1679)
    fix the problem that there is no libxgboost.dll (#1674)
    
    fix the problem that there is no libxgboost.dll built with Visual Studio.
    Fix a bug to handle Executable and Library with same name (xgboost) correctly. (#1669)
    
    add_library(libxgboost SHARED ${SOURCES}) builds a library named
    liblibxgboost.so; However, simply changing it to add_library(xgboost ...)
    won't work, as add_executable(xgboost ...) and add_library(xgbboost ...)
    will then have the same target name. This patch correctly handles the
    same-name situation through SET_TARGET_PROPERTIES.
    MS Visual Studio 2015 fix (#1530)
    
    Fixed to work with future versions of visual studio i.e., 2015
    
    MSVC has it's own section for setting compile parameters, it shouldn't need to fall into section below i.e., checking for c++11 as this is definitely already supported, though this isn't an issue for Visual Studio 2012, it breaks for later versions
    of visual studio i.e., 2015 when the default c++ is version 14.  Though still backward compatible with c++11
    Check for visual studio 12.0 and newer for c++11 support (#1330)
    cmake build system (#1314)
    
    * Changed c api to compile under MSVC
    
    * Include functional.h header for MSVC
    
    * Add cmake build
    Add release note for 1.2.0 in NEWS.md (#6063)
    
    * Update query_contributors.py to account for pagination
    
    * Add the release note for 1.2.0
    
    * Add release note for patch releases
    
    * Apply suggestions from code review
    
    * Fix typo
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Co-authored-by: John Zedlewski <904524+JohnZed@users.noreply.github.com>
    Add release note for 1.1.0 in NEWS.md (#5763)
    
    * Add release note for 1.1.0 in NEWS.md
    
    * Address reviewer's feedback
    Add release note for 1.0.0 in NEWS.md (#5329)
    
    * Add release note for 1.0.0
    
    * Fix a small bug in the Python script that compiles the list of contributors
    
    * Clarify governance of CI infrastructure; now PMC is formally in charge
    
    * Address reviewer comment
    
    * Fix typo
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    Release 0.82 (#4201)
    Document GPU objectives in NEWS. (#3865)
    Add another contributor for rabit update
    Release 0.81 version (#3864)
    
    * Release 0.81 version
    
    * Update NEWS.md
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Release version 0.80 (#3541)
    
    * Up versions
    
    * Write release note for 0.80
    Document 0.72.1 version (#3458)
    Release version 0.72 (#3337)
    Release version 0.71 (#3200)
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Tag version 0.7 (#2991)
    
    Document all changes made in year 2017
    Tag version 0.7 (#2975)
    
    * Tag version 0.7
    
    * Document all changes made in year 2016
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    Sklearn kwargs (#2338)
    
    * Added kwargs support for Sklearn API
    
    * Updated NEWS and CONTRIBUTORS
    
    * Fixed CONTRIBUTORS.md
    
    * Added clarification of **kwargs and test for proper usage
    
    * Fixed lint error
    
    * Fixed more lint errors and clf assigned but never used
    
    * Fixed more lint errors
    
    * Fixed more lint errors
    
    * Fixed issue with changes from different branch bleeding over
    
    * Fixed issue with changes from other branch bleeding over
    
    * Added note that kwargs may not be compatible with Sklearn
    
    * Fixed linting on kwargs note
    adding feature contributions to R and gblinear (#2295)
    
    * [gblinear] add features contribution prediction; fix DumpModel bug
    
    * [gbtree] minor changes to PredContrib
    
    * [R] add feature contribution prediction to R
    
    * [R] bump up version; update NEWS
    
    * [gblinear] fix the base_margin issue; fixes #1969
    
    * [R] list of matrices as output of multiclass feature contributions
    
    * [gblinear] make order of DumpModel coefficients consistent: group index changes the fastest
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Automatically remove nan from input data when it is sparse. (#2062)
    
    * [DATALoad] Automatically remove Nan when load from sparse matrix
    
    * add log
    [CORE] Refactor cache mechanism (#1540)
    Tag version 0.6 (#1422)
    Update NEWS.md
    [METHOD], add tree method option to prefer faster algo
    [DOC-JVM] Refactor JVM docs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    [docs] Fix typo in release notes
    
    small typo fix
    thanks
    [DOC] cleanup distributed training
    [LOG] Simplfy README.md add change logs.
    Update CHANGES.md
    Update CHANGES.md
    Added recent changes
    Update CHANGES.md
    Added recent changes
    Reformat CHANGES.md
    Added CV early stopping to CHANGES
    Create CHANGES.md
    Added tests for additional params in sklearn wrapper (+1 squashed commit)
    Squashed commits:
    [43892b9] Added tests for additional params in sklearn wrapper
    Update CHANGES.md
    Updated Changes
    update with comments on PR #450, fixed styles and updated CHANGES and CONTRIBUTORS
    ENH: Add visualization to python package
    quick fix of solaris problem in cranc check
    Update CHANGES.md
    0.4
    Update CHANGES.md
    update note
    Update CHANGES.md
    ok
    ok
    add change note
    add log
    Add link to XGBoost's Twitter handle (#6244)
    Consistent style for build status badge (#6203)
    Update README.md (#5346)
    Add Optuna badge to README.md (#5208)
    
    * Update README.md
    Mention dask in readme. [skip ci] (#4942)
    Update README.md (#4940)
    Mention Kubernetes on README (#4939)
    remove is_bootstrap parameter (#102)
    
    * apply openmp simd
    
    * clean __buildin detection, moving windows build check from xgboost project, add openmp support for vectorize reduce
    
    * apply openmp only to rabit
    
    * orgnize rabit signature
    
    * remove is_bootstrap, use load_checkpoint as implict flag
    
    * visual studio don't support latest openmp
    
    * orgnize omp declarations
    
    * replace memory copy with vector cast
    
    * Revert "replace memory copy with vector cast"
    
    This reverts commit 28de4792dcdff40d83d458510d23b7ef0b191d79.
    
    * Revert "orgnize omp declarations"
    
    This reverts commit 31341233d31ce93ccf34d700262b1f3f6690bbfe.
    
    * remove openmp settings, merge into a upcoming pr
    
    * mis
    
    * per feedback, update comments
    Added travis logo (#4344)
    Simplify README page (#4254)
    Activating Open Collective (#4244)
    
    * Added backers and sponsors on the README
    
    * Re-arrange sections
    
    * Resize AWS logo
    Add sponsors (#4222)
    Add Jenkins status badge (#4090)
    Update README.md
    [DOCS] Update link to readme (#3437)
    [GPU-Plugin] (#2227)
    
    * Add fast histogram algorithm
    * Fix Linux build
    * Add 'gpu_id' parameter
    change contribution link to open issues (#1834)
    Fixing a few typos (#1771)
    
    * Fixing a few typos
    
    * Fixing a few typos
    Add appveyor badge
    Broken Link in README (#1275)
    redirects funding info to UW page
    Update README.md
    Update README.md
    update year in LICENSE, conf.py and README.md files
    
    I found that year in files is not up-to-date
    Update README.md
    fix link
    Add Reference
    Update README.md
    [DIST] Add Distributed XGBoost on AWS Tutorial
    [DOC] reorg docs
    Update readme
    [doc] update news
    [DOC] Update R doc
    [DOC] cleanup distributed training
    [LOG] Simplfy README.md add change logs.
    [REFACTOR] cleanup structure
    Added Apache License badge
    Update README.md
    Update README.md
    Added PyPI badge to README
    Squashed 'subtree/rabit/' changes from fa99857..e81a11d
    
    e81a11d Merge pull request #25 from daiyl0320/master
    35c3b37 add retry mechanism to ConnectTracker and modify Listen backlog to 128 in rabit_traker.py
    c71ed6f try deply doxygen
    62e5647 try deply doxygen
    732f1c6 try
    2fa6e02 ok
    0537665 minor
    7b59dcb minor
    5934950 new doc
    f538187 ok
    44b6049 new doc
    387339b add more
    9d4397a chg
    2879a48 chg
    30e3110 ok
    9ff0301 add link translation
    6b629c2 k
    32e1955 ok
    8f4839d fix
    93137b2 ok
    7eeeb79 reload recommonmark
    a8f00cc minor
    19b0f01 ok
    dd01184 minor
    c1cdc19 minor
    fcf0f43 try rst
    cbc21ae try
    62ddfa7 tiny
    aefc05c final change
    2aee9b4 minor
    fe4e7c2 ok
    8001983 change to subtitle
    5ca33e4 ok
    88f7d24 update guide
    29d43ab add code
    fe8bb3b minor hack for readthedocs
    229c71d Merge branch 'master' of ssh://github.com/dmlc/rabit
    7424218 ok
    d1d45bb Update README.md
    1e8813f Update README.md
    1ccc990 Update README.md
    0323e06 remove readme
    679a835 remove theme
    7ea5b7c remove numpydoc to napoleon
    b73e2be Merge branch 'master' of ssh://github.com/dmlc/rabit
    1742283 ok
    1838e25 Update python-requirements.txt
    bc4e957 ok
    fba6fc2 ok
    0251101 ok
    d50b905 ok
    d4f2509 ok
    cdf401a ok
    fef0ef2 new doc
    cef360d ok
    c125d2a ok
    270a49e add requirments
    744f901 get the basic doc
    1cb5cad Merge branch 'master' of ssh://github.com/dmlc/rabit
    8cc07ba minor
    d74f126 Update .travis.yml
    52b3dcd Update .travis.yml
    099581b Update .travis.yml
    1258046 Update .travis.yml
    7addac9 Update Makefile
    0ea7adf Update .travis.yml
    f858856 Update travis_script.sh
    d8eac4a Update README.md
    3cc49ad lint and travis
    ceedf4e fix
    fd8920c fix win32
    8bbed35 modify
    9520b90 Merge pull request #14 from dmlc/hjk41
    df14bb1 fix type
    f441dc7 replace tab with blankspace
    2467942 remove unnecessary include
    181ef47 defined long long and ulonglong
    1582180 use int32_t to define int and int64_t to define long. in VC long is 32bit
    e0b7da0 fix
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: e81a11dd7ee3cff87a38a42901315821df018bae
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Adding dmlc stamp
    align logo with title
    Update README.md
    Document refactor
    
    change badge
    enable basic sphinx doc
    fixing broken basic_walkthrough links
    adding DMLC back to the title
    some more links and restructuring
    dropping raw graphlab url
    ok
    Update README.md
    Update README.md
    Update README.md
    some more changes to remove redundant information
    restructuring the README with an index
    Update README.md
    moving gitter chat up
    Check out vs. checkout
    
    Made it consistent across the README
    Fixed a few typos in README
    ok
    add list of contributors
    Update README.md
    Update README.md
    Update README.md
    refs and formatting changes
    Update README.md
    Update README.md
    update script
    Update README.md
    add travis conf, waiting for setting on travis-ci.org
    Update README.md
    Updated grammar for the README.md
    Update README.md
    0.4
    Update README.md
    add build instruction to doc
    Update README.md
    Update README.md
    Update README.md
    Added Gitter badge
    Update README.md
    Update README.md
    Update README.md
    
    Ensures OpenMP support
    Update README.md
    add highlights
    chg docs
    ok
    move documentation to repo
    chg docs
    Update README.md
    Redo readme modification
    Cancel readme modif
    Add slides to readme + group documentation together
    Squashed 'subtree/rabit/' changes from 50a66b3..18f4d6c
    
    18f4d6c remove rabit learn
    bcfbe51 fix dmlc io
    ad383b0 ok
    3b8c04a Merge branch 'master' of ssh://github.com/dmlc/rabit
    9dd97cc keepup with dmlc core
    ef13aaf ch
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: 18f4d6c0ba6c616fe1bf245938bdfc730180de4e
    remove rabit learn
    fix
    move distributed xgboost to wormhole
    add instruction to build with s3
    Update README.md
    add another solution to os x
    Update README.md
    add tuto to the README
    README
    update
    Squashed 'subtree/rabit/' changes from d4ec037..28ca7be
    
    28ca7be add linear readme
    ca4b20f add linear readme
    1133628 add linear readme
    6a11676 update docs
    a607047 Update build.sh
    2c1cfd8 complete yarn
    4f28e32 change formater
    2fbda81 fix stdin input
    3258bcf checkin yarn master
    67ebf81 allow setup from env variables
    9b6bf57 fix hdfs
    395d5c2 add make system
    88ce767 refactor io, initial hdfs file access need test
    19be870 chgs
    a1bd3c6 Merge branch 'master' of ssh://github.com/tqchen/rabit
    1a573f9 introduce input split
    29476f1 fix timer issue
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: 28ca7becbdf6503e6b1398588a969efb164c9701
    update docs
    Update README.md
    Update README.md
    Squashed 'subtree/rabit/' changes from 4ebe657..fb13cab
    
    fb13cab change makefile
    1479e37 fixed small bug in mpi submission script
    0ca7a63 Update README.md
    5ef4830 ok
    93a1338 chg note
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: fb13cab216b795f86dc90547b71c0f730766affa
    Update README.md
    ok
    chg note
    minor fix
    update note
    changes
    change R build script
    Squashed 'subtree/rabit/' content from commit c7282ac
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: c7282acb2a92e1d6a32614889ff466fec58937f3
    add languages
    chg
    desgin goal
    ok
    change toolkit to rabitlearn
    cosmetic changes to tutorial
    minor
    correct
    add api
    fix link
    Update readme with new win on Kaggle
    change usage
    cosmetic
    cosmetic changes
    chg robust to reliable
    chg interface
    before make rabit public
    change note
    change notes
    ok
    ok
    ok
    chg readme
    Update README.md
    Update README.md
    Update README.md
    fresh name fresh start
    Fixed README
    Added OS X OpenMP instructions
    initial version of allreduce
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    add auto build script
    same version
    
    reset changes
    make it cleaner
    OK
    add what is new
    change readme
    Update README.md
    Parallel execution of CV plus double inputted model
    ok
    Update README.md
    new theory: predict from cv + parametric rounds
    format README
    README
    set NFold CV from cmd args
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    link glc
    add glc comment
    ok
    ok
    add change note
    add log
    clean up
    add
    add
    add message about glc
    add base_margin
    add more note
    chg readme
    remake the wrapper
    check in io module
    modify readme
    chg readme
    start unity refactor
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    chg
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    ok
    add bing to author list
    ok
    update regression
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    chg license, README
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    finish readme
    add linear booster
    add ok
    sync everything
    update this folder
    update this folder
    Initial commit
    [jvm-packages] Fix up build for xgboost4j-gpu, xgboost4j-spark-gpu (#6216)
    
    * [CI] Clean up build for JVM packages
    
    * Use correct path for saving native lib
    
    * Fix groupId of maven-surefire-plugin
    
    * Fix stashing of xgboost4j_jar_gpu
    
    * [CI] Don't run xgboost4j-tester with GPU, since it doesn't use gpu_hist
    Move non-OpenMP gtest to GitHub Actions (#6210)
    [CI] Fix Docker build for CUDA 11 (#6202)
    [CI] Fix CTest by running it in a correct directory (#6104)
    
    * [CI] Fix CTest by running it in a correct directory
    
    * [CI] Do not run dmlc-core unit tests with sanitizer
    Refactor rabit tests (#6096)
    
    
    * Merge rabit tests into XGBoost.
    * Run them On CI.
    * Simplification for CMake scripts.
    [CI] Port CI fixes from the 1.2.0 branch (#6050)
    
    * Fix a unit test on CLI, to handle RC versions
    
    * [CI] Use mgpu machine to run gpu hist unit tests
    
    * [CI] Build GPU-enabled JAR artifact and deploy to xgboost-maven-repo
    [CI] Migrate linters to GitHub Actions (#6035)
    
    * [CI] Move lint to GitHub Actions
    
    * [CI] Move Doxygen to GitHub Actions
    
    * [CI] Move Sphinx build test to GitHub Actions
    
    * [CI] Reduce workload for Windows R tests
    
    * [CI] Move clang-tidy to Build stage
    [CI] Cancel builds on subsequent pushes (#6011)
    
    * [CI] Cancel builds on subsequent pushes
    
    * Use a more secure method
    
    * test commit
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    [CI] Assign larger /dev/shm to NCCL (#5966)
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    [CI] Fix broken Docker container 'cpu' (#5956)
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Support building XGBoost with CUDA 11 (#5808)
    
    * Change serialization test.
    * Add CUDA 11 tests on Linux CI.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Further improvements and savings in Jenkins pipeline (#5904)
    
    * Publish artifacts only on the master and release branches
    
    * Build CUDA only for Compute Capability 7.5 when building PRs
    
    * Run all Windows jobs in a single worker image
    
    * Build nightly XGBoost4J SNAPSHOT JARs with Scala 2.12 only
    
    * Show skipped Python tests on Windows
    
    * Make Graphviz optional for Python tests
    
    * Add back C++ tests
    
    * Unstash xgboost_cpp_tests
    
    * Fix label to CUDA 10.1
    
    * Install cuPy for CUDA 10.1
    
    * Install jsonschema
    
    * Address reviewer's feedback
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    [CI] update spark version to 3.0.0 (#5890)
    
    * [CI] update spark version to 3.0.0
    
    * Update Dockerfile.jvm_cross
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Enforce daily budget in Jenkins CI (#5884)
    
    * [CI] Throttle Jenkins CI
    
    * Don't use Jenkins master instance
    [CI] Simplify CMake build with modern CMake techniques (#5871)
    
    * [CI] Simplify CMake build
    
    * Make sure that plugins can be built
    
    * [CI] Install lz4 on Mac
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    [CI] Fix cuDF install; merge 'gpu' and 'cudf' test suite (#5814)
    [CI] Remove CUDA 9.0 from CI (#5745)
    Require Python 3.6+; drop Python 3.5 from CI (#5715)
    Upgrade to CUDA 10.0 (#5649) (#5652)
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)
    
    * [CI] Use Vault repository to re-gain access to devtoolset-4
    
    * Use manylinux2010 tag
    
    * Update Dockerfile.jvm
    
    * Fix rename_whl.py
    
    * Upgrade Pip, to handle manylinux2010 tag
    
    * Update insert_vcomp140.py
    
    * Update test_python.sh
    Fix non-openmp build. (#5566)
    
    
    * Add test to Jenkins.
    * Fix threading utils tests.
    * Require thread library.
    Fix CLI model IO. (#5535)
    
    
    * Add test for comparing Python and CLI training result.
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)
    Ensure that configured dmlc/build_config.h is picked up by Rabit and XGBoost (#5514)
    
    * Ensure that configured header (build_config.h) from dmlc-core is picked up by Rabit and XGBoost
    
    * Check which Rabit target is being used
    
    * Use CMake 3.13 in all Jenkins tests
    
    * Upgrade CMake in Travis CI
    
    * Install CMake using Kitware installer
    
    * Remove existing CMake (3.12.4)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Enable rabit test (#5358)" (#5377)
    
    This reverts commit 9a5efffebe9e527ca4dc27c281b06c221596f860.
    Enable rabit test (#5358)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    Add CMake option to run Undefined Behavior Sanitizer (UBSan) (#5211)
    
    * Fix related errors.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    [CI] Upload master branch artifacts to S3 root [skip ci] (#4979)
    [CI] Upload nightly builds to S3 (#4976)
    
    * Do not store built artifacts in the Jenkins master
    
    * Add wheel renaming script
    
    * Upload wheels to S3 bucket
    
    * Use env.GIT_COMMIT
    
    * Capture git hash correctly
    
    * Add missing import in Jenkinsfile
    
    * Address reviewer's comments
    
    * Put artifacts for pull requests in separate directory
    
    * No wildcard expansion in Windows CMD
    [CI] Run cuDF tests in Jenkins CI server (#4927)
    [CI] Raise timeout threshold in Jenkins (#4938)
    Use bundled gtest (#4900)
    
    * Suggest to use gtest bundled with dmlc
    
    * Use dmlc bundled gtest in all CI scripts
    
    * Make clang-tidy to use dmlc embedded gtest
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    [CI] Specify account ID when logging into ECR Docker registry (#4584)
    
    * [CI] Specify account ID when logging into ECR Docker registry
    
    * Do not display awscli login command
    [CI] Remove CUDA 8.0 from CI pipeline (#4580)
    Add support for cross-validation using query ID (#4474)
    
    * adding support for matrix slicing with query ID for cross-validation
    
    * hail mary test of unrar installation for windows tests
    
    * trying to modify tests to run in Github CI
    
    * Remove dependency on wget and unrar
    
    * Save error log from R test
    
    * Relax assertion in test_training
    
    * Use int instead of bool in C function interface
    
    * Revise R interface
    
    * Add XGDMatrixSliceDMatrixEx and keep old XGDMatrixSliceDMatrix for API compatibility
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    add cuda 10.1 support (#4468)
    [CI] Add Windows GPU to Jenkins CI pipeline (#4463)
    
    * Fix #4462: Use /MT flag consistently for MSVC target
    
    * First attempt at Windows CI
    
    * Distinguish stages in Linux and Windows pipelines
    
    * Try running CMake in Windows pipeline
    
    * Add build step
    [CI] Build XGBoost wheels with CUDA 9.0 (#4459)
    
    * [CI] Build XGBoost wheels with CUDA 9.0
    
    * Do not call archiveArtifacts for 8.0 wheel
    [BLOCKING][CI] Upgrade to Spark 2.4.3 (#4414)
    
    * [CI] Upgrade to Spark 2.4.2
    
    * Pass Spark version to build script
    
    * Allow multiple --build-arg in ci_build.sh
    
    * Fix syntax
    
    * Fix container name
    
    * Update pom.xml
    
    * Fix container name
    
    * Update Jenkinsfile
    
    * Update pom.xml
    
    * Update Dockerfile.jvm_cross
    Make CMakeLists.txt compatible with CMake 3.3 (#4420)
    
    * Make CMakeLists.txt compatible with CMake 3.3; require CMake 3.11 for MSVC
    
    * Use CMake 3.12 when sanitizer is enabled
    
    * Disable funroll-loops for MSVC
    
    * Use cmake version in container name
    
    * Add missing arg
    
    * Fix egrep use in ci_build.sh
    
    * Display CMake version
    
    * Do not set OpenMP_CXX_LIBRARIES for MSVC
    
    * Use cmake_minimum_required()
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [CI] Add external Docker build cache (#4331)
    jenkins build for cuda 10.0 (#4281)
    
    * jenkins build for cuda 10.0
    
    * yum install nccl2 for cuda 10.0
    Broken link for NCCL: cannot use CUDA 10.1 (#4232)
    support cuda 10.1 (#4223)
    
    * support cuda 10.1
    
    * add cuda 10.1 to jenkins build matrix
    Perform clang-tidy on both cpp and cuda source. (#4034)
    
    * Basic script for using compilation database.
    
    * Add `GENERATE_COMPILATION_DATABASE' to CMake.
    * Rearrange CMakeLists.txt.
    * Add basic python clang-tidy script.
    * Remove modernize-use-auto.
    * Add clang-tidy to Jenkins
    * Refine logic for correct path detection
    
    In Jenkins, the project root is of form /home/ubuntu/workspace/xgboost_PR-XXXX
    
    * Run clang-tidy in CUDA 9.2 container
    * Use clang_tidy container
    Disable retries in Jenkins CI, since we're now using On-Demand instances instead of Spot (#3948)
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Test wheels on CUDA 10.0 container for compatibility (#3838)
    Retry Jenkins CI tests up to 3 times to improve reliability (redux) (#3775)
    Retry Jenkins CI tests up to 3 times to improve reliability (#3769)
    Test wheel compatibility on CPU containers, for all pull requests (#3762)
    
    * Test wheel compatibility on CPU containers, for all pull requests
    
    * Run wheel test only when multi-GPU flag is not set
    Add multi-GPU unit test environment (#3741)
    
    * Add multi-GPU unit test environment
    
    * Better assertion message
    
    * Temporarily disable failing test
    
    * Distinguish between multi-GPU and single-GPU CPP tests
    
    * Consolidate Python tests. Use attributes to distinguish multi-GPU Python tests from single-CPU counterparts
    Separate out restricted and unrestricted tasks (#3736)
    [BLOCKING] Adding JVM doc build to Jenkins CI (#3567)
    
    * Adding Java/Scala doc build to Jenkins CI
    
    * Deploy built doc to S3 bucket
    
    * Build doc only for branches
    
    * Build doc first, to get doc faster for branch updates
    
    * Have ReadTheDocs download doc tarball from S3
    
    * Update JVM doc links
    
    * Put doc build commands in a script
    
    * Specify Spark 2.3+ requirement for XGBoost4J-Spark
    
    * Build GPU wheel without NCCL, to reduce binary size
    Upgrade cuda version to 9.2 for CI workflows (#3460)
    
    - Needed by the issue #3404
     - as v9.1 doesn't have a nccl2 release
    Build universal wheels using GPU CI (#3424)
    Cleanup old artefacts in Jenkins (#3361)
    Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container. (#3309)
    
    * Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container.
    
    * Add Google test
    Update Jenkins CI for GPU (#3294)
    [BUILD] Dockerfile and Jenkinsfile revisited (#2514)
    
    Includes:
      - Dockerfile changes
        - Dockerfile clean up
        - Fix execution privileges of files used from Dockerfile.
        - New Dockerfile entrypoint to replace with_user script
        - Defined a placeholders for CPU testing (script and Dockerfile)
      - Jenkinsfile
        - Jenkins file milestone defined
        - Single source code checkout and propagation via stash/unstash
        - Bash needs to be explicitly used in launching make build, since we need
    access to environment
        - Jenkinsfile build factory for cmake and make style of jobs
        - Archivation of artifacts (*.so, *.whl, *.egg) produced by cmake build
    
    Missing:
      - CPU testing
      - Python3 env build and testing
    [GPU-Plugin] Add basic continuous integration for GPU plugin. (#2431)
    [CI] Cancel builds on subsequent pushes (#6011)
    
    * [CI] Cancel builds on subsequent pushes
    
    * Use a more secure method
    
    * test commit
    [CI] Fix broken Docker container 'cpu' (#5956)
    Further improvements and savings in Jenkins pipeline (#5904)
    
    * Publish artifacts only on the master and release branches
    
    * Build CUDA only for Compute Capability 7.5 when building PRs
    
    * Run all Windows jobs in a single worker image
    
    * Build nightly XGBoost4J SNAPSHOT JARs with Scala 2.12 only
    
    * Show skipped Python tests on Windows
    
    * Make Graphviz optional for Python tests
    
    * Add back C++ tests
    
    * Unstash xgboost_cpp_tests
    
    * Fix label to CUDA 10.1
    
    * Install cuPy for CUDA 10.1
    
    * Install jsonschema
    
    * Address reviewer's feedback
    [CI] Reduce load on Windows CI pipeline (#5892)
    [CI] Enforce daily budget in Jenkins CI (#5884)
    
    * [CI] Throttle Jenkins CI
    
    * Don't use Jenkins master instance
    Add cupy to Windows CI (#5797)
    
    * Add cupy to Windows CI
    
    * Update Jenkinsfile-win64
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Update Jenkinsfile-win64
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Update tests/python-gpu/test_gpu_prediction.py
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    [CI] Remove CUDA 9.0 from Windows CI. (#5674)
    
    * Remove CUDA 9.0 on Windows CI.
    
    * Require cuda10 tag, to differentiate
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix skl nan tag. (#5538)
    Fix CLI model IO. (#5535)
    
    
    * Add test for comparing Python and CLI training result.
    [CI] Upload master branch artifacts to S3 root [skip ci] (#4979)
    [CI] Upload nightly builds to S3 (#4976)
    
    * Do not store built artifacts in the Jenkins master
    
    * Add wheel renaming script
    
    * Upload wheels to S3 bucket
    
    * Use env.GIT_COMMIT
    
    * Capture git hash correctly
    
    * Add missing import in Jenkinsfile
    
    * Address reviewer's comments
    
    * Put artifacts for pull requests in separate directory
    
    * No wildcard expansion in Windows CMD
    [CI] Add Python and C++ tests for Windows GPU target (#4469)
    
    * Add CMake option to use bundled gtest from dmlc-core, so that it is easy to build XGBoost with gtest on Windows
    
    * Consistently apply OpenMP flag to all targets. Force enable OpenMP when USE_CUDA is turned on.
    
    * Insert vcomp140.dll into Windows wheels
    
    * Add C++ and Python tests for CPU and GPU targets (CUDA 9.0, 10.0, 10.1)
    
    * Prevent spurious msbuild failure
    
    * Add GPU tests
    
    * Upgrade dmlc-core
    [CI] Add Windows GPU to Jenkins CI pipeline (#4463)
    
    * Fix #4462: Use /MT flag consistently for MSVC target
    
    * First attempt at Windows CI
    
    * Distinguish stages in Linux and Windows pipelines
    
    * Try running CMake in Windows pipeline
    
    * Add build step
    Remove R and JVM from appveyor. (#5922)
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    [R] Remove dependency on gendef for Visual Studio builds (fixes #5608) (#5764)
    
    * [R-package] Remove dependency on gendef for Visual Studio builds (fixes #5608)
    
    * clarify docs
    
    * removed debugging print statement
    
    * Make R CMake install more robust
    
    * Fix doc format; add ToC
    
    * Update build.rst
    
    * Fix AppVeyor
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Remove VC-2013 support. (#4701)
    
    * Removing it as it is not fully c++11 compliance.
    [R] Fix CRAN error for Mac OS X (#4672)
    
    * fix cran error for mac os x
    
    * ignore float on windows check for now
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [CI] Fix Windows tests (#4403)
    
    * Install binary igraph
    
    * Include Graphviz in PATH
    Fix test_gpu_coordinate. (#3974)
    
    * Fix test_gpu_coordinate.
    
    * Use `gpu_coord_descent` in test.
    * Reduce number of running rounds.
    
    * Remove nthread.
    
    * Use githubusercontent for r-appveyor.
    
    * Use githubusercontent in travis r tests.
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Dynamically allocate GPU histogram memory (#3519)
    
    * Expand histogram memory dynamically to prevent large allocations for large tree depths (e.g. > 15)
    
    * Remove GPU memory allocation messages. These are misleading as a large number of allocations are now dynamic.
    
    * Fix appveyor R test
    Updates for GPU CI tests (#3467)
    
    * Fail GPU CI after test failure
    
    * Fix GPU linear tests
    
    * Reduced number of GPU tests to speed up CI
    
    * Remove static allocations of device memory
    
    * Resolve illegal memory access for updater_fast_hist.cc
    
    * Fix broken r tests dependency
    
    * Update python install documentation for GPU
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Fix RMinGW build error: dependency 'data.table' not available (#3257)
    
    The R package dependency 'data.table' is apparently unavailable in Windows binary format, resulting into the following build errors:
    * https://ci.appveyor.com/project/tqchen/xgboost/build/1.0.1810/job/hhanvg0c2cqpn7bc
    * https://ci.appveyor.com/project/tqchen/xgboost/build/1.0.1811/job/hg65t9wb3rt1f5k8
    
    Fix: use type='both' to fall back to source when binary is unavailable
    change cmd to cmd.exe in appveyor (#3071)
    [R] fix for the 32 bit windows issue (#2994)
    
    * [R] disable thred_local for 32bit windows
    
    * [R] require C++11 and GNU make in DESCRIPTION
    
    * [R] enable 32+64 build and check in appveyor
    [Python] AppVeyor CI for Python wheel package (#2941)
    
    * Build python wheel artifacts for Windows
    
    * Remove Win32 target
    [R] AppVeyor CI for R package (#2954)
    
    * [R] fix finding R.exe with cmake on WIN when it is in PATH
    
    * [R] appveyor config for R package
    
    * [R] wrap the lines to make R check happier
    
    * [R] install only binary dep-packages in appveyor
    
    * [R] for MSVC appveyor, also build a binary for R package and keep as an artifact
    MinGW: shared library prefix and appveyor CI (#2539)
    
    * for MinGW, drop the 'lib' prefix from shared library name
    
    * fix defines for 'g++ 4.8 or higher' to include g++ >= 5
    
    * fix compile warnings
    
    * [Appveyor] add MinGW with python; remove redundant jobs
    
    * [Appveyor] also do python build for one of msvc jobs
    Cmake improvements (#2487)
    
    * Cmake improvements
    * Add google test to cmake
    [jvm-packages] Test xgboost4j on Windows (#2451)
    [jvm-packages] Another pack of build/CI improvements (#2422)
    
    * [jvm-packages] Fixed compilation on Windows
    
    * [jvm-packages] Build the JNI bindings on Appveyor
    
    * [jvm-packages] Build & test on OS X
    
    * [jvm-packages] Re-applied the CMake build changes reverted by #2395
    
    * Fixed Appveyor JVM build
    
    * Muted Maven on Travis
    
    * Don't link with libawt
    
    * "linux2"->"linux"
    
    Python2.x and 3.X use slightly different values for ``sys.platform``.
    GPU plug-in improvements + basic Windows continuous integration (#1752)
    
    * GPU Plugin: Reduce memory, improve performance, fix gcc compiler bug, add
    out of memory exceptions
    
    * Add basic Windows continuous integration for cmake VS2013, VS2015
    [REFACTOR] cleanup structure
    not working
    sleep
    give up for now
    checkin debug
    giveup for now, appveyor do not support openmp for msvc yet allow openmp to switch on
    use debug
    final
    final
    try disable omp
    ok
    ok
    incomplete appveyor
    ok
    ok
    ok
    finl
    ok
    ok
    ok
    rest
    rest
    update appvegor
    rename
    ok
    ok
    add appvegor
    Rename Ant Financial to Ant Group (#5827)
    Document addition of new committer @SmirnovEgorRu (#5762)
    Update affiliation of @hcho3 (#5292)
    add os.PathLike support for file paths to DMatrix and Booster Python classes (#4757)
    Update CONTRIBUTORS.md
    Simplify INI-style config reader using C++11 STL (#4478)
    
    * simplify the config.h file
    
    * revise config.h
    
    * revised config.h
    
    * revise format
    
    * revise format issues
    
    * revise whitespace issues
    
    * revise whitespace namespace format issues
    
    * revise namespace format issues
    
    * format issues
    
    * format issues
    
    * format issues
    
    * format issues
    
    * Revert submodule changes
    
    * minor change
    
    * Update src/common/config.h
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * address format issue from trivialfis
    
    * Use correct cub submodule
    Add support for cross-validation using query ID (#4474)
    
    * adding support for matrix slicing with query ID for cross-validation
    
    * hail mary test of unrar installation for windows tests
    
    * trying to modify tests to run in Github CI
    
    * Remove dependency on wget and unrar
    
    * Save error log from R test
    
    * Relax assertion in test_training
    
    * Use int instead of bool in C function interface
    
    * Revise R interface
    
    * Add XGDMatrixSliceDMatrixEx and keep old XGDMatrixSliceDMatrix for API compatibility
    Update CONTRIBUTORS.md (#4335)
    [REVIEW] Enable Multi-Node Multi-GPU functionality (#4095)
    
    * Initial commit to support multi-node multi-gpu xgboost using dask
    
    * Fixed NCCL initialization by not ignoring the opg parameter.
    
    - it now crashes on NCCL initialization, but at least we're attempting it properly
    
    * At the root node, perform a rabit::Allreduce to get initial sum_gradient across workers
    
    * Synchronizing in a couple of more places.
    
    - now the workers don't go down, but just hang
    - no more "wild" values of gradients
    - probably needs syncing in more places
    
    * Added another missing max-allreduce operation inside BuildHistLeftRight
    
    * Removed unnecessary collective operations.
    
    * Simplified rabit::Allreduce() sync of gradient sums.
    
    * Removed unnecessary rabit syncs around ncclAllReduce.
    
    - this improves performance _significantly_ (7x faster for overall training,
      20x faster for xgboost proper)
    
    * pulling in latest xgboost
    
    * removing changes to updater_quantile_hist.cc
    
    * changing use_nccl_opg initialization, removing unnecessary if statements
    
    * added definition for opaque ncclUniqueId struct to properly encapsulate GetUniqueId
    
    * placing struct defintion in guard to avoid duplicate code errors
    
    * addressing linting errors
    
    * removing
    
    * removing additional arguments to AllReduer initialization
    
    * removing distributed flag
    
    * making comm init symmetric
    
    * removing distributed flag
    
    * changing ncclCommInit to support multiple modalities
    
    * fix indenting
    
    * updating ncclCommInitRank block with necessary group calls
    
    * fix indenting
    
    * adding print statement, and updating accessor in vector
    
    * improving print statement to end-line
    
    * generalizing nccl_rank construction using rabit
    
    * assume device_ordinals is the same for every node
    
    * test, assume device_ordinals is identical for all nodes
    
    * test, assume device_ordinals is unique for all nodes
    
    * changing names of offset variable to be more descriptive, editing indenting
    
    * wrapping ncclUniqueId GetUniqueId() and aesthetic changes
    
    * adding synchronization, and tests for distributed
    
    * adding  to tests
    
    * fixing broken #endif
    
    * fixing initialization of gpu histograms, correcting errors in tests
    
    * adding to contributors list
    
    * adding distributed tests to jenkins
    
    * fixing bad path in distributed test
    
    * debugging
    
    * adding kubernetes for distributed tests
    
    * adding proper import for OrderedDict
    
    * adding urllib3==1.22 to address ordered_dict import error
    
    * added sleep to allow workers to save their models for comparison
    
    * adding name to GPU contributors under docs
    Update CONTRIBUTORS.md (#3999)
    update rabit (#3835)
    Update committer list (#3788)
    
    * Update committer list
    
    * Update CONTRIBUTORS.md
    
    * Minor format fix
    [jvm-packages] For training data with group, empty RDD partition threw exception (#3749) (#3750)
    [TREE] add interaction constraints (#3466)
    
    * add interaction constraints
    
    * enable both interaction and monotonic constraints at the same time
    
    * fix lint
    
    * add R test, fix lint, update demo
    
    * Use dmlc::JSONReader to express interaction constraints as nested lists; Use sparse arrays for bookkeeping
    
    * Add Python test for interaction constraints
    
    * make R interaction constraints parameter based on feature index instead of column names, fix R coding style
    
    * Fix lint
    
    * Add BlueTea88 to CONTRIBUTORS.md
    
    * Short circuit when no constraint is specified; address review comments
    
    * Add tutorial for feature interaction constraints
    
    * allow interaction constraints to be passed as string, remove redundant column_names argument
    
    * Fix typo
    
    * Address review comments
    
    * Add comments to Python test
    Add qid like ranklib format (#2749)
    
    * add qid for https://github.com/dmlc/xgboost/issues/2748
    
    * change names
    
    * change spaces
    
    * change qid to bst_uint type
    
    * change qid type to size_t
    
    * change qid first to SIZE_MAX
    
    * change qid type from size_t to uint64_t
    
    * update dmlc-core
    
    * fix qids name error
    
    * fix group_ptr_ error
    
    * Style fix
    
    * Add qid handling logic to SparsePage
    
    * New MetaInfo format + backward compatibility fix
    
    Old MetaInfo format (1.0) doesn't contain qid field. We still want to be able
    to read from MetaInfo files saved in old format. Also, define a new format
    (2.0) that contains the qid field. This way, we can distinguish files that
    contain qid and those that do not.
    
    * Update MetaInfo test
    
    * Simply group assignment logic
    
    * Explicitly set qid=nullptr in NativeDataIter
    
    NativeDataIter's callback does not support qid field. Users of NativeDataIter
    will need to call setGroup() function separately to set group information.
    
    * Save qids_ in SaveBinary()
    
    * Upgrade dmlc-core submodule
    
    * Add a test for reading qid
    
    * Add contributor
    
    * Check the size of qids_
    
    * Document qid format
    Fix tweedie handling of base_score (#3295)
    
    * fix tweedie margin calculations
    
    * add entry to contributors
    Refactor of FastHistMaker to allow for custom regularisation methods (#3335)
    
    * Refactor to allow for custom regularisation methods
    
    * Implement compositional SplitEvaluator framework
    
    * Fixed segfault when no monotone_constraints are supplied.
    
    * Change pid to parentID
    
    * test_monotone_constraints.py now passes
    
    * Refactor ColMaker and DistColMaker to use SplitEvaluator
    
    * Performance optimisation when no monotone_constraints specified
    
    * Fix linter messages
    
    * Fix a few more linter errors
    
    * Update the amalgamation
    
    * Add bounds check
    
    * Add check for leaf node
    
    * Fix linter error in param.h
    
    * Fix clang-tidy errors on CI
    
    * Fix incorrect function name
    
    * Fix clang-tidy error in updater_fast_hist.cc
    
    * Enable SSE2 for Win32 R MinGW
    
    Addresses https://github.com/dmlc/xgboost/pull/3335#issuecomment-400535752
    
    * Add contributor
    GPU binning and compression. (#3319)
    
    * GPU binning and compression.
    
    - binning and index compression are done inside the DeviceShard constructor
    - in case of a DMatrix with multiple row batches, it is first converted into a single row batch
    Release version 0.72 (#3337)
    CRAN Submission for 0.71.1 (#3311)
    
    * fix for CRAN manual checks
    
    * fix for CRAN manual checks
    
    * pass local check
    
    * fix variable naming style
    
    * Adding Philip's record
    Check existance of seed/nthread keys before checking their value. (#2669)
    Update CONTRIBUTORS.md (#2719)
    Update CONTRIBUTORS.md (#2350)
    Sklearn kwargs (#2338)
    
    * Added kwargs support for Sklearn API
    
    * Updated NEWS and CONTRIBUTORS
    
    * Fixed CONTRIBUTORS.md
    
    * Added clarification of **kwargs and test for proper usage
    
    * Fixed lint error
    
    * Fixed more lint errors and clf assigned but never used
    
    * Fixed more lint errors
    
    * Fixed more lint errors
    
    * Fixed issue with changes from different branch bleeding over
    
    * Fixed issue with changes from other branch bleeding over
    
    * Added note that kwargs may not be compatible with Sklearn
    
    * Fixed linting on kwargs note
    [GPU-Plugin] (#2227)
    
    * Add fast histogram algorithm
    * Fix Linux build
    * Add 'gpu_id' parameter
    Update CONTRIBUTORS.md
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [jvm-packages] XGBoost4j Windows fixes (#1639)
    
    * Changes for Mingw64 compilation to ensure long is a consistent size.
    
    Mainly impacts the Java API which would not compile, but there may be
    silent errors on Windows with large datasets before this patch (as long
    is 32-bits when compiled with mingw64 even in 64-bit mode).
    
    * Adding ifdefs to ensure it still compiles on MacOS
    
    * Makefile and create_jni.bat changes for Windows.
    
    * Switching XGDMatrixCreateFromCSREx JNI call to use size_t cast
    
    * Fixing lint error, adding profile switching to jvm-packages build to make create-jni.bat get called, adding myself to Contributors.Md
    Updated - fix merged (#1425)
    
    https://github.com/dmlc/xgboost/pull/1417
    Expose predictLeaf functionality in Scala XGBoostModel (#1351)
    Added contributor
    Update CONTRIBUTORS.md
    grammar/style fixes for "Introduction to Boosted Trees" docs
    Update CONTRIBUTORS.md
    Update CONTRIBUTORS.md
    Update CONTRIBUTORS.md
    [DOC] Add contributor
    Added Johan Manders to the list, asked by Tianqi Chen
    DOC: Updated contributors.md
    Add contributor
    DOC: Updated CONTRIBUTORS.md
    update with comments on PR #450, fixed styles and updated CHANGES and CONTRIBUTORS
    ENH: Add visualization to python package
    Document refactor
    
    change badge
    Update CONTRIBUTORS.md
    update more contributor names
    Update CONTRIBUTORS.md
    Update CONTRIBUTORS.md
    Update CONTRIBUTORS.md
    add list of contributors
    [R] Remove stringi dependency (#6109)
    
    * [R] Fix empty empty tests and a test warnings
    
    * [R] Remove stringi dependency (fix #5905)
    
    * Fix R lint check
    
    * [R] Fix automatic conversion to factor in R < 4.0.0 in xgb.model.dt.tree
    
    * Add `R` Makefile variable
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    C++14 for xgboost (#5664)
    Remove makefiles. (#5513)
    Rewrite setup.py. (#5271)
    
    The setup.py is rewritten.  This new script uses only Python code and provide customized
    implementation of setuptools commands.  This way users can run most of setuptools commands
    just like any other Python libraries.
    
    * Remove setup_pip.py
    * Remove soft links.
    * Define customized commands.
    * Remove shell script.
    * Remove makefile script.
    * Update the doc for building from source.
    Make `pip install xgboost*.tar.gz` work by fixing build-python.sh (#5241)
    
    * Make pip install xgboost*.tar.gz work by fixing build-python.sh
    
    * Simplify install doc
    
    * Add test
    
    * Install Miniconda for Linux target too
    
    * Build XGBoost only once in sdist
    
    * Try importing xgboost after installation
    
    * Don't set PYTHONPATH env var for sdist test
    [R] Enable OpenMP with AppleClang in XGBoost R package (#5240)
    
    * [R] Enable OpenMP with AppleClang in XGBoost R package
    
    * Dramatically simplify install doc
    [R] Robust endian detection in CRAN xgboost build (#5232)
    
    * [R] Robust endian detection in CRAN xgboost build
    
    * Check for external backtrace() lib
    
    * Update Makevars.win
    support bootstrap allreduce/broadcast (#98)
    
    * support run rabit tests as xgboost subproject using xgboost/dmlc-core
    
    * support tracker config set/get
    
    * remove redudant printf
    
    * remove redudant printf
    
    * add c++0x declaration
    
    * log allreduce/broadcast caller, engine should track caller stack for
    investigation
    
    * tracker support binary config format
    
    * Revert "tracker support binary config format"
    
    This reverts commit 2a28e5e2b55c200cb621af8d19f17ab1bc62503b.
    
    * remove caller, prototype fetch allreduce/broadcast results from resbuf
    
    * store cached allreduce/broadcast seq_no to tracker
    
    * allow restore all caches from other nodes
    
    * try new rabit collective cache, todo: recv_link seems down
    
    * link up cache restore with main recovery
    
    * cleanup load cache state
    
    * update cache api
    
    * pass test.mk
    
    * have a working tests
    
    * try to unify check into actionsummary
    
    * more logging to debug distributed hist three method issue
    
    * update rabit interface to support caller signature matching
    
    * splite seq_counter from cur_cache_seq to different variables
    
    * still see issue with inf loop
    
    * support debug print caller as well as allreduce op
    
    * cleanup
    
    * remove get/set cache from model_recover, adding recover in
    loadcheckpoint
    
    * clarify rabit cache strategy, cache is set only by successful collective
    call involving all nodes with unique cache key. if all nodes call
    getcache at same time, we keep rabit run collective call. If some nodes
    call getcache while others not, we backfill cache from those nodes with
    most entries
    
    * revert caller logs
    
    * fix lint error
    
    * fix engine mpi signature
    
    * support getcache by ref
    
    * allow result buffer presiet to filestream
    
    * add loging
    
    * try fix checkpoint failure recovery case
    
    * use int64_t to avoid overflow caused seq fault
    
    * try avoid int overflow
    
    * try fix checkpoint failure recovery case
    
    * try avoid seqno overflow to negative by offseting specifial flag value
    adding cache seq no to checkpoint/load checkpoint/check point ack to avoid
    confusion from cache recovery
    
    * fix cache seq assert error
    
    * remove loging, handle edge case
    
    * add extensive log to checkpoint state  with different seq no
    
    * fix lint errors
    
    * clean up comments before merge back to master
    
    * add logs to allreduce/broadcast/checkpoint
    
    * use unsinged int 32 and give seq no larger range
    
    * address remove allreduce dropseq code segment
    
    * using caller signature to filter bootstrapallreduces
    
    * remove get/set cache from empty
    
    * apply signature to reducer
    
    * apply signature to broadcast
    
    * add key to broadcat log
    
    * fix broadcast signature
    
    * fix default _line value for non linux system
    
    * adding comments, remove sleep(1)
    
    * fix osx build issue
    
    * try fix mpi
    
    * fix doc
    
    * fix engine_empty api
    
    * logging, adding more logs, restore immutable assertion
    
    * print unsinged int with ud
    
    * fix lint
    
    * rename seqtype to kSeq and KCache indicating it's usage
    apply kDiffSeq check to load_cache routine
    
    * comment allreduce/broadcast log
    
    * allow tests run on arm
    
    * enable flag to turn on / off cache
    
    * add log info alert if user choose to enable rabit bootstrap cache
    
    * add rabit_debug setting so user can use config to turn on
    
    * log flags when user turn on rabit_debug
    
    * force rabit restart if tracker assign -1 rank
    
    * use OPENMP to vecotrize reducer
    
    * address comment
    
    * Revert "address comment"
    
    This reverts commit 1dc61f33e7357dad8fa65528abeb81db92c5f9ed.
    
    * fix checkpoint size print 0
    
    * per feedback, remove DISABLEOPEMP, address race condition
    
    * - remove openmp from this pr
    - update name from cache to boostrapcache
    
    * add default value of signature macros
    
    * remove openmp from cmake file
    
    * Update src/allreduce_robust.cc
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Update src/allreduce_robust.cc
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * run test with cmake
    
    * remove openmp
    
    * fix cmake based tests
    
    * use cmake test fix darwin .dylib issue
    
    * move around rabit_signature definition due to windows build
    
    * misc, add c++ check in CMakeFile
    
    * per feedback
    
    * resolve CMake file
    
    * update rabit version
    Remove plugin, cuda related code in automake & autoconf files (#4789)
    
    * Build plugin example with CMake.
    
    * Remove plugin, cuda related code in automake & autoconf files.
    
    * Fix typo in GPU doc.
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Allow using external dmlc-core (#91)
    
    * Set `RABIT_BUILD_DMLC=1` if use dmlc-core in rabit
    
    * remove dmlc-core
    [rabit harden] Enable all tests (#90)
    
    * include osx in tests
    * address `time_wait` on port assignment
    * increase submit attempts.
    * cleanup tests
    Migrate pylint check to Python 3 (#4381)
    
    * Migrate lint to Python 3
    
    * Fix lint errors
    
    * Use Miniconda3 to use Python 3.7
    
    * Use latest pylint and astroid
    [rabit harden] replace hardcopy dmlc-core headers with submodule links (#86)
    
    * backport dmlc header changes to rabit
    
    * use gitmodule to reference latest dmlc header files
    
    * include ref to dmlc-core
    fix cmake
    
    * update cmake file, add cmake build traivs task
    
    * try force using g++-4.8
    
    * per feedback, update cmake
    [rabit harden] fix rabit tests (#81)
    
    
    * enable model recovery tests
    * force use gcc4.8 in Travis
    Remove dmlc logging. (#78)
    
    * Remove dmlc logging header.
    
    * Fix lint.
    Fix CRAN check warnings/notes (#3988)
    
    * fix
    
    * reorder declaration to match initialization
    Fix #3342 and h2oai/h2o4gpu#625: Save predictor parameters in model file (#3856)
    
    * Fix #3342 and h2oai/h2o4gpu#625: Save predictor parameters in model file
    
    This allows pickled models to retain predictor attributes, such as
    'predictor' (whether to use CPU or GPU) and 'n_gpu' (number of GPUs
    to use). Related: h2oai/h2o4gpu#625
    
    Closes #3342.
    
    TODO. Write a test.
    
    * Fix lint
    
    * Do not load GPU predictor into CPU-only XGBoost
    
    * Add a test for pickling GPU predictors
    
    * Make sample data big enough to pass multi GPU test
    
    * Update test_gpu_predictor.cu
    [Blocking] Fix #3840: Clean up logic for parsing tree_method parameter (#3849)
    
    * Clean up logic for converting tree_method to updater sequence
    
    * Use C++11 enum class for extra safety
    
    Compiler will give warnings if switch statements don't handle all
    possible values of C++11 enum class.
    
    Also allow enum class to be used as DMLC parameter.
    
    * Fix compiler error + lint
    
    * Address reviewer comment
    
    * Better docstring for DECLARE_FIELD_ENUM_CLASS
    
    * Fix lint
    
    * Add C++ test to see if tree_method is recognized
    
    * Fix clang-tidy error
    
    * Add test_learner.h to R package
    
    * Update comments
    
    * Fix lint error
    Add callback interface to re-direct console output (#3438)
    
    * Add callback interface to re-direct console output
    
    * Exempt TrackerLogger from custom logging
    
    * Fix lint
    For CRAN submission, remove all #pragma's that suppress compiler warnings (#3329)
    
    * For CRAN submission, remove all #pragma's that suppress compiler warnings
    
    A few headers in dmlc-core contain #pragma's that disable compiler warnings,
    which is against the CRAN submission policy. Fix the problem by removing
    the offending #pragma's as part of the command `make Rbuild`.
    
    This addresses issue #3322.
    
    * Fix script to improve Cygwin/MSYS compatibility
    
    We need this to pass rmingw CI test
    
    * Remove remove_warning_suppression_pragma.sh from packaged tarball
    CRAN Submission for 0.71.1 (#3311)
    
    * fix for CRAN manual checks
    
    * fix for CRAN manual checks
    
    * pass local check
    
    * fix variable naming style
    
    * Adding Philip's record
    Fix up `make pippack` command for building source package for PyPI (#3199)
    
    * Now `make pippack` works without any manual action: it will produce
      xgboost-[version].tar.gz, which one can use by typing
      `pip3 install xgboost-[version].tar.gz`.
    * Detect OpenMP-capable compilers (clang, gcc-5, gcc-7) on MacOS
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    Use -msse2 flag depending upon architecure while compiling the rabit code (#49)
    [GPU-Plugin] Major refactor (#2644)
    
    * Removal of redundant code/files.
    * Removal of exact namespace in GPU plugin
    * Revert double precision histograms to single precision for performance on Maxwell/Kepler
    fix build in case of spaces in path to make (#2619)
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    Several fixes (#2572)
    
    * repared serialization after update process; fixes #2545
    
    * non-stratified folds in python could omit some data instances
    
    * Makefile: fixes for older makes on windows; clean R-package too
    
    * make cub to be a shallow submodule
    
    * improve $(MAKE) recovery
    MinGW: shared library prefix and appveyor CI (#2539)
    
    * for MinGW, drop the 'lib' prefix from shared library name
    
    * fix defines for 'g++ 4.8 or higher' to include g++ >= 5
    
    * fix compile warnings
    
    * [Appveyor] add MinGW with python; remove redundant jobs
    
    * [Appveyor] also do python build for one of msvc jobs
    To compile on ARM cpu (#2513)
    To compile on ARM CPU (#46)
    Fix broken make on windows (#2499)
    
    * fix Makefile for make on windows
    
    * clean up compilation warnings
    
    * fix for `no file name for include` make warning
    [GPU-Plugin] Multi-GPU gpu_id bug fixes for grow_gpu_hist and grow_gpu methods, and additional documentation for the gpu plugin. (#2463)
    Fixed shared library loading in the Python package (#2461)
    
    * Fixed DLL name on Windows in ``xgboost.libpath``
    
    * Added support for OS X to ``xgboost.libpath``
    
    * Use .dylib for shared library on OS X
    
    This does not affect the JNI library, because it is not trully
    cross-platform in the Makefile-build anyway.
    [GPU-Plugin] Support for building to specific GPU architectures (#2390)
    
    * Support for builing gpu-plugins to specific GPU architectures
    1. Option GPU_COMPUTE_VER exposed from both Makefile and CMakeLists.txt
    2. updater_gpu documentation updated accordingly
    
    * Re-introduced GPU_COMPUTE_VER option in the cmake flow.
    This seems to fix the compile-time, rdc=true and copy-constructor related
    errors seen and discussed in PR #2390.
    [GPU-Plugin] Integration of a faster version of grow_gpu plugin into mainstream (#2360)
    
    * Integrating a faster version of grow_gpu plugin
    1. Removed the older files to reduce duplication
    2. Moved all of the grow_gpu files under 'exact' folder
    3. All of them are inside 'exact' namespace to avoid any conflicts
    4. Fixed a bug in benchmark.py while running only 'grow_gpu' plugin
    5. Added cub and googletest submodules to ease integration and unit-testing
    6. Updates to CMakeLists.txt to directly build cuda objects into libxgboost
    
    * Added support for building gpu plugins through make flow
    1. updated makefile and config.mk to add right targets
    2. added unit-tests for gpu exact plugin code
    
    * 1. Added support for building gpu plugin using 'make' flow as well
    2. Updated instructions for building and testing gpu plugin
    
    * Fix travis-ci errors for PR#2360
    1. lint errors on unit-tests
    2. removed googletest, instead depended upon dmlc-core provide gtest cache
    
    * Some more fixes to travis-ci lint failures PR#2360
    
    * Added Rory's copyrights to the files containing code from both.
    
    * updated copyright statement as per Rory's request
    
    * moved the static datasets into a script to generate them at runtime
    
    * 1. memory usage print when silent=0
    2. tests/ and test/ folder organization
    3. removal of the dependency of googletest for just building xgboost
    4. coding style updates for .cuh as well
    
    * Fixes for compilation warnings
    
    * add cuda object files as well when JVM_BINDINGS=ON
    ENH more makefile updates (#2133)
    
    This commit proposes a simpler single compiler specification for OSX and *nix. It also let's people override the setting on both systems, not just *nix.
    ENH add gcc/g++ before clang for macs (#2125)
    
    * ENH add gcc/g++ before clang for macs - will default to clang anyways and supports separate gcc installs
    
    * BUG missed a ) - :(
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    autoconf for solaris (#1880)
    travis: Add code coverage on success
    
    Update the code coverage of the project on codecov for easy viewing.
    
    Also the gcov on travis uses a different version which cannot
    find the directory of the given files, and it needs to be specified
    in the -o flag. Hence now we loop over the list of files and
    run them independently.
    Makefile: Add CPP code coverage
    Add make commands for tests
    
    This adds the make commands required to build and run tests.
    Applied FreeBSD support (#37)
    Add option on OSX to use macports (#1675)
    [jvm-packages] XGBoost4j Windows fixes (#1639)
    
    * Changes for Mingw64 compilation to ensure long is a consistent size.
    
    Mainly impacts the Java API which would not compile, but there may be
    silent errors on Windows with large datasets before this patch (as long
    is 32-bits when compiled with mingw64 even in 64-bit mode).
    
    * Adding ifdefs to ensure it still compiles on MacOS
    
    * Makefile and create_jni.bat changes for Windows.
    
    * Switching XGDMatrixCreateFromCSREx JNI call to use size_t cast
    
    * Fixing lint error, adding profile switching to jvm-packages build to make create-jni.bat get called, adding myself to Contributors.Md
    fix Makefile (#1579)
    
    * Update Makefile
    
    * Update Makefile
    Update Makefile (#1566)
    Fixed OpenMP installation on MacOSX with gcc-6 (#1460)
    
    * Fixed OpenMP installation on MacOSX with gcc-6
    
    - Modified makefile from gcc-5 to gcc-6
    - Removed deprecated install instructions from doc (gcc-5 was automatically forced if available in makefile on OSX)
    
    * Fixed OpenMP installation on MacOSX with gcc-6
    
    - Modified makefile from gcc-5 to gcc-6
    - Removed deprecated install instructions from doc (gcc-5 was automatically forced if available in makefile on OSX)
    PyPI (pip installation) setup for 0.6 code (#1445)
    
    * force gcc-5 or clang-omp for Mac OS, prepare for pip pack
    
    * add sklearn dep, make -j4
    
    * finalize PyPI submission
    
    * revert to Xcode clang for passing build #1468
    
    * force to clang, try to solve cmake travis error
    
    * remove sklearn dependency
    [PYTHON] Refactor trainnig API to use callback
    fix Makefile to use MAKE variable
    modify Makefile
    Update rabit
    rename files/packages
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    [PYTHON] Simplify training logic, update rabit lib
    refactor: librabit
    [R] make all customizations to meet strict standard of cran
    [TRAVIS] cleanup travis script
    [FIX] fix plugin system
    [PLUGIN] Add plugin system
    modify java wrapper settings for new refactor
    [R] enable R compile
    
    [R] Enable R build for windows and linux
    [IO] Enable external memory
    [LIBXGBOOST] pass demo running.
    [CLI] initial refactor of CLI
    [LEARNER] Init learner interface
    [Make] refactor build script to use config file
    [MAKE] fix makefile
    fix makefile warning when cc is defined
    add settings for OS X
    refactor jni code and rename libxgboostjavawrapper.so to libxgboost4j.so
    make gcc5 check silent when there's no gcc5
    update pip building, troubleshooting with new makefile, plus friendly error message when fail importing sklearn
    fix pushd problem of pip building, convert README to rst for PyPI
    Squashed 'subtree/rabit/' changes from fa99857..e81a11d
    
    e81a11d Merge pull request #25 from daiyl0320/master
    35c3b37 add retry mechanism to ConnectTracker and modify Listen backlog to 128 in rabit_traker.py
    c71ed6f try deply doxygen
    62e5647 try deply doxygen
    732f1c6 try
    2fa6e02 ok
    0537665 minor
    7b59dcb minor
    5934950 new doc
    f538187 ok
    44b6049 new doc
    387339b add more
    9d4397a chg
    2879a48 chg
    30e3110 ok
    9ff0301 add link translation
    6b629c2 k
    32e1955 ok
    8f4839d fix
    93137b2 ok
    7eeeb79 reload recommonmark
    a8f00cc minor
    19b0f01 ok
    dd01184 minor
    c1cdc19 minor
    fcf0f43 try rst
    cbc21ae try
    62ddfa7 tiny
    aefc05c final change
    2aee9b4 minor
    fe4e7c2 ok
    8001983 change to subtitle
    5ca33e4 ok
    88f7d24 update guide
    29d43ab add code
    fe8bb3b minor hack for readthedocs
    229c71d Merge branch 'master' of ssh://github.com/dmlc/rabit
    7424218 ok
    d1d45bb Update README.md
    1e8813f Update README.md
    1ccc990 Update README.md
    0323e06 remove readme
    679a835 remove theme
    7ea5b7c remove numpydoc to napoleon
    b73e2be Merge branch 'master' of ssh://github.com/dmlc/rabit
    1742283 ok
    1838e25 Update python-requirements.txt
    bc4e957 ok
    fba6fc2 ok
    0251101 ok
    d50b905 ok
    d4f2509 ok
    cdf401a ok
    fef0ef2 new doc
    cef360d ok
    c125d2a ok
    270a49e add requirments
    744f901 get the basic doc
    1cb5cad Merge branch 'master' of ssh://github.com/dmlc/rabit
    8cc07ba minor
    d74f126 Update .travis.yml
    52b3dcd Update .travis.yml
    099581b Update .travis.yml
    1258046 Update .travis.yml
    7addac9 Update Makefile
    0ea7adf Update .travis.yml
    f858856 Update travis_script.sh
    d8eac4a Update README.md
    3cc49ad lint and travis
    ceedf4e fix
    fd8920c fix win32
    8bbed35 modify
    9520b90 Merge pull request #14 from dmlc/hjk41
    df14bb1 fix type
    f441dc7 replace tab with blankspace
    2467942 remove unnecessary include
    181ef47 defined long long and ulonglong
    1582180 use int32_t to define int and int64_t to define long. in VC long is 32bit
    e0b7da0 fix
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: e81a11dd7ee3cff87a38a42901315821df018bae
    fix data file shipping confusions, force system compiling, correct libpath for pip
    Fix makefile typo
    Use homebrew gcc if available
    add multi-thread static link for MAC
    minor
    add necessary configrations for pip installation
    python package refactor into python-package
    add os lrt
    add requirments
    API refactor to make fault handling easy
    refactor and ci
    lint and travis
    Update Makefile
    update makefile
    change makefile to be compatible with r-travis
    Update Makefile
    add java wrapper
    fix
    modify
    checkin some micro optimization
    allow fpic
    fix python windows installation problem, enable mingw compile, but seems mingw dll was not fast in loading
    fix makefile
    add xgboost
    compile with dmlc
    fix rpack
    move stream to rabit part, support rabit on yarn
    Squashed 'subtree/rabit/' changes from d4ec037..28ca7be
    
    28ca7be add linear readme
    ca4b20f add linear readme
    1133628 add linear readme
    6a11676 update docs
    a607047 Update build.sh
    2c1cfd8 complete yarn
    4f28e32 change formater
    2fbda81 fix stdin input
    3258bcf checkin yarn master
    67ebf81 allow setup from env variables
    9b6bf57 fix hdfs
    395d5c2 add make system
    88ce767 refactor io, initial hdfs file access need test
    19be870 chgs
    a1bd3c6 Merge branch 'master' of ssh://github.com/tqchen/rabit
    1a573f9 introduce input split
    29476f1 fix timer issue
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: 28ca7becbdf6503e6b1398588a969efb164c9701
    allow setup from env variables
    remove mock from default build
    add vcd back
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    import vcd to eliminate note
    not build the vignette
    new Rpack
    chg back to g++
    Squashed 'subtree/rabit/' changes from fb13cab..1bb8fe9
    
    1bb8fe9 chg makefile
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: 1bb8fe96150175216cf5c59a21a7908418ae52b4
    quick fix seed
    chg makefile
    Squashed 'subtree/rabit/' changes from 4ebe657..fb13cab
    
    fb13cab change makefile
    1479e37 fixed small bug in mpi submission script
    0ca7a63 Update README.md
    5ef4830 ok
    93a1338 chg note
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: fb13cab216b795f86dc90547b71c0f730766affa
    change makefile
    Squashed 'subtree/rabit/' changes from 1db6449..85b7463
    
    85b7463 change def of reducer to take function ptr
    fe6366e add engine base
    a98720e more deps
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: 85b746394e0bed36a22ebb12beb8672616b39047
    change def of reducer to take function ptr
    add engine base
    remove xgpred
    add single instance prediction
    fix win compile
    fix of Rpack
    windows changes
    change R build script
    Squashed 'subtree/rabit/' changes from c7282ac..1db6449
    
    1db6449 remove include in -I, make things easier to direct compile
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: 1db6449b019902e99de446c84aa36e73c03eb888
    add sync module
    remove include in -I, make things easier to direct compile
    Squashed 'subtree/rabit/' content from commit c7282ac
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: c7282acb2a92e1d6a32614889ff466fec58937f3
    change makefile to lazy checkpt, fix col splt code
    fix bug in initialization of routing
    change default behavior to behave normal
    add auto caching of python in hadoop script, mock test module to python, with checkpt
    checkin broadcast python module
    add wrapper
    update doc
    cpplint pass
    add mock exec
    fix the row split recovery, add per iteration random number seed
    pas mock, need to fix rabit lib for not initialization
    add mock engine
    ok
    change file structure
    change hist update to lazy
    change allreduce lib to rabit library, xgboost now run with rabit
    add tracker print
    checkin makefile
    make wrapper ok
    check in allreduce tcp, check if there could be more concise form
    compile
    recheck column mode
    checkin skmaker
    ok
    finish mushroom example
    make clear seperation
    ok, now work on update position
    middle version
    intial version of sync wrapper
    chg back to g++
    add auto build script
    chg pack file
    chg version
    some fix to make it more c++
    fine fix
    gard GNU c
    some quick fix
    add
    more clean makevar
    pack script with cleanup
    chg  data
    improve pack script
    make it packable
    pass pedantic
    chg code guide
    pass build
    seems ok, need review destructors
    finish refactor, need debug
    complete refactor data.h, now replies on iterator to access column
    better error handling
    a fixed version
    add cvgrad stats, simplify data
    try to fix compile bug
    fix compilation on mac
    chg wrapper
    workable R wrapper
    add base_margin
    fix base score, and print message
    add no omp flag
    fix mac
    make xgcombine buffer work
    remake the wrapper
    first version that reproduce binary classification demo
    check in io module
    mv code into src
    start unity refactor
    ok
    simple chgs
    make regression module compatible with rank loss, now support weighted loss
    Lambda rank added
    lambda rank added
    rank pass toy
    big change, change interface to template, everything still OK
    backup makefile
    compatibility issue with openmp
    full omp support for regression
    modify tree so that training is standalone
    changes to reg booster
    chg makefile
    update this folder
    init commit
    Fix Python demo and doc. (#4545)
    
    * Remove old doc.
    * Fix checking __stdin__.
    Update README.rst (#4167)
    
    Fixes error when copy pasting.
    update macOS gcc@5 installation guide (#3003)
    
    After installing ``gcc@5``, ``CMAKE_C_COMPILER`` will not be set to gcc-5 in some macOS environment automatically and the installation of xgboost will still fail. Manually setting the compiler will solve the problem.
    Update README (#2204)
    
    I found the installation of the Python XGBoost package to be problematic as the documentation around compiler requirements was unclear, as discussed in #1501. I decided that I would improve the README.
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    PyPI (pip installation) setup for 0.6 code (#1445)
    
    * force gcc-5 or clang-omp for Mac OS, prepare for pip pack
    
    * add sklearn dep, make -j4
    
    * finalize PyPI submission
    
    * revert to Xcode clang for passing build #1468
    
    * force to clang, try to solve cmake travis error
    
    * remove sklearn dependency
    Remove pypi downloads badge (#1365)
    Updated obsolete installation instructions
    
    Fixed local compilation, and installation for R package and Python
    package. Modified the according documents.
    minor change dir
    adding right path to setup.py
    fix pushd problem of pip building, convert README to rst for PyPI
    Fix Windows 2016 build. (#5902)
    Require Python 3.6+; drop Python 3.5 from CI (#5715)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Use `scikit-learn` in extra dependencies. (#5310)
    Fix CMake build on Windows with setuptools. (#5280)
    Rewrite setup.py. (#5271)
    
    The setup.py is rewritten.  This new script uses only Python code and provide customized
    implementation of setuptools commands.  This way users can run most of setuptools commands
    just like any other Python libraries.
    
    * Remove setup_pip.py
    * Remove soft links.
    * Define customized commands.
    * Remove shell script.
    * Remove makefile script.
    * Update the doc for building from source.
    Declare Python 3.8 support in setup.py (#5274)
    Require Python 3.5+ in setup.py (#5021)
    [python package] include dmlc-tracker into xgb python pkg (#4731)
    Add optional dependencies to setup.py (#4655)
    Add native support for Dask (#4473)
    
    * Add native support for Dask
    
    * Add multi-GPU demo
    
    * Add sklearn example
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    Added language classifier for python (#4327)
    
    * Added language classifier for python
    
    * Removed python2 language classifier
    
    * Fix formatting
    open README with utf-8 and add gcc-8 (#3867)
    Fix relpath in setup.py on Windows (#3493)
    
    * Fix relpath in setup.py on Windows
    
    Fixes #3480.
    
    * Use only one lib file; use 4 space indent
    Update PyPI maintainer; use VERSION for binary wheels (#2992)
    Make lib path relatrive to fix setup error #1932 (#1947)
    Added license information (#1783)
    
    Added license information to the setup.py
    Enable flake8
    fix PyPi Description issue
    
    the description field was set to what should be the long_description field -- making a bit of a mess on PyPi
    [R] make all customizations to meet strict standard of cran
    change .md to .rst
    separate setup.py with pip installation, add trouble shooting page
    Update setup.py
    correct print for python 3
    fix pylint warnings
    fix data file shipping confusions, force system compiling, correct libpath for pip
    fix pylint in setup
    Restore Python3 compatibility
    Fix python setup: avoid import numpy in setup.py
    
    Currently `pip install xgboost` will raise traceback like this
    
    ```
    Traceback (most recent call last):
      File "<string>", line 20, in <module>
      File "/tmp/pip-build-IAdqYE/xgboost/setup.py", line 20, in <module>
        import xgboost
      File "./xgboost/__init__.py", line 8, in <module>
        from .core import DMatrix, Booster
      File "./xgboost/core.py", line 12, in <module>
        import numpy as np
    ImportError: No module named numpy
    ```
    
    We should avoid importing numpy in setup.py and let pip install numpy and scipy automatically.
    That's what `install_requires` for.
    switch back to the original version info
    add back setup.py after conflict resolving
    add platform if statement in setup.py for pip for pull #450 issuecomment-133795287
    Update setup.py
    update with comments on PR #450, fixed styles and updated CHANGES and CONTRIBUTORS
    add necessary configrations for pip installation
    python package refactor into python-package
    python package refactor into python-package
    add ignore
    fix pushd problem of pip building, convert README to rst for PyPI
    add necessary configrations for pip installation
    Improved sklearn compatibility (#5255)
    [CI] Update lint configuration to support latest pylint convention (#4971)
    
    * Update lint configuration
    
    * Use gcc 8 consistently in build instruction
    Migrate pylint check to Python 3 (#4381)
    
    * Migrate lint to Python 3
    
    * Fix lint errors
    
    * Use Miniconda3 to use Python 3.7
    
    * Use latest pylint and astroid
    reformat benchmark_tree.py to get rid of lint errors (#4126)
    Enforce naming style in Python lint (#3896)
    [PYTHON] Refactor trainnig API to use callback
    Fixed all lint errors
    fix lint errors in core
    ignore nested blocks
    Added pylintrc file
    Rewrite setup.py. (#5271)
    
    The setup.py is rewritten.  This new script uses only Python code and provide customized
    implementation of setuptools commands.  This way users can run most of setuptools commands
    just like any other Python libraries.
    
    * Remove setup_pip.py
    * Remove soft links.
    * Define customized commands.
    * Remove shell script.
    * Remove makefile script.
    * Update the doc for building from source.
    PyPI (pip installation) setup for 0.6 code (#1445)
    
    * force gcc-5 or clang-omp for Mac OS, prepare for pip pack
    
    * add sklearn dep, make -j4
    
    * finalize PyPI submission
    
    * revert to Xcode clang for passing build #1468
    
    * force to clang, try to solve cmake travis error
    
    * remove sklearn dependency
    [R] make all customizations to meet strict standard of cran
    fix pushd problem of pip building, convert README to rst for PyPI
    fix data file shipping confusions, force system compiling, correct libpath for pip
    add necessary configrations for pip installation
    Loop over `thrust::reduce`. (#6229)
    
    * Check input chunk size of dqdm.
    * Add doc for current limitation.
    Cleanup Python code. (#6223)
    
    
    * Remove pathlike as XGBoost 1.2 requires Python 3.6.
    * Move conditional import of dask/distributed into dask module.
    Enable categorical data support on Python DMatrix. (#6166)
    
    
    * Only pandas is recognized.
    [dask] Support more meta data on functional interface. (#6132)
    
    
    * Add base_margin, label_(lower|upper)_bound.
    * Test survival training with dask.
    Limit tree depth for GPU hist. (#6045)
    Optimize DMatrix build time. (#5877)
    
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Feature weights (#5962)
    [Breaking] Fix custom metric for multi output. (#5954)
    
    
    * Set output margin to true for custom metric.  This fixes only R and Python.
    Simplify the data backends. (#5893)
    Remove print. (#5867)
    Accept iterator in device dmatrix. (#5783)
    
    
    * Remove Device DMatrix.
    Move feature names and types of DMatrix from Python to C++. (#5858)
    
    
    * Add thread local return entry for DMatrix.
    * Save feature name and feature type in binary file.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Update document for model dump. (#5818)
    
    * Clarify the relationship between dump and save.
    * Mention the schema.
    Fix exception causes all over the codebase (#5787)
    [dask] Return GPU Series when input is from cuDF. (#5710)
    
    
    * Refactor predict function.
    Let XGBoostError inherit ValueError. (#5696)
    Implement Python data handler. (#5689)
    
    
    * Define data handlers for DMatrix.
    * Throw ValueError in scikit learn interface.
    Instruct Mac users to install libomp (#5606)
    [Breaking] Set output margin to True for custom objective. (#5564)
    
    * Set output margin to True for custom objective in Python and R.
    
    * Add a demo for writing multi-class custom objective function.
    
    * Run tests on selected demos.
    Don't use uint for threads. (#5542)
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Update Python doc. [skip ci] (#5517)
    
    * Update doc for copying booster. [skip ci]
    
    The issue is resolved in  #5312 .
    
    * Add version for new APIs. [skip ci]
    Enable parameter validation for skl. (#5477)
    Add support for dlpack, expose python docs for DeviceQuantileDMatrix (#5465)
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Device dmatrix (#5420)
    Support pandas SparseArray. (#5431)
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Define lazy isinstance for Python compat. (#5364)
    
    * Avoid importing datatable.
    * Fix #5363.
    Restore loading model from buffer. (#5360)
    [Breaking] Remove Scikit-Learn default parameters (#5130)
    
    * Simplify Scikit-Learn parameter management.
    
    * Copy base class for removing duplicated parameter signatures.
    * Set all parameters to None.
    * Handle None in set_param.
    * Extract the doc.
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Fix metainfo from DataFrame. (#5216)
    
    * Fix metainfo from DataFrame.
    
    * Unify helper functions for data and meta.
    Fix cupy without cudf import (#5219)
    Support dmatrix construction from cupy array (#5206)
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Disable parameter validation for Scikit-Learn interface. (#5167)
    
    * Disable parameter validation for now.
    
    Scikit-Learn passes all parameters down to XGBoost, whether they are used or
    not.
    
    * Add option `validate_parameters`.
    Fix feature_name crated from int64index dataframe. (#5081)
    Clean up Python 2 compatibility code. (#5161)
    Add base margin to sklearn interface. (#5151)
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Add better error message for invalid feature names (#5024)
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    [CI] Update lint configuration to support latest pylint convention (#4971)
    
    * Update lint configuration
    
    * Use gcc 8 consistently in build instruction
    Follow PEP 257 -- Docstring Conventions (#4959)
    Don't `set_params` at the end of `set_state`. (#4947)
    
    * Don't set_params at the end of set_state.
    
    * Also fix another issue found in dask prediction.
    
    * Add note about prediction.
    
    Don't support other prediction modes at the moment.
    Support feature names/types for cudf. (#4902)
    
    * Implement most of the pandas procedure for cudf except for type conversion.
    * Requires an array of interfaces in metainfo.
    Add support for cudf.Series (#4891)
    Rewrite Dask interface. (#4819)
    Fix DMatrix doc. (#4884)
    Deprecate set group (#4864)
    
    
    
    * Convert jvm package and R package.
    
    * Restore for compatibility.
    Complete cudf support. (#4850)
    
    
    * Handles missing value.
    * Accept all floating point and integer types.
    * Move to cudf 9.0 API.
    * Remove requirement on `null_count`.
    * Arbitrary column types support.
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    add os.PathLike support for file paths to DMatrix and Booster Python classes (#4757)
    Eliminate FutureWarning: Series.base is deprecated (#4337)
    
    * Remove all references to data.base
    
    Should eliminate the deprecation warning in issue #4300
    
    * Fix lint
    pytest tests/python fails if no pandas installed (#4620)
    
    * _maybe_pandas_xxx should return their arguments unchanged if no pandas installed
    
    * Tests should not assume pandas is installed
    
    * Mark tests which require pandas as such
    Implement tree model dump with code generator. (#4602)
    
    * Implement tree model dump with a code generator.
    
    * Split up generators.
    * Implement graphviz generator.
    * Use pattern matching.
    
    * [Breaking] Return a Source in `to_graphviz` instead of Digraph in Python package.
    
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add support for cross-validation using query ID (#4474)
    
    * adding support for matrix slicing with query ID for cross-validation
    
    * hail mary test of unrar installation for windows tests
    
    * trying to modify tests to run in Github CI
    
    * Remove dependency on wget and unrar
    
    * Save error log from R test
    
    * Relax assertion in test_training
    
    * Use int instead of bool in C function interface
    
    * Revise R interface
    
    * Add XGDMatrixSliceDMatrixEx and keep old XGDMatrixSliceDMatrix for API compatibility
    Ensure pandas DataFrame column names are treated as strings in type error message (#4481)
    Migrate pylint check to Python 3 (#4381)
    
    * Migrate lint to Python 3
    
    * Fix lint errors
    
    * Use Miniconda3 to use Python 3.7
    
    * Use latest pylint and astroid
    Don't store DMatrix handle until it's initialized. (#4317)
    
    * Use a temporary variable to store the handle.
    * Decode c++ error message.
    * Simple note about saved binary.
    Added trees_to_df() method for Booster class (#4153)
    
    * add test_parse_tree.py to tests/python
    
    * Fix formatting
    
    * Fix pylint error
    
    * Ignore 'no member' error for Pandas dataframe
    Fix #4163: always copy sliced data (#4165)
    
    * Revert "Accept numpy array view. (#4147)"
    
    This reverts commit a985a99cf0dacb26a5d734835473d492d3c2a0df.
    
    * Fix #4163: always copy sliced data
    
    * Remove print() from the test; check shape equality
    
    * Check if 'base' attribute exists
    
    * Fix lint
    
    * Address reviewer comment
    
    * Fix lint
    Accept numpy array view. (#4147)
    
    * Accept array view (slice) in metainfo.
    Update datatable usage (#4123)
    Update Python docstring for ranking functions (#4121)
    
    * Update Python docstring for ranking functions
    
    * Fix formatting
    Prevent training without setting up caches. (#4066)
    
    * Prevent training without setting up caches.
    
    * Add warning for internal functions.
    * Check number of features.
    
    * Address reviewer's comment.
    Check booster for dart in feature importance. (#4073)
    
    * Check booster for dart in feature importance.
    Deprecation warning for lists passed into DMatrix (#3970)
    
    * Ensure lists cannot be passed into DMatrix
    
    The documentation does not include lists as an allowed type for the data inputted into DMatrix. Despite this, a list can be passed in without an error. This change would prevent a list form being passed in directly.
    Fix #3894: Allow loading pickles without self.booster attributes (redux) (#3944)
    Address deprecation of Python ABC. (#3909)
    handle $PATH not being set in python library (#3845)
    
    Fixes #3844
    Document behavior of get_fscore() for zero-importance features (#3763)
    Fix #3714: preserve feature names when slicing DMatrix (#3766)
    
    * Fix #3714: preserve feature names when slicing DMatrix
    
    * Add test
    Add notes to doc (#3765)
    remove extra of (#3713)
    Better error message for failed library loading (#3690)
    
    * Better error message for failed lib loading
    
    * Address review comment + fix lint
    Revert #3677 and #3674 (#3678)
    
    * Revert "Add scikit-learn as dependency for doc build (#3677)"
    
    This reverts commit 308f664ade0547242608e21f6198c895415f03da.
    
    * Revert "Add scikit-learn tests (#3674)"
    
    This reverts commit d176a0fbc8165e3afe3e42ff464ab7b253211555.
    Add scikit-learn tests (#3674)
    
    * Add scikit-learn tests
    
    Goal is to pass scikit-learn's check_estimator() for XGBClassifier,
    XGBRegressor, and XGBRanker. It is actually not possible to do so
    entirely, since check_estimator() assumes that NaN is disallowed,
    but XGBoost allows for NaN as missing values. However, it is always
    good ideas to add some checks inspired by check_estimator().
    
    * Fix lint
    
    * Fix lint
    Update Python API doc (#3619)
    
    * Add XGBRanker to Python API doc
    
    * Show inherited members of XGBRegressor in API doc, since XGBRegressor uses default methods from XGBModel
    
    * Add table of contents to Python API doc
    
    * Skip JVM doc download if not available
    
    * Show inherited members for XGBRegressor and XGBRanker
    
    * Expose XGBRanker to Python XGBoost module directory
    
    * Add docstring to XGBRegressor.predict() and XGBRanker.predict()
    
    * Fix rendering errors in Python docstrings
    
    * Fix lint
    Add JSON model dump functionality (#3603)
    
    * Add JSON model dump functionality
    
    * Fix lint
    Fix accessing DMatrix.handle before set. (#3599)
    
    Close #3597.
    Revert "Fix #3485, #3540: Don't use dropout for predicting test sets" (#3563)
    
    * Revert "Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)"
    
    This reverts commit 44811f233071c5805d70c287abd22b155b732727.
    
    * Document behavior of predict() for DART booster
    
    * Add notice to parameter.rst
    Fix bug of using list(x) function when x is string (#3432)
    
    * Fix bug of using list(x) function when x is string
    
    list('abcdcba') = ['a', 'b', 'c', 'd', 'c', 'b', 'a']
    
    * Allow feature_names/feature_types to be of any type
    
    If feature_names/feature_types is iterable, e.g. tuple, list, then convert the value to list, except for string; otherwise construct a list with a single value
    
    * Delete excess whitespace
    
    * Fix whitespace to pass lint
    Add total_gain and total_cover importance measures (#3498)
    
    Add `'total_gain'` and `'total_cover'` as possible `importance_type`
    arguments to `Booster.get_score` in the Python package.
    
    `get_score` already accepts a `'gain'` argument, which returns each
    feature's average gain over all of its splits.  `'total_gain'` does the
    same, but returns a total rather than an average.  This seems more
    intuitively meaningful, and also matches the behavior of the R package's
    `xgb.importance` function.
    
    I also added an analogous `'total_cover'` command for consistency.
    
    This should resolve #3484.
    Improved library loading a bit (#3481)
    
    * Improved library loading a bit
    
    * Fixed indentation.
    
    * Fixes according to the discussion
    
    * Moved the comment to a separate line.
    * specified exception type
    Fix get_uint_info() (#3442)
    
    
    
    * Add regression test
    Add callback interface to re-direct console output (#3438)
    
    * Add callback interface to re-direct console output
    
    * Exempt TrackerLogger from custom logging
    
    * Fix lint
    Better doc for save_model() / load_model() (#3143)
    
    Be clear that they do not save Python-specific attributes
    Allow import via python datatable. (#3272)
    
    * Allow import via python datatable.
    
    * Write unit tests
    
    * Refactor dt API functions
    
    * Refactor python code
    
    * Lint fixes
    
    * Address review comments
    added python doc string for nthreads to dmatrix (#3363)
    Add validate_features option for Booster predict (#3323)
    
    * Add validate_features option for Booster predict
    
    * Fix trailing whitespace in docstring
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Documenting CSV loading into DMatrix (#3137)
    
    * Support CSV file in DMatrix
    
    We'd just need to expose the CSV parser in dmlc-core to the Python wrapper
    
    * Revert extra code; document existing CSV support
    
    CSV support is already there but undocumented
    
    * Add notice about categorical features
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    fix the typo in core.py (#2978)
    Fix performance of c_array in python core.py (#2786)
    Fix MultiIndex detection (breaks for latest pandas==0.21.0). (#2872)
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    Make __del__ method idempotent (#2627)
    
    Addresses Issue #2533.
    Fix issue 2670 (#2671)
    
    * fix issue 2670
    
    * add python<3.6 compatibility
    
    * fix Index
    
    * fix Index/MultiIndex
    
    * fix lint
    
    * fix W0622
    
    really nonsense
    
    * fix lambda
    
    * Trigger Travis
    
    * add test for MultiIndex
    
    * remove tailing whitespace
    Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation (#2530)
    
    * Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation from numpy arrays for python interface.
    [Python] Use appropriate integer types when calling native code. (#2361)
    
    Don't use implicit conversions to c_int, which incidentally happen to work
    on (some) 64-bit platforms, but:
    * may lead to truncation of the input value to a 32-bit signed int,
    * cause segfaults on some 32-bit architectures (tested on Ubuntu ARM,
      but is also the likely cause of issue #1707).
    
    Also, when passing references use explicit 64-bit integers, where needed,
    instead of c_ulong, which is not guaranteed to be this large.
    Minor cleanup (#2342)
    
    * Clean up demo of multiclass classification
    
    * Remove extra space
    Add prediction of feature contributions (#2003)
    
    * Add prediction of feature contributions
    
    This implements the idea described at http://blog.datadive.net/interpreting-random-forests/
    which tries to give insight in how a prediction is composed of its feature contributions
    and a bias.
    
    * Support multi-class models
    
    * Calculate learning_rate per-tree instead of using the one from the first tree
    
    * Do not rely on node.base_weight * learning_rate having the same value as the node mean value (aka leaf value, if it were a leaf); instead calculate them (lazily) on-the-fly
    
    * Add simple test for contributions feature
    
    * Check against param.num_nodes instead of checking for non-zero length
    
    * Loop over all roots instead of only the first
    A fix regarding the compatibility with python 2.6 (#1981)
    
    * A fix regarding the compatibility with python 2.6
    
    the syntax of {n: self.attr(n) for n in attr_names} is illegal in python 2.6
    
    * Update core.py
    
    add a space after comma
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    keep builtin evaluations while using customized evaluation function (#1624)
    
    * keep builtin evaluations while using customized evaluation function
    
    * fix concat bytes to str
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    make DMatrix._init_from_npy2d only copy data when necessary (#1637)
    
    * make DMatrix._init_from_npy2d only copy data when necessary
    
    When creating DMatrix from a 2d ndarray, it can unnecessarily copy the input data. This can be problematic when the data is already very large--running out of memory. The copy is temporary (going out of scope at the end of this function) but it still adds to peak memory usage.
    
    ``numpy.array`` copies its input no matter what by default. By adding ``copy=False``, it will only do so when necessary. Since XGDMatrixCreateFromMat is readonly on the input buffer, this copy is not needed.
    
    Also added comments explaining when a copy can happen (if data ordering/layout is wrong or if type is not 32-bit float).
    
    * remove whitespace
    More robust DMatrix creation from a sparse matrix (#1606)
    
    * [CORE] DMatrix from sparse w/ explicit #col #row; safer arg types
    
    * [python-package] c-api change for _init_from_csr _init_from_csc
    
    * fix spaces
    
    * [R-package] adopt the new XGDMatrixCreateFromCSCEx interface
    
    * [CORE] redirect old sparse creators to new ones
    [PYTHON] Refactor trainnig API to use callback
    py: replace attr_names() with attributes()
    attr_names for python interface; attribute deletion via set_attr
    fix VisibleDeprecationWarning
    Added other feature importances in python package (#1135)
    
    * added new function to calculate other feature importances
    
    * added capability to plot other feature importance measures
    
    * changed plotting default to fscore
    
    * added info on importance_type to boilerplate comment
    
    * updated text of error statement
    
    * added self module name to fix call
    
    * added unit test for feature importances
    
    * style fixes
    Bug mixing DMatrix's with and without feature names
    [py] split value histograms
    Enable flake8
    Verbose message: which fields have impropper data types
    
    A more verbose error message letting the user know which fields have impropper data types
    a more verbose field mismatch error message
    
    This error message can be hard to understand when there are several fields, as shown in the example below. This improves the error message, letting the user know which fields were unexpected or missing.
    
        import xgboost as xgb
        import pandas as pd
        train = pd.DataFrame({'a':[1], 'b':[2], 'c':[3], 'd':[4], 'f':[2], 'g':2, 'etc etc etc':[11]})
        dtrain = xgb.DMatrix(train.drop('d', axis=1), train.d)
        test = pd.DataFrame({'a':[1], 'b':[2], 'c':[1], 'd':[4], 'e':[2], 'f':[2], 'g':2, 'etc etc etc':[11]})
        dtest = xgb.DMatrix(test)
        modl = xgb.train({}, dtrain)
        modl.predict(dtest)
    
    
        # ValueError: feature_names mismatch: [u'a', u'b', u'c', u'etc etc etc', u'f', u'g'] [u'a', u'b', u'c', u'd', u'e', u'etc etc etc', u'f', u'g']
    [PYTHON-DIST] Distributed xgboost python training API.
    [DIST] Add Distributed XGBoost on AWS Tutorial
    Make missing handling consistent with sklearn's portion of the python package
    __copy__ calls __deepcopy__ with an argument
    fix signature of __deepcopy__ method
    Fixed all lint errors
    fix lint errors in core
    Cleanup pandas support
    python 2.6 compatibility tweak
    
    replacing set literal {} with set() for python 2.6 compatibility (plus reformatting the line)
    python: multiple eval_metrics changes
    
    - allows feval to return a list of tuples (name, error/score value)
    - changed behavior for multiple eval_metrics in conjunction with
    early_stopping: Instead of raising an error, the last passed evel_metric
    (or last entry in return value of feval) is used for early stopping
    - allows list of eval_metrics in dict-typed params
    - unittest for new features / behavior
    
    documentation updated
    
    - example for assigning a list to 'eval_metric'
    - note about early stopping on last passed eval metric
    
    - info msg for used eval metric added
    Cleaned up some code
    Added back feature names
    fixes typo in error message
    Bool gets mapped to i instead of int
    More Pandas dtypes and more flexible variable naming
    
    - Pandas DataFrame supports more dtypes than 'int64', 'float64' and 'bool', therefor added a bunch of extra dtypes for the data variable.
    - From now on the label variable can be a Pandas DataFrame with the same dtypes as the data variable.
    - If label is a Pandas DataFrame will be converted to float.
    - If no feature_types is set, the data dtypes will be converted to 'int' or 'float'.
    - The feature_names may contain every character except [, ] or <
    Support non-str column names
    python DMatrix now accepts pandas DataFrame
    Change to properties
    Add feature_types
    Fix numpy array check logic
    Fix python setup: avoid import numpy in setup.py
    
    Currently `pip install xgboost` will raise traceback like this
    
    ```
    Traceback (most recent call last):
      File "<string>", line 20, in <module>
      File "/tmp/pip-build-IAdqYE/xgboost/setup.py", line 20, in <module>
        import xgboost
      File "./xgboost/__init__.py", line 8, in <module>
        from .core import DMatrix, Booster
      File "./xgboost/core.py", line 12, in <module>
        import numpy as np
    ImportError: No module named numpy
    ```
    
    We should avoid importing numpy in setup.py and let pip install numpy and scipy automatically.
    That's what `install_requires` for.
    Cleanup str roundtrip using ctypes
    BUG: incorrect model_file results in segfault
    Use ctypes
    ENH: allow python to handle feature names
    add platform if statement in setup.py for pip for pull #450 issuecomment-133795287
    update with comments on PR #450, fixed styles and updated CHANGES and CONTRIBUTORS
    add necessary configrations for pip installation
    enable basic sphinx doc
    python package refactor into python-package
    fix sklearn best score
    fix wrapper dict
    STY: Fix lint errors
    REF: Combine eval_metric and feval to one parameter
    DOC: Point to parameter.md for eval_metric
    DOC: Document verbose_eval
    ENH: Allow for silent evaluation
    ENH: Allow possibly negative evaluation metrics.
    ENH: Allow early stopping in sklearn API.
    API refactor to make fault handling easy
    last check
    make  python lint
    Update xgboost.py
    ENH: Allow missing = 0
    BUG: XGBError -> XGBoostError
    ENH: Allow settable missing value in sklearn api.
    checkin copy
    Added classes_ attribute to scikit-learn wrapper.
    Fix early stopping in python wrapper
    Update xgboost.py
    CLN: Remove unused import. Fix comment.
    change numpy to bytearray as buffer
    fix pkl problem
    Update xgboost.py
    update version to be consistent with python
    fix saveraw
    ENH: Don't use tempfiles for save/load
    ENH: Make XGBModel pickleable.
    fix typo
    Update xgboost.py
    COMPAT: Decode bytes object for Python 3.
    ENH: Add n_classes_ to fitted classifier.
    DOC: Add docstrings to user-facing classes.
    bugfix setup
    enable msvc win32 project
    fix python windows installation problem, enable mingw compile, but seems mingw dll was not fast in loading
    remove eval_metric
    add more params
    remove print in Python get_fscore()
    add flag variable in Python get_fscore() to control printing
    correct format
    record training progress
    reorder parameters
    add more params in sklearn wrapper.
    OK
    some initial try of cachefiles
    *Fix Sklearn.grid_search error
    fix indent warning by flake8
    *Fix XGBClassifier super()
    bugfix
    early_stopping_rounds for train() in Python wrapper :fire:
    Fix some stuff
    Initial commit
    early stopping for Python wrapper
    early stopping for Python wrapper
    early stopping for Python wrapper
    Fixed the dll import for relative paths + various cleanup.
    
    - DLL import now works when __file__ is a relative path
    - Various PEP8 and whitespace fixes + whitespace cleanup
    - Docstring fixes (conform to numpydoc)
    - Added __all__ to the module
    - Fixed mutable default values
    - Removed print statements
    - py2/py3-compatible string-type checks
    - Replace asserts with proper exceptions
    - Make classes new-style (derive from object)
    cleanup multi-node
    add dump statistics
    make wrapper ok
    add predict leaf indices
    delete old cvpack
    python 3 encoding
    add create from csc
    fix doc
    small fix to the doc
    add cv for python
    Update xgboost.py
    chg
    make some changes to cv
    chg
    chg fobj back to obj, to keep parameter name unchanged
    move custom obj build in into booster
    fix ntreelimit
    adjust weight
    add ntree limit
    windows check
    check unity back
    make py work
    Update xgboost.py
    MSVS DLL Project for Python wrapper (ver.3 on win64)
    fix magic so that it can detect binary file
    move ncol, row to booster, add set/get uint info
    seems ok
    chg
    fix typo
    fix typo
    add base_margin
    fix mac
    enforce putting iteration numbers in train
    make it compatible with old code
    ok
    python module pass basic test
    remake the wrapper
    add rand seeds back
    ok
    make python random seed invariant in each round
    Compatibility with both Python 2(.7) and 3
    force handle as void_p, seems fix mac problem
    add return type for xgboost, don't know if it is mac problem
    add bing to author list
    a correct version
    fix numpy convert
    change data format to include weight in binary file, add get weight to python
    some fix
    faster convert to numpy array
    add cutomized training
    fix
    add interact mode
    add python interface for xgboost
    finish python lib
    finish matrix
    good
    important change to regrank interface, need some more test
    try python
    Fix typo in dask interface. (#6240)
    Loop over `thrust::reduce`. (#6229)
    
    * Check input chunk size of dqdm.
    * Add doc for current limitation.
    [dask] Test for data initializaton. (#6226)
    Cleanup Python code. (#6223)
    
    
    * Remove pathlike as XGBoost 1.2 requires Python 3.6.
    * Move conditional import of dask/distributed into dask module.
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    Update base margin dask (#6155)
    
    * Add `base-margin`
    * Add `output_margin` to regressor.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Add DaskDeviceQuantileDMatrix demo. (#6156)
    [dask] Support more meta data on functional interface. (#6132)
    
    
    * Add base_margin, label_(lower|upper)_bound.
    * Test survival training with dask.
    [dask] Refactor meta data handling. (#6130)
    Allow kwargs in dask predict (#6117)
    [Breaking] Fix .predict() method and add .predict_proba() in xgboost.dask.DaskXGBClassifier (#5986)
    Fix dask predict shape infer. (#5989)
    [Dask] Asyncio support. (#5862)
    Dask device dmatrix (#5901)
    
    
    * Fix softprob with empty dmatrix.
    [dask] Return GPU Series when input is from cuDF. (#5710)
    
    
    * Refactor predict function.
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Update Python doc. [skip ci] (#5517)
    
    * Update doc for copying booster. [skip ci]
    
    The issue is resolved in  #5312 .
    
    * Add version for new APIs. [skip ci]
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    [dask] Fix missing value for scikit-learn interface. (#5435)
    [dask] Accept other inputs for prediction. (#5428)
    
    
    * Returns a series when input is dataframe.
    
    * Merge assert client.
    [dask] Check non-equal when setting threads. (#5421)
    
    * Check non-equal.
    
    `nthread` can be restored from internal parameter, which is mis-interpreted as
    user defined parameter.
    
    * Check None.
    [dask] Enable gridsearching with skl. (#5417)
    [dask] Honor `nthreads` from dask worker. (#5414)
    [dask] Order the prediction result. (#5416)
    [dask] Use `DMLC_TASK_ID`. (#5415)
    [dask] passed through verbose for dask fit (#5413)
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    [Breaking] Remove Scikit-Learn default parameters (#5130)
    
    * Simplify Scikit-Learn parameter management.
    
    * Copy base class for removing duplicated parameter signatures.
    * Set all parameters to None.
    * Handle None in set_param.
    * Extract the doc.
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Note for `DaskDMatrix`. (#5144)
    
    * Brief introduction to `DaskDMatrix`.
    
    * Add xgboost.dask.train to API doc
    Support dask dataframe as y for classifier. (#5077)
    
    * Support dask dataframe as y for classifier.
    
    * Lint.
    Assert dask client at early stage. (#5048)
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Don't `set_params` at the end of `set_state`. (#4947)
    
    * Don't set_params at the end of set_state.
    
    * Also fix another issue found in dask prediction.
    
    * Add note about prediction.
    
    Don't support other prediction modes at the moment.
    Fix dask prediction. (#4941)
    
    * Fix dask prediction.
    
    * Add better error messages for wrong partition.
    Use `cudf.concat` explicitly. (#4918)
    
    * Use `cudf.concat` explicitly.
    
    * Add test.
    Resolve dask performance issues (#4914)
    
    * Set dask client.map as impure function
    
    * Remove nrows
    
    * Remove slow check in verbose mode
    Rewrite Dask interface. (#4819)
    [python package] include dmlc-tracker into xgb python pkg (#4731)
    Support Dask 2.0 (#4617)
    Update doc for feature constraints and `n_gpus`. (#4596)
    
    * Update doc for feature constraints.
    
    * Fix some warnings.
    
    * Clean up doc for `n_gpus`.
    Fix dask API sphinx docstrings (#4507)
    
    * Fix dask API sphinx docstrings
    
    * Update GPU docs page
    Add native support for Dask (#4473)
    
    * Add native support for Dask
    
    * Add multi-GPU demo
    
    * Add sklearn example
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    Don't set seed on CLI interface. (#5563)
    Serialise booster after training to reset state (#5484)
    
    * Serialise booster after training to reset state
    
    * Prevent process_type being set on load
    
    * Check for correct updater sequence
    Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)
    [Breaking] Remove Scikit-Learn default parameters (#5130)
    
    * Simplify Scikit-Learn parameter management.
    
    * Copy base class for removing duplicated parameter signatures.
    * Set all parameters to None.
    * Handle None in set_param.
    * Extract the doc.
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Clean up Python 2 compatibility code. (#5161)
    [Breaking] Remove `learning_rates` in Python. (#5155)
    
    * Remove `learning_rates`.
    
    It's been deprecated since we have callback.
    
    * Set `before_iteration` of `reset_learning_rate` to False to preserve
      the initial learning rate, and comply to the term "reset".
    
    Closes #4709.
    
    * Tests for various `tree_method`.
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Fix early stopping in the Python package (#4638)
    
    * Fix #4630, #4421: Preserve correct ordering between metrics, and always use last metric for early stopping
    
    * Clarify semantics of early stopping in presence of multiple valid sets and metrics
    
    * Add a test
    
    * Fix lint
    Add support for cross-validation using query ID (#4474)
    
    * adding support for matrix slicing with query ID for cross-validation
    
    * hail mary test of unrar installation for windows tests
    
    * trying to modify tests to run in Github CI
    
    * Remove dependency on wget and unrar
    
    * Save error log from R test
    
    * Relax assertion in test_training
    
    * Use int instead of bool in C function interface
    
    * Revise R interface
    
    * Add XGDMatrixSliceDMatrixEx and keep old XGDMatrixSliceDMatrix for API compatibility
    Migrate pylint check to Python 3 (#4381)
    
    * Migrate lint to Python 3
    
    * Fix lint errors
    
    * Use Miniconda3 to use Python 3.7
    
    * Use latest pylint and astroid
    Fix #3663: Allow sklearn API to use callbacks (#3682)
    
    * Fix #3663: Allow sklearn API to use callbacks
    
    * Fix lint
    
    * Add Callback API to Python API doc
    Revert #3677 and #3674 (#3678)
    
    * Revert "Add scikit-learn as dependency for doc build (#3677)"
    
    This reverts commit 308f664ade0547242608e21f6198c895415f03da.
    
    * Revert "Add scikit-learn tests (#3674)"
    
    This reverts commit d176a0fbc8165e3afe3e42ff464ab7b253211555.
    Add scikit-learn tests (#3674)
    
    * Add scikit-learn tests
    
    Goal is to pass scikit-learn's check_estimator() for XGBClassifier,
    XGBRegressor, and XGBRanker. It is actually not possible to do so
    entirely, since check_estimator() assumes that NaN is disallowed,
    but XGBoost allows for NaN as missing values. However, it is always
    good ideas to add some checks inspired by check_estimator().
    
    * Fix lint
    
    * Fix lint
    Update Python API doc (#3619)
    
    * Add XGBRanker to Python API doc
    
    * Show inherited members of XGBRegressor in API doc, since XGBRegressor uses default methods from XGBModel
    
    * Add table of contents to Python API doc
    
    * Skip JVM doc download if not available
    
    * Show inherited members for XGBRegressor and XGBRanker
    
    * Expose XGBRanker to Python XGBoost module directory
    
    * Add docstring to XGBRegressor.predict() and XGBRanker.predict()
    
    * Fix rendering errors in Python docstrings
    
    * Fix lint
    allow arbitrary cross validation fold indices (#3353)
    
    * allow arbitrary cross validation fold indices
    
     - use training indices passed to `folds` parameter in `training.cv`
     - update doc string
    
    * add tests for arbitrary fold indices
    Several fixes (#2572)
    
    * repared serialization after update process; fixes #2545
    
    * non-stratified folds in python could omit some data instances
    
    * Makefile: fixes for older makes on windows; clean R-package too
    
    * make cub to be a shallow submodule
    
    * improve $(MAKE) recovery
    option to shuffle data in mknfolds (#1459)
    
    * option to shuffle data in mknfolds
    
    * removed possibility to run as stand alone test
    
    * split function def in 2 lines for lint
    
    * option to shuffle data in mknfolds
    
    * removed possibility to run as stand alone test
    
    * split function def in 2 lines for lint
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [python-package] Provide a learning_rates parameter to xgb.cv() (#1770)
    
    * Allow using learning_rates parameter when doing CV
    
    - Create a new `callback_cv` method working when called from `xgb.cv()`
    - Rename existing `callback` into `callback_train` and make it the default callback
    - Get the logic out of the callbacks and place it into a common helper
    
    * Add a learning_rates parameter to cv()
    
    * lint
    
    * remove caller explicit reference
    
    * callback is aware of its calling context
    
    * remove caller argument
    
    * remove learning_rates param
    
    * restore learning_rates for training, but deprecated
    
    * lint
    
    * lint line too long
    
    * quick example for predefined callbacks
    Fix typo - eval_metric in param should be dictionary (#1791)
    Fix mknfold using new StratifiedKFold API (#1660)
    remove a redundant sentence, and a word 'and' (#1526)
    
    * fix a typo
    
    * fix a typo and some code format
    
    * Update training.py
    
    * delete redundant sentence
    [PYTHON] Refactor trainnig API to use callback
    fix VisibleDeprecationWarning
    [py] eta decay bugfix
    Enable flake8
    improved docstring for folds in cv function
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    changed the param show_progress by verbose_eval in cv and aggcv functions
    stratified cv for python wrapper
    
    finalize docstring
    python cv bugfixing
    
    - fixed bug if both eval_metrics xgb-param and
    metrics param of cv function have been set
    - cv early stopping output looks now like the one of xgb.train
    cv: fixed devision by zero exception
    
    - show_progress=False or show_progress=0 led to devision by zero exception
    fixed wrong iter when using training continuation
    docstring typo
    updating docs for cv
    modifying cv show_progress to allow print-every-n behavior
    small verbose_eval fixes
    
    - ensures same behavior for verbose_eval=0 and verbose_eval=False
    - fix printing last eval message if early_stopping_rounds is set, but xgb
      runs to the end
    Python verbose_eval extension
    
    This is an extension of the verbose_eval abilities.
    
    Removed some trailing-whitespaces
    Fixed minor lint issue
    Clarification for best_ntree_limit
    best_ntree_limit attribute added
    
    - best_ntree_limit as new booster atrribute added
    - usage of bst.best_ntree_limit in python doc added
    - fixed wrong 'best_iteration' after training continuation
    Update training.py
    
    pylint compliancy
    Update training.py
    
    avoid dict comprehension for python 2.6 compatibility
    Clarification for learning_rates
    grammar correction
    python: multiple eval_metrics changes
    
    - allows feval to return a list of tuples (name, error/score value)
    - changed behavior for multiple eval_metrics in conjunction with
    early_stopping: Instead of raising an error, the last passed evel_metric
    (or last entry in return value of feval) is used for early stopping
    - allows list of eval_metrics in dict-typed params
    - unittest for new features / behavior
    
    documentation updated
    
    - example for assigning a list to 'eval_metric'
    - note about early stopping on last passed eval metric
    
    - info msg for used eval metric added
    early stopping for CV (python)
    bugfix type-check xgb_model param
    python train additions
    
    + training continuation of existing model
    + maximize parameter just like in R package (whether  to maximize feval)
    Suppress more evaluation verbose during training
    Fixed bug in eta decay (+2 squashed commits)
    Squashed commits:
    [b67caf2] Fix build
    [365ceaa] Fixed bug in eta decay
    fixed "Exactly one space required after comma"
    learning_rates per boosting round
    fix training.py for evals_result in python3
    One line was too long
    Updated the documentation a bit
    
    Will upload some demos for guide-python later.
    Removed extra spaces
    Update training.py
    Update training.py
    
    Made changes to training.py to make sure all eval_metric information get passed to evals_result. Previous version lost and mislabeled data in evals_result when using more than one eval_metric.
    
    Structure of eval_metric is now:
    eval_metric[evals][eval_metric] = list of metrics
    
    Example:
    
    >>> dtrain = xgb.DMatrix('agaricus.txt.train', silent=True)
    >>> dtest = xgb.DMatrix('agaricus.txt.test', silent=True)
    
    >>> param = [('max_depth', 2), ('objective', 'binary:logistic'), ('bst:eta', 0.01), ('eval_metric', 'logloss'), ('eval_metric', 'error')]
    
    >>> watchlist  = [(dtest,'eval'), (dtrain,'train')]
    >>> num_round = 3
    >>> evals_result = {}
    >>> bst = xgb.train(param, dtrain, num_round, watchlist, evals_result=evals_result)
    
    >>> print(evals_result['eval']['logloss'])
    >>> print(evals_result)
    
    Prints:
    
    ['0.684877', '0.676767', '0.668817']
    
    {'train': {'logloss': ['0.684954', '0.676917', '0.669036'], 'error': ['0.04652', '0.04652', '0.04652']}, 'eval': {'logloss': ['0.684877', '0.676767', '0.668817'], 'error': ['0.042831', '0.042831', '0.042831']}}
    CV returns ndarray or DataFrame
    bugfix evals_result regex
    checkin all python
    Cleanup Python code. (#6223)
    
    
    * Remove pathlike as XGBoost 1.2 requires Python 3.6.
    * Move conditional import of dask/distributed into dask module.
    Fix exception causes all over the codebase (#5787)
    [dask] Return GPU Series when input is from cuDF. (#5710)
    
    
    * Refactor predict function.
    Implement Python data handler. (#5689)
    
    
    * Define data handlers for DMatrix.
    * Throw ValueError in scikit learn interface.
    Define lazy isinstance for Python compat. (#5364)
    
    * Avoid importing datatable.
    * Fix #5363.
    Save Scikit-Learn attributes into learner attributes. (#5245)
    
    * Remove the recommendation for pickle.
    
    * Save skl attributes in booster.attr
    
    * Test loading scikit-learn model with native booster.
    Support dmatrix construction from cupy array (#5206)
    Fix feature_name crated from int64index dataframe. (#5081)
    Clean up Python 2 compatibility code. (#5161)
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Use `cudf.concat` explicitly. (#4918)
    
    * Use `cudf.concat` explicitly.
    
    * Add test.
    Support feature names/types for cudf. (#4902)
    
    * Implement most of the pandas procedure for cudf except for type conversion.
    * Requires an array of interfaces in metainfo.
    Add support for cudf.Series (#4891)
    Rewrite Dask interface. (#4819)
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    add os.PathLike support for file paths to DMatrix and Booster Python classes (#4757)
    Fix Python demo and doc. (#4545)
    
    * Remove old doc.
    * Fix checking __stdin__.
    Add native support for Dask (#4473)
    
    * Add native support for Dask
    
    * Add multi-GPU demo
    
    * Add sklearn example
    Migrate pylint check to Python 3 (#4381)
    
    * Migrate lint to Python 3
    
    * Fix lint errors
    
    * Use Miniconda3 to use Python 3.7
    
    * Use latest pylint and astroid
    Update datatable usage (#4123)
    Allow import via python datatable. (#3272)
    
    * Allow import via python datatable.
    
    * Write unit tests
    
    * Refactor dt API functions
    
    * Refactor python code
    
    * Lint fixes
    
    * Address review comments
    Fix MultiIndex detection (breaks for latest pandas==0.21.0). (#2872)
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Fix mknfold using new StratifiedKFold API (#1660)
    add scikit-learn v0.18 compatibility (#1636)
    
    * add scikit-learn v0.18 compatibility
    
    import KFold & StratifiedKFold from sklearn.model_selection instead of sklearn.cross_validation
    
    * change DeprecationWarning to ImportError
    
    DeprecationWarning isn't an exception, so it should work the other way around.
    [PYTHON] Refactor trainnig API to use callback
    DOC/TST: Fix Python sklearn dep
    Enable flake8
    [PYTHON-DIST] Distributed xgboost python training API.
    temp compatibility with sklearn
    stratified cv for python wrapper
    
    finalize docstring
    Fixed all lint errors
    Cleanup pandas support
    Fix cls typo. (#6247)
    Limit tree depth for GPU hist. (#6045)
    Fix scikit learn cls doc. (#6041)
    Feature weights (#5962)
    Fix sklearn doc. (#5980)
    Disable feature validation on sklearn predict prob. (#5953)
    
    
    * Fix issue when scikit learn interface receives transformed inputs.
    Add new skl model attribute for number of features (#5780)
    Let XGBoostError inherit ValueError. (#5696)
    Implement Python data handler. (#5689)
    
    
    * Define data handlers for DMatrix.
    * Throw ValueError in scikit learn interface.
    Update document. (#5572)
    Assert matching length of evaluation inputs. (#5540)
    Fix skl nan tag. (#5538)
    Fix checking booster. (#5505)
    
    * Use `get_params()` instead of `getattr` intrinsic.
    Enable parameter validation for skl. (#5477)
    Fix simple typo: information.c -> information (#5384)
    
    Closes #5383
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    Improved sklearn compatibility (#5255)
    Export JSON config in `get_params`. (#5256)
    Save Scikit-Learn attributes into learner attributes. (#5245)
    
    * Remove the recommendation for pickle.
    
    * Save skl attributes in booster.attr
    
    * Test loading scikit-learn model with native booster.
    Add constraint parameters to Scikit-Learn interface. (#5227)
    
    * Add document for constraints.
    
    * Fix a format error in doc for objective function.
    [Breaking] Remove Scikit-Learn default parameters (#5130)
    
    * Simplify Scikit-Learn parameter management.
    
    * Copy base class for removing duplicated parameter signatures.
    * Set all parameters to None.
    * Handle None in set_param.
    * Extract the doc.
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Disable parameter validation for Scikit-Learn interface. (#5167)
    
    * Disable parameter validation for now.
    
    Scikit-Learn passes all parameters down to XGBoost, whether they are used or
    not.
    
    * Add option `validate_parameters`.
    Add base margin to sklearn interface. (#5151)
    Allow using RandomState object from Numpy in sklearn interface. (#5049)
    Update GPU doc. (#4953)
    Fix incorrectly displayed Note in the doc (#4943)
    [Breaking] Update sklearn interface. (#4929)
    
    
    * Remove nthread, seed, silent. Add tree_method, gpu_id, num_parallel_tree. Fix #4909.
    * Check data shape. Fix #4896.
    * Check element of eval_set is tuple. Fix #4875
    *  Add doc for random_state with hogwild. Fixes #4919
    Rewrite Dask interface. (#4819)
    Fix early stopping in the Python package (#4638)
    
    * Fix #4630, #4421: Preserve correct ordering between metrics, and always use last metric for early stopping
    
    * Clarify semantics of early stopping in presence of multiple valid sets and metrics
    
    * Add a test
    
    * Fix lint
    Add warning when save_model() is called from scikit-learn interface (#4632)
    Update doc for feature constraints and `n_gpus`. (#4596)
    
    * Update doc for feature constraints.
    
    * Fix some warnings.
    
    * Clean up doc for `n_gpus`.
    Set reg_lambda=1e-5 for scikit-learn-like random forest classes. (#4558)
    Fix docstring for XGBModel.predict() [skip ci] (#4592)
    Remove remaining reg:linear. (#4544)
    Fix #4497: Enable feature importance property for DART booster (#4525)
    Migrate pylint check to Python 3 (#4381)
    
    * Migrate lint to Python 3
    
    * Fix lint errors
    
    * Use Miniconda3 to use Python 3.7
    
    * Use latest pylint and astroid
    Mark Scikit-Learn RF interface as experimental in doc. (#4258)
    
    * Mark Scikit-Learn RF interface as experimental in doc.
    Brought the silent parameter for the SKLearn-like API back, marked it deprecated. (#4255)
    
    * Brought the silent parameter for the SKLearn-like API back, marked it deprecated.
    
    - added deprecation notice and warning
    - removed silent from the tests for the SKLearn-like API
    Added SKLearn-like random forest Python API. (#4148)
    
    
    * Added SKLearn-like random forest Python API.
    
    - added XGBRFClassifier and XGBRFRegressor classes to SKL-like xgboost API
    - also added n_gpus and gpu_id parameters to SKL classes
    - added documentation describing how to use xgboost for random forests,
      as well as existing caveats
    Update Python docstring for ranking functions (#4121)
    
    * Update Python docstring for ranking functions
    
    * Fix formatting
    enable xgb_model in scklearn XGBClassifier and test. (#4092)
    
    * Enable xgb_model parameter in XGClassifier scikit-learn API
    
    https://github.com/dmlc/xgboost/issues/3049
    
    * add test_XGBClassifier_resume():
    
    test for xgb_model parameter in XGBClassifier API.
    
    * Update test_with_sklearn.py
    
    * Fix lint
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    scikit-learn api section documentation correction (#3967)
    
    * update description of early stopping rounds
    
    the description of early stopping round was quite inconsistent in the scikit-learn api section since the fit paragraph tells that when early stopping rounds occurs, the last iteration is returned not the best one, but the predict paragraph tells that when the predict is called without ntree_limit specified, then ntree_limit is equals to best_ntree_limit.
    
    Thus, when reading the fit part, one could think that it is needed to specify what is the best iter when calling the predict, but when reading the predict part, then the best iter is given by default, it is the last iter that you have to specify if needed.
    
    * Update sklearn.py
    
    * Update sklearn.py
    
    fix doc according to the python_lightweight_test error
    Fix #3894: Allow loading pickles without self.booster attributes (#3938)
    
    The addition of self.booster attribute broke backward compatibility.
    use gain for sklearn feature_importances_ (#3876)
    
    * use gain for sklearn feature_importances_
    
    `gain` is a better feature importance criteria than the currently used `weight`
    
    * added importance_type to class
    
    * fixed test
    
    * white space
    
    * fix variable name
    
    * fix deprecation warning
    
    * fix exp array
    
    * white spaces
    Fix coef_ and intercept_ signature to be compatible with sklearn.RFECV (#3873)
    
    * Fix coef_ and intercept_ signature to be compatible with sklearn.RFECV
    
    * Fix lint
    
    * Fix lint
    Fix #3747: Add coef_ and intercept_ as properties of sklearn wrapper (#3855)
    
    * Fix #3747: Add coef_ and intercept_ as properties of sklearn wrapper
    
    Scikit-learn expects linear learners to expose `coef_` and `intercept_`
    as properties.
    
    Closes #3747.
    
    * Fix lint
    Allow XGBRanker sklearn interface to use other xgboost ranking objectives (#3848)
    Recommend pickling as the way to save XGBClassifier / XGBRegressor / XGBRanker (#3829)
    
    The `save_model()` and `load_model()` method only saves the part of the model
    that's common to all language interfaces and do not preserve Python-specific
    attributes, such as `feature_names`. More crucially, label encoder is not
    preserved either; this is needed for the scikit-learn wrapper, since you may
    have string labels.
    
    Fix: Explicitly recommend pickling as the way to save scikit-learn model
    objects.
    Allow sklearn grid search over parameters specified as kwargs (#3791)
    Add notes to doc (#3765)
    Fix #3663: Allow sklearn API to use callbacks (#3682)
    
    * Fix #3663: Allow sklearn API to use callbacks
    
    * Fix lint
    
    * Add Callback API to Python API doc
    Revert #3677 and #3674 (#3678)
    
    * Revert "Add scikit-learn as dependency for doc build (#3677)"
    
    This reverts commit 308f664ade0547242608e21f6198c895415f03da.
    
    * Revert "Add scikit-learn tests (#3674)"
    
    This reverts commit d176a0fbc8165e3afe3e42ff464ab7b253211555.
    Add scikit-learn tests (#3674)
    
    * Add scikit-learn tests
    
    Goal is to pass scikit-learn's check_estimator() for XGBClassifier,
    XGBRegressor, and XGBRanker. It is actually not possible to do so
    entirely, since check_estimator() assumes that NaN is disallowed,
    but XGBoost allows for NaN as missing values. However, it is always
    good ideas to add some checks inspired by check_estimator().
    
    * Fix lint
    
    * Fix lint
    Fix #3648: XGBClassifier.predict() should return margin scores when output_margin=True (#3651)
    
    * Fix #3648: XGBClassifier.predict() should return margin scores when output_margin=True
    
    * Fix tests to reflect correct implementation of XGBClassifier.predict(output_margin=True)
    
    * Fix flaky test test_with_sklearn.test_sklearn_api_gblinear
    Add validate_features parameter to sklearn API (#3653)
    Update Python API doc (#3619)
    
    * Add XGBRanker to Python API doc
    
    * Show inherited members of XGBRegressor in API doc, since XGBRegressor uses default methods from XGBModel
    
    * Add table of contents to Python API doc
    
    * Skip JVM doc download if not available
    
    * Show inherited members for XGBRegressor and XGBRanker
    
    * Expose XGBRanker to Python XGBoost module directory
    
    * Add docstring to XGBRegressor.predict() and XGBRanker.predict()
    
    * Fix rendering errors in Python docstrings
    
    * Fix lint
    sklearn api for ranking (#3560)
    
    * added xgbranker
    
    * fixed predict method and ranking test
    
    * reformatted code in accordance with pep8
    
    * fixed lint error
    
    * fixed docstring and added checks on objective
    
    * added ranking demo for python
    
    * fixed suffix in rank.py
    Revert "Fix #3485, #3540: Don't use dropout for predicting test sets" (#3563)
    
    * Revert "Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)"
    
    This reverts commit 44811f233071c5805d70c287abd22b155b732727.
    
    * Document behavior of predict() for DART booster
    
    * Add notice to parameter.rst
    python sklearn api: defaulting to best_ntree_limit if defined, otherwise current behaviour (#3445)
    
    * python sklearn api: defaulting to best_ntree_limit if defined, otherwise current behaviour
    
    * Fix whitespace
    Save and load model in sklearn API (#3192)
    
    * Add (load|save)_model to XGBModel
    
    * Add docstring
    
    * Fix docstring
    
    * Fix mixed use of space and tab
    
    * Add a test
    
    * Fix Flake8 style errors
    Remove output_margin from XGBClassifier.predict_proba argument list. (#3343)
    Sklearn: validation set weights (#2354)
    
    * Add option to use weights when evaluating metrics in validation sets
    
    * Add test for validation-set weights functionality
    
    * simplify case with no weights for test sets
    
    * fix lint issues
    adding some docs based on `core.Boost.predict` (#1865)
    delete duplicated code in python-package (#2985)
    Add xgb_model parameter to sklearn fit (#2623)
    
    Adding xgb_model paramter allows the continuation of model training.
    Model has to be saved by calling `model.get_booster().save_model(path)`
    Check existance of seed/nthread keys before checking their value. (#2669)
    python: follow the default warning filters of Python (#2666)
    
    * python: follow the default warning filters of Python
    
    https://docs.python.org/3/library/warnings.html#default-warning-filters
    
    * update tests
    
    * update tests
    Update sklearn API to pass along n_jobs to DMatrix creation (#2658)
    Fix typo in sklearn documentation (#2580)
    [python-package] fix sklearn n_jobs/nthreads and seed/random_state bug  (#2378)
    
    * add a testcase causing RuntimeError
    
    * move seed/random_state/nthread/n_jobs check to get_xgb_params()
    
    * fix failed test
    Sklearn kwargs (#2338)
    
    * Added kwargs support for Sklearn API
    
    * Updated NEWS and CONTRIBUTORS
    
    * Fixed CONTRIBUTORS.md
    
    * Added clarification of **kwargs and test for proper usage
    
    * Fixed lint error
    
    * Fixed more lint errors and clf assigned but never used
    
    * Fixed more lint errors
    
    * Fixed more lint errors
    
    * Fixed issue with changes from different branch bleeding over
    
    * Fixed issue with changes from other branch bleeding over
    
    * Added note that kwargs may not be compatible with Sklearn
    
    * Fixed linting on kwargs note
    Sklearn convention update (#2323)
    
    * Added n_jobs and random_state to keep up to date with sklearn API.
    Deprecated nthread and seed.  Added tests for new params and
    deprecations.
    
    * Fixed docstring to reflect updates to n_jobs and random_state.
    
    * Fixed whitespace issues and removed nose import.
    
    * Added deprecation note for nthread and seed in docstring.
    
    * Attempted fix of deprecation tests.
    
    * Second attempted fix to tests.
    
    * Set n_jobs to 1.
    Add option to choose booster in scikit intreface (gbtree by default) (#2303)
    
    * Add option to choose booster in scikit intreface (gbtree by default)
    
    * Add option to choose booster in scikit intreface: complete docstring.
    
    * Fix XGBClassifier to work with booster option
    
    * Added test case for gblinear booster
    adding sample weights for XGBRegressor (was this forgotten?) (#1874)
    Move feature_importances_ to base XGBModel for XGBRegressor access (#1591)
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [py] fix label encoding of eval sets in sklearn API (#1244)
    Preserve the actal objective used on the booster
    
    Save the actual objective used on xgboost.train.
    
    Not saving it was giving problem in predict_proba, as issue  #1215
    Update sklearn.py
    [PYTHON] Refactor trainnig API to use callback
    cosmetic change
    
    cosmetic change of putting space after comma compared to previous edit.
    XGBModel.fit had a call to DMatrix without missing=self.missing. fixed that
    [py] added apply function in sklearn API to return the predicted leaves
    Added other feature importances in python package (#1135)
    
    * added new function to calculate other feature importances
    
    * added capability to plot other feature importance measures
    
    * changed plotting default to fscore
    
    * added info on importance_type to boilerplate comment
    
    * updated text of error statement
    
    * added self module name to fix call
    
    * added unit test for feature importances
    
    * style fixes
    DOC/TST: Fix Python sklearn dep
    Bug mixing DMatrix's with and without feature names
    BUG: XGBClassifier.feature_importances_ raises ValueError if input is pandas DataFrame
    Enable flake8
    change type of xgbclassifier.classes_ from list to numpy array
    XGBClassifier.feature_importances_ compatible with sklearn RFECV
    return best_ntree_limit if early stopped
    Created decorator function so that custom objective function passed to the constructor are more consistent with the sklearn conventions. Added comments in the doc string
    Added the possibility to use custom objective function in the sklearn API
    add feature_importances_ property for XGBClassifier
    Fixes #725
    Fixed all lint errors
    Cleanup pandas support
    sklearn_wrapper additions
    
    added output_margin & ntree_limit to predict and predict_proba
    correcting wrong default values
    fixed too long lines
    added missing params
    fix sklearn.py for evals_result in python3
    Removed th last few trailing whitespaces
    Removed trailing whitespaces and Change Error to XGBoostError
    Made eval_results for sklearn output the same structure as in the new training.py
    
    Changed the name of eval_results to evals_result, so that the naming is the same in training.py and sklearn.py
    
    Made the structure of evals_result the same as in training.py, the names of the keys are different:
    
    In sklearn.py you cannot name your evals_result, but they are automatically called 'validation_0', 'validation_1' etc.
    The dict evals_result will output something like: {'validation_0': {'logloss': ['0.674800', '0.657121']}, 'validation_1': {'logloss': ['0.63776', '0.58372']}}
    
    In training.py you can name your multiple evals_result with a watchlist like: watchlist  = [(dtest,'eval'), (dtrain,'train')]
    The dict evals_result will output something like: {'train': {'logloss': ['0.68495', '0.67691']}, 'eval': {'logloss': ['0.684877', '0.676767']}}
    
    You can access the evals_result using the evals_result() function.
    Too many branches and unused key
    update sklearn.py because evals_result in training.py changed
    
    Because I changed the training.py, the sklearn.py had to be changed also to be able to read all the data form evals_result.
    make XGBClassifier.score compatible with arrays
    enable basic sphinx doc
    checkin all python
    Rewrite setup.py. (#5271)
    
    The setup.py is rewritten.  This new script uses only Python code and provide customized
    implementation of setuptools commands.  This way users can run most of setuptools commands
    just like any other Python libraries.
    
    * Remove setup_pip.py
    * Remove soft links.
    * Define customized commands.
    * Remove shell script.
    * Remove makefile script.
    * Update the doc for building from source.
    add dll_path for cygwin users (#4499)
    Migrate pylint check to Python 3 (#4381)
    
    * Migrate lint to Python 3
    
    * Fix lint errors
    
    * Use Miniconda3 to use Python 3.7
    
    * Use latest pylint and astroid
    freebsd support in libpath.py (#3247)
    Fixed shared library loading in the Python package (#2461)
    
    * Fixed DLL name on Windows in ``xgboost.libpath``
    
    * Added support for OS X to ``xgboost.libpath``
    
    * Use .dylib for shared library on OS X
    
    This does not affect the JNI library, because it is not trully
    cross-platform in the Makefile-build anyway.
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [python-package] modify libpath.py and fix typos (#1594)
    
    * Update Makefile
    
    * Update Makefile
    
    * modify __init__.py
    
    * modified libpath.py and fixed typos
    Fixes typo "candicate" (#1508)
    Make python package wheelable (#1500)
    
    Currently xgboost can only be installed by running:
    
        python setup.py install
    
    Now it can be packaged (in binary form) as a wheel and installed like:
    
        pip install xgboost-0.6-py2-none-any.whl
    
    distutils and wheel install `data_files` differently than setuptools.
    setuptools will install the `data_files` in the package directory whereas the
    others install it in `sys.prefix`. By adding `sys.prefix` to the list of
    directories to check for the shared library, xgboost can now be distributed as
    a wheel.
    Enable flake8
    [LIBXGBOOST] pass demo running.
    separate setup.py with pip installation, add trouble shooting page
    Fix python setup: avoid import numpy in setup.py
    
    Currently `pip install xgboost` will raise traceback like this
    
    ```
    Traceback (most recent call last):
      File "<string>", line 20, in <module>
      File "/tmp/pip-build-IAdqYE/xgboost/setup.py", line 20, in <module>
        import xgboost
      File "./xgboost/__init__.py", line 8, in <module>
        from .core import DMatrix, Booster
      File "./xgboost/core.py", line 12, in <module>
        import numpy as np
    ImportError: No module named numpy
    ```
    
    We should avoid importing numpy in setup.py and let pip install numpy and scipy automatically.
    That's what `install_requires` for.
    Fix exception causes all over the codebase (#5787)
    Allow pass fmap to importance plot (#5719)
    
    Co-authored-by: Peter Jung <peter.jung@heureka.cz>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Note for `DaskDMatrix`. (#5144)
    
    * Brief introduction to `DaskDMatrix`.
    
    * Add xgboost.dask.train to API doc
    Implement tree model dump with code generator. (#4602)
    
    * Implement tree model dump with a code generator.
    
    * Split up generators.
    * Implement graphviz generator.
    * Use pattern matching.
    
    * [Breaking] Return a Source in `to_graphviz` instead of Digraph in Python package.
    
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Update doc for feature constraints and `n_gpus`. (#4596)
    
    * Update doc for feature constraints.
    
    * Fix some warnings.
    
    * Clean up doc for `n_gpus`.
    Address some sphinx warnings and errors, add doc for building doc. (#4589)
    Migrate pylint check to Python 3 (#4381)
    
    * Migrate lint to Python 3
    
    * Fix lint errors
    
    * Use Miniconda3 to use Python 3.7
    
    * Use latest pylint and astroid
    Add parameter to make node type configurable in plot tree (#3859)
    
    * add parameters 'conditionNodeParams' and 'leafNodeParams' to function `to_graphviz` enable to configure node type
    Update Python API doc (#3619)
    
    * Add XGBRanker to Python API doc
    
    * Show inherited members of XGBRegressor in API doc, since XGBRegressor uses default methods from XGBModel
    
    * Add table of contents to Python API doc
    
    * Skip JVM doc download if not available
    
    * Show inherited members for XGBRegressor and XGBRanker
    
    * Expose XGBRanker to Python XGBoost module directory
    
    * Add docstring to XGBRegressor.predict() and XGBRanker.predict()
    
    * Fix rendering errors in Python docstrings
    
    * Fix lint
    update doc string for grid parameter (#2647)
    
    * update doc string for grid parameter
    
    * update doc string for grid parameter
    Add show_values option to feature importances plot (#2351)
    
    Adding an option to remove the values from the features importances plot in Python.
    Add option to choose booster in scikit intreface (gbtree by default) (#2303)
    
    * Add option to choose booster in scikit intreface (gbtree by default)
    
    * Add option to choose booster in scikit intreface: complete docstring.
    
    * Fix XGBClassifier to work with booster option
    
    * Added test case for gblinear booster
    fix ylim with max_num_features in python plot_importance (#1974)
    added the max_features parameter to the plot_importance function. (#1963)
    
    * added the max_features parameter to the plot_importance function.
    
    * renamed max_features parameter to max_num_features for better understanding
    
    * removed unwanted character in docstring
    python package tree plotting support fmap (#1856)
    
    * to_graphviz and plot_tree support fmap
    
    * [python-package] add model_plot docstring
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Added other feature importances in python package (#1135)
    
    * added new function to calculate other feature importances
    
    * added capability to plot other feature importance measures
    
    * changed plotting default to fscore
    
    * added info on importance_type to boilerplate comment
    
    * updated text of error statement
    
    * added self module name to fix call
    
    * added unit test for feature importances
    
    * style fixes
    Enable flake8
    Separate dependencies and lightweight test env for Python
    Fixed all lint errors
    Python: adjusts plot_importance ylim
    Allow plot function to handle XGBModel
    Add feature_types
    Fix for python 3
    ENH: Add visualization to python package
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    Fix typo in xgboost.callback.early_stop docstring (#6071)
    Fix uninitialized value bug in xgboost callback (#5463)
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Clean up Python 2 compatibility code. (#5161)
    [Breaking] Remove `learning_rates` in Python. (#5155)
    
    * Remove `learning_rates`.
    
    It's been deprecated since we have callback.
    
    * Set `before_iteration` of `reset_learning_rate` to False to preserve
      the initial learning rate, and comply to the term "reset".
    
    Closes #4709.
    
    * Tests for various `tree_method`.
    eval_metrics print fixed (#4803)
    Empty evaluation list in early stopping should produce meaningful error message (#4633)
    
    * Empty evaluation list should not break early stopping
    
    * Fix lint
    
    * Update callback.py
    Migrate pylint check to Python 3 (#4381)
    
    * Migrate lint to Python 3
    
    * Fix lint errors
    
    * Use Miniconda3 to use Python 3.7
    
    * Use latest pylint and astroid
    Add AUC-PR to list of metrics to maximize for early stopping (#3936)
    Fix #3397: early_stop callback does not maximize metric of form NDCG@n- (#3685)
    
    * Fix #3397: early_stop callback does not maximize metric of form NDCG@n-
    
    Early stopping callback makes splits with '-' letter, which interferes
    with metrics of form NDCG@n-. As a result, XGBoost tries to minimize
    NDCG@n-, where it should be maximized instead.
    
    Fix. Specify maxsplit=1.
    
    * Python 2.x compatibility fix
    Fix #3663: Allow sklearn API to use callbacks (#3682)
    
    * Fix #3663: Allow sklearn API to use callbacks
    
    * Fix lint
    
    * Add Callback API to Python API doc
    Fix for ZeroDivisionError when verbose_eval equals to 0. (#3115)
    Fix typo (#2818)
    
    Fix typo
    bugfix: when metric's name contains `-` (#2090)
    
    When metric's name contains `-`, Python will complain about insufficient arguments to unpack.
    print_evaluation callback output on last iteration (#2036)
    
    verbose_eval docs claim it will log the last iteration (http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train). this is also consistent w/the behavior from 0.4. not a huge deal but I found it handy to see the last iter's result b/c my period is usually large.
    
    this doesn't address logging the last stage found by early_stopping (as noted in docs) as I'm not sure how to do that.
    [python-package] Provide a learning_rates parameter to xgb.cv() (#1770)
    
    * Allow using learning_rates parameter when doing CV
    
    - Create a new `callback_cv` method working when called from `xgb.cv()`
    - Rename existing `callback` into `callback_train` and make it the default callback
    - Get the logic out of the callbacks and place it into a common helper
    
    * Add a learning_rates parameter to cv()
    
    * lint
    
    * remove caller explicit reference
    
    * callback is aware of its calling context
    
    * remove caller argument
    
    * remove learning_rates param
    
    * restore learning_rates for training, but deprecated
    
    * lint
    
    * lint line too long
    
    * quick example for predefined callbacks
    [python-package] Fix the issue #1439 (#1666)
    
    *Fix 1439
            *Fix python_wrapper when eval set name contain '-' will cause early_stop maximize variable con't set to True propely
    
    Change-Id: Ib0595afd4ae7b445a84c00a3a8faeccc506c6d13
    Fix how maximize_metric value is determined in early_stop (#1451)
    
    * Update callback.py
    
    * Update callback.py
    fixed error when eval False (#1271)
    [PYTHON] Refactor trainnig API to use callback
    Cleanup Python code. (#6223)
    
    
    * Remove pathlike as XGBoost 1.2 requires Python 3.6.
    * Move conditional import of dask/distributed into dask module.
    Add high level tests for categorical data. (#6179)
    
    * Fix unique.
    Enable categorical data support on Python DMatrix. (#6166)
    
    
    * Only pandas is recognized.
    Add back support for scipy.sparse.coo_matrix (#6162)
    Modin DF support (#6055)
    
    * Modin DF support
    
    * mode change
    
    * tests were added, ci env was extended
    
    * mode change
    
    * Remove redundant installation of modin
    
    * Add a pytest skip marker for modin
    
    * Install Modin[ray] from PyPI
    
    * fix interfering
    
    * avoid extra conversion
    
    * delete cv test for modin
    
    * revert cv function
    
    Co-authored-by: ShvetsKS <kirill.shvets@intel.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Feature weights (#5962)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Fix missing data warning. (#5969)
    
    * Fix data warning.
    
    * Add numpy/scipy test.
    Dask device dmatrix (#5901)
    
    
    * Fix softprob with empty dmatrix.
    Simplify the data backends. (#5893)
    Accept iterator in device dmatrix. (#5783)
    
    
    * Remove Device DMatrix.
    Move feature names and types of DMatrix from Python to C++. (#5858)
    
    
    * Add thread local return entry for DMatrix.
    * Save feature name and feature type in binary file.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Speed up python test (#5752)
    
    * Speed up tests
    
    * Prevent DeviceQuantileDMatrix initialisation with numpy
    
    * Use joblib.memory
    
    * Use RandomState
    Implement Python data handler. (#5689)
    
    
    * Define data handlers for DMatrix.
    * Throw ValueError in scikit learn interface.
    Fix typo in tracker logging (#5994)
    Clean up Python 2 compatibility code. (#5161)
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    [CI] Update lint configuration to support latest pylint convention (#4971)
    
    * Update lint configuration
    
    * Use gcc 8 consistently in build instruction
    [python package] include dmlc-tracker into xgb python pkg (#4731)
    [python-package] remove unused imports (#5776)
    Require Python 3.6+; drop Python 3.5 from CI (#5715)
    Device dmatrix (#5420)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    Clean up Python 2 compatibility code. (#5161)
    Rewrite Dask interface. (#4819)
    [python package] include dmlc-tracker into xgb python pkg (#4731)
    Add native support for Dask (#4473)
    
    * Add native support for Dask
    
    * Add multi-GPU demo
    
    * Add sklearn example
    Added SKLearn-like random forest Python API. (#4148)
    
    
    * Added SKLearn-like random forest Python API.
    
    - added XGBRFClassifier and XGBRFRegressor classes to SKL-like xgboost API
    - also added n_gpus and gpu_id parameters to SKL classes
    - added documentation describing how to use xgboost for random forests,
      as well as existing caveats
    Update Python API doc (#3619)
    
    * Add XGBRanker to Python API doc
    
    * Show inherited members of XGBRegressor in API doc, since XGBRegressor uses default methods from XGBModel
    
    * Add table of contents to Python API doc
    
    * Skip JVM doc download if not available
    
    * Show inherited members for XGBRegressor and XGBRanker
    
    * Expose XGBRanker to Python XGBoost module directory
    
    * Add docstring to XGBRegressor.predict() and XGBRanker.predict()
    
    * Fix rendering errors in Python docstrings
    
    * Fix lint
    [python-package] modify __init__.py (#1587)
    
    * Update Makefile
    
    * Update Makefile
    
    * modify __init__.py
    Enable flake8
    [PYTHON-DIST] Distributed xgboost python training API.
    [DOC] Update R doc
    update pip building, troubleshooting with new makefile, plus friendly error message when fail importing sklearn
    Fix python setup: avoid import numpy in setup.py
    
    Currently `pip install xgboost` will raise traceback like this
    
    ```
    Traceback (most recent call last):
      File "<string>", line 20, in <module>
      File "/tmp/pip-build-IAdqYE/xgboost/setup.py", line 20, in <module>
        import xgboost
      File "./xgboost/__init__.py", line 8, in <module>
        from .core import DMatrix, Booster
      File "./xgboost/core.py", line 12, in <module>
        import numpy as np
    ImportError: No module named numpy
    ```
    
    We should avoid importing numpy in setup.py and let pip install numpy and scipy automatically.
    That's what `install_requires` for.
    ENH: Add visualization to python package
    enable basic sphinx doc
    checkin all python
    Bump version to 1.3.0 snapshot in master (#6052)
    Bump version to 1.2.0 snapshot in master (#5733)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    upgrade version num (#4670)
    
    * upgrade version num
    
    * missign changes
    
    * fix version script
    
    * change versions
    
    * rm files
    
    * Update CMakeLists.txt
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    Bump Python version number (#4285)
    Release 0.81 version (#3864)
    
    * Release 0.81 version
    
    * Update NEWS.md
    Release version 0.80 (#3541)
    
    * Up versions
    
    * Write release note for 0.80
    Release version 0.72 (#3337)
    Release version 0.71 (#3200)
    Tag version 0.7 (#2975)
    
    * Tag version 0.7
    
    * Document all changes made in year 2016
    Tag version 0.6 (#1422)
    Fix python setup: avoid import numpy in setup.py
    
    Currently `pip install xgboost` will raise traceback like this
    
    ```
    Traceback (most recent call last):
      File "<string>", line 20, in <module>
      File "/tmp/pip-build-IAdqYE/xgboost/setup.py", line 20, in <module>
        import xgboost
      File "./xgboost/__init__.py", line 8, in <module>
        from .core import DMatrix, Booster
      File "./xgboost/core.py", line 12, in <module>
        import numpy as np
    ImportError: No module named numpy
    ```
    
    We should avoid importing numpy in setup.py and let pip install numpy and scipy automatically.
    That's what `install_requires` for.
    Add Python binding for rabit ops. (#5743)
    Clean up Python 2 compatibility code. (#5161)
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    stylistic fix (#1789)
    
    * stylistic fix
    
    * try multiple repos
    
    * fix
    
    * fix
    [PYTHON] Refactor trainnig API to use callback
    Enable flake8
    allow common python output in single node
    Fix rabit
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    python package refactor into python-package
    add msd
    Update the list of winning solutions (#6222)
    Update the list of winning solutions using XGBoost (#6192)
    
    
    Co-authored-by: divya <divyachauhan661@gmail.com>
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [Doc] Add list of winning solutions in data science competitions using XGBoost  (#6177)
    [Doc] Add dtreeviz as a showcase example of integration with 3rd-party software (#6013)
    Add Neptune and Optuna to list of examples (#5528)
    Update demo readme's use case section with BentoML (#4400)
    Add link to InfoWorld 2019 award (#4116)
    Add a new winning solution to demo/README.md (#2778)
    add Qubole example (#2401)
    Update README.md (#2202)
    
    Add a link to a demo for the proposed PHP XGBoost wrapper.
    Update md grammar for the README.md (#2141)
    [jvm-packages] add Spark and XGBoost tutorial (#1649)
    
    * add back train method but mark as deprecated
    
    * add Spark and XGBoost tutorial
    
    * fix scalastyle error
    Update README.md - added windows binaries (#1600)
    
    Added a link to the nightly windows binaries hosted on Guido Tapia's (my) blog
    Added KDD Cup 2016 competition (#1596)
    
    merged thanks
    Practical XGBoost in Python online course (#1542)
    link to talk (video+slides) by Tianqi at Los Angeles Data Science meetup (#1254)
    
    * link to talk (video+slides) by Tianqi
    
    * benchmark
    Update README.md
    
    Winning solution added: "Homesite Quote Conversion" (@Kaggle)
    Update README.md
    Add a new winning solution to demo/README.md
    Update README.md
    Add tpot to Tools using XGBoost
    Complete Guide to Parameter Tuning in XGBoost
    Plugging in gp_xgboost_gridsearch
    Update README.md
    Update README.md
    [DOC] reorg docs
    [doc] update news
    move TOC under title
    add TOC, simplied text in the solution section
    Awesome-XGBoost, first commit
    [DOC] cleanup distributed training
    Update README.md
    
    Link for line 26 was wrong, it pointed out again for the last demo. I was reading the readme and found the subtle inconsistence. Please, accept this minor change. It works correctly now.
    Update README.md
    
    Fixed broken link for R 'First N Trees' sample.
    Document refactor
    
    change badge
    python package refactor into python-package
    chg docs
    move documentation to repo
    update demo readme
    Fixed minor typos.
    Update README.md
    Update README.md
    add predict leaf indices
    Update README.md
    Update README.md
    add glm
    add cv for python
    Update README.md
    fix
    remove R for now
    fix doc
    ok
    push python examples in
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Implement robust regularization in 'survival:aft' objective (#5473)
    
    * Robust regularization of AFT gradient and hessian
    
    * Fix AFT doc; expose it to tutorial TOC
    
    * Apply robust regularization to uncensored case too
    
    * Revise unit test slightly
    
    * Fix lint
    
    * Update test_survival.py
    
    * Use GradientPairPrecise
    
    * Remove unused variables
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Implement robust regularization in 'survival:aft' objective (#5473)
    
    * Robust regularization of AFT gradient and hessian
    
    * Fix AFT doc; expose it to tutorial TOC
    
    * Apply robust regularization to uncensored case too
    
    * Revise unit test slightly
    
    * Fix lint
    
    * Update test_survival.py
    
    * Use GradientPairPrecise
    
    * Remove unused variables
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Add DMatrix usage examples to c-api-demo (#5854)
    
    * Add DMatrix usage examples to c-api-demo
    
    * Add XGDMatrixCreateFromCSREx example
    
    * Add XGDMatrixCreateFromCSCEx example
    [CI] Test C API demo (#6159)
    
    * Fix CMake install config to use dependencies
    
    * [CI] Test C API demo
    
    * Explicitly cast num_feature, to avoid warning in Linux
    Add XGBoosterGetNumFeature (#5856)
    
    - add GetNumFeature to Learner
    - add XGBoosterGetNumFeature to C API
    - update c-api-demo accordingly
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Fix compilation error (#5215)
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    C API example (#4333)
    [CI] Test C API demo (#6159)
    
    * Fix CMake install config to use dependencies
    
    * [CI] Test C API demo
    
    * Explicitly cast num_feature, to avoid warning in Linux
    Enable OpenMP with Apple Clang (Mac default compiler) (#5146)
    
    * Add OpenMP as CMake target
    
    * Require CMake 3.12, to allow linking OpenMP target to objxgboost
    
    * Specify OpenMP compiler flag for CUDA host compiler
    
    * Require CMake 3.16+ if the OS is Mac OSX
    
    * Use AppleClang in Mac tests.
    
    * Update dmlc-core
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add c-api-demo to .gitignore (#5855)
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    C API example (#4333)
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    fix typo "customized" (#5515)
    fix typo (#3188)
    Minor edits to coding style (#2835)
    
    * Some minor changes to the code style
    
    Some minor changes to the code style in file basic_walkthrough.py
    
    * coding style changes
    
    * coding style changes arrcording PEP8
    
    * Update basic_walkthrough.py
    Fix comment in cross_validation.py (#1923)
    
    cv() doesn't output std_value because show_stdv is set to False.
    [PYTHON] Refactor trainnig API to use callback
    Update demo scripts to use installed python library
    add cv for python
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [Breaking] Fix custom metric for multi output. (#5954)
    
    
    * Set output margin to true for custom metric.  This fixes only R and Python.
    [Breaking] Set output margin to True for custom objective. (#5564)
    
    * Set output margin to True for custom objective in Python and R.
    
    * Add a demo for writing multi-class custom objective function.
    
    * Run tests on selected demos.
    Add cache name back to external_memory.py files. (#6088)
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Minor edits to coding style (#2835)
    
    * Some minor changes to the code style
    
    Some minor changes to the code style in file basic_walkthrough.py
    
    * coding style changes
    
    * coding style changes arrcording PEP8
    
    * Update basic_walkthrough.py
    add more docs
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Access xgboost eval metrics by using sklearn
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Minor edits to coding style (#2835)
    
    * Some minor changes to the code style
    
    Some minor changes to the code style in file basic_walkthrough.py
    
    * coding style changes
    
    * coding style changes arrcording PEP8
    
    * Update basic_walkthrough.py
    Add support for Gamma regression (#1258)
    
    * Add support for Gamma regression
    
    * Use base_score to replace the lp_bias
    
    * Remove the lp_bias config block
    
    * Add a demo for running gamma regression in Python
    
    * Typo fix
    
    * Revise the description for objective
    
    * Add a script to generate the autoclaims dataset
    [Breaking] Set output margin to True for custom objective. (#5564)
    
    * Set output margin to True for custom objective in Python and R.
    
    * Add a demo for writing multi-class custom objective function.
    
    * Run tests on selected demos.
    Doc and demo for customized metric and obj. (#4598)
    
    Co-Authored-By: Theodore Vasiloudis <theodoros.vasiloudis@gmail.com>
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Update evals_result.py
    Demo for accessing eval metrics in xgboost
    Fix a comment in demo to use correct reference (#6190)
    
    Co-authored-by: John Quitto Graham <johnq@dgx07.aselab.nvidia.com>
    Update Python custom objective demo. (#5981)
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Doc and demo for customized metric and obj. (#4598)
    
    Co-Authored-By: Theodore Vasiloudis <theodoros.vasiloudis@gmail.com>
    Add option to disable default metric (#3606)
    Fix #3598: document that custom objective can't contain colon (:) (#3601)
    Minor edits to coding style (#2835)
    
    * Some minor changes to the code style
    
    Some minor changes to the code style in file basic_walkthrough.py
    
    * coding style changes
    
    * coding style changes arrcording PEP8
    
    * Update basic_walkthrough.py
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    fix a typo and some code format (#1470)
    
    * fix a typo
    
    * fix a typo and some code format
    Update demo scripts to use installed python library
    move python example
    [Breaking] Set output margin to True for custom objective. (#5564)
    
    * Set output margin to True for custom objective in Python and R.
    
    * Add a demo for writing multi-class custom objective function.
    
    * Run tests on selected demos.
    Doc and demo for customized metric and obj. (#4598)
    
    Co-Authored-By: Theodore Vasiloudis <theodoros.vasiloudis@gmail.com>
    Fix typo: cutomize -> customize (#3073)
    Added evals result demos
    Cleaned up guide-python directory.
    Document refactor
    
    change badge
    add more docs
    add predict leaf indices
    add glm
    add cv for python
    fix doc
    ok
    push python examples in
    move python example
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    Validate weights are positive values. (#6115)
    Accept iterator in device dmatrix. (#5783)
    
    
    * Remove Device DMatrix.
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    [Breaking] Set output margin to True for custom objective. (#5564)
    
    * Set output margin to True for custom objective in Python and R.
    
    * Add a demo for writing multi-class custom objective function.
    
    * Run tests on selected demos.
    Minor edits to coding style (#2835)
    
    * Some minor changes to the code style
    
    Some minor changes to the code style in file basic_walkthrough.py
    
    * coding style changes
    
    * coding style changes arrcording PEP8
    
    * Update basic_walkthrough.py
    cause this code test pickle the booster, so change bst2 -> bst3
    checkin copy
    allow booster to be pickable, add copy function
    Update demo scripts to use installed python library
    add create from csc
    Update basic_walkthrough.py
    move python example
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Minor edits to coding style (#2835)
    
    * Some minor changes to the code style
    
    Some minor changes to the code style in file basic_walkthrough.py
    
    * coding style changes
    
    * coding style changes arrcording PEP8
    
    * Update basic_walkthrough.py
    Update demo scripts to use installed python library
    move python example
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Add base margin to sklearn interface. (#5151)
    Minor edits to coding style (#2835)
    
    * Some minor changes to the code style
    
    Some minor changes to the code style in file basic_walkthrough.py
    
    * coding style changes
    
    * coding style changes arrcording PEP8
    
    * Update basic_walkthrough.py
    Minor changes to code style (#2805)
    
    Some minor changes to code style in file 'boost_from_prediction.py'.
    Update demo scripts to use installed python library
    move python example
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Minor edits to coding style (#2835)
    
    * Some minor changes to the code style
    
    Some minor changes to the code style in file basic_walkthrough.py
    
    * coding style changes
    
    * coding style changes arrcording PEP8
    
    * Update basic_walkthrough.py
    Update predict leaf indices (#2135)
    
    * Updated sklearn_parallel.py for soon-to-be-deprecated modules
    
    * Updated predict_leaf_indices.py; Use python3 print() as other exmaples and removed unused module
    Update demo scripts to use installed python library
    add predict leaf indices
    move python example
    Implement Python data handler. (#5689)
    
    
    * Define data handlers for DMatrix.
    * Throw ValueError in scikit learn interface.
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Minor edits to coding style (#2835)
    
    * Some minor changes to the code style
    
    Some minor changes to the code style in file basic_walkthrough.py
    
    * coding style changes
    
    * coding style changes arrcording PEP8
    
    * Update basic_walkthrough.py
    Updated sklearn_examples.py for soon-to-be-deprecated modules (#2117)
    fix DeprecationWarning on sklearn.cross_validation (#2075)
    
    * fix DeprecationWarning on sklearn.cross_validation
    
    * fix syntax
    
    * fix kfold n_split issue
    
    * fix mistype
    
    * fix n_splits multiple value issue
    
    * split should pass a iterable
    
    * use np.arange instead of xrange, py3 compatibility
    python package refactor into python-package
    DOC: Add early stopping example
    EX: Make separate example for fork issue.
    EX: Show example of pickling and parallel use.
    minor
    Update demo scripts to use installed python library
    Fix some stuff
    Initial commit
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Updated sklearn_parallel.py for soon-to-be-deprecated modules (#2134)
    EX: Make separate example for fork issue.
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Minor edits to coding style (#2835)
    
    * Some minor changes to the code style
    
    Some minor changes to the code style in file basic_walkthrough.py
    
    * coding style changes
    
    * coding style changes arrcording PEP8
    
    * Update basic_walkthrough.py
    Update demo scripts to use installed python library
    add glm
    Feature weights (#5962)
    Example JSON model parser and Schema. (#5137)
    [Breaking] Don't save leaf child count in JSON. (#6094)
    
    The field is deprecated and not used anywhere in XGBoost.
    Feature weights (#5962)
    Example JSON model parser and Schema. (#5137)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Update demo scripts to use installed python library
    Compatibility with both Python 2(.7) and 3
    add dump nice to regression demo
    update regression
    change the regression demo data set
    adding regression demo
    separate binary classification and regression demo
    fix fmap
    add mushroom
    change the regression demo data set
    change the regression demo data set
    Fix prediction heuristic (#5955)
    
    
    * Relax check for prediction.
    * Relax test in spark test.
    * Add tests in C++.
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    remove dependency on bst
    Update machine.conf
    chg all settings to obj
    update regression
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Fix typo in demo/regression/README.md
    move documentation to repo
    add dump nice to regression demo
    update regression
    small fix
    change the regression demo data set
    remove test directory
    adding regression demo
    separate binary classification and regression demo
    ok
    fix fmap
    add dump2json
    add pathdump
    finish mushroom
    add mushroom
    Compatibility with both Python 2(.7) and 3
    update regression
    adding regression demo
    separate binary classification and regression demo
    add mushroom
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Update demo scripts to use installed python library
    alrite
    chg higgs back
    add set leaf, constructor of tstats now rely on param
    chg wrapper
    Compatibility with both Python 2(.7) and 3
    chg all settings to obj
    minor changes
    new speed test
    speedtest
    a correct version
    ok
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Update demo scripts to use installed python library
    add higgs example
    chg wrapper
    change rank order output to follow kaggle convention
    Compatibility with both Python 2(.7) and 3
    a correct version
    ok
    Remove silent from R demos. (#5675)
    
    * Remove silent from R demos.
    
    * Vignettes.
    replace nround with nrounds to match actual parameter (#3592)
    change higgs script, remove R wrapper
    add higgs example
    Remove silent from R demos. (#5675)
    
    * Remove silent from R demos.
    
    * Vignettes.
    replace nround with nrounds to match actual parameter (#3592)
    Update speedtest.R
    add speedtest.R by -f
    hide xgb.Boost
    modification of higgs-pred.R
    change higgs script, remove R wrapper
    add higgs example
    fixed some typos (#1814)
    Update README.md
    Update README.md
    Update README.md
    add vignette
    add higgs example
    add base_margin
    ok
    Update README.md
    chng few things
    minor changes
    Update README.md
    ok
    ok
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Update demo scripts to use installed python library
    remove dependency on bst
    chg python back
    add higgs example
    chg wrapper
    Compatibility with both Python 2(.7) and 3
    chg all settings to obj
    a correct version
    ok
    Compatibility with both Python 2(.7) and 3
    minor changes
    ok
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Update demo scripts to use installed python library
    fix regression
    higgs cv
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [DIST] Add Distributed XGBoost on AWS Tutorial
    [DIST] Add Distributed XGBoost on AWS Tutorial
    remove dependency on bst
    Update mushroom.conf
    chg all settings to obj
    ok
    update regression
    remove test directory
    separate binary classification and regression demo
    add a test folder
    add smart decision of nfeatures
    fix col maker, make it default
    add pathdump
    add mushroom classification
    add mushroom
    Remove makefiles. (#5513)
    fixed some typos (#1814)
    resolved dead link in demo/distributed-training/README.md (#1484)
    [DIST] Add Distributed XGBoost on AWS Tutorial
    [DOC] cleanup distributed training
    [DIST] Add Distributed XGBoost on AWS Tutorial
    separate binary classification and regression demo
    add mushroom
    Fix #3638: Binary classification demo should produce LIBSVM with 0-based indexing (#3652)
    update libsvm file to start with 1 index
    Update demo scripts to use installed python library
    Compatibility with both Python 2(.7) and 3
    separate binary classification and regression demo
    fix fmap
    add mushroom
    [DIST] Add Distributed XGBoost on AWS Tutorial
    remove dependency on bst
    Update mushroom.conf
    chg all settings to obj
    ok
    update regression
    remove test directory
    separate binary classification and regression demo
    add a test folder
    add smart decision of nfeatures
    fix col maker, make it default
    add pathdump
    add mushroom classification
    add mushroom
    separate binary classification and regression demo
    add mushroom
    Fix typo (#5399)
    Fix link in binary classification demo README.md (#3918) (#3919)
    Fix #3609: Removed unused parameter 'use_buffer' (#3610)
    Add JSON dump functionality documentation (#3600)
    Fix minor typos  (#2842)
    
    * Some minor changes to the code style
    
    Some minor changes to the code style in file basic_walkthrough.py
    
    * coding style changes
    
    * coding style changes arrcording PEP8
    
    * Update basic_walkthrough.py
    
    * Fix minor typo
    
    * Minor edits to coding style
    
    Minor edits to coding style following the proposals of PEP8.
    Nonreproducible sequence of evaluations fixed (#2153)
    
    As `num_round=2` there is no `0003.model` file after training.
    [DOC] reorg docs
    Adding some details on nthread parameter
    
    I got this information about nthread='real cpu count' from https://github.com/dmlc/xgboost/blob/7cb449c4a75c2a16f6dfea5244ce959d998344b1/java/xgboost4j-demo/src/main/java/org/dmlc/xgboost4j/demo/ExternalMemory.java#L50
    Please confirm if this note is still valid before merging this change!
    fixing some typos
    fix links to wiki
    Update README.md
    move documentation to repo
    make wrapper ok
    make clear seperation
    small fix
    remove test directory
    separate binary classification and regression demo
    ok
    fix fmap
    add dump2json
    add pathdump
    finish mushroom
    add mushroom
    separate binary classification and regression demo
    add mushroom
    Compatibility with both Python 2(.7) and 3
    separate binary classification and regression demo
    add mushroom
    Update demo for ranking. (#5154)
    [doc] Fix link in rank demo README.md . (#3759)
    sklearn api for ranking (#3560)
    
    * added xgbranker
    
    * fixed predict method and ranking test
    
    * reformatted code in accordance with pep8
    
    * fixed lint error
    
    * fixed docstring and added checks on objective
    
    * added ranking demo for python
    
    * fixed suffix in rank.py
    Update broken links (#3565)
    
    Fix #3559
    Fix #3562
    fixed some typos (#1814)
    move documentation to repo
    moved data processing to wgetdata.sh (#3666)
    small change
    do not need to dump in rank
    small change
    fix small bug
    chg all settings to obj
    chg scripts
    chg rank demo
    Download data set from web site
    Add LETOR MQ2008 for rank demo
    small change
    rank pass toy
    add dump nice to regression demo
    update regression
    small fix
    change the regression demo data set
    remove test directory
    adding regression demo
    separate binary classification and regression demo
    ok
    fix fmap
    add dump2json
    add pathdump
    finish mushroom
    add mushroom
    Update demo for ranking. (#5154)
    sklearn api for ranking (#3560)
    
    * added xgbranker
    
    * fixed predict method and ranking test
    
    * reformatted code in accordance with pep8
    
    * fixed lint error
    
    * fixed docstring and added checks on objective
    
    * added ranking demo for python
    
    * fixed suffix in rank.py
    Update demo for ranking. (#5154)
    sklearn api for ranking (#3560)
    
    * added xgbranker
    
    * fixed predict method and ranking test
    
    * reformatted code in accordance with pep8
    
    * fixed lint error
    
    * fixed docstring and added checks on objective
    
    * added ranking demo for python
    
    * fixed suffix in rank.py
    remove dependency on bst
    Update mq2008.conf
    small change
    small change
    chg scripts
    chg rank demo
    small change
    new lambda rank interface
    ok
    bug fix in pairwise rank
    cleanup code
    small bug in ndcg eval
    Add LETOR MQ2008 for rank demo
    small change
    update regression
    moved data processing to wgetdata.sh (#3666)
    Update broken links (#3565)
    
    Fix #3559
    Fix #3562
    chg scripts
    Compatibility with both Python 2(.7) and 3
    cleanup code
    Add LETOR MQ2008 for rank demo
    [R] add a demo of multi-class classification R version (#3695)
    
    * add a demo of multi-class classification R version
    
    * add a demo of multi-class classification result
    
    * add intro to the demo readme
    
    * Delete train.md
    
    * Update README.md
    Typo fixed (#3784)
    
    The word 'make' was been repeated twice, fixed to single.
    [R] add a demo of multi-class classification R version (#3695)
    
    * add a demo of multi-class classification R version
    
    * add a demo of multi-class classification result
    
    * add intro to the demo readme
    
    * Delete train.md
    
    * Update README.md
    fix links to wiki
    Update README.md
    chg
    chg
    multi class
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Fix minor typos  (#2842)
    
    * Some minor changes to the code style
    
    Some minor changes to the code style in file basic_walkthrough.py
    
    * coding style changes
    
    * coding style changes arrcording PEP8
    
    * Update basic_walkthrough.py
    
    * Fix minor typo
    
    * Minor edits to coding style
    
    Minor edits to coding style following the proposals of PEP8.
    fix bug for demo/multiclass_classification/train.py (#2747)
    Minor cleanup (#2342)
    
    * Clean up demo of multiclass classification
    
    * Remove extra space
    Update demo scripts to use installed python library
    remove dependency on bst
    chg wrapper
    check in softmax multiclass
    support for multiclass output prob
    Compatibility with both Python 2(.7) and 3
    chg
    chg
    Update train.py
    multi class
    demo
    demo
    demo
    update libsvm file to start with 1 index
    move python example
    chg
    add python interface for xgboost
    move python example
    move python example
    chg
    add python interface for xgboost
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    update libsvm file to start with 1 index
    move python example
    chg
    add python interface for xgboost
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add support for Gamma regression (#1258)
    
    * Add support for Gamma regression
    
    * Use base_score to replace the lp_bias
    
    * Remove the lp_bias config block
    
    * Add a demo for running gamma regression in Python
    
    * Typo fix
    
    * Revise the description for objective
    
    * Add a script to generate the autoclaims dataset
    Improve doc and demo for dask. (#4907)
    
    * Add a readme with link to doc.
    * Add more comments in the demonstrations code.
    * Workaround https://github.com/dask/distributed/issues/3081 .
    Rewrite Dask interface. (#4819)
    Improve doc and demo for dask. (#4907)
    
    * Add a readme with link to doc.
    * Add more comments in the demonstrations code.
    * Workaround https://github.com/dask/distributed/issues/3081 .
    Rewrite Dask interface. (#4819)
    [dask] Honor `nthreads` from dask worker. (#5414)
    Fix dask prediction. (#4941)
    
    * Fix dask prediction.
    
    * Add better error messages for wrong partition.
    Improve doc and demo for dask. (#4907)
    
    * Add a readme with link to doc.
    * Add more comments in the demonstrations code.
    * Workaround https://github.com/dask/distributed/issues/3081 .
    Rewrite Dask interface. (#4819)
    Improve doc and demo for dask. (#4907)
    
    * Add a readme with link to doc.
    * Add more comments in the demonstrations code.
    * Workaround https://github.com/dask/distributed/issues/3081 .
    Rewrite Dask interface. (#4819)
    Add native support for Dask (#4473)
    
    * Add native support for Dask
    
    * Add multi-GPU demo
    
    * Add sklearn example
    Add DaskDeviceQuantileDMatrix demo. (#6156)
    [dask] Honor `nthreads` from dask worker. (#5414)
    Don't `set_params` at the end of `set_state`. (#4947)
    
    * Don't set_params at the end of set_state.
    
    * Also fix another issue found in dask prediction.
    
    * Add note about prediction.
    
    Don't support other prediction modes at the moment.
    Fix dask prediction. (#4941)
    
    * Fix dask prediction.
    
    * Add better error messages for wrong partition.
    Improve doc and demo for dask. (#4907)
    
    * Add a readme with link to doc.
    * Add more comments in the demonstrations code.
    * Workaround https://github.com/dask/distributed/issues/3081 .
    Rewrite Dask interface. (#4819)
    add msd
    ok
    add readme
    ok
    add msd
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    add conf
    remove dependency on bst
    Update machine.conf
    chg all settings to obj
    update regression
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Some guidelines on device memory usage (#5038)
    
    * Add memory usage demo
    
    * Update documentation
    Update GPU plug-in documentation link (#3130)
    Update GPU acceleration demo (#2617)
    
    * Update GPU acceleration demo
    
    * Fix parameter formatting
    GPU Plugin: Add subsample, colsample_bytree, colsample_bylevel (#1895)
    GPU Plugin: Add bosch demo, update build instructions (#1872)
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    Fix demo typo (#2632)
    Update GPU acceleration demo (#2617)
    
    * Update GPU acceleration demo
    
    * Fix parameter formatting
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    replace nround with nrounds to match actual parameter (#3592)
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    fixed some typos (#1814)
    Document refactor
    
    change badge
    Update understandingXGBoostModel.Rmd
    
    a typo for the dimension of the test set
    wording + presentation Otto rmarkdown
    small change in the wording of Otto R markdown
    wording
    add CSS
    fix
    small changes in RMarkdown
    improve tree graph
    Update understandingXGBoostModel.Rmd
    minor changes
    new rmarkdown
    some minor fix
    parameter change in OTTO ramarkdown
    wording
    trees
    OTTO markdown improvement
    OTTO markdown
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    replace nround with nrounds to match actual parameter (#3592)
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    update links dmlc
    change name
    add kaggle otto folder
    Fix typo in demo (#4156)
    clean
    clean
    change name
    Update readme.md
    add kaggle otto folder
    Remove linking RMM library. (#6146)
    
    * Remove linking RMM library.
    
    * RMM is now header only.
    
    * Remove remaining reference.
    Work around a compiler bug in MacOS AppleClang 11 (#6103)
    
    * Workaround a compiler bug in MacOS AppleClang
    
    * [CI] Run C++ test with MacOS Catalina + AppleClang 11.0.3
    
    * [CI] Migrate cmake_test on MacOS from Travis CI to GitHub Actions
    
    * Install OpenMP runtime
    
    * [CI] Use CMake to locate lz4 lib
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Introducing DPC++-based plugin (predictor, objective function) supporting oneAPI programming model (#5825)
    
    * Added plugin with DPC++-based predictor and objective function
    
    * Update CMakeLists.txt
    
    * Update regression_obj_oneapi.cc
    
    * Added README.md for OneAPI plugin
    
    * Added OneAPI predictor support to gbtree
    
    * Update README.md
    
    * Merged kernels in gradient computation. Enabled multiple loss functions with DPC++ backend
    
    * Aligned plugin CMake files with latest master changes. Fixed whitespace typos
    
    * Removed debug output
    
    * [CI] Make oneapi_plugin a CMake target
    
    * Added tests for OneAPI plugin for predictor and obj. functions
    
    * Temporarily switched to default selector for device dispacthing in OneAPI plugin to enable execution in environments without gpus
    
    * Updated readme file.
    
    * Fixed USM usage in predictor
    
    * Removed workaround with explicit templated names for DPC++ kernels
    
    * Fixed warnings in plugin tests
    
    * Fix CMake build of gtest
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Simplify CMake build with modern CMake techniques (#5871)
    
    * [CI] Simplify CMake build
    
    * Make sure that plugins can be built
    
    * [CI] Install lz4 on Mac
    Remove plugin, cuda related code in automake & autoconf files (#4789)
    
    * Build plugin example with CMake.
    
    * Remove plugin, cuda related code in automake & autoconf files.
    
    * Fix typo in GPU doc.
    Remove plugin, cuda related code in automake & autoconf files (#4789)
    
    * Build plugin example with CMake.
    
    * Remove plugin, cuda related code in automake & autoconf files.
    
    * Fix typo in GPU doc.
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    [FIX] fix plugin system
    [PLUGIN] Add plugin system
    Add link to GPU documentation (#5437)
    Remove plugin, cuda related code in automake & autoconf files (#4789)
    
    * Build plugin example with CMake.
    
    * Remove plugin, cuda related code in automake & autoconf files.
    
    * Fix typo in GPU doc.
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    [GPU-Plugin] Various fixes (#2579)
    
    * Fix test large
    
    * Add check for max_depth 0
    
    * Update readme
    
    * Add LBS specialisation for dense data
    
    * Add bst_gpair_precise
    
    * Temporarily disable accuracy tests on test_large.py
    
    * Solve unused variable compiler warning
    
    * Fix max_bin > 1024 error
    [GPU-Plugin] Add load balancing search to gpu_hist. Add compressed iterator. (#2504)
    [GPU-Plugin] Multi-GPU gpu_id bug fixes for grow_gpu_hist and grow_gpu methods, and additional documentation for the gpu plugin. (#2463)
    [GPU-Plugin] Change GPU plugin to use tree_method parameter, bump cmake version to 3.5 for GPU plugin, add compute architecture 3.5, remove unused cmake files (#2455)
    [GPU-Plugin] Make node_idx type 32 bit for hist algo. Set default n_gpus to 1. (#2445)
    [GPU-Plugin] Support for building to specific GPU architectures (#2390)
    
    * Support for builing gpu-plugins to specific GPU architectures
    1. Option GPU_COMPUTE_VER exposed from both Makefile and CMakeLists.txt
    2. updater_gpu documentation updated accordingly
    
    * Re-introduced GPU_COMPUTE_VER option in the cmake flow.
    This seems to fix the compile-time, rdc=true and copy-constructor related
    errors seen and discussed in PR #2390.
    [GPU-Plugin] Multi-GPU for grow_gpu_hist histogram method using NVIDIA NCCL. (#2395)
    [GPU-Plugin] Integration of a faster version of grow_gpu plugin into mainstream (#2360)
    
    * Integrating a faster version of grow_gpu plugin
    1. Removed the older files to reduce duplication
    2. Moved all of the grow_gpu files under 'exact' folder
    3. All of them are inside 'exact' namespace to avoid any conflicts
    4. Fixed a bug in benchmark.py while running only 'grow_gpu' plugin
    5. Added cub and googletest submodules to ease integration and unit-testing
    6. Updates to CMakeLists.txt to directly build cuda objects into libxgboost
    
    * Added support for building gpu plugins through make flow
    1. updated makefile and config.mk to add right targets
    2. added unit-tests for gpu exact plugin code
    
    * 1. Added support for building gpu plugin using 'make' flow as well
    2. Updated instructions for building and testing gpu plugin
    
    * Fix travis-ci errors for PR#2360
    1. lint errors on unit-tests
    2. removed googletest, instead depended upon dmlc-core provide gtest cache
    
    * Some more fixes to travis-ci lint failures PR#2360
    
    * Added Rory's copyrights to the files containing code from both.
    
    * updated copyright statement as per Rory's request
    
    * moved the static datasets into a script to generate them at runtime
    
    * 1. memory usage print when silent=0
    2. tests/ and test/ folder organization
    3. removal of the dependency of googletest for just building xgboost
    4. coding style updates for .cuh as well
    
    * Fixes for compilation warnings
    
    * add cuda object files as well when JVM_BINDINGS=ON
    [GPU Plugin] Fast histogram speed improvements. Updated benchmarks. (#2258)
    [GPU-Plugin] (#2227)
    
    * Add fast histogram algorithm
    * Fix Linux build
    * Add 'gpu_id' parameter
    Fix cmake build for linux. Update GPU benchmarks. (#1904)
    GPU Plugin: Add subsample, colsample_bytree, colsample_bylevel (#1895)
    GPU Plugin: Add bosch demo, update build instructions (#1872)
    Add benchmarks, fix GCC build (#1848)
    Update build instructions,  improve memory usage (#1811)
    Add GPU accelerated tree construction plugin (#1679)
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [PLUGIN] Add plugin system
    Remove plugin, cuda related code in automake & autoconf files (#4789)
    
    * Build plugin example with CMake.
    
    * Remove plugin, cuda related code in automake & autoconf files.
    
    * Fix typo in GPU doc.
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    [FIX] fix plugin system
    [PLUGIN] Add plugin system
    [CI] Simplify CMake build with modern CMake techniques (#5871)
    
    * [CI] Simplify CMake build
    
    * Make sure that plugins can be built
    
    * [CI] Install lz4 on Mac
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    [LZ4] enable 16 bit index
    [LZ] Improve lz4 format
    [FIX] fix plugin system
    [PLUGIN] Add plugin system
    Introducing DPC++-based plugin (predictor, objective function) supporting oneAPI programming model (#5825)
    
    * Added plugin with DPC++-based predictor and objective function
    
    * Update CMakeLists.txt
    
    * Update regression_obj_oneapi.cc
    
    * Added README.md for OneAPI plugin
    
    * Added OneAPI predictor support to gbtree
    
    * Update README.md
    
    * Merged kernels in gradient computation. Enabled multiple loss functions with DPC++ backend
    
    * Aligned plugin CMake files with latest master changes. Fixed whitespace typos
    
    * Removed debug output
    
    * [CI] Make oneapi_plugin a CMake target
    
    * Added tests for OneAPI plugin for predictor and obj. functions
    
    * Temporarily switched to default selector for device dispacthing in OneAPI plugin to enable execution in environments without gpus
    
    * Updated readme file.
    
    * Fixed USM usage in predictor
    
    * Removed workaround with explicit templated names for DPC++ kernels
    
    * Fixed warnings in plugin tests
    
    * Fix CMake build of gtest
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Introducing DPC++-based plugin (predictor, objective function) supporting oneAPI programming model (#5825)
    
    * Added plugin with DPC++-based predictor and objective function
    
    * Update CMakeLists.txt
    
    * Update regression_obj_oneapi.cc
    
    * Added README.md for OneAPI plugin
    
    * Added OneAPI predictor support to gbtree
    
    * Update README.md
    
    * Merged kernels in gradient computation. Enabled multiple loss functions with DPC++ backend
    
    * Aligned plugin CMake files with latest master changes. Fixed whitespace typos
    
    * Removed debug output
    
    * [CI] Make oneapi_plugin a CMake target
    
    * Added tests for OneAPI plugin for predictor and obj. functions
    
    * Temporarily switched to default selector for device dispacthing in OneAPI plugin to enable execution in environments without gpus
    
    * Updated readme file.
    
    * Fixed USM usage in predictor
    
    * Removed workaround with explicit templated names for DPC++ kernels
    
    * Fixed warnings in plugin tests
    
    * Fix CMake build of gtest
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Introducing DPC++-based plugin (predictor, objective function) supporting oneAPI programming model (#5825)
    
    * Added plugin with DPC++-based predictor and objective function
    
    * Update CMakeLists.txt
    
    * Update regression_obj_oneapi.cc
    
    * Added README.md for OneAPI plugin
    
    * Added OneAPI predictor support to gbtree
    
    * Update README.md
    
    * Merged kernels in gradient computation. Enabled multiple loss functions with DPC++ backend
    
    * Aligned plugin CMake files with latest master changes. Fixed whitespace typos
    
    * Removed debug output
    
    * [CI] Make oneapi_plugin a CMake target
    
    * Added tests for OneAPI plugin for predictor and obj. functions
    
    * Temporarily switched to default selector for device dispacthing in OneAPI plugin to enable execution in environments without gpus
    
    * Updated readme file.
    
    * Fixed USM usage in predictor
    
    * Removed workaround with explicit templated names for DPC++ kernels
    
    * Fixed warnings in plugin tests
    
    * Fix CMake build of gtest
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Introducing DPC++-based plugin (predictor, objective function) supporting oneAPI programming model (#5825)
    
    * Added plugin with DPC++-based predictor and objective function
    
    * Update CMakeLists.txt
    
    * Update regression_obj_oneapi.cc
    
    * Added README.md for OneAPI plugin
    
    * Added OneAPI predictor support to gbtree
    
    * Update README.md
    
    * Merged kernels in gradient computation. Enabled multiple loss functions with DPC++ backend
    
    * Aligned plugin CMake files with latest master changes. Fixed whitespace typos
    
    * Removed debug output
    
    * [CI] Make oneapi_plugin a CMake target
    
    * Added tests for OneAPI plugin for predictor and obj. functions
    
    * Temporarily switched to default selector for device dispacthing in OneAPI plugin to enable execution in environments without gpus
    
    * Updated readme file.
    
    * Fixed USM usage in predictor
    
    * Removed workaround with explicit templated names for DPC++ kernels
    
    * Fixed warnings in plugin tests
    
    * Fix CMake build of gtest
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Update dmlc-core submodule (#3221)
    
    * Update dmlc-core submodule
    
    * Fix dense_parser to work with the latest dmlc-core
    
    * Specify location of Google Test
    
    * Add more source files in dmlc-minimum to get latest dmlc-core working
    
    * Update dmlc-core submodule
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Update dmlc-core
    [PLUGIN] Add densify parser
    Enable auto-locking of issues closed long ago (#3821)
    
    * Enable auto-locking of issues closed long ago
    
    Issues that were closed more than 90 days ago will be locked automatically so
    that no additional comments would be allowed. We will use a bot to do
    this: https://probot.github.io/apps/lock/
    
    Background: As a maintainer, I often see people leaving comments to old issue
    posts that were closed long ago. Those comments are hard to discover and assist
    with, since they get buried under list of other active issues.
    
    With the change, users who want to follow up with an old issue would be asked
    to file a new issue.
    
    * Exempt `feature-request` from auto locking
    
    * Disable comment to avoid triggering notification
    Display Sponsor button, link to OpenCollective (#5325)
    [DOCS] Update link to readme (#3437)
    Move non-OpenMP gtest to GitHub Actions (#6210)
    [CI] Test C API demo (#6159)
    
    * Fix CMake install config to use dependencies
    
    * [CI] Test C API demo
    
    * Explicitly cast num_feature, to avoid warning in Linux
    Enable building rabit on Windows (#6105)
    Work around a compiler bug in MacOS AppleClang 11 (#6103)
    
    * Workaround a compiler bug in MacOS AppleClang
    
    * [CI] Run C++ test with MacOS Catalina + AppleClang 11.0.3
    
    * [CI] Migrate cmake_test on MacOS from Travis CI to GitHub Actions
    
    * Install OpenMP runtime
    
    * [CI] Use CMake to locate lz4 lib
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    [CI] Improve JVM test in GitHub Actions (#5930)
    
    * [CI] Improve JVM test in GitHub Actions
    
    * Use env var for Wagon options [skip ci]
    
    * Move the retry flag to pom.xml [skip ci]
    
    * Export env var RABIT_MOCK to run Spark tests [skip ci]
    
    * Correct location of env var
    
    * Re-try up to 5 times [skip ci]
    
    * Don't run distributed training test on Windows
    
    * Fix typo
    
    * Update main.yml
    Remove win2016 jvm github action test. (#6042)
    [CI] Migrate linters to GitHub Actions (#6035)
    
    * [CI] Move lint to GitHub Actions
    
    * [CI] Move Doxygen to GitHub Actions
    
    * [CI] Move Sphinx build test to GitHub Actions
    
    * [CI] Reduce workload for Windows R tests
    
    * [CI] Move clang-tidy to Build stage
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    Cache dependencies on Github Action. (#5928)
    Setup github action. (#5917)
    Fix typo in CI. [skip ci] (#5919)
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Move warning about empty dataset. (#5998)
    Thread-safe prediction by making the prediction cache thread-local. (#5853)
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    [Breaking] Fix custom metric for multi output. (#5954)
    
    
    * Set output margin to true for custom metric.  This fixes only R and Python.
    Fix prediction heuristic (#5955)
    
    
    * Relax check for prediction.
    * Relax test in spark test.
    * Add tests in C++.
    [BLOCKING] Remove to_string. (#5934)
    Add XGBoosterGetNumFeature (#5856)
    
    - add GetNumFeature to Learner
    - add XGBoosterGetNumFeature to C API
    - update c-api-demo accordingly
    Move feature names and types of DMatrix from Python to C++. (#5858)
    
    
    * Add thread local return entry for DMatrix.
    * Save feature name and feature type in binary file.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Fix loading old model. (#5724)
    
    
    * Add test.
    Better message when no GPU is found. (#5594)
    Set device in device dmatrix. (#5596)
    Avoid rabit calls in learner configuration (#5581)
    Fix configuration I load model. (#5562)
    Group aware GPU sketching. (#5551)
    
    * Group aware GPU weighted sketching.
    
    * Distribute group weights to each data point.
    * Relax the test.
    * Validate input meta info.
    * Fix metainfo copy ctor.
    Write binary header. (#5532)
    Remove distcol updater. (#5507)
    
    Closes #5498.
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix dump model. (#5485)
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Split up `LearnerImpl`. (#5350)
    Move thread local entry into Learner. (#5396)
    
    * Move thread local entry into Learner.
    
    This is an attempt to workaround CUDA context issue in static variable, where
    the CUDA context can be released before device vector.
    
    * Add PredictionEntry to thread local entry.
    
    This eliminates one copy of prediction vector.
    
    * Don't define CUDA C API in a namespace.
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Pass shared pointer instead of raw pointer to Learner. (#5302)
    
    Extracted from https://github.com/dmlc/xgboost/pull/5220 .
    Export JSON config in `get_params`. (#5256)
    Fix compilation error due to 64-bit integer narrowing to size_t (#5250)
    Save Scikit-Learn attributes into learner attributes. (#5245)
    
    * Remove the recommendation for pickle.
    
    * Save skl attributes in booster.attr
    
    * Test loading scikit-learn model with native booster.
    Better error message about loading pickled model. (#5236)
    
    
    Print the link to new tutorial.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    Disable parameter validation for Scikit-Learn interface. (#5167)
    
    * Disable parameter validation for now.
    
    Scikit-Learn passes all parameters down to XGBoost, whether they are used or
    not.
    
    * Add option `validate_parameters`.
    Fix wrapping GPU ID and prevent data copying. (#5160)
    
    * Removed some data copying.
    
    * Make sure gpu_id is valid before any configuration is carried out.
    Parameter validation (#5157)
    
    
    
    * Unused code.
    
    * Split up old colmaker parameters from train param.
    
    * Fix dart.
    
    * Better name.
    Tests and documents for new JSON routines. (#5120)
    Fix metric name loading. (#5122)
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Small refinements for JSON model. (#5112)
    
    * Naming consistency.
    
    * Remove duplicated test.
    Model IO in JSON. (#5110)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Remove dead code in colmaker. (#5105)
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    [Breaking] Update sklearn interface. (#4929)
    
    
    * Remove nthread, seed, silent. Add tree_method, gpu_id, num_parallel_tree. Fix #4909.
    * Check data shape. Fix #4896.
    * Check element of eval_set is tuple. Fix #4875
    *  Add doc for random_state with hogwild. Fixes #4919
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Check deprecated `n_gpus`. (#4908)
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    monitor for distributed envorinment. (#4829)
    
    * Collect statistics from other ranks in monitor.
    
    * Workaround old GCC bug.
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Prevent copying data to host. (#4795)
    [BREAKING] prevent multi-gpu usage (#4749)
    
    * prevent multi-gpu usage
    
    * fix distributed test
    
    * combine gpu predictor tests
    
    * set upper bound on n_gpus
    Remove some unused functions as reported by cppcheck (#4743)
    Fix model parameter recovery (#4738)
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Feature interaction for GPU Hist. (#4534)
    
    * GPU hist Interaction Constraints.
    * Duplicate related parameters.
    * Add tests for CPU interaction constraint.
    * Add better error reporting.
    * Thorough tests.
    Deprecate single node multi-gpu mode (#4579)
    
    * deprecate multi-gpu training
    
    * add single node
    
    * add warning
    Offload some configurations into GBM. (#4553)
    
    
    This is part 1 of refactoring configuration.
    
    * Move tree heuristic configurations.
    * Split up declarations and definitions for GBTree.
    * Implement UseGPU in gbm.
    Enforce exclusion between pred_interactions=True and pred_interactions=True (#4522)
    Fix prediction from loaded pickle. (#4516)
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Change obj name to `reg:squarederror` in learner. (#4427)
    
    
    * Change memory dump size in R test.
    Allow unique prediction vector for each input matrix (#4275)
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Make `HistCutMatrix::Init' be aware of groups. (#4115)
    
    
    * Add checks for group size.
    * Simple docs.
    * Search group index during hist cut matrix initialization.
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    Prevent training without setting up caches. (#4066)
    
    * Prevent training without setting up caches.
    
    * Add warning for internal functions.
    * Check number of features.
    
    * Address reviewer's comment.
    Fix ignoring dart in updater configuration. (#4024)
    
    * Fix ignoring dart in updater configuration.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    Fix Typo in learner.cc (#3902)
    Remove unnecessary warning when 'gblinear' is selected (#3888)
    Document current limitation in number of features (#3886)
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Fix #3342 and h2oai/h2o4gpu#625: Save predictor parameters in model file (#3856)
    
    * Fix #3342 and h2oai/h2o4gpu#625: Save predictor parameters in model file
    
    This allows pickled models to retain predictor attributes, such as
    'predictor' (whether to use CPU or GPU) and 'n_gpu' (number of GPUs
    to use). Related: h2oai/h2o4gpu#625
    
    Closes #3342.
    
    TODO. Write a test.
    
    * Fix lint
    
    * Do not load GPU predictor into CPU-only XGBoost
    
    * Add a test for pickling GPU predictors
    
    * Make sample data big enough to pass multi GPU test
    
    * Update test_gpu_predictor.cu
    [Blocking] Fix #3840: Clean up logic for parsing tree_method parameter (#3849)
    
    * Clean up logic for converting tree_method to updater sequence
    
    * Use C++11 enum class for extra safety
    
    Compiler will give warnings if switch statements don't handle all
    possible values of C++11 enum class.
    
    Also allow enum class to be used as DMLC parameter.
    
    * Fix compiler error + lint
    
    * Address reviewer comment
    
    * Better docstring for DECLARE_FIELD_ENUM_CLASS
    
    * Fix lint
    
    * Add C++ test to see if tree_method is recognized
    
    * Fix clang-tidy error
    
    * Add test_learner.h to R package
    
    * Update comments
    
    * Fix lint error
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Add option to disable default metric (#3606)
    Revert "Fix #3485, #3540: Don't use dropout for predicting test sets" (#3563)
    
    * Revert "Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)"
    
    This reverts commit 44811f233071c5805d70c287abd22b155b732727.
    
    * Document behavior of predict() for DART booster
    
    * Add notice to parameter.rst
    Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)
    
    * Fix #3485, #3540: Don't use dropout for predicting test sets
    
    Dropout (for DART) should only be used at training time.
    
    * Add regression test
    Fix model saving for 'count:possion': max_delta_step as Booster attribute (#3515)
    
    * Save max_delta_step as an extra attribute of Booster
    
    Fixes #3509 and #3026, where `max_delta_step` parameter gets lost during serialization.
    
    * fix lint
    
    * Use camel case for global constant
    
    * disable local variable case in clang-tidy
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Update gpu_hist algorithm (#2901)
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    [WIP] Extract prediction into separate interface (#2531)
    
    * [WIP] Extract prediction into separate interface
    
    * Add copyright, fix linter errors
    
    * Add predictor to amalgamation
    
    * Fix documentation
    
    * Move prediction cache into predictor, add GBTreeModel
    
    * Updated predictor doc comments
    [GPU-Plugin] Change GPU plugin to use tree_method parameter, bump cmake version to 3.5 for GPU plugin, add compute architecture 3.5, remove unused cmake files (#2455)
    Add prediction of feature contributions (#2003)
    
    * Add prediction of feature contributions
    
    This implements the idea described at http://blog.datadive.net/interpreting-random-forests/
    which tries to give insight in how a prediction is composed of its feature contributions
    and a bias.
    
    * Support multi-class models
    
    * Calculate learning_rate per-tree instead of using the one from the first tree
    
    * Do not rely on node.base_weight * learning_rate having the same value as the node mean value (aka leaf value, if it were a leaf); instead calculate them (lazily) on-the-fly
    
    * Add simple test for contributions feature
    
    * Check against param.num_nodes instead of checking for non-zero length
    
    * Loop over all roots instead of only the first
    Store metrics with learner (#2241)
    
    Storing and then loading a model loses any eval_metric that was
    provided. This causes implementations that always store/load, like
    xgboost4j-spark, to be unable to eval with the desired metric.
    Improve multi-threaded performance (#2104)
    
    * Add UpdatePredictionCache() option to updaters
    
    Some updaters (e.g. fast_hist) has enough information to quickly compute
    prediction cache for the training data. Each updater may override
    UpdaterPredictionCache() method to update the prediction cache. Note: this
    trick does not apply to validation data.
    
    * Respond to code review
    
    * Disable some debug messages by default
    * Document UpdatePredictionCache() interface
    * Remove base_margin logic from UpdatePredictionCache() implementation
    * Do not take pointer to cfg, as reference may get stale
    
    * Improve multi-threaded performance
    
    * Use columnwise accessor to accelerate ApplySplit() step,
      with support for a compressed representation
    * Parallel sort for evaluation step
    * Inline BuildHist() function
    * Cache gradient pairs when building histograms in BuildHist()
    
    * Add missing #if macro
    
    * Respond to code review
    
    * Use wrapper to enable parallel sort on Linux
    
    * Fix C++ compatibility issues
    
    * MSVC doesn't support unsigned in OpenMP loops
    * gcc 4.6 doesn't support using keyword
    
    * Fix lint issues
    
    * Respond to code review
    
    * Fix bug in ApplySplitSparseData()
    
    * Attempting to read beyond the end of a sparse column
    * Mishandling the case where an entire range of rows have missing values
    
    * Fix training continuation bug
    
    Disable UpdatePredictionCache() in the first iteration. This way, we can
    accomodate the scenario where we build off of an existing (nonempty) ensemble.
    
    * Add regression test for fast_hist
    
    * Respond to code review
    
    * Add back old version of ApplySplitSparseData
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Histogram Optimized Tree Grower (#1940)
    
    * Support histogram-based algorithm + multiple tree growing strategy
    
    * Add a brand new updater to support histogram-based algorithm, which buckets
      continuous features into discrete bins to speed up training. To use it, set
      `tree_method = fast_hist` to configuration.
    * Support multiple tree growing strategies. For now, two policies are supported:
      * `grow_policy=depthwise` (default):  favor splitting at nodes closest to the
        root, i.e. grow depth-wise.
      * `grow_policy=lossguide`: favor splitting at nodes with highest loss change
    * Improve single-threaded performance
      * Unroll critical loops
      * Introduce specialized code for dense data (i.e. no missing values)
    * Additional training parameters: `max_leaves`, `max_bin`, `grow_policy`, `verbose`
    
    * Adding a small test for hist method
    
    * Fix memory error in row_set.h
    
    When std::vector is resized, a reference to one of its element may become
    stale. Any such reference must be updated as well.
    
    * Resolve cross-platform compilation issues
    
    * Versions of g++ older than 4.8 lacks support for a few C++11 features, e.g.
      alignas(*) and new initializer syntax. To support g++ 4.6, use pre-C++11
      initializer and remove alignas(*).
    * Versions of MSVC older than 2015 does not support alignas(*). To support
      MSVC 2012, remove alignas(*).
    * For g++ 4.8 and newer, alignas(*) is enabled for performance benefits.
    * Some old compilers (MSVC 2012, g++ 4.6) do not support template aliases
      (which uses `using` to declate type aliases). So always use `typedef`.
    
    * Fix a host of CI issues
    
    * Remove dependency for libz on osx
    * Fix heading for hist_util
    * Fix minor style issues
    * Add missing #include
    * Remove extraneous logging
    
    * Enable tree_method=hist in R
    
    * Renaming HistMaker to GHistBuilder to avoid confusion
    
    * Fix R integration
    
    * Respond to style comments
    
    * Consistent tie-breaking for priority queue using timestamps
    
    * Last-minute style fixes
    
    * Fix issuecomment-271977647
    
    The way we quantize data is broken. The agaricus data consists of all
    categorical values. When NAs are converted into 0's,
    `HistCutMatrix::Init` assign both 0's and 1's to the same single bin.
    
    Why? gmat only the smallest value (0) and an upper bound (2), which is twice
    the maximum value (1). Add the maximum value itself to gmat to fix the issue.
    
    * Fix issuecomment-272266358
    
    * Remove padding from cut values for the continuous case
    * For categorical/ordinal values, use midpoints as bin boundaries to be safe
    
    * Fix CI issue -- do not use xrange(*)
    
    * Fix corner case in quantile sketch
    
    Signed-off-by: Philip Cho <chohyu01@cs.washington.edu>
    
    * Adding a test for an edge case in quantile sketcher
    
    max_bin=2 used to cause an exception.
    
    * Fix fast_hist test
    
    The test used to require a strictly increasing Test AUC for all examples.
    One of them exhibits a small blip in Test AUC before achieving a Test AUC
    of 1. (See bottom.)
    
    Solution: do not require monotonic increase for this particular example.
    
    [0] train-auc:0.99989 test-auc:0.999497
    [1] train-auc:1 test-auc:0.999749
    [2] train-auc:1 test-auc:0.999749
    [3] train-auc:1 test-auc:0.999749
    [4] train-auc:1 test-auc:0.999749
    [5] train-auc:1 test-auc:0.999497
    [6] train-auc:1 test-auc:1
    [7] train-auc:1 test-auc:1
    [8] train-auc:1 test-auc:1
    [9] train-auc:1 test-auc:1
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Fixing a few typos (#1771)
    
    * Fixing a few typos
    
    * Fixing a few typos
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    Fix the issue 1474 (#1615)
    
    * Fix 1474
    
    * Fix crash issue when saving and loading poisson model
    
    * Rollback the wrong fix
    [CORE] Refactor cache mechanism (#1540)
    no exception throwing within omp parallel; set nthread in Learner (#1421)
    Fixes for multiple and default metric (#1239)
    
    * fix multiple evaluation metrics
    
    * create DefaultEvalMetric only when really necessary
    
    * py test for #1239
    
    * make travis happy
    create DefaultEvalMetric only when really necessary
    fixes for lint
    methods to delete an attribute and get names of available attributes
    in Configure, set random seed only for uninitialized model
    [METHOD], add tree method option to prefer faster algo
    Fix multi-class loading
    Fix continue training in CLI
    [PYTHON-DIST] Distributed xgboost python training API.
    [R] make all customizations to meet strict standard of cran
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [LEARNER] refactor learner
    [LEARNER] Init learner interface
    [REFACTOR] cleanup structure
    spelling, wording, and doc fixes in c++ code
    
    I was reading through the code and fixing some things in the comments.
    Only a few trivial actual code changes were made to make things more
    readable.
    lint learner finish
    io part refactor
    add with pbuffer info to model, allow xgb model to be saved in a more memory compact way
    add poisson regression
    minor shadow fix
    fix platform dependent thing
    half ram support
    need more check
    xgboost update for dmlc changes
    safe fix
    push backward compatible fix
    compile with dmlc
    add objective
    move stream to rabit part, support rabit on yarn
    add saveload to raw
    quick fix seed
    better warning at multiclass, fix cran check
    add single instance prediction
    add sync module
    change makefile to lazy checkpt, fix col splt code
    now support distributed evaluation
    add base64 model format
    fix the row split recovery, add per iteration random number seed
    rm boost str
    pas mock, need to fix rabit lib for not initialization
    add rabit checkpoint to xgb
    change allreduce lib to rabit library, xgboost now run with rabit
    remove warning from MSVC need another round of check
    add predict leaf indices
    checkin continue training
    still need to test row merge
    recheck column mode
    fix load bug
    add part_load col
    finish mushroom example
    new change for mpi
    some changes
    ok
    add cv for python
    remove using std from cpp
    move sprintf into std
    add ntree limit
    fix print problem, fix Tong's email format
    fix some windows type conversion warning
    finish refactor, need debug
    complete refactor data.h, now replies on iterator to access column
    change omp loop var to bst_omp_uint, add XGB_DLL to wrapper
    remove dependency on bst
    clean up warnings from msvc
    add changes
    check in linear model
    chg root index to booster info, need review
    add apratio
    change row subsample to prob
    refresher test
    add base_margin
    fix base score, and print message
    ok
    check in softmax multiclass
    remake the wrapper
    first version that reproduce binary classification demo
    check in io module
    mv code into src
    start unity refactor
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Refactor the CLI. (#5574)
    
    
    * Enable parameter validation.
    * Enable JSON.
    * Catch `dmlc::Error`.
    * Show help message.
    Don't set seed on CLI interface. (#5563)
    Fix CLI model IO. (#5535)
    
    
    * Add test for comparing Python and CLI training result.
    Pass shared pointer instead of raw pointer to Learner. (#5302)
    
    Extracted from https://github.com/dmlc/xgboost/pull/5220 .
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Modify caching allocator/vector and fix issues relating to inability to train large datasets (#4615)
    Fix C++11 config parser (#4521)
    
    * Fix C++11 config parser
    * Use raw strings to improve readability of regex
    * Fix compilation for GCC 5.x
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Simplify INI-style config reader using C++11 STL (#4478)
    
    * simplify the config.h file
    
    * revise config.h
    
    * revised config.h
    
    * revise format
    
    * revise format issues
    
    * revise whitespace issues
    
    * revise whitespace namespace format issues
    
    * revise namespace format issues
    
    * format issues
    
    * format issues
    
    * format issues
    
    * format issues
    
    * Revert submodule changes
    
    * minor change
    
    * Update src/common/config.h
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * address format issue from trivialfis
    
    * Use correct cub submodule
    max_digits10 guarantees float decimal roundtrip (#4435)
    
    2 additional digits are not needed to guarantee that casting the decimal representation will result in the same float, see https://github.com/dmlc/xgboost/issues/3980#issuecomment-458702440
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Save outputs in high precision in CLI prediction (#3356)
    
    Currently, `CLIPredict()` saves prediction results in default 6-digit precision which causes precision loss. This PR sets precision to a level so that the conversion back to `bst_float` is lossless.
    
    Related: #3298.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    fix bug in loading config for pred task (#2795)
    Improve multi-threaded performance (#2104)
    
    * Add UpdatePredictionCache() option to updaters
    
    Some updaters (e.g. fast_hist) has enough information to quickly compute
    prediction cache for the training data. Each updater may override
    UpdaterPredictionCache() method to update the prediction cache. Note: this
    trick does not apply to validation data.
    
    * Respond to code review
    
    * Disable some debug messages by default
    * Document UpdatePredictionCache() interface
    * Remove base_margin logic from UpdatePredictionCache() implementation
    * Do not take pointer to cfg, as reference may get stale
    
    * Improve multi-threaded performance
    
    * Use columnwise accessor to accelerate ApplySplit() step,
      with support for a compressed representation
    * Parallel sort for evaluation step
    * Inline BuildHist() function
    * Cache gradient pairs when building histograms in BuildHist()
    
    * Add missing #if macro
    
    * Respond to code review
    
    * Use wrapper to enable parallel sort on Linux
    
    * Fix C++ compatibility issues
    
    * MSVC doesn't support unsigned in OpenMP loops
    * gcc 4.6 doesn't support using keyword
    
    * Fix lint issues
    
    * Respond to code review
    
    * Fix bug in ApplySplitSparseData()
    
    * Attempting to read beyond the end of a sparse column
    * Mishandling the case where an entire range of rows have missing values
    
    * Fix training continuation bug
    
    Disable UpdatePredictionCache() in the first iteration. This way, we can
    accomodate the scenario where we build off of an existing (nonempty) ensemble.
    
    * Add regression test for fast_hist
    
    * Respond to code review
    
    * Add back old version of ApplySplitSparseData
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    Fix typos and messages in docs (#1723)
    [CORE] Refactor cache mechanism (#1540)
    Fix issue #1236: cli_main crashes when dumping count:poisson model (#1253)
    fix cli_main crashes when using count:poisson regression
    Fix continue training in CLI
    [CLI] Fix model save problem
    [DIST] fix distirbuted setting
    [LIBXGBOOST] pass demo running.
    [CLI] initial refactor of CLI
    Option for generating device debug info. (#6168)
    
    * Supply `-G;-src-in-ptx` when `USE_DEVICE_DEBUG` is set and debug mode is selected.
    * Refactor CMake script to gather all CUDA configuration.
    * Use CMAKE_CUDA_ARCHITECTURES.  Close #6029.
    * Add compute 80.  Close #5999
    Don't link imported target. (#6093)
    Update GPUTreeShap (#6064)
    
    * Update GPUTreeShap
    
    * Update src/CMakeLists.txt
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    GPUTreeShap (#6038)
    Add CMake flag to log C API invocations, to aid debugging (#5925)
    
    * Add CMake flag to log C API invocations, to aid debugging
    
    * Remove unnecessary parentheses
    Add option to enable all compiler warnings in GCC/Clang (#5897)
    
    * Add option to enable all compiler warnings in GCC/Clang
    
    * Fix -Wall for CUDA sources
    
    * Make -Wall private req for xgboost-r
    Support building XGBoost with CUDA 11 (#5808)
    
    * Change serialization test.
    * Add CUDA 11 tests on Linux CI.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Simplify CMake build with modern CMake techniques (#5871)
    
    * [CI] Simplify CMake build
    
    * Make sure that plugins can be built
    
    * [CI] Install lz4 on Mac
    Define _CRT_SECURE_NO_WARNINGS to remove unneeded warnings in MSVC (#5434)
    C++14 for xgboost (#5664)
    Enhance nvtx support. (#5636)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Fix non-openmp build. (#5566)
    
    
    * Add test to Jenkins.
    * Fix threading utils tests.
    * Require thread library.
    Explicitly use UTF-8 codepage when using MSVC (#5197)
    
    * Explicitly use UTF-8 codepage when using MSVC
    
    * Fix build with CUDA enabled
    Enable OpenMP with Apple Clang (Mac default compiler) (#5146)
    
    * Add OpenMP as CMake target
    
    * Require CMake 3.12, to allow linking OpenMP target to objxgboost
    
    * Specify OpenMP compiler flag for CUDA host compiler
    
    * Require CMake 3.16+ if the OS is Mac OSX
    
    * Use AppleClang in Mac tests.
    
    * Update dmlc-core
    Implement training observer. (#5088)
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Use configure_file() to configure version only (#4974)
    
    * Avoid writing build_config.h
    
    * Remove build_config.h all together.
    
    * Lint.
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    unittests mock, cleanup (#111)
    
    * cleanup, fix issue involved after remove is_bootstrap parameter
    
    * misc
    
    * clean
    
    * add unittests
    Clean up cmake script and code includes (#106)
    
    * Clean up CMake scripts and related include paths.
    * Add unittests.
    Remove plugin, cuda related code in automake & autoconf files (#4789)
    
    * Build plugin example with CMake.
    
    * Remove plugin, cuda related code in automake & autoconf files.
    
    * Fix typo in GPU doc.
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Overload device memory allocation (#4532)
    
    * Group source files, include headers in source files
    
    * Overload device memory allocation
    [CI] Add Windows GPU to Jenkins CI pipeline (#4463)
    
    * Fix #4462: Use /MT flag consistently for MSVC target
    
    * First attempt at Windows CI
    
    * Distinguish stages in Linux and Windows pipelines
    
    * Try running CMake in Windows pipeline
    
    * Add build step
    Make CMakeLists.txt compatible with CMake 3.3 (#4420)
    
    * Make CMakeLists.txt compatible with CMake 3.3; require CMake 3.11 for MSVC
    
    * Use CMake 3.12 when sanitizer is enabled
    
    * Disable funroll-loops for MSVC
    
    * Use cmake version in container name
    
    * Add missing arg
    
    * Fix egrep use in ci_build.sh
    
    * Display CMake version
    
    * Do not set OpenMP_CXX_LIBRARIES for MSVC
    
    * Use cmake_minimum_required()
     Combine thread launches into single launch per tree for gpu_hist (#4343)
    
    * Combine thread launches into single launch per tree for gpu_hist
    algorithm.
    
    * Address deprecation warning
    
    * Add manual column sampler constructor
    
    * Turn off omp dynamic to get a guaranteed number of threads
    
    * Enable openmp in cuda code
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Remove silent parameter. (#5476)
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Fix and optimize logger (#4002)
    
    * Fix logging switch statement.
    
    * Remove debug_verbose_ in AllReducer.
    
    * Don't construct the stream when not needed.
    
    * Make default constructor deleted.
    
    * Remove redundant IsVerbose.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Add callback interface to re-direct console output (#3438)
    
    * Add callback interface to re-direct console output
    
    * Exempt TrackerLogger from custom logging
    
    * Fix lint
    Have ConsoleLogger log to stderr instead of stdout (#1714)
    
    On Unix systems, it's common for programs to read their input from stdin, and
    write their output to stdout.  Messages should be written to stderr, where they
    won't corrupt a program's output, and where they can be seen by the user even
    if the output is being redirected.
    
    This is mostly a problem when XGBoost is being used from Python or from another
    program.
    [R] make all customizations to meet strict standard of cran
    [LIBXGBOOST] pass demo running.
    Reduce compile warnings (#6198)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Dask device dmatrix (#5901)
    
    
    * Fix softprob with empty dmatrix.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add check for length of weights. (#4872)
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    remove device shards (#4867)
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    More explict sharding methods for device memory (#4396)
    
    * Rename the Reshard method to Shard
    
    * Add a new Reshard method for sharding a vector that's already sharded
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Enable running objectives with 0 GPU. (#3878)
    
    * Enable running objectives with 0 GPU.
    
    * Enable 0 GPU for objectives.
    * Add doc for GPU objectives.
    * Fix some objectives defaulted to running on all GPUs.
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Multi-GPU support in GPUPredictor. (#3738)
    
    * Multi-GPU support in GPUPredictor.
    
    - GPUPredictor is multi-GPU
    - removed DeviceMatrix, as it has been made obsolete by using HostDeviceVector in DMatrix
    
    * Replaced pointers with spans in GPUPredictor.
    
    * Added a multi-GPU predictor test.
    
    * Fix multi-gpu test.
    
    * Fix n_rows < n_gpus.
    
    * Reinitialize shards when GPUSet is changed.
    * Tests range of data.
    
    * Remove commented code.
    
    * Remove commented code.
    Address #2754, accuracy issues with gpu_hist (#3793)
    
    * Address windows compilation error
    
    * Do not allow divide by zero in weight calculation
    
    * Update tests
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Prevent multiclass Hessian approaching 0 (#3304)
    
    * Prevent Hessian in multiclass objective becoming zero
    
    * Set default learning rate to 0.5 for "coord_descent" linear updater
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [LIBXGBOOST] pass demo running.
    [LEARNER] refactor learner
    [GBM] Finish migrate all gbms
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [METRIC] all metric move finished
    [OBJ] Add basic objective function and registry
    Add check for length of weights. (#4872)
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Enable running objectives with 0 GPU. (#3878)
    
    * Enable running objectives with 0 GPU.
    
    * Enable 0 GPU for objectives.
    * Add doc for GPU objectives.
    * Fix some objectives defaulted to running on all GPUs.
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Multi-GPU support in GPUPredictor. (#3738)
    
    * Multi-GPU support in GPUPredictor.
    
    - GPUPredictor is multi-GPU
    - removed DeviceMatrix, as it has been made obsolete by using HostDeviceVector in DMatrix
    
    * Replaced pointers with spans in GPUPredictor.
    
    * Added a multi-GPU predictor test.
    
    * Fix multi-gpu test.
    
    * Fix n_rows < n_gpus.
    
    * Reinitialize shards when GPUSet is changed.
    * Tests range of data.
    
    * Remove commented code.
    
    * Remove commented code.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Pseudo-huber loss metric added (#5647)
    
    
    - Add pseudo huber loss objective.
    - Add pseudo huber loss metric.
    
    Co-authored-by: Reetz <s02reetz@iavgroup.local>
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Add `rmsle` metric and `reg:squaredlogerror` objective (#4541)
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    C++14 for xgboost (#5664)
    Use thrust functions instead of custom functions (#5544)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Move segment sorter to common (#5378)
    
    - move segment sorter to common
    - this is the first of a handful of pr's that splits the larger pr #5326
    - it moves this facility to common (from ranking objective class), so that it can be
        used for metric computation
    - it also wraps all the bald device pointers into span.
    Fix metainfo from DataFrame. (#5216)
    
    * Fix metainfo from DataFrame.
    
    * Unify helper functions for data and meta.
    implementation of map ranking algorithm on gpu (#5129)
    
    * - implementation of map ranking algorithm
      - also effected necessary suggestions mentioned in the earlier ranking pr's
      - made some performance improvements to the ndcg algo as well
    - ndcg ltr implementation on gpu (#5004)
    
    * - ndcg ltr implementation on gpu
      - this is a follow-up to the pairwise ltr implementation
    Fix external memory race in colmaker. (#4980)
    
    
    * Move `GetColDensity` out of omp parallel block.
    Pairwise ranking objective implementation on gpu (#4873)
    
    * - pairwise ranking objective implementation on gpu
       - there are couple of more algorithms (ndcg and map) for which support will be added
         as follow-up pr's
       - with no label groups defined, get gradient is 90x faster on gpu (120m instance
         mortgage dataset)
       - it can perform by an order of magnitude faster with ~ 10 groups (and adequate cores
         for the cpu implementation)
    
    * Add JSON config to rank obj.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Implementation of hinge loss for binary classification (#3477)
    Pairwise ranking objective implementation on gpu (#4873)
    
    * - pairwise ranking objective implementation on gpu
       - there are couple of more algorithms (ndcg and map) for which support will be added
         as follow-up pr's
       - with no label groups defined, get gradient is 90x faster on gpu (120m instance
         mortgage dataset)
       - it can perform by an order of magnitude faster with ~ 10 groups (and adequate cores
         for the cpu implementation)
    
    * Add JSON config to rank obj.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Remove some unused functions as reported by cppcheck (#4743)
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    added code for instance based weighing for rank objectives (#3379)
    
    * added code for instance based weighing for rank objectives
    
    * Fix lint
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Integer gradient summation for GPU histogram algorithm. (#2681)
    Update rank_obj.cc (#2126)
    
    typo: PairwieRankObj -> PairwiseRankObj
    Fix issue introduced from correction to log2 (#1837)
    
    https://github.com/dmlc/xgboost/pull/1642
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    correct CalcDCG in rank_metric.cc and rank_obj.cc (#1642)
    
    * correct CalcDCG in rank_metric.cc
    
    DCG use log base-2, however `std::log` returns log base-e.
    
    * correct CalcDCG in rank_obj.cc
    
    DCG use log base-2, however `std::log` returns log base-e.
    
    * use std::log2 instead of std::log
    
     make it more elegant
    
    * use std::log2 instead of std::log
    
    make it more elegant
    [LIBXGBOOST] pass demo running.
    [LEARNER] refactor learner
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [METRIC] all metric move finished
    [OBJ] Add basic objective function and registry
    Move warning about empty dataset. (#5998)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Fix release degradation (#5720)
    
    * fix release degradation, related to 5666
    
    * less resizes
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Pseudo-huber loss metric added (#5647)
    
    
    - Add pseudo huber loss objective.
    - Add pseudo huber loss metric.
    
    Co-authored-by: Reetz <s02reetz@iavgroup.local>
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add number of columns to native data iterator. (#5202)
    
    * Change native data iter into an adapter.
    Add check for length of weights. (#4872)
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Removed deprecated gpu objectives. (#4690)
    Add `rmsle` metric and `reg:squaredlogerror` objective (#4541)
    Fix tweedie metric string. (#4543)
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Enable running objectives with 0 GPU. (#3878)
    
    * Enable running objectives with 0 GPU.
    
    * Enable 0 GPU for objectives.
    * Add doc for GPU objectives.
    * Fix some objectives defaulted to running on all GPUs.
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Multi-GPU support in GPUPredictor. (#3738)
    
    * Multi-GPU support in GPUPredictor.
    
    - GPUPredictor is multi-GPU
    - removed DeviceMatrix, as it has been made obsolete by using HostDeviceVector in DMatrix
    
    * Replaced pointers with spans in GPUPredictor.
    
    * Added a multi-GPU predictor test.
    
    * Fix multi-gpu test.
    
    * Fix n_rows < n_gpus.
    
    * Reinitialize shards when GPUSet is changed.
    * Tests range of data.
    
    * Remove commented code.
    
    * Remove commented code.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Refactor the CLI. (#5574)
    
    
    * Enable parameter validation.
    * Enable JSON.
    * Catch `dmlc::Error`.
    * Show help message.
    Fix compilation on Mac OSX High Sierra (10.13) (#5597)
    
    * Fix compilation on Mac OSX High Sierra
    
    * [CI] Build Mac OSX binary wheel using Travis CI
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Fix tweedie handling of base_score (#3295)
    
    * fix tweedie margin calculations
    
    * add entry to contributors
    Fix for issue 3306. (#3324)
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    [core] fix slow predict-caching with many classes (#3109)
    
    * fix prediction caching inefficiency for multiclass
    
    * silence some warnings
    
    * redundant if
    
    * workaround for R v3.4.3 bug; fixes #3081
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    AVX gradients (#2878)
    
    * AVX gradients
    
    * Add google test for AVX
    
    * Create fallback implementation, remove fma instruction
    
    * Improved accuracy of AVX exp function
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    add default to poisson -> max_delta_step to enable loading/saving/dumping of model (#1781)
    Tweedie Regression Post-Rebase (#1737)
    
    * add support for tweedie regression
    
    * added back readme line that was accidentally deleted
    
    * fixed linting errors
    
    * add support for tweedie regression
    
    * added back readme line that was accidentally deleted
    
    * fixed linting errors
    
    * rebased with upstream master and added R example
    
    * changed parameter name to tweedie_variance_power
    
    * linting error fix
    
    * refactored tweedie-nloglik metric to be more like the other parameterized metrics
    
    * added upper and lower bound check to tweedie metric
    
    * add support for tweedie regression
    
    * added back readme line that was accidentally deleted
    
    * fixed linting errors
    
    * added upper and lower bound check to tweedie metric
    
    * added back readme line that was accidentally deleted
    
    * rebased with upstream master and added R example
    
    * rebased again on top of upstream master
    
    * linting error fix
    
    * added upper and lower bound check to tweedie metric
    
    * rebased with master
    
    * lint fix
    
    * removed whitespace at end of line 186 - elementwise_metric.cc
    Add support for Gamma regression (#1258)
    
    * Add support for Gamma regression
    
    * Use base_score to replace the lp_bias
    
    * Remove the lp_bias config block
    
    * Add a demo for running gamma regression in Python
    
    * Typo fix
    
    * Revise the description for objective
    
    * Add a script to generate the autoclaims dataset
    [PLUGIN] Add plugin system
    [LIBXGBOOST] pass demo running.
    [LEARNER] refactor learner
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [METRIC] all metric move finished
    [OBJ] Add basic objective function and registry
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Pairwise ranking objective implementation on gpu (#4873)
    
    * - pairwise ranking objective implementation on gpu
       - there are couple of more algorithms (ndcg and map) for which support will be added
         as follow-up pr's
       - with no label groups defined, get gradient is 90x faster on gpu (120m instance
         mortgage dataset)
       - it can perform by an order of magnitude faster with ~ 10 groups (and adequate cores
         for the cpu implementation)
    
    * Add JSON config to rank obj.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Implementation of hinge loss for binary classification (#3477)
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    [LIBXGBOOST] pass demo running.
    [METRIC] all metric move finished
    [OBJ] Add basic objective function and registry
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Fix r interaction constraints (#5543)
    
    * Unify the parsing code.
    
    * Cleanup.
    Unify max nodes. (#5497)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Move bitfield into common. (#4737)
    
    
    * Prepare for columnar format support.
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Fix race condition in interaction constraint. (#4587)
    
    * Split up the kernel to sync write.
    
    * QueryNode is no-longer used in Query, but kept for testing.
    Feature interaction for GPU Hist. (#4534)
    
    * GPU hist Interaction Constraints.
    * Duplicate related parameters.
    * Add tests for CPU interaction constraint.
    * Add better error reporting.
    * Thorough tests.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Model IO in JSON. (#5110)
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    [LIBXGBOOST] pass demo running.
    [GBM] remove need to explicit InitModel, rename save/load
    [TREE] Move colmaker
    [TREE] Enable updater registry
    More categorical tests and disable shap sparse test. (#6219)
    
    * Fix tree load with 32 category.
    Add high level tests for categorical data. (#6179)
    
    * Fix unique.
    Reduce compile warnings (#6198)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Improve JSON format for categorical features. (#6128)
    
    * Gather categories for all nodes.
    [Breaking] Don't save leaf child count in JSON. (#6094)
    
    The field is deprecated and not used anywhere in XGBoost.
    Expand categorical node. (#6028)
    
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Add JSON schema to model dump. (#5660)
    Requires setting leaf stat when expanding tree. (#5501)
    
    * Fix GPU Hist feature importance.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Check whether current updater can modify a tree. (#5406)
    
    * Check whether current updater can modify a tree.
    
    * Fix tree model JSON IO for pruned trees.
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Fix num_roots to be 1. (#5165)
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Model IO in JSON. (#5110)
    [Breaking] Remove num roots. (#5059)
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Remove initializing stringstream reference. (#4788)
    Implement tree model dump with code generator. (#4602)
    
    * Implement tree model dump with a code generator.
    
    * Split up generators.
    * Implement graphviz generator.
    * Use pattern matching.
    
    * [Breaking] Return a Source in `to_graphviz` instead of Digraph in Python package.
    
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Avoid generating NaNs in UnwoundPathSum (#3943)
    
    * Avoid generating NaNs in UnwoundPathSum.
    
    Kudos to Jakub Zakrzewski for tracking down the bug.
    
    * Add a test
    Combine TreeModel and RegTree (#3995)
    Fix #3702: do not round up integer thresholds for integer features in JSON dump (#3717)
    Increase precision of bst_float values in tree dumps (#3298)
    
    * Increase precision of bst_float values in tree dumps
    
    * Increase precision of bst_float values in tree dumps
    
    * Fix lint error and switch precision to right float variable
    
    * Fix clang-tidy error
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    [TREE] Move colmaker
    [TREE] Enable updater registry
    [TREE] Finalize regression tree refactor
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Check whether current updater can modify a tree. (#5406)
    
    * Check whether current updater can modify a tree.
    
    * Fix tree model JSON IO for pruned trees.
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    [Breaking] Remove num roots. (#5059)
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    Remove leaf vector, add tree serialisation test, fix Windows tests (#3989)
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Remove accidental SparsePage copies (#3583)
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Changing omp_get_num_threads to omp_get_max_threads (#1831)
    
    * Updating dmlc-core
    
    * Changing omp_get_num_threads to omp_get_max_threads
    [CORE] The update process for a tree model, and its application to feature importance (#1670)
    
    * [CORE] allow updating trees in an existing model
    
    * [CORE] in refresh updater, allow keeping old leaf values and update stats only
    
    * [R-package] xgb.train mod to allow updating trees in an existing model
    
    * [R-package] added check for nrounds when is_update
    
    * [CORE] merge parameter declaration changes; unify their code style
    
    * [CORE] move the update-process trees initialization to Configure; rename default process_type to 'default'; fix the trees and trees_to_update sizes comparison check
    
    * [R-package] unit tests for the update process type
    
    * [DOC] documentation for process_type parameter; improved docs for updater, Gamma and Tweedie; added some parameter aliases; metrics indentation and some were non-documented
    
    * fix my sloppy merge conflict resolutions
    
    * [CORE] add a TreeProcessType enum
    
    * whitespace fix
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [LIBXGBOOST] pass demo running.
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [TREE] finish move of updater
    [TREE] Move the files to target refactor location
    [REFACTOR] cleanup structure
    lint half way
    update with new rabit api
    add sync module
    change hist update to lazy
    change allreduce lib to rabit library, xgboost now run with rabit
    refresher is now distributed
    move nthread to local var
    fix som solaris
    complete refactor data.h, now replies on iterator to access column
    rename SparseBatch to RowBatch
    change omp loop var to bst_omp_uint, add XGB_DLL to wrapper
    clean up warnings from msvc
    tstats now depend on param
    templatize refresher
    refactor grad stats to be like visitor
    chg root index to booster info, need review
    refresher test
    add tree refresher, need review
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    C++14 for xgboost (#5664)
    Requires setting leaf stat when expanding tree. (#5501)
    
    * Fix GPU Hist feature importance.
    [Breaking] Remove num roots. (#5059)
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Restrict access to `cfg_` in gbm. (#4801)
    
    * Restrict access to `cfg_` in gbm.
    
    * Verify having correct updaters.
    
    * Remove `grow_global_histmaker`
    
    This updater is the same as `grow_histmaker`.  The former is not in our
    document so we just remove it.
    Remove some unused functions as reported by cppcheck (#4743)
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
     Require leaf statistics when expanding tree (#4015)
    
    * Cache left and right gradient sums
    
    * Require leaf statistics when expanding tree
    Reduce tree expand boilerplate code (#4008)
    Remove leaf vector, add tree serialisation test, fix Windows tests (#3989)
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Remove obsoleted QuantileHistMaker. (#3761)
    
    Fix #3755.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    [jvm-packages] fix errors in example (#3719)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * instrumentation
    
    * use log console
    
    * better measurement
    
    * fix erros in example
    
    * update histmaker
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Remove accidental SparsePage copies (#3583)
    Fix #3402: wrong fid crashes distributed algorithm (#3535)
    
    * Fix #3402: wrong fid crashes distributed algorithm
    
    The bug was introduced by the recent DMatrix refactor (#3301). It was partially
    fixed by #3408 but the example in #3402 was still failing. The example in #3402
    will succeed after this fix is applied.
    
    * Explicitly specify "this" to prevent compile error
    
    * Add regression test
    
    * Add distributed test to Travis matrix
    
    * Install kubernetes Python package as dependency of dmlc tracker
    
    * Add Python dependencies
    
    * Add compile step
    
    * Reduce size of regression test case
    
    * Further reduce size of test
    Correct mistake from dmatrix refactor (#3408)
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Integer gradient summation for GPU histogram algorithm. (#2681)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Automatically remove nan from input data when it is sparse. (#2062)
    
    * [DATALoad] Automatically remove Nan when load from sparse matrix
    
    * add log
    Changing omp_get_num_threads to omp_get_max_threads (#1831)
    
    * Updating dmlc-core
    
    * Changing omp_get_num_threads to omp_get_max_threads
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [PYTHON-DIST] Distributed xgboost python training API.
    Update dmlc-core
    [APPROX] Make global proposal default, add group ptr solution
    [TREE] Enable global proposal for faster speed
    [TREE] switch to two pass
    [TREE] Cleanup some functions, add utility function for two pass
    [TREE] Refactor histmaker
    [LIBXGBOOST] pass demo running.
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [TREE] finish move of updater
    [TREE] Move the files to target refactor location
    [REFACTOR] cleanup structure
    lint half way
    checkin some micro optimization
    fix python windows installation problem, enable mingw compile, but seems mingw dll was not fast in loading
    need more check
    update with new rabit api
    add sync module
    change hist update to lazy
    change allreduce lib to rabit library, xgboost now run with rabit
    remove warning from MSVC need another round of check
    check pipe, commit optimization for hist
    get multinode in
    compile
    cqmaker ok
    only need to add in create hist col base
    hack to make the propose fast in one pass, start sketchmaker
    simplify
    sorted base sketch maker
    check in basemaker
    check in two bad ones, start think of column distribut cut row
    alrite
    fix regression
    ok
    fix compile, need final leaf node?
    a version that compile
    first ver
    ok
    everything is ready, except for propose
    finish find split, next to do quantile sketch
    incomplete histmaker
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Distributed optimizations for 'hist' method with CPUs (#5557)
    
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Check whether current updater can modify a tree. (#5406)
    
    * Check whether current updater can modify a tree.
    
    * Fix tree model JSON IO for pruned trees.
    Fix pruner. (#5335)
    
    
    * Honor the tree depth.
    * Prevent pruning pruned node.
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    [Breaking] Remove num roots. (#5059)
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Combine TreeModel and RegTree (#3995)
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    [LIBXGBOOST] pass demo running.
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [TREE] finish move of updater
    [TREE] Move the files to target refactor location
    [REFACTOR] cleanup structure
    fix minor typo
    spelling, wording, and doc fixes in c++ code
    
    I was reading through the code and fixing some things in the comments.
    Only a few trivial actual code changes were made to make things more
    readable.
    lint half way
    add in sync
    move sync tree to pruner, pruner is now distributed
    remove using std from cpp
    fix print problem, fix Tong's email format
    complete refactor data.h, now replies on iterator to access column
    chg root index to booster info, need review
    fix num parallel tree
    fix base score, and print message
    pass fmatrix as const
    mv code into src
    start unity refactor
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    CPU predict performance improvement (#6127)
    
    Co-authored-by: ShvetsKS <kirill.shvets@intel.com>
    Unify evaluation functions. (#6037)
    Feature weights (#5962)
    Thread-safe prediction by making the prediction cache thread-local. (#5853)
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Add explicit template specialization for portability (#5921)
    
    * Add explicit template specializations
    
    * Adding Specialization for FileAdapterBatch
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Add float32 histogram (#5624)
    
    * new single_precision_histogram param was added.
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Co-authored-by: fis <jm.yuan@outlook.com>
    Distributed optimizations for 'hist' method with CPUs (#5557)
    
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    skip missing lookup if nothing is missing in CPU hist partition kernel. (#5644)
    
    * [xgboost] skip missing lookup if nothing is missing
    Optimizations for RNG in InitData kernel (#5522)
    
    * optimizations for subsampling in InitData
    
    * optimizations for subsampling in InitData
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Requires setting leaf stat when expanding tree. (#5501)
    
    * Fix GPU Hist feature importance.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Reducing memory consumption for 'hist' method on CPU (#5334)
    Optimized ApplySplit, BuildHist and UpdatePredictCache functions on CPU (#5244)
    
    * Split up sparse and dense build hist kernels.
    * Add `PartitionBuilder`.
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Optimized BuildHist function (#5156)
    Optimized EvaluateSplut function (#5138)
    
    
    * Add block based threading utilities.
    Quick fix for memory leak in CPU Hist. (#5153)
    
    
    
    Closes https://github.com/dmlc/xgboost/issues/3579 .
    
    * Don't use map.
    added tracking execution time for UpdatePredictionCache function (#5107)
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    [Breaking] Remove num roots. (#5059)
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Revert #4529 (#5008)
    
    * Revert " Optimize ‘hist’ for multi-core CPU (#4529)"
    
    This reverts commit 4d6590be3c9a043d44d9e4fe0a456a9f8179ec72.
    
    * Fix build
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    [blocking] fix parallel eval_split of hist updater (#4851)
    
    * Don't call rabit functions inside parallel loop.
    [HOTFIX] distributed training with hist method  (#4716)
    
    * add parallel test for hist.EvalualiteSplit
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * fix OMP schedule policy
    
    * fix clang-tidy
    
    * add logging: total_num_bins
    
    * fix
    
    * fix
    
    * test
    
    * replace guided OPENMP policy with static in updater_quantile_hist.cc
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
     Optimize ‘hist’ for multi-core CPU (#4529)
    
    * Initial performance optimizations for xgboost
    
    * remove includes
    
    * revert float->double
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * Check existence of _mm_prefetch and __builtin_prefetch
    
    * Fix lint
    
    * optimizations for CPU
    
    * appling comments in review
    
    * add some comments, code refactoring
    
    * fixing issues in CI
    
    * adding runtime checks
    
    * remove 1 extra check
    
    * remove extra checks in BuildHist
    
    * remove checks
    
    * add debug info
    
    * added debug info
    
    * revert changes
    
    * added comments
    
    * Apply suggestions from code review
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * apply review comments
    
    * Remove unused function CreateNewNodes()
    
    * Add descriptive comment on node_idx variable in QuantileHistMaker::Builder::BuildHistsBatch()
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Use feature interaction constraints to narrow search space for split candidates (#4341)
    
    * Use feature interaction constraints to narrow search space for split candidates.
    
    * fix clang-tidy broken at updater_quantile_hist.cc:535:3
    
    * make const
    
    * fix
    
    * try to fix exception thrown in java_test
    
    * fix suspected mistake which cause EvaluateSplit error
    
    * try fix
    
    * Fix bug: feature ID and node ID swapped in argument
    
    * Rename CheckValidation() to CheckFeatureConstraint() for clarity
    
    * Do not create temporary vector validFeatures, to enable parallelism
    Optimizations of pre-processing for 'hist' tree method (#4310)
    
    * oprimizations for pre-processing
    
    * code cleaning
    
    * code cleaning
    
    * code cleaning after review
    
    * Apply suggestions from code review
    
    Co-Authored-By: SmirnovEgorRu <egor.smirnov@intel.com>
    Use Monitor in quantile hist. (#4273)
    Optimisations for gpu_hist. (#4248)
    
    * Optimisations for gpu_hist.
    
    * Use streams to overlap operations.
    
    * ColumnSampler now uses HostDeviceVector to prevent repeatedly copying feature vectors to the device.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    more correct way to build node stats in distributed fast hist (#4140)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * more changes
    
    * temp
    
    * update
    
    * udpate rabit
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * broadcast subsampled feature correctly
    
    * init col
    
    * temp
    
    * col sampling
    
    * fix histmastrix init
    
    * fix col sampling
    
    * remove cout
    
    * fix out of bound access
    
    * fix core dump
    
    remove core dump file
    
    * update
    
    * add fid
    
    * update
    
    * revert some changes
    
    * temp
    
    * temp
    
    * pass all tests
    
    * bring back some tests
    
    * recover some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * recover column init part
    
    * more recovery
    
    * fix core dumps
    
    * code clean
    
    * revert some changes
    
    * fix test compilation issue
    
    * fix lint issue
    
    * resolve compilation issue
    
    * fix issues of lint caused by rebase
    
    * fix stylistic changes and change variable names
    
    * modularize depth width
    
    * address the comments
    
    * fix failed tests
    
    * wrap perf timers with class
    
    * temp
    
    * pass all lossguide
    
    * pass tests
    
    * add comments
    
    * more changes
    
    * use separate flow for single and tests
    
    * add test for lossguide hist
    
    * remove duplications
    
    * syncing stats for only once
    
    * recover more changes
    
    * recover more changes
    
    * fix root-stats
    
    * simplify code
    
    * remove outdated comments
    Separate Depthwidth and Lossguide growing policy in fast histogram (#4102)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * more changes
    
    * temp
    
    * update
    
    * udpate rabit
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * broadcast subsampled feature correctly
    
    * init col
    
    * temp
    
    * col sampling
    
    * fix histmastrix init
    
    * fix col sampling
    
    * remove cout
    
    * fix out of bound access
    
    * fix core dump
    
    remove core dump file
    
    * disbale test temporarily
    
    * update
    
    * add fid
    
    * print perf data
    
    * update
    
    * revert some changes
    
    * temp
    
    * temp
    
    * pass all tests
    
    * bring back some tests
    
    * recover some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * recover column init part
    
    * more recovery
    
    * fix core dumps
    
    * code clean
    
    * revert some changes
    
    * fix test compilation issue
    
    * fix lint issue
    
    * resolve compilation issue
    
    * fix issues of lint caused by rebase
    
    * fix stylistic changes and change variable names
    
    * use regtree internal function
    
    * modularize depth width
    
    * address the comments
    
    * fix failed tests
    
    * wrap perf timers with class
    
    * fix lint
    
    * fix num_leaves count
    
    * fix indention
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * merge
    
    * fix compilation
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
     Require leaf statistics when expanding tree (#4015)
    
    * Cache left and right gradient sums
    
    * Require leaf statistics when expanding tree
    Reduce tree expand boilerplate code (#4008)
    Fix and optimize logger (#4002)
    
    * Fix logging switch statement.
    
    * Remove debug_verbose_ in AllReducer.
    
    * Don't construct the stream when not needed.
    
    * Make default constructor deleted.
    
    * Remove redundant IsVerbose.
    Column sampling at individual nodes (splits). (#3971)
    
    * Column sampling at individual nodes (splits).
    
    * Documented colsample_bynode parameter.
    
    - also updated documentation for colsample_by* parameters
    
    * Updated documentation.
    
    * GetFeatureSet() returns shared pointer to std::vector.
    
    * Sync sampled columns across multiple processes.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Remove leaf vector, add tree serialisation test, fix Windows tests (#3989)
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Fix r interaction constraints (#5543)
    
    * Unify the parsing code.
    
    * Cleanup.
    Fix parsing empty vector in parameter. (#5087)
    Fix r interaction constraints (#5543)
    
    * Unify the parsing code.
    
    * Cleanup.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Loop over `thrust::reduce`. (#6229)
    
    * Check input chunk size of dqdm.
    * Add doc for current limitation.
    Add categorical data support to GPU Hist. (#6164)
    Unify evaluation functions. (#6037)
    Feature weights (#5962)
    Split Features into Groups to Compute Histograms in Shared Memory (#5795)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    gpu_hist performance tweaks (#5707)
    
    * Remove device vectors
    
    * Remove allreduce synchronize
    
    * Remove double buffer
    Reduce device synchronisation (#5631)
    
    * Reduce device synchronisation
    
    * Initialise pinned memory
    Enhance nvtx support. (#5636)
    Refactor gpu_hist split evaluation (#5610)
    
    * Refactor
    
    * Rewrite evaluate splits
    
    * Add more tests
    Refactor the CLI. (#5574)
    
    
    * Enable parameter validation.
    * Enable JSON.
    * Catch `dmlc::Error`.
    * Show help message.
    For histograms, opting into maximum shared memory available per block. (#5491)
    gpu_hist performance fixes (#5558)
    
    * Remove unnecessary cuda API calls
    
    * Fix histogram memory growth
    Fix r interaction constraints (#5543)
    
    * Unify the parsing code.
    
    * Cleanup.
    Purge device_helpers.cuh (#5534)
    
    * Simplifications with caching_device_vector
    
    * Purge device helpers
    Unify max nodes. (#5497)
    Requires setting leaf stat when expanding tree. (#5501)
    
    * Fix GPU Hist feature importance.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix memory usage of device sketching (#5407)
    Partial rewrite EllpackPage (#5352)
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Model IO in JSON. (#5110)
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Pairwise ranking objective implementation on gpu (#4873)
    
    * - pairwise ranking objective implementation on gpu
       - there are couple of more algorithms (ndcg and map) for which support will be added
         as follow-up pr's
       - with no label groups defined, get gradient is 90x faster on gpu (120m instance
         mortgage dataset)
       - it can perform by an order of magnitude faster with ~ 10 groups (and adequate cores
         for the cpu implementation)
    
    * Add JSON config to rank obj.
    Set correct file permission. (#4964)
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Use heuristic to select histogram node, avoid rabit call (#4951)
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    remove device shards (#4867)
    Support gamma in GPU_Hist. (#4874)
    
    
    * Just prevent building the tree instead of using an explicit pruner.
    Move ellpack page construction into DMatrix (#4833)
    Some comments for row partitioner. (#4832)
    Restrict access to `cfg_` in gbm. (#4801)
    
    * Restrict access to `cfg_` in gbm.
    
    * Verify having correct updaters.
    
    * Remove `grow_global_histmaker`
    
    This updater is the same as `grow_histmaker`.  The former is not in our
    document so we just remove it.
    further cleanup of single process multi-GPU code (#4810)
    
    
    * use subspan in gpu predictor instead of copying
    * Revise `HostDeviceVector`
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Modify caching allocator/vector and fix issues relating to inability to train large datasets (#4615)
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Refactor out row partitioning logic from gpu_hist, introduce caching device vectors (#4554)
    Feature interaction for GPU Hist. (#4534)
    
    * GPU hist Interaction Constraints.
    * Duplicate related parameters.
    * Add tests for CPU interaction constraint.
    * Add better error reporting.
    * Thorough tests.
    Optimizations for quantisation on device (#4572)
    
    * - do not create device vectors for the entire sparse page while computing histograms...
       - while creating the compressed histogram indices, the row vector is created for the entire
         sparse page batch. this is needless as we only process chunks at a time based on a slice
         of the total gpu memory
       - this pr will allocate only as much as required to store the ppropriate row indices and the entries
    
    * - do not dereference row_ptrs once the device_vector has been created to elide host copies of those counts
       - instead, grab the entry counts directly from the sparsepage
    - training with external memory - part 2 of 2 (#4526)
    
    * - training with external memory - part 2 of 2
       - when external memory support is enabled, building of histogram indices are
         done incrementally for every sparse page
       - the entire set of input data is divided across multiple gpu's and the relative
         row positions within each device is tracked when building the compressed histogram buffer
       - this was tested using a mortgage dataset containing ~ 670m rows before 4xt4's could be
         saturated
    Overload device memory allocation (#4532)
    
    * Group source files, include headers in source files
    
    * Overload device memory allocation
    Refactor histogram building code for gpu_hist (#4528)
     Smarter choice of histogram construction for distributed gpu_hist (#4519)
    
    * Smarter choice of histogram construction for distributed gpu_hist
    
    * Limit omp team size in ExecuteShards
    - training with external memory part 1 of 2 (#4486)
    
    * - training with external memory part 1 of 2
       - this pr focuses on computing the quantiles using multiple gpus on a
         dataset that uses the external cache capabilities
       - there will a follow-up pr soon after this that will support creation
         of histogram indices on large dataset as well
       - both of these changes are required to support training with external memory
       - the sparse pages in dmatrix are taken in batches and the the cut matrices
         are incrementally built
       - also snuck in some (perf) changes related to sketches aggregation amongst multiple
         features across multiple sparse page batches. instead of aggregating the summary
         inside each device and merged later, it is aggregated in-place when the device
         is working on different rows but the same feature
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    More explict sharding methods for device memory (#4396)
    
    * Rename the Reshard method to Shard
    
    * Add a new Reshard method for sharding a vector that's already sharded
     Combine thread launches into single launch per tree for gpu_hist (#4343)
    
    * Combine thread launches into single launch per tree for gpu_hist
    algorithm.
    
    * Address deprecation warning
    
    * Add manual column sampler constructor
    
    * Turn off omp dynamic to get a guaranteed number of threads
    
    * Enable openmp in cuda code
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Fix Histogram allocation. (#4347)
    
    * Fix Histogram allocation.
    
    nidx_map is cleared after `Reset`, but histogram data size isn't changed hence
    histogram recycling is used in later iterations.  After a reset(building new
    tree), newly allocated node will start from 0, while recycling always choose
    the node with smallest index, which happens to be our newly allocated node 0.
    Retire DVec class in favour of c++20 style span for device memory. (#4293)
    Further optimisations for gpu_hist. (#4283)
    
    - Fuse final update position functions into a single more efficient kernel
    
    - Refactor gpu_hist with a more explicit ellpack  matrix representation
    Optimisations for gpu_hist. (#4248)
    
    * Optimisations for gpu_hist.
    
    * Use streams to overlap operations.
    
    * ColumnSampler now uses HostDeviceVector to prevent repeatedly copying feature vectors to the device.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Remove various synchronisations from cuda API calls, instrument monitor (#4205)
    
    * Remove various synchronisations from cuda API calls, instrument monitor
    with nvtx profiler ranges.
    minor fix: log InitDataOnce() only when it is actually called (#4206)
    [REVIEW] Enable Multi-Node Multi-GPU functionality (#4095)
    
    * Initial commit to support multi-node multi-gpu xgboost using dask
    
    * Fixed NCCL initialization by not ignoring the opg parameter.
    
    - it now crashes on NCCL initialization, but at least we're attempting it properly
    
    * At the root node, perform a rabit::Allreduce to get initial sum_gradient across workers
    
    * Synchronizing in a couple of more places.
    
    - now the workers don't go down, but just hang
    - no more "wild" values of gradients
    - probably needs syncing in more places
    
    * Added another missing max-allreduce operation inside BuildHistLeftRight
    
    * Removed unnecessary collective operations.
    
    * Simplified rabit::Allreduce() sync of gradient sums.
    
    * Removed unnecessary rabit syncs around ncclAllReduce.
    
    - this improves performance _significantly_ (7x faster for overall training,
      20x faster for xgboost proper)
    
    * pulling in latest xgboost
    
    * removing changes to updater_quantile_hist.cc
    
    * changing use_nccl_opg initialization, removing unnecessary if statements
    
    * added definition for opaque ncclUniqueId struct to properly encapsulate GetUniqueId
    
    * placing struct defintion in guard to avoid duplicate code errors
    
    * addressing linting errors
    
    * removing
    
    * removing additional arguments to AllReduer initialization
    
    * removing distributed flag
    
    * making comm init symmetric
    
    * removing distributed flag
    
    * changing ncclCommInit to support multiple modalities
    
    * fix indenting
    
    * updating ncclCommInitRank block with necessary group calls
    
    * fix indenting
    
    * adding print statement, and updating accessor in vector
    
    * improving print statement to end-line
    
    * generalizing nccl_rank construction using rabit
    
    * assume device_ordinals is the same for every node
    
    * test, assume device_ordinals is identical for all nodes
    
    * test, assume device_ordinals is unique for all nodes
    
    * changing names of offset variable to be more descriptive, editing indenting
    
    * wrapping ncclUniqueId GetUniqueId() and aesthetic changes
    
    * adding synchronization, and tests for distributed
    
    * adding  to tests
    
    * fixing broken #endif
    
    * fixing initialization of gpu histograms, correcting errors in tests
    
    * adding to contributors list
    
    * adding distributed tests to jenkins
    
    * fixing bad path in distributed test
    
    * debugging
    
    * adding kubernetes for distributed tests
    
    * adding proper import for OrderedDict
    
    * adding urllib3==1.22 to address ordered_dict import error
    
    * added sleep to allow workers to save their models for comparison
    
    * adding name to GPU contributors under docs
    Fix incorrect device in multi-GPU algorithm (#4161)
    Use nccl group calls to prevent from dead lock. (#4113)
    
    * launch all reduce sequentially.
    * Fix gpu_exact test memory leak.
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
     Require leaf statistics when expanding tree (#4015)
    
    * Cache left and right gradient sums
    
    * Require leaf statistics when expanding tree
    Use Span in GPU exact updater. (#4020)
    
    * Use Span in GPU exact updater.
    
    * Add a small test.
    Reduce tree expand boilerplate code (#4008)
    Fix and optimize logger (#4002)
    
    * Fix logging switch statement.
    
    * Remove debug_verbose_ in AllReducer.
    
    * Don't construct the stream when not needed.
    
    * Make default constructor deleted.
    
    * Remove redundant IsVerbose.
    Column sampling at individual nodes (splits). (#3971)
    
    * Column sampling at individual nodes (splits).
    
    * Documented colsample_bynode parameter.
    
    - also updated documentation for colsample_by* parameters
    
    * Updated documentation.
    
    * GetFeatureSet() returns shared pointer to std::vector.
    
    * Sync sampled columns across multiple processes.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Single precision histograms on GPU (#3965)
    
    * Allow single precision histogram summation in gpu_hist
    
    * Add python test, reduce run-time of gpu_hist tests
    
    * Update documentation
    GPU performance logging/improvements (#3945)
    
    - Improved GPU performance logging
    
    - Only use one execute shards function
    
    - Revert performance regression on multi-GPU
    
    - Use threads to launch NCCL AllReduce
    Improve update position function for gpu_hist (#3895)
    Minor refactor of split evaluation in gpu_hist (#3889)
    
    * Refactor evaluate split into shard
    
    * Use span in evaluate split
    
    * Update google tests
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Add basic unittests for gpu-hist method. (#3785)
    
    * Split building histogram into separated class.
    * Extract `InitCompressedRow` definition.
    * Basic tests for gpu-hist.
    * Document the code more verbosely.
    * Removed `HistCutUnit`.
    * Removed some duplicated copies in `GPUHistMaker`.
    * Implement LCG and use it in tests.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Implement devices to devices reshard. (#3721)
    
    * Force clearing device memory before Reshard.
    * Remove calculating row_segments for gpu_hist and gpu_sketch.
    * Guard against changing device.
    Fixed an uninitialized pointer. (#3703)
    Fixed the performance regression within EvaluateSplits(). (#3680)
    
    - it turns out creating an std::vector on every call is faster
      than cudaMallocHost()/cudaFreeHost()
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    GPU memory usage fixes + column sampling refactor (#3635)
    
    * Remove thrust copy calls
    
    * Fix  histogram memory usage
    
    * Cap extreme histogram memory usage
    
    * More efficient column sampling
    
    * Use column sampler across updaters
    
    * More efficient split evaluation on GPU with column sampling
    Merge generic device helper functions into gpu set. (#3626)
    
    * Remove the use of old NDevices* functions.
    * Use GPUSet in timer.h.
    Dynamically allocate GPU histogram memory (#3519)
    
    * Expand histogram memory dynamically to prevent large allocations for large tree depths (e.g. > 15)
    
    * Remove GPU memory allocation messages. These are misleading as a large number of allocations are now dynamic.
    
    * Fix appveyor R test
    Added finding quantiles on GPU. (#3393)
    
    * Added finding quantiles on GPU.
    
    - this includes datasets where weights are assigned to data rows
    - as the quantiles found by the new algorithm are not the same
      as those found by the old one, test thresholds in
        tests/python-gpu/test_gpu_updaters.py have been adjusted.
    
    * Adjustments and improved testing for finding quantiles on the GPU.
    
    - added C++ tests for the DeviceSketch() function
    - reduced one of the thresholds in test_gpu_updaters.py
    - adjusted the cuts found by the find_cuts_k kernel
    Resolve GPU bug on large files (#3472)
    
    Remove calls to thrust copy, fix indexing bug
    Upgrading to NCCL2 (#3404)
    
    * Upgrading to NCCL2
    
    * Part - II of NCCL2 upgradation
    
     - Doc updates to build with nccl2
     - Dockerfile.gpu update for a correct CI build with nccl2
     - Updated FindNccl package to have env-var NCCL_ROOT to take precedence
    
    * Upgrading to v9.2 for CI workflow, since it has the nccl2 binaries available
    
    * Added NCCL2 license + copy the nccl binaries into /usr location for the FindNccl module to find
    
    * Set LD_LIBRARY_PATH variable to pick nccl2 binary at runtime
    
    * Need the nccl2 library download instructions inside Dockerfile.release as well
    
    * Use NCCL2 as a static library
    Shared memory atomics while building histogram (#3384)
    
    * Use shared memory atomics for building histograms, whenever possible
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    GPU binning and compression. (#3319)
    
    * GPU binning and compression.
    
    - binning and index compression are done inside the DeviceShard constructor
    - in case of a DMatrix with multiple row batches, it is first converted into a single row batch
    Multi-GPU HostDeviceVector. (#3287)
    
    * Multi-GPU HostDeviceVector.
    
    - HostDeviceVector instances can now span multiple devices, defined by GPUSet struct
    - the interface of HostDeviceVector has been modified accordingly
    - GPU objective functions are now multi-GPU
    - GPU predicting from cache is now multi-GPU
    - avoiding omp_set_num_threads() calls
    - other minor changes
    Fix issue #3264, accuracy issues on k80 GPUs. (#3293)
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Change reduce operation from thrust to cub. Fix for cuda 9.1 error (#3218)
    
    * Change reduce operation from thrust to cub. Fix for cuda 9.1 runtime error
    
    * Unit test sum reduce
    Added back UpdatePredictionCache() in updater_gpu_hist.cu. (#3120)
    
    * Added back UpdatePredictionCache() in updater_gpu_hist.cu.
    
    - it had been there before, but wasn't ported to the new version
      of updater_gpu_hist.cu
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Fix GPU bugs (#3051)
    
    * Change uint to unsigned int
    
    * Fix no root predictions bug
    
    * Remove redundant splitting due to numerical instability
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Avoid repeated cuda API call in GPU predictor and only synchronize used GPUs (#2936)
    Fix several GPU bugs (#2916)
    
    * Fix #2905
    
    * Fix gpu_exact test failures
    
    * Fix bug in GPU prediction where multiple calls to batch prediction can produce incorrect results
    
    * Fix GPU documentation formatting
    Monotone constraints for gpu_hist (#2904)
    Update gpu_hist algorithm (#2901)
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    Add warnings for large labels when using GPU histogram algorithms (#2834)
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    Integer gradient summation for GPU histogram algorithm. (#2681)
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    [GPU-Plugin] Major refactor (#2644)
    
    * Removal of redundant code/files.
    * Removal of exact namespace in GPU plugin
    * Revert double precision histograms to single precision for performance on Maxwell/Kepler
    Remove skmaker. (#5971)
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Remove gpu_exact tree method (#4742)
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Update gpu_hist algorithm (#2901)
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    [GPU-Plugin] Major refactor (#2644)
    
    * Removal of redundant code/files.
    * Removal of exact namespace in GPU plugin
    * Revert double precision histograms to single precision for performance on Maxwell/Kepler
    Disallow multiple roots for tree_method=hist (#1979)
    
    As discussed in issue #1978, tree_method=hist ignores the parameter
    param.num_roots; it simply assumes that the tree has only one root. In
    particular, when InitData() method initializes row_set_collection_, it simply
    assigns all rows to node 0, the value that's hard-coded.
    
    For now, the updater will simply fail when num_roots exceeds 1. I will revise
    the updater soon to support multiple roots.
    [LIBXGBOOST] pass demo running.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    [Breaking] Remove num roots. (#5059)
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Get rid of a few trivial compiler warnings. (#4312)
    Add PushCSC for SparsePage. (#4193)
    
    * Add PushCSC for SparsePage.
    
    * Move Push* definitions into cc file.
    * Add std:: prefix to `size_t` make clang++ happy.
    * Address monitor count == 0.
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Remove accidental SparsePage copies (#3583)
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Integer gradient summation for GPU histogram algorithm. (#2681)
    Changing omp_get_num_threads to omp_get_max_threads (#1831)
    
    * Updating dmlc-core
    
    * Changing omp_get_num_threads to omp_get_max_threads
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [PYTHON-DIST] Distributed xgboost python training API.
    [TREE] Cleanup some functions, add utility function for two pass
    [TREE] Refactor histmaker
    [MEM] Add rowset struct to save memory with billion level rows
    [R] make all customizations to meet strict standard of cran
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [TREE] finish move of updater
    [TREE] Move the files to target refactor location
    [REFACTOR] cleanup structure
    lint half way
    fix python windows installation problem, enable mingw compile, but seems mingw dll was not fast in loading
    more capacity for base
    need more check
    add sync module
    fix the row split recovery, add per iteration random number seed
    change allreduce lib to rabit library, xgboost now run with rabit
    remove warning from MSVC need another round of check
    check pipe, commit optimization for hist
    cqmaker ok
    only need to add in create hist col base
    checkin skmaker
    updated base
    check in basemaker
    Unify evaluation functions. (#6037)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Use feature interaction constraints to narrow search space for split candidates (#4341)
    
    * Use feature interaction constraints to narrow search space for split candidates.
    
    * fix clang-tidy broken at updater_quantile_hist.cc:535:3
    
    * make const
    
    * fix
    
    * try to fix exception thrown in java_test
    
    * fix suspected mistake which cause EvaluateSplit error
    
    * try fix
    
    * Fix bug: feature ID and node ID swapped in argument
    
    * Rename CheckValidation() to CheckFeatureConstraint() for clarity
    
    * Do not create temporary vector validFeatures, to enable parallelism
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Add LASSO (#3429)
    
    * Allow multiple split constraints
    
    * Replace RidgePenalty with ElasticNet
    
    * Add test for checking Ridge, LASSO, and Elastic Net are implemented
    Refactor of FastHistMaker to allow for custom regularisation methods (#3335)
    
    * Refactor to allow for custom regularisation methods
    
    * Implement compositional SplitEvaluator framework
    
    * Fixed segfault when no monotone_constraints are supplied.
    
    * Change pid to parentID
    
    * test_monotone_constraints.py now passes
    
    * Refactor ColMaker and DistColMaker to use SplitEvaluator
    
    * Performance optimisation when no monotone_constraints specified
    
    * Fix linter messages
    
    * Fix a few more linter errors
    
    * Update the amalgamation
    
    * Add bounds check
    
    * Add check for leaf node
    
    * Fix linter error in param.h
    
    * Fix clang-tidy errors on CI
    
    * Fix incorrect function name
    
    * Fix clang-tidy error in updater_fast_hist.cc
    
    * Enable SSE2 for Win32 R MinGW
    
    Addresses https://github.com/dmlc/xgboost/pull/3335#issuecomment-400535752
    
    * Add contributor
    Unify evaluation functions. (#6037)
    Refactor gpu_hist split evaluation (#5610)
    
    * Refactor
    
    * Rewrite evaluate splits
    
    * Add more tests
    Fix r interaction constraints (#5543)
    
    * Unify the parsing code.
    
    * Cleanup.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Clean up after removing `gpu_exact`. (#4777)
    
    * Removed unused functions.
    * Removed unused parameters.
    * Move ValueConstraints into constraints.cuh since it's now only used in GPU_Hist.
    Move bitfield into common. (#4737)
    
    
    * Prepare for columnar format support.
    Feature interaction for GPU Hist. (#4534)
    
    * GPU hist Interaction Constraints.
    * Duplicate related parameters.
    * Add tests for CPU interaction constraint.
    * Add better error reporting.
    * Thorough tests.
    [R] Check warnings explicitly for model compatibility tests (#6114)
    
    * [R] Check warnings explicitly for model compatibility tests
    
    * Address reviewer's feedback
    Unify evaluation functions. (#6037)
    [R] Add a compatibility layer to load Booster object from an old RDS file (#5940)
    
    * [R] Add a compatibility layer to load Booster from an old RDS
    * Modify QuantileHistMaker::LoadConfig() to be backward compatible with 1.1.x
    * Add a big warning about compatibility in QuantileHistMaker::LoadConfig()
    * Add testing suite
    * Discourage use of saveRDS() in CRAN doc
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Add float32 histogram (#5624)
    
    * new single_precision_histogram param was added.
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Co-authored-by: fis <jm.yuan@outlook.com>
    Distributed optimizations for 'hist' method with CPUs (#5557)
    
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Optimizations for RNG in InitData kernel (#5522)
    
    * optimizations for subsampling in InitData
    
    * optimizations for subsampling in InitData
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Reducing memory consumption for 'hist' method on CPU (#5334)
    Optimized ApplySplit, BuildHist and UpdatePredictCache functions on CPU (#5244)
    
    * Split up sparse and dense build hist kernels.
    * Add `PartitionBuilder`.
    Optimized BuildHist function (#5156)
    Optimized EvaluateSplut function (#5138)
    
    
    * Add block based threading utilities.
    Quick fix for memory leak in CPU Hist. (#5153)
    
    
    
    Closes https://github.com/dmlc/xgboost/issues/3579 .
    
    * Don't use map.
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Revert #4529 (#5008)
    
    * Revert " Optimize ‘hist’ for multi-core CPU (#4529)"
    
    This reverts commit 4d6590be3c9a043d44d9e4fe0a456a9f8179ec72.
    
    * Fix build
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
     Optimize ‘hist’ for multi-core CPU (#4529)
    
    * Initial performance optimizations for xgboost
    
    * remove includes
    
    * revert float->double
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * Check existence of _mm_prefetch and __builtin_prefetch
    
    * Fix lint
    
    * optimizations for CPU
    
    * appling comments in review
    
    * add some comments, code refactoring
    
    * fixing issues in CI
    
    * adding runtime checks
    
    * remove 1 extra check
    
    * remove extra checks in BuildHist
    
    * remove checks
    
    * add debug info
    
    * added debug info
    
    * revert changes
    
    * added comments
    
    * Apply suggestions from code review
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * apply review comments
    
    * Remove unused function CreateNewNodes()
    
    * Add descriptive comment on node_idx variable in QuantileHistMaker::Builder::BuildHistsBatch()
    Optimizations of pre-processing for 'hist' tree method (#4310)
    
    * oprimizations for pre-processing
    
    * code cleaning
    
    * code cleaning
    
    * code cleaning after review
    
    * Apply suggestions from code review
    
    Co-Authored-By: SmirnovEgorRu <egor.smirnov@intel.com>
    Use Monitor in quantile hist. (#4273)
    more correct way to build node stats in distributed fast hist (#4140)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * more changes
    
    * temp
    
    * update
    
    * udpate rabit
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * broadcast subsampled feature correctly
    
    * init col
    
    * temp
    
    * col sampling
    
    * fix histmastrix init
    
    * fix col sampling
    
    * remove cout
    
    * fix out of bound access
    
    * fix core dump
    
    remove core dump file
    
    * update
    
    * add fid
    
    * update
    
    * revert some changes
    
    * temp
    
    * temp
    
    * pass all tests
    
    * bring back some tests
    
    * recover some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * recover column init part
    
    * more recovery
    
    * fix core dumps
    
    * code clean
    
    * revert some changes
    
    * fix test compilation issue
    
    * fix lint issue
    
    * resolve compilation issue
    
    * fix issues of lint caused by rebase
    
    * fix stylistic changes and change variable names
    
    * modularize depth width
    
    * address the comments
    
    * fix failed tests
    
    * wrap perf timers with class
    
    * temp
    
    * pass all lossguide
    
    * pass tests
    
    * add comments
    
    * more changes
    
    * use separate flow for single and tests
    
    * add test for lossguide hist
    
    * remove duplications
    
    * syncing stats for only once
    
    * recover more changes
    
    * recover more changes
    
    * fix root-stats
    
    * simplify code
    
    * remove outdated comments
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Separate Depthwidth and Lossguide growing policy in fast histogram (#4102)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * more changes
    
    * temp
    
    * update
    
    * udpate rabit
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * broadcast subsampled feature correctly
    
    * init col
    
    * temp
    
    * col sampling
    
    * fix histmastrix init
    
    * fix col sampling
    
    * remove cout
    
    * fix out of bound access
    
    * fix core dump
    
    remove core dump file
    
    * disbale test temporarily
    
    * update
    
    * add fid
    
    * print perf data
    
    * update
    
    * revert some changes
    
    * temp
    
    * temp
    
    * pass all tests
    
    * bring back some tests
    
    * recover some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * recover column init part
    
    * more recovery
    
    * fix core dumps
    
    * code clean
    
    * revert some changes
    
    * fix test compilation issue
    
    * fix lint issue
    
    * resolve compilation issue
    
    * fix issues of lint caused by rebase
    
    * fix stylistic changes and change variable names
    
    * use regtree internal function
    
    * modularize depth width
    
    * address the comments
    
    * fix failed tests
    
    * wrap perf timers with class
    
    * fix lint
    
    * fix num_leaves count
    
    * fix indention
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * merge
    
    * fix compilation
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Add categorical data support to GPU Hist. (#6164)
    Unify evaluation functions. (#6037)
    Make binary bin search reusable. (#6058)
    
    * Move binary search row to hist util.
    * Remove dead code.
    Unify max nodes. (#5497)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Pairwise ranking objective implementation on gpu (#4873)
    
    * - pairwise ranking objective implementation on gpu
       - there are couple of more algorithms (ndcg and map) for which support will be added
         as follow-up pr's
       - with no label groups defined, get gradient is 90x faster on gpu (120m instance
         mortgage dataset)
       - it can perform by an order of magnitude faster with ~ 10 groups (and adequate cores
         for the cpu implementation)
    
    * Add JSON config to rank obj.
    Clean up after removing `gpu_exact`. (#4777)
    
    * Removed unused functions.
    * Removed unused parameters.
    * Move ValueConstraints into constraints.cuh since it's now only used in GPU_Hist.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Retire DVec class in favour of c++20 style span for device memory. (#4293)
    Optimisations for gpu_hist. (#4248)
    
    * Optimisations for gpu_hist.
    
    * Use streams to overlap operations.
    
    * ColumnSampler now uses HostDeviceVector to prevent repeatedly copying feature vectors to the device.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
     Require leaf statistics when expanding tree (#4015)
    
    * Cache left and right gradient sums
    
    * Require leaf statistics when expanding tree
    Reduce tree expand boilerplate code (#4008)
    Single precision histograms on GPU (#3965)
    
    * Allow single precision histogram summation in gpu_hist
    
    * Add python test, reduce run-time of gpu_hist tests
    
    * Update documentation
    Add basic unittests for gpu-hist method. (#3785)
    
    * Split building histogram into separated class.
    * Extract `InitCompressedRow` definition.
    * Basic tests for gpu-hist.
    * Document the code more verbosely.
    * Removed `HistCutUnit`.
    * Removed some duplicated copies in `GPUHistMaker`.
    * Implement LCG and use it in tests.
    GPU memory usage fixes + column sampling refactor (#3635)
    
    * Remove thrust copy calls
    
    * Fix  histogram memory usage
    
    * Cap extreme histogram memory usage
    
    * More efficient column sampling
    
    * Use column sampler across updaters
    
    * More efficient split evaluation on GPU with column sampling
    Shared memory atomics while building histogram (#3384)
    
    * Use shared memory atomics for building histograms, whenever possible
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Fix several GPU bugs (#2916)
    
    * Fix #2905
    
    * Fix gpu_exact test failures
    
    * Fix bug in GPU prediction where multiple calls to batch prediction can produce incorrect results
    
    * Fix GPU documentation formatting
    Monotone constraints for gpu_hist (#2904)
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    Add warnings for large labels when using GPU histogram algorithms (#2834)
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    Integer gradient summation for GPU histogram algorithm. (#2681)
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    [GPU-Plugin] Major refactor (#2644)
    
    * Removal of redundant code/files.
    * Removal of exact namespace in GPU plugin
    * Revert double precision histograms to single precision for performance on Maxwell/Kepler
    Unify evaluation functions. (#6037)
    Feature weights (#5962)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Remove distcol updater. (#5507)
    
    Closes #5498.
    Requires setting leaf stat when expanding tree. (#5501)
    
    * Fix GPU Hist feature importance.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Remove unnecessary DMatrix methods (#5324)
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    Parameter validation (#5157)
    
    
    
    * Unused code.
    
    * Split up old colmaker parameters from train param.
    
    * Fix dart.
    
    * Better name.
    Remove dead code in colmaker. (#5105)
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    [Breaking] Remove num roots. (#5059)
    Don't use 0 for "fresh leaf". (#5084)
    
    
    * Allow using right child as marker for Exact tree_method.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Fix repeated split and 0 cover nodes (#5010)
    Fix external memory race in colmaker. (#4980)
    
    
    * Move `GetColDensity` out of omp parallel block.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Remove some unused functions as reported by cppcheck (#4743)
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Offload some configurations into GBM. (#4553)
    
    
    This is part 1 of refactoring configuration.
    
    * Move tree heuristic configurations.
    * Split up declarations and definitions for GBTree.
    * Implement UseGPU in gbm.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Get rid of a few trivial compiler warnings. (#4312)
    Optimisations for gpu_hist. (#4248)
    
    * Optimisations for gpu_hist.
    
    * Use streams to overlap operations.
    
    * ColumnSampler now uses HostDeviceVector to prevent repeatedly copying feature vectors to the device.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
     Require leaf statistics when expanding tree (#4015)
    
    * Cache left and right gradient sums
    
    * Require leaf statistics when expanding tree
    Reduce tree expand boilerplate code (#4008)
    Column sampling at individual nodes (splits). (#3971)
    
    * Column sampling at individual nodes (splits).
    
    * Documented colsample_bynode parameter.
    
    - also updated documentation for colsample_by* parameters
    
    * Updated documentation.
    
    * GetFeatureSet() returns shared pointer to std::vector.
    
    * Sync sampled columns across multiple processes.
    Remove leaf vector, add tree serialisation test, fix Windows tests (#3989)
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    GPU memory usage fixes + column sampling refactor (#3635)
    
    * Remove thrust copy calls
    
    * Fix  histogram memory usage
    
    * Cap extreme histogram memory usage
    
    * More efficient column sampling
    
    * Use column sampler across updaters
    
    * More efficient split evaluation on GPU with column sampling
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Remove accidental SparsePage copies (#3583)
    Add LASSO (#3429)
    
    * Allow multiple split constraints
    
    * Replace RidgePenalty with ElasticNet
    
    * Add test for checking Ridge, LASSO, and Elastic Net are implemented
    Refactor of FastHistMaker to allow for custom regularisation methods (#3335)
    
    * Refactor to allow for custom regularisation methods
    
    * Implement compositional SplitEvaluator framework
    
    * Fixed segfault when no monotone_constraints are supplied.
    
    * Change pid to parentID
    
    * test_monotone_constraints.py now passes
    
    * Refactor ColMaker and DistColMaker to use SplitEvaluator
    
    * Performance optimisation when no monotone_constraints specified
    
    * Fix linter messages
    
    * Fix a few more linter errors
    
    * Update the amalgamation
    
    * Add bounds check
    
    * Add check for leaf node
    
    * Fix linter error in param.h
    
    * Fix clang-tidy errors on CI
    
    * Fix incorrect function name
    
    * Fix clang-tidy error in updater_fast_hist.cc
    
    * Enable SSE2 for Win32 R MinGW
    
    Addresses https://github.com/dmlc/xgboost/pull/3335#issuecomment-400535752
    
    * Add contributor
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Monotone constraints for gpu_hist (#2904)
    Integer gradient summation for GPU histogram algorithm. (#2681)
    [GPU-Plugin] Multi-GPU gpu_id bug fixes for grow_gpu_hist and grow_gpu methods, and additional documentation for the gpu plugin. (#2463)
    Improve multi-threaded performance (#2104)
    
    * Add UpdatePredictionCache() option to updaters
    
    Some updaters (e.g. fast_hist) has enough information to quickly compute
    prediction cache for the training data. Each updater may override
    UpdaterPredictionCache() method to update the prediction cache. Note: this
    trick does not apply to validation data.
    
    * Respond to code review
    
    * Disable some debug messages by default
    * Document UpdatePredictionCache() interface
    * Remove base_margin logic from UpdatePredictionCache() implementation
    * Do not take pointer to cfg, as reference may get stale
    
    * Improve multi-threaded performance
    
    * Use columnwise accessor to accelerate ApplySplit() step,
      with support for a compressed representation
    * Parallel sort for evaluation step
    * Inline BuildHist() function
    * Cache gradient pairs when building histograms in BuildHist()
    
    * Add missing #if macro
    
    * Respond to code review
    
    * Use wrapper to enable parallel sort on Linux
    
    * Fix C++ compatibility issues
    
    * MSVC doesn't support unsigned in OpenMP loops
    * gcc 4.6 doesn't support using keyword
    
    * Fix lint issues
    
    * Respond to code review
    
    * Fix bug in ApplySplitSparseData()
    
    * Attempting to read beyond the end of a sparse column
    * Mishandling the case where an entire range of rows have missing values
    
    * Fix training continuation bug
    
    Disable UpdatePredictionCache() in the first iteration. This way, we can
    accomodate the scenario where we build off of an existing (nonempty) ensemble.
    
    * Add regression test for fast_hist
    
    * Respond to code review
    
    * Add back old version of ApplySplitSparseData
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Automatically remove nan from input data when it is sparse. (#2062)
    
    * [DATALoad] Automatically remove Nan when load from sparse matrix
    
    * add log
    Changing omp_get_num_threads to omp_get_max_threads (#1831)
    
    * Updating dmlc-core
    
    * Changing omp_get_num_threads to omp_get_max_threads
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [TREE] Experimental version of monotone constraint (#1516)
    
    * [TREE] Experimental version of monotone constraint
    
    * Allow default detection of montone option
    
    * loose the condition of strict check
    
    * Update gbtree.cc
    [TREE] Remove gap constraint, make tree construction more robust
    [MEM] Add rowset struct to save memory with billion level rows
    change the formula of fsplit value
    [LIBXGBOOST] pass demo running.
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [TREE] finish move of updater
    [TREE] Refactor colmaker
    [TREE] Move colmaker
    [REFACTOR] cleanup structure
    spelling, wording, and doc fixes in c++ code
    
    I was reading through the code and fixing some things in the comments.
    Only a few trivial actual code changes were made to make things more
    readable.
    lint half way
    add a indicator opt
    checkin some micro optimization
    minor shadow fix
    add small boundary checking
    larger boundary in edge case
    fix wrapper checkNAN
    fix solaris
    Avoid some Cran check error messages
    sorted base sketch maker
    add in sync
    ok, now work on update position
    middle version
    add bitmap .
    ok
    fix cxx98
    more ignore
    pass trival test
    check unity back
    complete refactor data.h, now replies on iterator to access column
    change omp loop var to bst_omp_uint, add XGB_DLL to wrapper
    clean up warnings from msvc
    a fixed version
    add cvgrad stats, simplify data
    tstats now depend on param
    refactor grad stats to be like visitor
    check in linear model
    chg root index to booster info, need review
    change row subsample to prob
    refresher test
    update tree maker to make it more robust
    fix num parallel tree
    first version that reproduce binary classification demo
    pass fmatrix as const
    mv code into src
    refactor config
    start unity refactor
    Unify evaluation functions. (#6037)
    Limit tree depth for GPU hist. (#6045)
    Add float32 histogram (#5624)
    
    * new single_precision_histogram param was added.
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Co-authored-by: fis <jm.yuan@outlook.com>
    Fix r interaction constraints (#5543)
    
    * Unify the parsing code.
    
    * Cleanup.
    Unify max nodes. (#5497)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Accept other gradient types for split entry. (#5467)
    Fix pruner. (#5335)
    
    
    * Honor the tree depth.
    * Prevent pruning pruned node.
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Parameter validation (#5157)
    
    
    
    * Unused code.
    
    * Split up old colmaker parameters from train param.
    
    * Fix dart.
    
    * Better name.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    Fix parsing empty vector in parameter. (#5087)
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Revert #4529 (#5008)
    
    * Revert " Optimize ‘hist’ for multi-core CPU (#4529)"
    
    This reverts commit 4d6590be3c9a043d44d9e4fe0a456a9f8179ec72.
    
    * Fix build
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Clean up after removing `gpu_exact`. (#4777)
    
    * Removed unused functions.
    * Removed unused parameters.
    * Move ValueConstraints into constraints.cuh since it's now only used in GPU_Hist.
     Optimize ‘hist’ for multi-core CPU (#4529)
    
    * Initial performance optimizations for xgboost
    
    * remove includes
    
    * revert float->double
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * Check existence of _mm_prefetch and __builtin_prefetch
    
    * Fix lint
    
    * optimizations for CPU
    
    * appling comments in review
    
    * add some comments, code refactoring
    
    * fixing issues in CI
    
    * adding runtime checks
    
    * remove 1 extra check
    
    * remove extra checks in BuildHist
    
    * remove checks
    
    * add debug info
    
    * added debug info
    
    * revert changes
    
    * added comments
    
    * Apply suggestions from code review
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * apply review comments
    
    * Remove unused function CreateNewNodes()
    
    * Add descriptive comment on node_idx variable in QuantileHistMaker::Builder::BuildHistsBatch()
    Feature interaction for GPU Hist. (#4534)
    
    * GPU hist Interaction Constraints.
    * Duplicate related parameters.
    * Add tests for CPU interaction constraint.
    * Add better error reporting.
    * Thorough tests.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
     Require leaf statistics when expanding tree (#4015)
    
    * Cache left and right gradient sums
    
    * Require leaf statistics when expanding tree
    Column sampling at individual nodes (splits). (#3971)
    
    * Column sampling at individual nodes (splits).
    
    * Documented colsample_bynode parameter.
    
    - also updated documentation for colsample_by* parameters
    
    * Updated documentation.
    
    * GetFeatureSet() returns shared pointer to std::vector.
    
    * Sync sampled columns across multiple processes.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Remove leaf vector, add tree serialisation test, fix Windows tests (#3989)
    Single precision histograms on GPU (#3965)
    
    * Allow single precision histogram summation in gpu_hist
    
    * Add python test, reduce run-time of gpu_hist tests
    
    * Update documentation
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Address #2754, accuracy issues with gpu_hist (#3793)
    
    * Address windows compilation error
    
    * Do not allow divide by zero in weight calculation
    
    * Update tests
    Remove NoConstraint. (#3792)
    [TREE] add interaction constraints (#3466)
    
    * add interaction constraints
    
    * enable both interaction and monotonic constraints at the same time
    
    * fix lint
    
    * add R test, fix lint, update demo
    
    * Use dmlc::JSONReader to express interaction constraints as nested lists; Use sparse arrays for bookkeeping
    
    * Add Python test for interaction constraints
    
    * make R interaction constraints parameter based on feature index instead of column names, fix R coding style
    
    * Fix lint
    
    * Add BlueTea88 to CONTRIBUTORS.md
    
    * Short circuit when no constraint is specified; address review comments
    
    * Add tutorial for feature interaction constraints
    
    * allow interaction constraints to be passed as string, remove redundant column_names argument
    
    * Fix typo
    
    * Address review comments
    
    * Add comments to Python test
    Added finding quantiles on GPU. (#3393)
    
    * Added finding quantiles on GPU.
    
    - this includes datasets where weights are assigned to data rows
    - as the quantiles found by the new algorithm are not the same
      as those found by the old one, test thresholds in
        tests/python-gpu/test_gpu_updaters.py have been adjusted.
    
    * Adjustments and improved testing for finding quantiles on the GPU.
    
    - added C++ tests for the DeviceSketch() function
    - reduced one of the thresholds in test_gpu_updaters.py
    - adjusted the cuts found by the find_cuts_k kernel
    Add LASSO (#3429)
    
    * Allow multiple split constraints
    
    * Replace RidgePenalty with ElasticNet
    
    * Add test for checking Ridge, LASSO, and Elastic Net are implemented
    Refactor of FastHistMaker to allow for custom regularisation methods (#3335)
    
    * Refactor to allow for custom regularisation methods
    
    * Implement compositional SplitEvaluator framework
    
    * Fixed segfault when no monotone_constraints are supplied.
    
    * Change pid to parentID
    
    * test_monotone_constraints.py now passes
    
    * Refactor ColMaker and DistColMaker to use SplitEvaluator
    
    * Performance optimisation when no monotone_constraints specified
    
    * Fix linter messages
    
    * Fix a few more linter errors
    
    * Update the amalgamation
    
    * Add bounds check
    
    * Add check for leaf node
    
    * Fix linter error in param.h
    
    * Fix clang-tidy errors on CI
    
    * Fix incorrect function name
    
    * Fix clang-tidy error in updater_fast_hist.cc
    
    * Enable SSE2 for Win32 R MinGW
    
    Addresses https://github.com/dmlc/xgboost/pull/3335#issuecomment-400535752
    
    * Add contributor
    GPU binning and compression. (#3319)
    
    * GPU binning and compression.
    
    - binning and index compression are done inside the DeviceShard constructor
    - in case of a DMatrix with multiple row batches, it is first converted into a single row batch
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    'hist': Montonic Constraints (#3085)
    
    * Extended monotonic constraints support to 'hist' tree method.
    
    * Added monotonic constraints tests.
    
    * Fix the signature of NoConstraint::CalcSplitGain()
    
    * Document monotonic constraint support in 'hist'
    
    * Update signature of Update to account for latest refactor
    Fix several GPU bugs (#2916)
    
    * Fix #2905
    
    * Fix gpu_exact test failures
    
    * Fix bug in GPU prediction where multiple calls to batch prediction can produce incorrect results
    
    * Fix GPU documentation formatting
    Monotone constraints for gpu_hist (#2904)
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    Integer gradient summation for GPU histogram algorithm. (#2681)
    Patch to improve multithreaded performance scaling (#2493)
    
    * Patch to improve multithreaded performance scaling
    
    Change parallel strategy for histogram construction.
    Instead of partitioning data rows among multiple threads, partition feature
    columns instead. Useful heuristics for assigning partitions have been adopted
    from LightGBM project.
    
    * Add missing header to satisfy MSVC
    
    * Restore max_bin and related parameters to TrainParam
    
    * Fix lint error
    
    * inline functions do not require static keyword
    
    * Feature grouping algorithm accepting FastHistParam
    
    Feature grouping algorithm accepts many parameters (3+), and it gets annoying to
    pass them one by one. Instead, simply pass the reference to FastHistParam. The
    definition of FastHistParam has been moved to a separate header file to
    accomodate this change.
    [GPU-Plugin] Unify gpu_gpair/bst_gpair. Refactor. (#2477)
    [GPU-Plugin] Make node_idx type 32 bit for hist algo. Set default n_gpus to 1. (#2445)
    [GPU-Plugin] Multi-GPU for grow_gpu_hist histogram method using NVIDIA NCCL. (#2395)
    [GPU-Plugin] (#2227)
    
    * Add fast histogram algorithm
    * Fix Linux build
    * Add 'gpu_id' parameter
    Improve multi-threaded performance (#2104)
    
    * Add UpdatePredictionCache() option to updaters
    
    Some updaters (e.g. fast_hist) has enough information to quickly compute
    prediction cache for the training data. Each updater may override
    UpdaterPredictionCache() method to update the prediction cache. Note: this
    trick does not apply to validation data.
    
    * Respond to code review
    
    * Disable some debug messages by default
    * Document UpdatePredictionCache() interface
    * Remove base_margin logic from UpdatePredictionCache() implementation
    * Do not take pointer to cfg, as reference may get stale
    
    * Improve multi-threaded performance
    
    * Use columnwise accessor to accelerate ApplySplit() step,
      with support for a compressed representation
    * Parallel sort for evaluation step
    * Inline BuildHist() function
    * Cache gradient pairs when building histograms in BuildHist()
    
    * Add missing #if macro
    
    * Respond to code review
    
    * Use wrapper to enable parallel sort on Linux
    
    * Fix C++ compatibility issues
    
    * MSVC doesn't support unsigned in OpenMP loops
    * gcc 4.6 doesn't support using keyword
    
    * Fix lint issues
    
    * Respond to code review
    
    * Fix bug in ApplySplitSparseData()
    
    * Attempting to read beyond the end of a sparse column
    * Mishandling the case where an entire range of rows have missing values
    
    * Fix training continuation bug
    
    Disable UpdatePredictionCache() in the first iteration. This way, we can
    accomodate the scenario where we build off of an existing (nonempty) ensemble.
    
    * Add regression test for fast_hist
    
    * Respond to code review
    
    * Add back old version of ApplySplitSparseData
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Rename parameter in fast_hist to disambiguate (#1962)
    Histogram Optimized Tree Grower (#1940)
    
    * Support histogram-based algorithm + multiple tree growing strategy
    
    * Add a brand new updater to support histogram-based algorithm, which buckets
      continuous features into discrete bins to speed up training. To use it, set
      `tree_method = fast_hist` to configuration.
    * Support multiple tree growing strategies. For now, two policies are supported:
      * `grow_policy=depthwise` (default):  favor splitting at nodes closest to the
        root, i.e. grow depth-wise.
      * `grow_policy=lossguide`: favor splitting at nodes with highest loss change
    * Improve single-threaded performance
      * Unroll critical loops
      * Introduce specialized code for dense data (i.e. no missing values)
    * Additional training parameters: `max_leaves`, `max_bin`, `grow_policy`, `verbose`
    
    * Adding a small test for hist method
    
    * Fix memory error in row_set.h
    
    When std::vector is resized, a reference to one of its element may become
    stale. Any such reference must be updated as well.
    
    * Resolve cross-platform compilation issues
    
    * Versions of g++ older than 4.8 lacks support for a few C++11 features, e.g.
      alignas(*) and new initializer syntax. To support g++ 4.6, use pre-C++11
      initializer and remove alignas(*).
    * Versions of MSVC older than 2015 does not support alignas(*). To support
      MSVC 2012, remove alignas(*).
    * For g++ 4.8 and newer, alignas(*) is enabled for performance benefits.
    * Some old compilers (MSVC 2012, g++ 4.6) do not support template aliases
      (which uses `using` to declate type aliases). So always use `typedef`.
    
    * Fix a host of CI issues
    
    * Remove dependency for libz on osx
    * Fix heading for hist_util
    * Fix minor style issues
    * Add missing #include
    * Remove extraneous logging
    
    * Enable tree_method=hist in R
    
    * Renaming HistMaker to GHistBuilder to avoid confusion
    
    * Fix R integration
    
    * Respond to style comments
    
    * Consistent tie-breaking for priority queue using timestamps
    
    * Last-minute style fixes
    
    * Fix issuecomment-271977647
    
    The way we quantize data is broken. The agaricus data consists of all
    categorical values. When NAs are converted into 0's,
    `HistCutMatrix::Init` assign both 0's and 1's to the same single bin.
    
    Why? gmat only the smallest value (0) and an upper bound (2), which is twice
    the maximum value (1). Add the maximum value itself to gmat to fix the issue.
    
    * Fix issuecomment-272266358
    
    * Remove padding from cut values for the continuous case
    * For categorical/ordinal values, use midpoints as bin boundaries to be safe
    
    * Fix CI issue -- do not use xrange(*)
    
    * Fix corner case in quantile sketch
    
    Signed-off-by: Philip Cho <chohyu01@cs.washington.edu>
    
    * Adding a test for an edge case in quantile sketcher
    
    max_bin=2 used to cause an exception.
    
    * Fix fast_hist test
    
    The test used to require a strictly increasing Test AUC for all examples.
    One of them exhibits a small blip in Test AUC before achieving a Test AUC
    of 1. (See bottom.)
    
    Solution: do not require monotonic increase for this particular example.
    
    [0] train-auc:0.99989 test-auc:0.999497
    [1] train-auc:1 test-auc:0.999749
    [2] train-auc:1 test-auc:0.999749
    [3] train-auc:1 test-auc:0.999749
    [4] train-auc:1 test-auc:0.999749
    [5] train-auc:1 test-auc:0.999497
    [6] train-auc:1 test-auc:1
    [7] train-auc:1 test-auc:1
    [8] train-auc:1 test-auc:1
    [9] train-auc:1 test-auc:1
    [CORE] The update process for a tree model, and its application to feature importance (#1670)
    
    * [CORE] allow updating trees in an existing model
    
    * [CORE] in refresh updater, allow keeping old leaf values and update stats only
    
    * [R-package] xgb.train mod to allow updating trees in an existing model
    
    * [R-package] added check for nrounds when is_update
    
    * [CORE] merge parameter declaration changes; unify their code style
    
    * [CORE] move the update-process trees initialization to Configure; rename default process_type to 'default'; fix the trees and trees_to_update sizes comparison check
    
    * [R-package] unit tests for the update process type
    
    * [DOC] documentation for process_type parameter; improved docs for updater, Gamma and Tweedie; added some parameter aliases; metrics indentation and some were non-documented
    
    * fix my sloppy merge conflict resolutions
    
    * [CORE] add a TreeProcessType enum
    
    * whitespace fix
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Update build instructions,  improve memory usage (#1811)
    Fixing a few typos (#1771)
    
    * Fixing a few typos
    
    * Fixing a few typos
    Add GPU accelerated tree construction plugin (#1679)
    [TREE] Experimental version of monotone constraint (#1516)
    
    * [TREE] Experimental version of monotone constraint
    
    * Allow default detection of montone option
    
    * loose the condition of strict check
    
    * Update gbtree.cc
    no exception throwing within omp parallel; set nthread in Learner (#1421)
    Fix ambiguous call to abs(c or c++). (#1308)
    [APPROX] Make global proposal default, add group ptr solution
    [LIBXGBOOST] pass demo running.
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [TREE] finish move of updater
    [TREE] Move colmaker
    [REFACTOR] cleanup structure
    spelling, wording, and doc fixes in c++ code
    
    I was reading through the code and fixing some things in the comments.
    Only a few trivial actual code changes were made to make things more
    readable.
    Update param.h
    
     enforce parallel option to 0 for now for stable result
    lint half way
    add a indicator opt
    auto turn on optimization
    checkin some micro optimization
    add max_delta_step
    Update param.h
    update with new rabit api
    potential BUG in skmaker?
    checkin skmaker
    a version that compile
    first ver
    refresher is now distributed
    middle version
    ok
    remove using std from cpp
    fix new warning
    fix param.h
    fix som solaris
    check unity back
    change things back
    clean up warnings from msvc
    a fixed version
    add cvgrad stats, simplify data
    tstats now depend on param
    refactor grad stats to be like visitor
    mv code into src
    start unity refactor
    Add categorical data support to GPU Hist. (#6164)
    Unify evaluation functions. (#6037)
    Refactor gpu_hist split evaluation (#5610)
    
    * Refactor
    
    * Rewrite evaluate splits
    
    * Add more tests
    Split Features into Groups to Compute Histograms in Shared Memory (#5795)
    Unify evaluation functions. (#6037)
    Reduce device synchronisation (#5631)
    
    * Reduce device synchronisation
    
    * Initialise pinned memory
    Make binary bin search reusable. (#6058)
    
    * Move binary search row to hist util.
    * Remove dead code.
    Split Features into Groups to Compute Histograms in Shared Memory (#5795)
    For histograms, opting into maximum shared memory available per block. (#5491)
    Partial rewrite EllpackPage (#5352)
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Add categorical data support to GPU Hist. (#6164)
    Unify evaluation functions. (#6037)
    Fix evaluate root split. (#5948)
    Refactor gpu_hist split evaluation (#5610)
    
    * Refactor
    
    * Rewrite evaluate splits
    
    * Add more tests
    Unify evaluation functions. (#6037)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    gpu_hist performance tweaks (#5707)
    
    * Remove device vectors
    
    * Remove allreduce synchronize
    
    * Remove double buffer
    Use non-synchronising scan (#5560)
    Use thrust functions instead of custom functions (#5544)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Some comments for row partitioner. (#4832)
    Refactor out row partitioning logic from gpu_hist, introduce caching device vectors (#4554)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Enhance nvtx support. (#5636)
    Purge device_helpers.cuh (#5534)
    
    * Simplifications with caching_device_vector
    
    * Purge device helpers
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Partial rewrite EllpackPage (#5352)
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Purge device_helpers.cuh (#5534)
    
    * Simplifications with caching_device_vector
    
    * Purge device helpers
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Unify evaluation functions. (#6037)
    gpu_hist performance tweaks (#5707)
    
    * Remove device vectors
    
    * Remove allreduce synchronize
    
    * Remove double buffer
    Reduce device synchronisation (#5631)
    
    * Reduce device synchronisation
    
    * Initialise pinned memory
    gpu_hist performance fixes (#5558)
    
    * Remove unnecessary cuda API calls
    
    * Fix histogram memory growth
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Some comments for row partitioner. (#4832)
    Refactor out row partitioning logic from gpu_hist, introduce caching device vectors (#4554)
    Loop over `thrust::reduce`. (#6229)
    
    * Check input chunk size of dqdm.
    * Add doc for current limitation.
    Make binary bin search reusable. (#6058)
    
    * Move binary search row to hist util.
    * Remove dead code.
    Use `cudaOccupancyMaxPotentialBlockSize` to calculate the block size. (#5926)
    Split Features into Groups to Compute Histograms in Shared Memory (#5795)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    For histograms, opting into maximum shared memory available per block. (#5491)
    Partial rewrite EllpackPage (#5352)
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Split Features into Groups to Compute Histograms in Shared Memory (#5795)
    Unify set index data. (#6062)
    Revert "Remove warning about memset. (#6003)" (#6020)
    
    This reverts commit 12e3fb6a6cb601c58d39ed54253e6e50f1513ccc.
    Remove warning about memset. (#6003)
    Unify CPU hist sketching (#5880)
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Implement weighted sketching for adapter. (#5760)
    
    
    * Bounded memory tests.
    * Fixed memory estimation.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Add float32 histogram (#5624)
    
    * new single_precision_histogram param was added.
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Co-authored-by: fis <jm.yuan@outlook.com>
    Port R compatibility patches from 1.0.0 release branch (#5577)
    
    * Don't use memset to set struct when compiling for R
    
    * Support 32-bit Solaris target for R package
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Reducing memory consumption for 'hist' method on CPU (#5334)
    Partial rewrite EllpackPage (#5352)
    Optimized ApplySplit, BuildHist and UpdatePredictCache functions on CPU (#5244)
    
    * Split up sparse and dense build hist kernels.
    * Add `PartitionBuilder`.
    Wide dataset quantile performance improvement (#5306)
    Testing hist_util (#5251)
    
    * Rank tests
    
    * Remove categorical split specialisation
    
    * Extend tests to multiple features, switch to WQSketch
    
    * Add tests for SparseCuts
    
    * Add external memory quantile tests, fix some existing tests
    Optimized BuildHist function (#5156)
    Use adapters for SparsePageDMatrix (#5092)
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    fix: reset hit counter for next batch (#5035)
    Revert #4529 (#5008)
    
    * Revert " Optimize ‘hist’ for multi-core CPU (#4529)"
    
    This reverts commit 4d6590be3c9a043d44d9e4fe0a456a9f8179ec72.
    
    * Fix build
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Use configure_file() to configure version only (#4974)
    
    * Avoid writing build_config.h
    
    * Remove build_config.h all together.
    
    * Lint.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    [HOTFIX] distributed training with hist method  (#4716)
    
    * add parallel test for hist.EvalualiteSplit
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * fix OMP schedule policy
    
    * fix clang-tidy
    
    * add logging: total_num_bins
    
    * fix
    
    * fix
    
    * test
    
    * replace guided OPENMP policy with static in updater_quantile_hist.cc
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
     Optimize ‘hist’ for multi-core CPU (#4529)
    
    * Initial performance optimizations for xgboost
    
    * remove includes
    
    * revert float->double
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * Check existence of _mm_prefetch and __builtin_prefetch
    
    * Fix lint
    
    * optimizations for CPU
    
    * appling comments in review
    
    * add some comments, code refactoring
    
    * fixing issues in CI
    
    * adding runtime checks
    
    * remove 1 extra check
    
    * remove extra checks in BuildHist
    
    * remove checks
    
    * add debug info
    
    * added debug info
    
    * revert changes
    
    * added comments
    
    * Apply suggestions from code review
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * apply review comments
    
    * Remove unused function CreateNewNodes()
    
    * Add descriptive comment on node_idx variable in QuantileHistMaker::Builder::BuildHistsBatch()
    - fix issues with training with external memory on cpu (#4487)
    
    * - fix issues with training with external memory on cpu
       - use the batch size to determine the correct number of rows in a batch
       - use the right number of threads in omp parallalization if the batch size
         is less than the default omp max threads (applicable for the last batch)
    
    * - handle scenarios where last batch size is < available number of threads
    - augment tests such that we can test all scenarios (batch size <, >, = number of threads)
    Revert hist init optimization. (#4502)
    Optimizations of pre-processing for 'hist' tree method (#4310)
    
    * oprimizations for pre-processing
    
    * code cleaning
    
    * code cleaning
    
    * code cleaning after review
    
    * Apply suggestions from code review
    
    Co-Authored-By: SmirnovEgorRu <egor.smirnov@intel.com>
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Prevent empty quantiles in fast hist (#4155)
    
    * Prevent empty quantiles
    
    * Revise and improve unit tests for quantile hist
    
    * Remove unnecessary comment
    
    * Add #2943 as a test case
    
    * Skip test if no sklearn
    
    * Revise misleading comments
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Make `HistCutMatrix::Init' be aware of groups. (#4115)
    
    
    * Add checks for group size.
    * Simple docs.
    * Search group index during hist cut matrix initialization.
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Separate Depthwidth and Lossguide growing policy in fast histogram (#4102)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * more changes
    
    * temp
    
    * update
    
    * udpate rabit
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * broadcast subsampled feature correctly
    
    * init col
    
    * temp
    
    * col sampling
    
    * fix histmastrix init
    
    * fix col sampling
    
    * remove cout
    
    * fix out of bound access
    
    * fix core dump
    
    remove core dump file
    
    * disbale test temporarily
    
    * update
    
    * add fid
    
    * print perf data
    
    * update
    
    * revert some changes
    
    * temp
    
    * temp
    
    * pass all tests
    
    * bring back some tests
    
    * recover some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * recover column init part
    
    * more recovery
    
    * fix core dumps
    
    * code clean
    
    * revert some changes
    
    * fix test compilation issue
    
    * fix lint issue
    
    * resolve compilation issue
    
    * fix issues of lint caused by rebase
    
    * fix stylistic changes and change variable names
    
    * use regtree internal function
    
    * modularize depth width
    
    * address the comments
    
    * fix failed tests
    
    * wrap perf timers with class
    
    * fix lint
    
    * fix num_leaves count
    
    * fix indention
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * merge
    
    * fix compilation
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    Performance optimizations for Intel CPUs (#3957)
    
    * Initial performance optimizations for xgboost
    
    * remove includes
    
    * revert float->double
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * Check existence of _mm_prefetch and __builtin_prefetch
    
    * Fix lint
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Implement devices to devices reshard. (#3721)
    
    * Force clearing device memory before Reshard.
    * Remove calculating row_segments for gpu_hist and gpu_sketch.
    * Guard against changing device.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    GPU memory usage fixes + column sampling refactor (#3635)
    
    * Remove thrust copy calls
    
    * Fix  histogram memory usage
    
    * Cap extreme histogram memory usage
    
    * More efficient column sampling
    
    * Use column sampler across updaters
    
    * More efficient split evaluation on GPU with column sampling
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Remove accidental SparsePage copies (#3583)
    Refactor parts of fast histogram utilities (#3564)
    
    * Refactor parts of fast histogram utilities
    
    * Removed byte packing from column matrix
    Added finding quantiles on GPU. (#3393)
    
    * Added finding quantiles on GPU.
    
    - this includes datasets where weights are assigned to data rows
    - as the quantiles found by the new algorithm are not the same
      as those found by the old one, test thresholds in
        tests/python-gpu/test_gpu_updaters.py have been adjusted.
    
    * Adjustments and improved testing for finding quantiles on the GPU.
    
    - added C++ tests for the DeviceSketch() function
    - reduced one of the thresholds in test_gpu_updaters.py
    - adjusted the cuts found by the find_cuts_k kernel
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Fix incorrect minimum value in quantile generation (#3167)
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    [GPU-Plugin] Fix gpu_hist to allow matrices with more than just 2^{32} elements. Also fixed CPU hist algorithm. (#2518)
    Patch to improve multithreaded performance scaling (#2493)
    
    * Patch to improve multithreaded performance scaling
    
    Change parallel strategy for histogram construction.
    Instead of partitioning data rows among multiple threads, partition feature
    columns instead. Useful heuristics for assigning partitions have been adopted
    from LightGBM project.
    
    * Add missing header to satisfy MSVC
    
    * Restore max_bin and related parameters to TrainParam
    
    * Fix lint error
    
    * inline functions do not require static keyword
    
    * Feature grouping algorithm accepting FastHistParam
    
    Feature grouping algorithm accepts many parameters (3+), and it gets annoying to
    pass them one by one. Instead, simply pass the reference to FastHistParam. The
    definition of FastHistParam has been moved to a separate header file to
    accomodate this change.
    Fix performance degradation of BuildHist on Windows (#2243)
    
    Reported in issue #2165. Dynamic scheduling of OpenMP loops involve
    implicit synchronization. To implement synchronization, libgomp uses futex
    (fast userspace mutex), whereas MinGW uses kernel-space mutex, which is more
    costly. With chunk size of 1, synchronization overhead may become prohibitive
    on Windows machines.
    
    Solution: use 'guided' schedule to minimize the number of syncs
    Improve multi-threaded performance (#2104)
    
    * Add UpdatePredictionCache() option to updaters
    
    Some updaters (e.g. fast_hist) has enough information to quickly compute
    prediction cache for the training data. Each updater may override
    UpdaterPredictionCache() method to update the prediction cache. Note: this
    trick does not apply to validation data.
    
    * Respond to code review
    
    * Disable some debug messages by default
    * Document UpdatePredictionCache() interface
    * Remove base_margin logic from UpdatePredictionCache() implementation
    * Do not take pointer to cfg, as reference may get stale
    
    * Improve multi-threaded performance
    
    * Use columnwise accessor to accelerate ApplySplit() step,
      with support for a compressed representation
    * Parallel sort for evaluation step
    * Inline BuildHist() function
    * Cache gradient pairs when building histograms in BuildHist()
    
    * Add missing #if macro
    
    * Respond to code review
    
    * Use wrapper to enable parallel sort on Linux
    
    * Fix C++ compatibility issues
    
    * MSVC doesn't support unsigned in OpenMP loops
    * gcc 4.6 doesn't support using keyword
    
    * Fix lint issues
    
    * Respond to code review
    
    * Fix bug in ApplySplitSparseData()
    
    * Attempting to read beyond the end of a sparse column
    * Mishandling the case where an entire range of rows have missing values
    
    * Fix training continuation bug
    
    Disable UpdatePredictionCache() in the first iteration. This way, we can
    accomodate the scenario where we build off of an existing (nonempty) ensemble.
    
    * Add regression test for fast_hist
    
    * Respond to code review
    
    * Add back old version of ApplySplitSparseData
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Histogram Optimized Tree Grower (#1940)
    
    * Support histogram-based algorithm + multiple tree growing strategy
    
    * Add a brand new updater to support histogram-based algorithm, which buckets
      continuous features into discrete bins to speed up training. To use it, set
      `tree_method = fast_hist` to configuration.
    * Support multiple tree growing strategies. For now, two policies are supported:
      * `grow_policy=depthwise` (default):  favor splitting at nodes closest to the
        root, i.e. grow depth-wise.
      * `grow_policy=lossguide`: favor splitting at nodes with highest loss change
    * Improve single-threaded performance
      * Unroll critical loops
      * Introduce specialized code for dense data (i.e. no missing values)
    * Additional training parameters: `max_leaves`, `max_bin`, `grow_policy`, `verbose`
    
    * Adding a small test for hist method
    
    * Fix memory error in row_set.h
    
    When std::vector is resized, a reference to one of its element may become
    stale. Any such reference must be updated as well.
    
    * Resolve cross-platform compilation issues
    
    * Versions of g++ older than 4.8 lacks support for a few C++11 features, e.g.
      alignas(*) and new initializer syntax. To support g++ 4.6, use pre-C++11
      initializer and remove alignas(*).
    * Versions of MSVC older than 2015 does not support alignas(*). To support
      MSVC 2012, remove alignas(*).
    * For g++ 4.8 and newer, alignas(*) is enabled for performance benefits.
    * Some old compilers (MSVC 2012, g++ 4.6) do not support template aliases
      (which uses `using` to declate type aliases). So always use `typedef`.
    
    * Fix a host of CI issues
    
    * Remove dependency for libz on osx
    * Fix heading for hist_util
    * Fix minor style issues
    * Add missing #include
    * Remove extraneous logging
    
    * Enable tree_method=hist in R
    
    * Renaming HistMaker to GHistBuilder to avoid confusion
    
    * Fix R integration
    
    * Respond to style comments
    
    * Consistent tie-breaking for priority queue using timestamps
    
    * Last-minute style fixes
    
    * Fix issuecomment-271977647
    
    The way we quantize data is broken. The agaricus data consists of all
    categorical values. When NAs are converted into 0's,
    `HistCutMatrix::Init` assign both 0's and 1's to the same single bin.
    
    Why? gmat only the smallest value (0) and an upper bound (2), which is twice
    the maximum value (1). Add the maximum value itself to gmat to fix the issue.
    
    * Fix issuecomment-272266358
    
    * Remove padding from cut values for the continuous case
    * For categorical/ordinal values, use midpoints as bin boundaries to be safe
    
    * Fix CI issue -- do not use xrange(*)
    
    * Fix corner case in quantile sketch
    
    Signed-off-by: Philip Cho <chohyu01@cs.washington.edu>
    
    * Adding a test for an edge case in quantile sketcher
    
    max_bin=2 used to cause an exception.
    
    * Fix fast_hist test
    
    The test used to require a strictly increasing Test AUC for all examples.
    One of them exhibits a small blip in Test AUC before achieving a Test AUC
    of 1. (See bottom.)
    
    Solution: do not require monotonic increase for this particular example.
    
    [0] train-auc:0.99989 test-auc:0.999497
    [1] train-auc:1 test-auc:0.999749
    [2] train-auc:1 test-auc:0.999749
    [3] train-auc:1 test-auc:0.999749
    [4] train-auc:1 test-auc:0.999749
    [5] train-auc:1 test-auc:0.999497
    [6] train-auc:1 test-auc:1
    [7] train-auc:1 test-auc:1
    [8] train-auc:1 test-auc:1
    [9] train-auc:1 test-auc:1
    Feature weights (#5962)
    Introducing DPC++-based plugin (predictor, objective function) supporting oneAPI programming model (#5825)
    
    * Added plugin with DPC++-based predictor and objective function
    
    * Update CMakeLists.txt
    
    * Update regression_obj_oneapi.cc
    
    * Added README.md for OneAPI plugin
    
    * Added OneAPI predictor support to gbtree
    
    * Update README.md
    
    * Merged kernels in gradient computation. Enabled multiple loss functions with DPC++ backend
    
    * Aligned plugin CMake files with latest master changes. Fixed whitespace typos
    
    * Removed debug output
    
    * [CI] Make oneapi_plugin a CMake target
    
    * Added tests for OneAPI plugin for predictor and obj. functions
    
    * Temporarily switched to default selector for device dispacthing in OneAPI plugin to enable execution in environments without gpus
    
    * Updated readme file.
    
    * Fixed USM usage in predictor
    
    * Removed workaround with explicit templated names for DPC++ kernels
    
    * Fixed warnings in plugin tests
    
    * Fix CMake build of gtest
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Prevent copying SimpleDMatrix. (#5453)
    
    * Set default dtor for SimpleDMatrix to initialize default copy ctor, which is
    deleted due to unique ptr.
    
    * Remove commented code.
    * Remove warning for calling host function (std::max).
    * Remove warning for initialization order.
    * Remove warning for unused variables.
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Enable running objectives with 0 GPU. (#3878)
    
    * Enable running objectives with 0 GPU.
    
    * Enable 0 GPU for objectives.
    * Add doc for GPU objectives.
    * Fix some objectives defaulted to running on all GPUs.
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Fix gpu devices. (#3693)
    
    * Fix gpu_set normalized and unnormalized.
    * Fix DeviceSpan.
    Merge generic device helper functions into gpu set. (#3626)
    
    * Remove the use of old NDevices* functions.
    * Use GPUSet in timer.h.
    Fix continue training in CLI
    [DISK] Add shard option to disk
    Optimize cpu sketch allreduce for sparse data. (#6009)
    
    * Bypass RABIT serialization reducer and use custom allgather based merging.
    Unify CPU hist sketching (#5880)
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Catch dmlc::Error. (#3751)
    
    Fix #3643.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Implement devices to devices reshard. (#3721)
    
    * Force clearing device memory before Reshard.
    * Remove calculating row_segments for gpu_hist and gpu_sketch.
    * Guard against changing device.
    Fix gpu devices. (#3693)
    
    * Fix gpu_set normalized and unnormalized.
    * Fix DeviceSpan.
    Unify evaluation functions. (#6037)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Optimized ApplySplit, BuildHist and UpdatePredictCache functions on CPU (#5244)
    
    * Split up sparse and dense build hist kernels.
    * Add `PartitionBuilder`.
    Revert #4529 (#5008)
    
    * Revert " Optimize ‘hist’ for multi-core CPU (#4529)"
    
    This reverts commit 4d6590be3c9a043d44d9e4fe0a456a9f8179ec72.
    
    * Fix build
     Optimize ‘hist’ for multi-core CPU (#4529)
    
    * Initial performance optimizations for xgboost
    
    * remove includes
    
    * revert float->double
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * Check existence of _mm_prefetch and __builtin_prefetch
    
    * Fix lint
    
    * optimizations for CPU
    
    * appling comments in review
    
    * add some comments, code refactoring
    
    * fixing issues in CI
    
    * adding runtime checks
    
    * remove 1 extra check
    
    * remove extra checks in BuildHist
    
    * remove checks
    
    * add debug info
    
    * added debug info
    
    * revert changes
    
    * added comments
    
    * Apply suggestions from code review
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * apply review comments
    
    * Remove unused function CreateNewNodes()
    
    * Add descriptive comment on node_idx variable in QuantileHistMaker::Builder::BuildHistsBatch()
    Optimizations of pre-processing for 'hist' tree method (#4310)
    
    * oprimizations for pre-processing
    
    * code cleaning
    
    * code cleaning
    
    * code cleaning after review
    
    * Apply suggestions from code review
    
    Co-Authored-By: SmirnovEgorRu <egor.smirnov@intel.com>
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    Fix issue #2800 (#2817)
    
    Problem:
    Fast histogram updater crashes whenever subsampling picks zero rows
    
    Diagnosis:
    Row set data structure uses "nullptr" internally to indicate a non-existent
    row set. Since you cannot take the address of the first element of an empty
    vector, a valid row set ends up getting "nullptr" as well.
    
    Fix:
    Use an arbitrary value (not equal to "nullptr") to bypass nullptr check.
    [GPU-Plugin] Fix gpu_hist to allow matrices with more than just 2^{32} elements. Also fixed CPU hist algorithm. (#2518)
    Improve multi-threaded performance (#2104)
    
    * Add UpdatePredictionCache() option to updaters
    
    Some updaters (e.g. fast_hist) has enough information to quickly compute
    prediction cache for the training data. Each updater may override
    UpdaterPredictionCache() method to update the prediction cache. Note: this
    trick does not apply to validation data.
    
    * Respond to code review
    
    * Disable some debug messages by default
    * Document UpdatePredictionCache() interface
    * Remove base_margin logic from UpdatePredictionCache() implementation
    * Do not take pointer to cfg, as reference may get stale
    
    * Improve multi-threaded performance
    
    * Use columnwise accessor to accelerate ApplySplit() step,
      with support for a compressed representation
    * Parallel sort for evaluation step
    * Inline BuildHist() function
    * Cache gradient pairs when building histograms in BuildHist()
    
    * Add missing #if macro
    
    * Respond to code review
    
    * Use wrapper to enable parallel sort on Linux
    
    * Fix C++ compatibility issues
    
    * MSVC doesn't support unsigned in OpenMP loops
    * gcc 4.6 doesn't support using keyword
    
    * Fix lint issues
    
    * Respond to code review
    
    * Fix bug in ApplySplitSparseData()
    
    * Attempting to read beyond the end of a sparse column
    * Mishandling the case where an entire range of rows have missing values
    
    * Fix training continuation bug
    
    Disable UpdatePredictionCache() in the first iteration. This way, we can
    accomodate the scenario where we build off of an existing (nonempty) ensemble.
    
    * Add regression test for fast_hist
    
    * Respond to code review
    
    * Add back old version of ApplySplitSparseData
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Histogram Optimized Tree Grower (#1940)
    
    * Support histogram-based algorithm + multiple tree growing strategy
    
    * Add a brand new updater to support histogram-based algorithm, which buckets
      continuous features into discrete bins to speed up training. To use it, set
      `tree_method = fast_hist` to configuration.
    * Support multiple tree growing strategies. For now, two policies are supported:
      * `grow_policy=depthwise` (default):  favor splitting at nodes closest to the
        root, i.e. grow depth-wise.
      * `grow_policy=lossguide`: favor splitting at nodes with highest loss change
    * Improve single-threaded performance
      * Unroll critical loops
      * Introduce specialized code for dense data (i.e. no missing values)
    * Additional training parameters: `max_leaves`, `max_bin`, `grow_policy`, `verbose`
    
    * Adding a small test for hist method
    
    * Fix memory error in row_set.h
    
    When std::vector is resized, a reference to one of its element may become
    stale. Any such reference must be updated as well.
    
    * Resolve cross-platform compilation issues
    
    * Versions of g++ older than 4.8 lacks support for a few C++11 features, e.g.
      alignas(*) and new initializer syntax. To support g++ 4.6, use pre-C++11
      initializer and remove alignas(*).
    * Versions of MSVC older than 2015 does not support alignas(*). To support
      MSVC 2012, remove alignas(*).
    * For g++ 4.8 and newer, alignas(*) is enabled for performance benefits.
    * Some old compilers (MSVC 2012, g++ 4.6) do not support template aliases
      (which uses `using` to declate type aliases). So always use `typedef`.
    
    * Fix a host of CI issues
    
    * Remove dependency for libz on osx
    * Fix heading for hist_util
    * Fix minor style issues
    * Add missing #include
    * Remove extraneous logging
    
    * Enable tree_method=hist in R
    
    * Renaming HistMaker to GHistBuilder to avoid confusion
    
    * Fix R integration
    
    * Respond to style comments
    
    * Consistent tie-breaking for priority queue using timestamps
    
    * Last-minute style fixes
    
    * Fix issuecomment-271977647
    
    The way we quantize data is broken. The agaricus data consists of all
    categorical values. When NAs are converted into 0's,
    `HistCutMatrix::Init` assign both 0's and 1's to the same single bin.
    
    Why? gmat only the smallest value (0) and an upper bound (2), which is twice
    the maximum value (1). Add the maximum value itself to gmat to fix the issue.
    
    * Fix issuecomment-272266358
    
    * Remove padding from cut values for the continuous case
    * For categorical/ordinal values, use midpoints as bin boundaries to be safe
    
    * Fix CI issue -- do not use xrange(*)
    
    * Fix corner case in quantile sketch
    
    Signed-off-by: Philip Cho <chohyu01@cs.washington.edu>
    
    * Adding a test for an edge case in quantile sketcher
    
    max_bin=2 used to cause an exception.
    
    * Fix fast_hist test
    
    The test used to require a strictly increasing Test AUC for all examples.
    One of them exhibits a small blip in Test AUC before achieving a Test AUC
    of 1. (See bottom.)
    
    Solution: do not require monotonic increase for this particular example.
    
    [0] train-auc:0.99989 test-auc:0.999497
    [1] train-auc:1 test-auc:0.999749
    [2] train-auc:1 test-auc:0.999749
    [3] train-auc:1 test-auc:0.999749
    [4] train-auc:1 test-auc:0.999749
    [5] train-auc:1 test-auc:0.999497
    [6] train-auc:1 test-auc:1
    [7] train-auc:1 test-auc:1
    [8] train-auc:1 test-auc:1
    [9] train-auc:1 test-auc:1
    Use dmlc stream when URI protocol is not local file. (#5857)
    Ensure that LoadSequentialFile() actually read the whole file (#5831)
    Port R compatibility patches from 1.0.0 release branch (#5577)
    
    * Don't use memset to set struct when compiling for R
    
    * Support 32-bit Solaris target for R package
    Model IO in JSON. (#5110)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Add IO utilities. (#5091)
    
    
    * Add fixed size stream for reading model stream.
    * Add file extension.
    Fix external memory race in colmaker. (#4980)
    
    
    * Move `GetColDensity` out of omp parallel block.
    A simple Json implementation for future use. (#4708)
    
    * A simple Json implementation for future use.
    More categorical tests and disable shap sparse test. (#6219)
    
    * Fix tree load with 32 category.
    Expand categorical node. (#6028)
    
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Ensure that LoadSequentialFile() actually read the whole file (#5831)
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix changing locale. (#5314)
    
    * Fix changing locale.
    
    * Don't use locale guard.
    
    As number parsing is implemented in house, we don't need locale.
    
    * Update doc.
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Save Scikit-Learn attributes into learner attributes. (#5245)
    
    * Remove the recommendation for pickle.
    
    * Save skl attributes in booster.attr
    
    * Test loading scikit-learn model with native booster.
    Support FreeBSD (#5233)
    
    * Fix build on FreeBSD
    
    * Use __linux__ macro
    Convenient methods for JSON integer. (#5089)
    
    * Fix parsing empty object.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Fix parsing empty json object. (#4868)
    
    * Fix parsing empty json object.
    
    * Better error message.
    Add Json integer, remove specialization. (#4739)
    A simple Json implementation for future use. (#4708)
    
    * A simple Json implementation for future use.
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Refactor the CLI. (#5574)
    
    
    * Enable parameter validation.
    * Enable JSON.
    * Catch `dmlc::Error`.
    * Show help message.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix data loading (#4862)
    
    * Fix loading text data.
    * Fix config regex.
    * Try to explain the error better in exception.
    * Update doc.
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Fix C++11 config parser (#4521)
    
    * Fix C++11 config parser
    * Use raw strings to improve readability of regex
    * Fix compilation for GCC 5.x
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Simplify INI-style config reader using C++11 STL (#4478)
    
    * simplify the config.h file
    
    * revise config.h
    
    * revised config.h
    
    * revise format
    
    * revise format issues
    
    * revise whitespace issues
    
    * revise whitespace namespace format issues
    
    * revise namespace format issues
    
    * format issues
    
    * format issues
    
    * format issues
    
    * format issues
    
    * Revert submodule changes
    
    * minor change
    
    * Update src/common/config.h
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * address format issue from trivialfis
    
    * Use correct cub submodule
    Use int instead of char in CLI config parser (#3976)
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    [TRAVIS] cleanup travis script
    [TREE] finish move of updater
    [TREE] Move the files to target refactor location
    [REFACTOR] cleanup structure
    lint half way
    more robust config parser
    mv code into src
    refactor config
    start unity refactor
    rank pass toy
    make style more like Google style
    Comments added
    add config
    [dask] Test for data initializaton. (#6226)
    Handle duplicated values in sketching. (#6178)
    
    
    * Accumulate weights in duplicated values.
    * Fix device id in iterative dmatrix.
    Use default allocator in sketching. (#6182)
    Support categorical data in GPU sketching. (#6137)
    Merge extract cuts into QuantileContainer. (#6125)
    
    
    * Use pruning for initial summary construction.
    Unify CPU hist sketching (#5880)
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Loop over `thrust::reduce`. (#6229)
    
    * Check input chunk size of dqdm.
    * Add doc for current limitation.
    Loop over copy_if (#6201)
    
    * Loop over copy_if
    
    * Catch OOM.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Handle duplicated values in sketching. (#6178)
    
    
    * Accumulate weights in duplicated values.
    * Fix device id in iterative dmatrix.
    Use default allocator in sketching. (#6182)
    Add categorical data support to GPU Hist. (#6164)
    Fall back to CUB allocator if RMM memory pool is not set up (#6150)
    
    * Fall back to CUB allocator if RMM memory pool is not set up
    
    * Fix build
    
    * Prevent memory leak
    
    * Add note about lack of memory initialisation
    
    * Add check for other fast allocators
    
    * Set use_cub_allocator_ to true when RMM is not enabled
    
    * Fix clang-tidy
    
    * Do not demangle symbol; add check to ensure Linux+Clang/GCC combo
    Use caching allocator from RMM, when RMM is enabled (#6131)
    Make binary bin search reusable. (#6058)
    
    * Move binary search row to hist util.
    * Remove dead code.
    GPUTreeShap (#6038)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    gpu_hist performance tweaks (#5707)
    
    * Remove device vectors
    
    * Remove allreduce synchronize
    
    * Remove double buffer
    Reduce device synchronisation (#5631)
    
    * Reduce device synchronisation
    
    * Initialise pinned memory
    Refactor the CLI. (#5574)
    
    
    * Enable parameter validation.
    * Enable JSON.
    * Catch `dmlc::Error`.
    * Show help message.
    Use cudaDeviceGetAttribute instead of cudaGetDeviceProperties (#5570)
    For histograms, opting into maximum shared memory available per block. (#5491)
    gpu_hist performance fixes (#5558)
    
    * Remove unnecessary cuda API calls
    
    * Fix histogram memory growth
    Use thrust functions instead of custom functions (#5544)
    Purge device_helpers.cuh (#5534)
    
    * Simplifications with caching_device_vector
    
    * Purge device helpers
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Device dmatrix (#5420)
    Fix memory usage of device sketching (#5407)
    Move segment sorter to common (#5378)
    
    - move segment sorter to common
    - this is the first of a handful of pr's that splits the larger pr #5326
    - it moves this facility to common (from ranking objective class), so that it can be
        used for metric computation
    - it also wraps all the bald device pointers into span.
    Pass shared pointer instead of raw pointer to Learner. (#5302)
    
    Extracted from https://github.com/dmlc/xgboost/pull/5220 .
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Implement cudf construction with adapters. (#5189)
    implementation of map ranking algorithm on gpu (#5129)
    
    * - implementation of map ranking algorithm
      - also effected necessary suggestions mentioned in the earlier ranking pr's
      - made some performance improvements to the ndcg algo as well
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Pairwise ranking objective implementation on gpu (#4873)
    
    * - pairwise ranking objective implementation on gpu
       - there are couple of more algorithms (ndcg and map) for which support will be added
         as follow-up pr's
       - with no label groups defined, get gradient is 90x faster on gpu (120m instance
         mortgage dataset)
       - it can perform by an order of magnitude faster with ~ 10 groups (and adequate cores
         for the cpu implementation)
    
    * Add JSON config to rank obj.
    Span: use `size_t' for index_type,  add `front' and `back'. (#4935)
    
    * Use `size_t' for index_type.  Add `front' and `back'.
    
    * Remove a batch of `static_cast'.
    Resolve dask performance issues (#4914)
    
    * Set dask client.map as impure function
    
    * Remove nrows
    
    * Remove slow check in verbose mode
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Complete cudf support. (#4850)
    
    
    * Handles missing value.
    * Accept all floating point and integer types.
    * Move to cudf 9.0 API.
    * Remove requirement on `null_count`.
    * Arbitrary column types support.
    further cleanup of single process multi-GPU code (#4810)
    
    
    * use subspan in gpu predictor instead of copying
    * Revise `HostDeviceVector`
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Modify caching allocator/vector and fix issues relating to inability to train large datasets (#4615)
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Refactor out row partitioning logic from gpu_hist, introduce caching device vectors (#4554)
    Set the appropriate device before freeing device memory... (#4566)
    
    * - set the appropriate device before freeing device memory...
       - pr #4532 added a global memory tracker/logger to keep track of number of (de)allocations
         and peak memory usage on a per device basis.
       - this pr adds the appropriate check to make sure that the (de)allocation counts and memory usages
         makes sense for the device. since verbosity is typically increased on debug/non-retail builds.
    * - pre-create cub allocators and reuse them
       - create them once and not resize them dynamically. we need to ensure that these allocators
         are created and destroyed exactly once so that the appropriate device id's are set
    Overload device memory allocation (#4532)
    
    * Group source files, include headers in source files
    
    * Overload device memory allocation
     Smarter choice of histogram construction for distributed gpu_hist (#4519)
    
    * Smarter choice of histogram construction for distributed gpu_hist
    
    * Limit omp team size in ExecuteShards
    add cuda 10.1 support (#4468)
     Combine thread launches into single launch per tree for gpu_hist (#4343)
    
    * Combine thread launches into single launch per tree for gpu_hist
    algorithm.
    
    * Address deprecation warning
    
    * Add manual column sampler constructor
    
    * Turn off omp dynamic to get a guaranteed number of threads
    
    * Enable openmp in cuda code
    Retire DVec class in favour of c++20 style span for device memory. (#4293)
    Optimisations for gpu_hist. (#4248)
    
    * Optimisations for gpu_hist.
    
    * Use streams to overlap operations.
    
    * ColumnSampler now uses HostDeviceVector to prevent repeatedly copying feature vectors to the device.
    Mark CUDA 10.1 as unsupported. (#4265)
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Remove various synchronisations from cuda API calls, instrument monitor (#4205)
    
    * Remove various synchronisations from cuda API calls, instrument monitor
    with nvtx profiler ranges.
    [REVIEW] Enable Multi-Node Multi-GPU functionality (#4095)
    
    * Initial commit to support multi-node multi-gpu xgboost using dask
    
    * Fixed NCCL initialization by not ignoring the opg parameter.
    
    - it now crashes on NCCL initialization, but at least we're attempting it properly
    
    * At the root node, perform a rabit::Allreduce to get initial sum_gradient across workers
    
    * Synchronizing in a couple of more places.
    
    - now the workers don't go down, but just hang
    - no more "wild" values of gradients
    - probably needs syncing in more places
    
    * Added another missing max-allreduce operation inside BuildHistLeftRight
    
    * Removed unnecessary collective operations.
    
    * Simplified rabit::Allreduce() sync of gradient sums.
    
    * Removed unnecessary rabit syncs around ncclAllReduce.
    
    - this improves performance _significantly_ (7x faster for overall training,
      20x faster for xgboost proper)
    
    * pulling in latest xgboost
    
    * removing changes to updater_quantile_hist.cc
    
    * changing use_nccl_opg initialization, removing unnecessary if statements
    
    * added definition for opaque ncclUniqueId struct to properly encapsulate GetUniqueId
    
    * placing struct defintion in guard to avoid duplicate code errors
    
    * addressing linting errors
    
    * removing
    
    * removing additional arguments to AllReduer initialization
    
    * removing distributed flag
    
    * making comm init symmetric
    
    * removing distributed flag
    
    * changing ncclCommInit to support multiple modalities
    
    * fix indenting
    
    * updating ncclCommInitRank block with necessary group calls
    
    * fix indenting
    
    * adding print statement, and updating accessor in vector
    
    * improving print statement to end-line
    
    * generalizing nccl_rank construction using rabit
    
    * assume device_ordinals is the same for every node
    
    * test, assume device_ordinals is identical for all nodes
    
    * test, assume device_ordinals is unique for all nodes
    
    * changing names of offset variable to be more descriptive, editing indenting
    
    * wrapping ncclUniqueId GetUniqueId() and aesthetic changes
    
    * adding synchronization, and tests for distributed
    
    * adding  to tests
    
    * fixing broken #endif
    
    * fixing initialization of gpu histograms, correcting errors in tests
    
    * adding to contributors list
    
    * adding distributed tests to jenkins
    
    * fixing bad path in distributed test
    
    * debugging
    
    * adding kubernetes for distributed tests
    
    * adding proper import for OrderedDict
    
    * adding urllib3==1.22 to address ordered_dict import error
    
    * added sleep to allow workers to save their models for comparison
    
    * adding name to GPU contributors under docs
    Fix for windows compilation (#4139)
    Use nccl group calls to prevent from dead lock. (#4113)
    
    * launch all reduce sequentially.
    * Fix gpu_exact test memory leak.
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
    Use Span in GPU exact updater. (#4020)
    
    * Use Span in GPU exact updater.
    
    * Add a small test.
    Fix and optimize logger (#4002)
    
    * Fix logging switch statement.
    
    * Remove debug_verbose_ in AllReducer.
    
    * Don't construct the stream when not needed.
    
    * Make default constructor deleted.
    
    * Remove redundant IsVerbose.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Initialized AllReducer counters to 0. (#3987)
    Single precision histograms on GPU (#3965)
    
    * Allow single precision histogram summation in gpu_hist
    
    * Add python test, reduce run-time of gpu_hist tests
    
    * Update documentation
    GPU performance logging/improvements (#3945)
    
    - Improved GPU performance logging
    
    - Only use one execute shards function
    
    - Revert performance regression on multi-GPU
    
    - Use threads to launch NCCL AllReduce
    Improve update position function for gpu_hist (#3895)
    Minor refactor of split evaluation in gpu_hist (#3889)
    
    * Refactor evaluate split into shard
    
    * Use span in evaluate split
    
    * Update google tests
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Add basic unittests for gpu-hist method. (#3785)
    
    * Split building histogram into separated class.
    * Extract `InitCompressedRow` definition.
    * Basic tests for gpu-hist.
    * Document the code more verbosely.
    * Removed `HistCutUnit`.
    * Removed some duplicated copies in `GPUHistMaker`.
    * Implement LCG and use it in tests.
    Catch dmlc::Error. (#3751)
    
    Fix #3643.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Implement devices to devices reshard. (#3721)
    
    * Force clearing device memory before Reshard.
    * Remove calculating row_segments for gpu_hist and gpu_sketch.
    * Guard against changing device.
    Fix gpu devices. (#3693)
    
    * Fix gpu_set normalized and unnormalized.
    * Fix DeviceSpan.
    Merge generic device helper functions into gpu set. (#3626)
    
    * Remove the use of old NDevices* functions.
    * Use GPUSet in timer.h.
    Dynamically allocate GPU histogram memory (#3519)
    
    * Expand histogram memory dynamically to prevent large allocations for large tree depths (e.g. > 15)
    
    * Remove GPU memory allocation messages. These are misleading as a large number of allocations are now dynamic.
    
    * Fix appveyor R test
    Added finding quantiles on GPU. (#3393)
    
    * Added finding quantiles on GPU.
    
    - this includes datasets where weights are assigned to data rows
    - as the quantiles found by the new algorithm are not the same
      as those found by the old one, test thresholds in
        tests/python-gpu/test_gpu_updaters.py have been adjusted.
    
    * Adjustments and improved testing for finding quantiles on the GPU.
    
    - added C++ tests for the DeviceSketch() function
    - reduced one of the thresholds in test_gpu_updaters.py
    - adjusted the cuts found by the find_cuts_k kernel
    Upgrading to NCCL2 (#3404)
    
    * Upgrading to NCCL2
    
    * Part - II of NCCL2 upgradation
    
     - Doc updates to build with nccl2
     - Dockerfile.gpu update for a correct CI build with nccl2
     - Updated FindNccl package to have env-var NCCL_ROOT to take precedence
    
    * Upgrading to v9.2 for CI workflow, since it has the nccl2 binaries available
    
    * Added NCCL2 license + copy the nccl binaries into /usr location for the FindNccl module to find
    
    * Set LD_LIBRARY_PATH variable to pick nccl2 binary at runtime
    
    * Need the nccl2 library download instructions inside Dockerfile.release as well
    
    * Use NCCL2 as a static library
    GPU binning and compression. (#3319)
    
    * GPU binning and compression.
    
    - binning and index compression are done inside the DeviceShard constructor
    - in case of a DMatrix with multiple row batches, it is first converted into a single row batch
    Fix for issue 3306. (#3324)
    Multi-GPU HostDeviceVector. (#3287)
    
    * Multi-GPU HostDeviceVector.
    
    - HostDeviceVector instances can now span multiple devices, defined by GPUSet struct
    - the interface of HostDeviceVector has been modified accordingly
    - GPU objective functions are now multi-GPU
    - GPU predicting from cache is now multi-GPU
    - avoiding omp_set_num_threads() calls
    - other minor changes
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Change reduce operation from thrust to cub. Fix for cuda 9.1 error (#3218)
    
    * Change reduce operation from thrust to cub. Fix for cuda 9.1 runtime error
    
    * Unit test sum reduce
    Fixed the bug with illegal memory access in test_large_sizes.py with 4 GPUs. (#3068)
    
    - thrust::copy() called from dvec::copy() for gpairs invoked a GPU kernel instead of
      cudaMemcpy()
    - this resulted in illegal memory access if the GPU running the kernel could not access
      the data being copied
    - new version of dvec::copy() for thrust::device_ptr iterators calls cudaMemcpy(),
      avoiding the problem.
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    [GPU-Plugin] Major refactor (#2644)
    
    * Removal of redundant code/files.
    * Removal of exact namespace in GPU plugin
    * Revert double precision histograms to single precision for performance on Maxwell/Kepler
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    [GPU-Plugin] Add throw of asserts and added compute compatibility error check. (#2565)
    
    * [GPU-Plugin] Added compute compatibility error check, added verbose timing
    [GPU-Plugin] Various fixes (#2579)
    
    * Fix test large
    
    * Add check for max_depth 0
    
    * Update readme
    
    * Add LBS specialisation for dense data
    
    * Add bst_gpair_precise
    
    * Temporarily disable accuracy tests on test_large.py
    
    * Solve unused variable compiler warning
    
    * Fix max_bin > 1024 error
    [GPU-Plugin] Fix gpu_hist to allow matrices with more than just 2^{32} elements. Also fixed CPU hist algorithm. (#2518)
    [GPU-Plugin] Improved load balancing search (#2521)
    [GPU-Plugin] Add load balancing search to gpu_hist. Add compressed iterator. (#2504)
    [GPU-Plugin] Resolve double compilation issue (#2479)
    [GPU-Plugin] Multi-GPU gpu_id bug fixes for grow_gpu_hist and grow_gpu methods, and additional documentation for the gpu plugin. (#2463)
    [GPU-Plugin] Multi-GPU for grow_gpu_hist histogram method using NVIDIA NCCL. (#2395)
    [GPU-Plugin] Integration of a faster version of grow_gpu plugin into mainstream (#2360)
    
    * Integrating a faster version of grow_gpu plugin
    1. Removed the older files to reduce duplication
    2. Moved all of the grow_gpu files under 'exact' folder
    3. All of them are inside 'exact' namespace to avoid any conflicts
    4. Fixed a bug in benchmark.py while running only 'grow_gpu' plugin
    5. Added cub and googletest submodules to ease integration and unit-testing
    6. Updates to CMakeLists.txt to directly build cuda objects into libxgboost
    
    * Added support for building gpu plugins through make flow
    1. updated makefile and config.mk to add right targets
    2. added unit-tests for gpu exact plugin code
    
    * 1. Added support for building gpu plugin using 'make' flow as well
    2. Updated instructions for building and testing gpu plugin
    
    * Fix travis-ci errors for PR#2360
    1. lint errors on unit-tests
    2. removed googletest, instead depended upon dmlc-core provide gtest cache
    
    * Some more fixes to travis-ci lint failures PR#2360
    
    * Added Rory's copyrights to the files containing code from both.
    
    * updated copyright statement as per Rory's request
    
    * moved the static datasets into a script to generate them at runtime
    
    * 1. memory usage print when silent=0
    2. tests/ and test/ folder organization
    3. removal of the dependency of googletest for just building xgboost
    4. coding style updates for .cuh as well
    
    * Fixes for compilation warnings
    
    * add cuda object files as well when JVM_BINDINGS=ON
    [GPU Plugin] Fast histogram speed improvements. Updated benchmarks. (#2258)
    [GPU-Plugin] (#2227)
    
    * Add fast histogram algorithm
    * Fix Linux build
    * Add 'gpu_id' parameter
    GPU Plugin: Add subsample, colsample_bytree, colsample_bylevel (#1895)
    GPU Plugin: Add bosch demo, update build instructions (#1872)
    Add benchmarks, fix GCC build (#1848)
    Update build instructions,  improve memory usage (#1811)
    GPU plug-in improvements + basic Windows continuous integration (#1752)
    
    * GPU Plugin: Reduce memory, improve performance, fix gcc compiler bug, add
    out of memory exceptions
    
    * Add basic Windows continuous integration for cmake VS2013, VS2015
    Add GPU accelerated tree construction plugin (#1679)
    Expand categorical node. (#6028)
    
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Ignore columnar alignment requirement. (#4928)
    
    * Better error message for wrong type.
    * Fix stride size.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Unify set index data. (#6062)
    Unify evaluation functions. (#6037)
    Make binary bin search reusable. (#6058)
    
    * Move binary search row to hist util.
    * Remove dead code.
    Optimize cpu sketch allreduce for sparse data. (#6009)
    
    * Bypass RABIT serialization reducer and use custom allgather based merging.
    Unify CPU hist sketching (#5880)
    Cleanup on device sketch. (#5874)
    
    * Remove old functions.
    
    * Merge weighted and un-weighted into a common interface.
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Implement weighted sketching for adapter. (#5760)
    
    
    * Bounded memory tests.
    * Fixed memory estimation.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Add float32 histogram (#5624)
    
    * new single_precision_histogram param was added.
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Co-authored-by: fis <jm.yuan@outlook.com>
    Expose device sketching in header. (#5747)
    Group aware GPU sketching. (#5551)
    
    * Group aware GPU weighted sketching.
    
    * Distribute group weights to each data point.
    * Relax the test.
    * Validate input meta info.
    * Fix metainfo copy ctor.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Prevent copying SimpleDMatrix. (#5453)
    
    * Set default dtor for SimpleDMatrix to initialize default copy ctor, which is
    deleted due to unique ptr.
    
    * Remove commented code.
    * Remove warning for calling host function (std::max).
    * Remove warning for initialization order.
    * Remove warning for unused variables.
    Reducing memory consumption for 'hist' method on CPU (#5334)
    Device dmatrix (#5420)
    Fix memory usage of device sketching (#5407)
    Partial rewrite EllpackPage (#5352)
    Sketching from adapters (#5365)
    
    * Sketching from adapters
    
    * Add weights test
    Optimized ApplySplit, BuildHist and UpdatePredictCache functions on CPU (#5244)
    
    * Split up sparse and dense build hist kernels.
    * Add `PartitionBuilder`.
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Wide dataset quantile performance improvement (#5306)
    Testing hist_util (#5251)
    
    * Rank tests
    
    * Remove categorical split specialisation
    
    * Extend tests to multiple features, switch to WQSketch
    
    * Add tests for SparseCuts
    
    * Add external memory quantile tests, fix some existing tests
    Optimized BuildHist function (#5156)
    Add CMake option to run Undefined Behavior Sanitizer (UBSan) (#5211)
    
    * Fix related errors.
    Revert #4529 (#5008)
    
    * Revert " Optimize ‘hist’ for multi-core CPU (#4529)"
    
    This reverts commit 4d6590be3c9a043d44d9e4fe0a456a9f8179ec72.
    
    * Fix build
    Span: use `size_t' for index_type,  add `front' and `back'. (#4935)
    
    * Use `size_t' for index_type.  Add `front' and `back'.
    
    * Remove a batch of `static_cast'.
    Move ellpack page construction into DMatrix (#4833)
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
     Optimize ‘hist’ for multi-core CPU (#4529)
    
    * Initial performance optimizations for xgboost
    
    * remove includes
    
    * revert float->double
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * Check existence of _mm_prefetch and __builtin_prefetch
    
    * Fix lint
    
    * optimizations for CPU
    
    * appling comments in review
    
    * add some comments, code refactoring
    
    * fixing issues in CI
    
    * adding runtime checks
    
    * remove 1 extra check
    
    * remove extra checks in BuildHist
    
    * remove checks
    
    * add debug info
    
    * added debug info
    
    * revert changes
    
    * added comments
    
    * Apply suggestions from code review
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * apply review comments
    
    * Remove unused function CreateNewNodes()
    
    * Add descriptive comment on node_idx variable in QuantileHistMaker::Builder::BuildHistsBatch()
    - training with external memory part 1 of 2 (#4486)
    
    * - training with external memory part 1 of 2
       - this pr focuses on computing the quantiles using multiple gpus on a
         dataset that uses the external cache capabilities
       - there will a follow-up pr soon after this that will support creation
         of histogram indices on large dataset as well
       - both of these changes are required to support training with external memory
       - the sparse pages in dmatrix are taken in batches and the the cut matrices
         are incrementally built
       - also snuck in some (perf) changes related to sketches aggregation amongst multiple
         features across multiple sparse page batches. instead of aggregating the summary
         inside each device and merged later, it is aggregated in-place when the device
         is working on different rows but the same feature
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Optimizations of pre-processing for 'hist' tree method (#4310)
    
    * oprimizations for pre-processing
    
    * code cleaning
    
    * code cleaning
    
    * code cleaning after review
    
    * Apply suggestions from code review
    
    Co-Authored-By: SmirnovEgorRu <egor.smirnov@intel.com>
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Make `HistCutMatrix::Init' be aware of groups. (#4115)
    
    
    * Add checks for group size.
    * Simple docs.
    * Search group index during hist cut matrix initialization.
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    Performance optimizations for Intel CPUs (#3957)
    
    * Initial performance optimizations for xgboost
    
    * remove includes
    
    * revert float->double
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * Check existence of _mm_prefetch and __builtin_prefetch
    
    * Fix lint
    Single precision histograms on GPU (#3965)
    
    * Allow single precision histogram summation in gpu_hist
    
    * Add python test, reduce run-time of gpu_hist tests
    
    * Update documentation
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Add basic unittests for gpu-hist method. (#3785)
    
    * Split building histogram into separated class.
    * Extract `InitCompressedRow` definition.
    * Basic tests for gpu-hist.
    * Document the code more verbosely.
    * Removed `HistCutUnit`.
    * Removed some duplicated copies in `GPUHistMaker`.
    * Implement LCG and use it in tests.
    Implement devices to devices reshard. (#3721)
    
    * Force clearing device memory before Reshard.
    * Remove calculating row_segments for gpu_hist and gpu_sketch.
    * Guard against changing device.
    GPU memory usage fixes + column sampling refactor (#3635)
    
    * Remove thrust copy calls
    
    * Fix  histogram memory usage
    
    * Cap extreme histogram memory usage
    
    * More efficient column sampling
    
    * Use column sampler across updaters
    
    * More efficient split evaluation on GPU with column sampling
    Refactor parts of fast histogram utilities (#3564)
    
    * Refactor parts of fast histogram utilities
    
    * Removed byte packing from column matrix
    Added finding quantiles on GPU. (#3393)
    
    * Added finding quantiles on GPU.
    
    - this includes datasets where weights are assigned to data rows
    - as the quantiles found by the new algorithm are not the same
      as those found by the old one, test thresholds in
        tests/python-gpu/test_gpu_updaters.py have been adjusted.
    
    * Adjustments and improved testing for finding quantiles on the GPU.
    
    - added C++ tests for the DeviceSketch() function
    - reduced one of the thresholds in test_gpu_updaters.py
    - adjusted the cuts found by the find_cuts_k kernel
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    Integer gradient summation for GPU histogram algorithm. (#2681)
    [GPU-Plugin] Fix gpu_hist to allow matrices with more than just 2^{32} elements. Also fixed CPU hist algorithm. (#2518)
    Patch to improve multithreaded performance scaling (#2493)
    
    * Patch to improve multithreaded performance scaling
    
    Change parallel strategy for histogram construction.
    Instead of partitioning data rows among multiple threads, partition feature
    columns instead. Useful heuristics for assigning partitions have been adopted
    from LightGBM project.
    
    * Add missing header to satisfy MSVC
    
    * Restore max_bin and related parameters to TrainParam
    
    * Fix lint error
    
    * inline functions do not require static keyword
    
    * Feature grouping algorithm accepting FastHistParam
    
    Feature grouping algorithm accepts many parameters (3+), and it gets annoying to
    pass them one by one. Instead, simply pass the reference to FastHistParam. The
    definition of FastHistParam has been moved to a separate header file to
    accomodate this change.
    Improve multi-threaded performance (#2104)
    
    * Add UpdatePredictionCache() option to updaters
    
    Some updaters (e.g. fast_hist) has enough information to quickly compute
    prediction cache for the training data. Each updater may override
    UpdaterPredictionCache() method to update the prediction cache. Note: this
    trick does not apply to validation data.
    
    * Respond to code review
    
    * Disable some debug messages by default
    * Document UpdatePredictionCache() interface
    * Remove base_margin logic from UpdatePredictionCache() implementation
    * Do not take pointer to cfg, as reference may get stale
    
    * Improve multi-threaded performance
    
    * Use columnwise accessor to accelerate ApplySplit() step,
      with support for a compressed representation
    * Parallel sort for evaluation step
    * Inline BuildHist() function
    * Cache gradient pairs when building histograms in BuildHist()
    
    * Add missing #if macro
    
    * Respond to code review
    
    * Use wrapper to enable parallel sort on Linux
    
    * Fix C++ compatibility issues
    
    * MSVC doesn't support unsigned in OpenMP loops
    * gcc 4.6 doesn't support using keyword
    
    * Fix lint issues
    
    * Respond to code review
    
    * Fix bug in ApplySplitSparseData()
    
    * Attempting to read beyond the end of a sparse column
    * Mishandling the case where an entire range of rows have missing values
    
    * Fix training continuation bug
    
    Disable UpdatePredictionCache() in the first iteration. This way, we can
    accomodate the scenario where we build off of an existing (nonempty) ensemble.
    
    * Add regression test for fast_hist
    
    * Respond to code review
    
    * Add back old version of ApplySplitSparseData
    Histogram Optimized Tree Grower (#1940)
    
    * Support histogram-based algorithm + multiple tree growing strategy
    
    * Add a brand new updater to support histogram-based algorithm, which buckets
      continuous features into discrete bins to speed up training. To use it, set
      `tree_method = fast_hist` to configuration.
    * Support multiple tree growing strategies. For now, two policies are supported:
      * `grow_policy=depthwise` (default):  favor splitting at nodes closest to the
        root, i.e. grow depth-wise.
      * `grow_policy=lossguide`: favor splitting at nodes with highest loss change
    * Improve single-threaded performance
      * Unroll critical loops
      * Introduce specialized code for dense data (i.e. no missing values)
    * Additional training parameters: `max_leaves`, `max_bin`, `grow_policy`, `verbose`
    
    * Adding a small test for hist method
    
    * Fix memory error in row_set.h
    
    When std::vector is resized, a reference to one of its element may become
    stale. Any such reference must be updated as well.
    
    * Resolve cross-platform compilation issues
    
    * Versions of g++ older than 4.8 lacks support for a few C++11 features, e.g.
      alignas(*) and new initializer syntax. To support g++ 4.6, use pre-C++11
      initializer and remove alignas(*).
    * Versions of MSVC older than 2015 does not support alignas(*). To support
      MSVC 2012, remove alignas(*).
    * For g++ 4.8 and newer, alignas(*) is enabled for performance benefits.
    * Some old compilers (MSVC 2012, g++ 4.6) do not support template aliases
      (which uses `using` to declate type aliases). So always use `typedef`.
    
    * Fix a host of CI issues
    
    * Remove dependency for libz on osx
    * Fix heading for hist_util
    * Fix minor style issues
    * Add missing #include
    * Remove extraneous logging
    
    * Enable tree_method=hist in R
    
    * Renaming HistMaker to GHistBuilder to avoid confusion
    
    * Fix R integration
    
    * Respond to style comments
    
    * Consistent tie-breaking for priority queue using timestamps
    
    * Last-minute style fixes
    
    * Fix issuecomment-271977647
    
    The way we quantize data is broken. The agaricus data consists of all
    categorical values. When NAs are converted into 0's,
    `HistCutMatrix::Init` assign both 0's and 1's to the same single bin.
    
    Why? gmat only the smallest value (0) and an upper bound (2), which is twice
    the maximum value (1). Add the maximum value itself to gmat to fix the issue.
    
    * Fix issuecomment-272266358
    
    * Remove padding from cut values for the continuous case
    * For categorical/ordinal values, use midpoints as bin boundaries to be safe
    
    * Fix CI issue -- do not use xrange(*)
    
    * Fix corner case in quantile sketch
    
    Signed-off-by: Philip Cho <chohyu01@cs.washington.edu>
    
    * Adding a test for an edge case in quantile sketcher
    
    max_bin=2 used to cause an exception.
    
    * Fix fast_hist test
    
    The test used to require a strictly increasing Test AUC for all examples.
    One of them exhibits a small blip in Test AUC before achieving a Test AUC
    of 1. (See bottom.)
    
    Solution: do not require monotonic increase for this particular example.
    
    [0] train-auc:0.99989 test-auc:0.999497
    [1] train-auc:1 test-auc:0.999749
    [2] train-auc:1 test-auc:0.999749
    [3] train-auc:1 test-auc:0.999749
    [4] train-auc:1 test-auc:0.999749
    [5] train-auc:1 test-auc:0.999497
    [6] train-auc:1 test-auc:1
    [7] train-auc:1 test-auc:1
    [8] train-auc:1 test-auc:1
    [9] train-auc:1 test-auc:1
    Unify evaluation functions. (#6037)
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Port R compatibility patches from 1.0.0 release branch (#5577)
    
    * Don't use memset to set struct when compiling for R
    
    * Support 32-bit Solaris target for R package
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Pass shared pointer instead of raw pointer to Learner. (#5302)
    
    Extracted from https://github.com/dmlc/xgboost/pull/5220 .
    Remove use of std::cout from R package (#5261)
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    Implement training observer. (#5088)
    Optimize DMatrix build time. (#5877)
    
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Add number of columns to native data iterator. (#5202)
    
    * Change native data iter into an adapter.
    Group builder modified for incremental building (#5098)
    External data adapters (#5044)
    
    * Use external data adapters as lightweight intermediate layer between external data and DMatrix
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Add PushCSC for SparsePage. (#4193)
    
    * Add PushCSC for SparsePage.
    
    * Move Push* definitions into cc file.
    * Add std:: prefix to `size_t` make clang++ happy.
    * Address monitor count == 0.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    [TRAVIS] cleanup travis script
    [TREE] Refactor to new logging
    [TREE] move tree model
    [REFACTOR] cleanup structure
    fix all utils
    lint half way
    simplify and parallelize data builder
    ok
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Offload some configurations into GBM. (#4553)
    
    
    This is part 1 of refactoring configuration.
    
    * Move tree heuristic configurations.
    * Split up declarations and definitions for GBTree.
    * Implement UseGPU in gbm.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Fix gpu devices. (#3693)
    
    * Fix gpu_set normalized and unnormalized.
    * Fix DeviceSpan.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Remove xgboost's thread_local and switch to dmlc::ThreadLocalStore (#2121)
    
    * Remove xgboost's own version of thread_local and switch to dmlc::ThreadLocalStore (#2109)
    
    * Update dmlc-core
    [JVM] Make JVM Serializable
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    [R] make all customizations to meet strict standard of cran
    [LIBXGBOOST] pass demo running.
    Add categorical data support to GPU predictor. (#6165)
    Move feature names and types of DMatrix from Python to C++. (#5858)
    
    
    * Add thread local return entry for DMatrix.
    * Save feature name and feature type in binary file.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Implement extend method for meta info. (#5800)
    
    * Implement extend for host device vector.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Lazy initialization of device vector. (#5173)
    
    * Lazy initialization of device vector.
    
    * Fix #5162.
    
    * Disable copy constructor of HostDeviceVector.  Prevents implicit copying.
    
    * Fix CPU build.
    
    * Bring back move assignment operator.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    further cleanup of single process multi-GPU code (#4810)
    
    
    * use subspan in gpu predictor instead of copying
    * Revise `HostDeviceVector`
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    More explict sharding methods for device memory (#4396)
    
    * Rename the Reshard method to Shard
    
    * Add a new Reshard method for sharding a vector that's already sharded
    Improve HostDeviceVector exception safety (#4301)
    
    
    * make the assignments of HostDeviceVector exception safe.
    * storing a dummy GPUDistribution instance in HDV for CPU based code.
    * change testxgboost binary location to build directory.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    GPU memory usage fixes + column sampling refactor (#3635)
    
    * Remove thrust copy calls
    
    * Fix  histogram memory usage
    
    * Cap extreme histogram memory usage
    
    * More efficient column sampling
    
    * Use column sampler across updaters
    
    * More efficient split evaluation on GPU with column sampling
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Multi-GPU HostDeviceVector. (#3287)
    
    * Multi-GPU HostDeviceVector.
    
    - HostDeviceVector instances can now span multiple devices, defined by GPUSet struct
    - the interface of HostDeviceVector has been modified accordingly
    - GPU objective functions are now multi-GPU
    - GPU predicting from cache is now multi-GPU
    - avoiding omp_set_num_threads() calls
    - other minor changes
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Feature weights (#5962)
    Use dmlc stream when URI protocol is not local file. (#5857)
    Ensure that LoadSequentialFile() actually read the whole file (#5831)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Save Scikit-Learn attributes into learner attributes. (#5245)
    
    * Remove the recommendation for pickle.
    
    * Save skl attributes in booster.attr
    
    * Test loading scikit-learn model with native booster.
    Add IO utilities. (#5091)
    
    
    * Add fixed size stream for reading model stream.
    * Add file extension.
    Fix external memory race in colmaker. (#4980)
    
    
    * Move `GetColDensity` out of omp parallel block.
    A simple Json implementation for future use. (#4708)
    
    * A simple Json implementation for future use.
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    [LEARNER] refactor learner
    [TREE] Enable updater registry
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Throw error when not compiled with NCCL. (#5170)
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Optimize cpu sketch allreduce for sparse data. (#6009)
    
    * Bypass RABIT serialization reducer and use custom allgather based merging.
    Unify CPU hist sketching (#5880)
    Cleanup on device sketch. (#5874)
    
    * Remove old functions.
    
    * Merge weighted and un-weighted into a common interface.
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Remove unweighted GK quantile. (#5816)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Fix out-of-bound array access in WQSummary::SetPrune() (#5493)
    Prevent copying SimpleDMatrix. (#5453)
    
    * Set default dtor for SimpleDMatrix to initialize default copy ctor, which is
    deleted due to unique ptr.
    
    * Remove commented code.
    * Remove warning for calling host function (std::max).
    * Remove warning for initialization order.
    * Remove warning for unused variables.
    Wide dataset quantile performance improvement (#5306)
    Added finding quantiles on GPU. (#3393)
    
    * Added finding quantiles on GPU.
    
    - this includes datasets where weights are assigned to data rows
    - as the quantiles found by the new algorithm are not the same
      as those found by the old one, test thresholds in
        tests/python-gpu/test_gpu_updaters.py have been adjusted.
    
    * Adjustments and improved testing for finding quantiles on the GPU.
    
    - added C++ tests for the DeviceSketch() function
    - reduced one of the thresholds in test_gpu_updaters.py
    - adjusted the cuts found by the find_cuts_k kernel
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replacing cout with LOG (#3076)
    
    * change cout to LOG
    
    * lint fix
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    Histogram Optimized Tree Grower (#1940)
    
    * Support histogram-based algorithm + multiple tree growing strategy
    
    * Add a brand new updater to support histogram-based algorithm, which buckets
      continuous features into discrete bins to speed up training. To use it, set
      `tree_method = fast_hist` to configuration.
    * Support multiple tree growing strategies. For now, two policies are supported:
      * `grow_policy=depthwise` (default):  favor splitting at nodes closest to the
        root, i.e. grow depth-wise.
      * `grow_policy=lossguide`: favor splitting at nodes with highest loss change
    * Improve single-threaded performance
      * Unroll critical loops
      * Introduce specialized code for dense data (i.e. no missing values)
    * Additional training parameters: `max_leaves`, `max_bin`, `grow_policy`, `verbose`
    
    * Adding a small test for hist method
    
    * Fix memory error in row_set.h
    
    When std::vector is resized, a reference to one of its element may become
    stale. Any such reference must be updated as well.
    
    * Resolve cross-platform compilation issues
    
    * Versions of g++ older than 4.8 lacks support for a few C++11 features, e.g.
      alignas(*) and new initializer syntax. To support g++ 4.6, use pre-C++11
      initializer and remove alignas(*).
    * Versions of MSVC older than 2015 does not support alignas(*). To support
      MSVC 2012, remove alignas(*).
    * For g++ 4.8 and newer, alignas(*) is enabled for performance benefits.
    * Some old compilers (MSVC 2012, g++ 4.6) do not support template aliases
      (which uses `using` to declate type aliases). So always use `typedef`.
    
    * Fix a host of CI issues
    
    * Remove dependency for libz on osx
    * Fix heading for hist_util
    * Fix minor style issues
    * Add missing #include
    * Remove extraneous logging
    
    * Enable tree_method=hist in R
    
    * Renaming HistMaker to GHistBuilder to avoid confusion
    
    * Fix R integration
    
    * Respond to style comments
    
    * Consistent tie-breaking for priority queue using timestamps
    
    * Last-minute style fixes
    
    * Fix issuecomment-271977647
    
    The way we quantize data is broken. The agaricus data consists of all
    categorical values. When NAs are converted into 0's,
    `HistCutMatrix::Init` assign both 0's and 1's to the same single bin.
    
    Why? gmat only the smallest value (0) and an upper bound (2), which is twice
    the maximum value (1). Add the maximum value itself to gmat to fix the issue.
    
    * Fix issuecomment-272266358
    
    * Remove padding from cut values for the continuous case
    * For categorical/ordinal values, use midpoints as bin boundaries to be safe
    
    * Fix CI issue -- do not use xrange(*)
    
    * Fix corner case in quantile sketch
    
    Signed-off-by: Philip Cho <chohyu01@cs.washington.edu>
    
    * Adding a test for an edge case in quantile sketcher
    
    max_bin=2 used to cause an exception.
    
    * Fix fast_hist test
    
    The test used to require a strictly increasing Test AUC for all examples.
    One of them exhibits a small blip in Test AUC before achieving a Test AUC
    of 1. (See bottom.)
    
    Solution: do not require monotonic increase for this particular example.
    
    [0] train-auc:0.99989 test-auc:0.999497
    [1] train-auc:1 test-auc:0.999749
    [2] train-auc:1 test-auc:0.999749
    [3] train-auc:1 test-auc:0.999749
    [4] train-auc:1 test-auc:0.999749
    [5] train-auc:1 test-auc:0.999497
    [6] train-auc:1 test-auc:1
    [7] train-auc:1 test-auc:1
    [8] train-auc:1 test-auc:1
    [9] train-auc:1 test-auc:1
    [TRAVIS] cleanup travis script
    [FIX] fix plugin system
    [LIBXGBOOST] pass demo running.
    [TREE] finish move of updater
    [TREE] Move the files to target refactor location
    [REFACTOR] cleanup structure
    spelling, wording, and doc fixes in c++ code
    
    I was reading through the code and fixing some things in the comments.
    Only a few trivial actual code changes were made to make things more
    readable.
    remove debug messages fix lint
    fix SetCombine and SetPrune bug
    fix all utils
    remove print
    cap second order gradient
    fix quantile for edge case, make logloss evaluation capped for extreme values
    add debuglog for quantile
    change allreduce lib to rabit library, xgboost now run with rabit
    start check windows compatiblity
    remove warning from MSVC need another round of check
    get multinode in
    compile
    checkin skmaker
    sorted base sketch maker
    fix regression
    fix compile, need final leaf node?
    a version that compile
    ok for now
    optimize heavy hitter
    chg begin end type
    unified gk and wq
    commit in quantile test
    mostly correct\n
    init check
    fix bug in queue2summary
    before test quantile
    quantile
    remv debug
    add quantile sketch
    Handle duplicated values in sketching. (#6178)
    
    
    * Accumulate weights in duplicated values.
    * Fix device id in iterative dmatrix.
    Use default allocator in sketching. (#6182)
    Support categorical data in GPU sketching. (#6137)
    Merge extract cuts into QuantileContainer. (#6125)
    
    
    * Use pruning for initial summary construction.
    Dask device dmatrix (#5901)
    
    
    * Fix softprob with empty dmatrix.
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Expand categorical node. (#6028)
    
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Implement robust regularization in 'survival:aft' objective (#5473)
    
    * Robust regularization of AFT gradient and hessian
    
    * Fix AFT doc; expose it to tutorial TOC
    
    * Apply robust regularization to uncensored case too
    
    * Revise unit test slightly
    
    * Fix lint
    
    * Update test_survival.py
    
    * Use GradientPairPrecise
    
    * Remove unused variables
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Handle duplicated values in sketching. (#6178)
    
    
    * Accumulate weights in duplicated values.
    * Fix device id in iterative dmatrix.
    Use default allocator in sketching. (#6182)
    Merge extract cuts into QuantileContainer. (#6125)
    
    
    * Use pruning for initial summary construction.
    Unify CPU hist sketching (#5880)
    Fix sketch size calculation. (#5898)
    Cleanup on device sketch. (#5874)
    
    * Remove old functions.
    
    * Merge weighted and un-weighted into a common interface.
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Implement iterative DMatrix. (#5837)
    Implement weighted sketching for adapter. (#5760)
    
    
    * Bounded memory tests.
    * Fixed memory estimation.
    Expose device sketching in header. (#5747)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Enhance nvtx support. (#5636)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    monitor for distributed envorinment. (#4829)
    
    * Collect statistics from other ranks in monitor.
    
    * Workaround old GCC bug.
    Implement tree model dump with code generator. (#4602)
    
    * Implement tree model dump with a code generator.
    
    * Split up generators.
    * Implement graphviz generator.
    * Use pattern matching.
    
    * [Breaking] Return a Source in `to_graphviz` instead of Digraph in Python package.
    
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Remove various synchronisations from cuda API calls, instrument monitor (#4205)
    
    * Remove various synchronisations from cuda API calls, instrument monitor
    with nvtx profiler ranges.
    Add PushCSC for SparsePage. (#4193)
    
    * Add PushCSC for SparsePage.
    
    * Move Push* definitions into cc file.
    * Add std:: prefix to `size_t` make clang++ happy.
    * Address monitor count == 0.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Fix and optimize logger (#4002)
    
    * Fix logging switch statement.
    
    * Remove debug_verbose_ in AllReducer.
    
    * Don't construct the stream when not needed.
    
    * Make default constructor deleted.
    
    * Remove redundant IsVerbose.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    GPU performance logging/improvements (#3945)
    
    - Improved GPU performance logging
    
    - Only use one execute shards function
    
    - Revert performance regression on multi-GPU
    
    - Use threads to launch NCCL AllReduce
    Fix gpu devices. (#3693)
    
    * Fix gpu_set normalized and unnormalized.
    * Fix DeviceSpan.
    Merge generic device helper functions into gpu set. (#3626)
    
    * Remove the use of old NDevices* functions.
    * Use GPUSet in timer.h.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Replacing cout with LOG (#3076)
    
    * change cout to LOG
    
    * lint fix
    Avoid repeated cuda API call in GPU predictor and only synchronize used GPUs (#2936)
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Use configure_file() to configure version only (#4974)
    
    * Avoid writing build_config.h
    
    * Remove build_config.h all together.
    
    * Lint.
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    [LIBXGBOOST] pass demo running.
    [TREE] finish move of updater
    [TREE] Move the files to target refactor location
    [REFACTOR] cleanup structure
    spelling, wording, and doc fixes in c++ code
    
    I was reading through the code and fixing some things in the comments.
    Only a few trivial actual code changes were made to make things more
    readable.
    fix all utils
    compile with dmlc
    Handle duplicated values in sketching. (#6178)
    
    
    * Accumulate weights in duplicated values.
    * Fix device id in iterative dmatrix.
    Add high level tests for categorical data. (#6179)
    
    * Fix unique.
    Use default allocator in sketching. (#6182)
    Add categorical data support to GPU Hist. (#6164)
    Support categorical data in GPU sketching. (#6137)
    Merge extract cuts into QuantileContainer. (#6125)
    
    
    * Use pruning for initial summary construction.
    Unify CPU hist sketching (#5880)
    Fix sketch size calculation. (#5898)
    Cleanup on device sketch. (#5874)
    
    * Remove old functions.
    
    * Merge weighted and un-weighted into a common interface.
    fix device sketch with weights in external memory mode (#5870)
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Implement weighted sketching for adapter. (#5760)
    
    
    * Bounded memory tests.
    * Fixed memory estimation.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Expose device sketching in header. (#5747)
    Group aware GPU sketching. (#5551)
    
    * Group aware GPU weighted sketching.
    
    * Distribute group weights to each data point.
    * Relax the test.
    * Validate input meta info.
    * Fix metainfo copy ctor.
    Device dmatrix (#5420)
    Fix memory usage of device sketching (#5407)
    Sketching from adapters (#5365)
    
    * Sketching from adapters
    
    * Add weights test
    Wide dataset quantile performance improvement (#5306)
    Testing hist_util (#5251)
    
    * Rank tests
    
    * Remove categorical split specialisation
    
    * Extend tests to multiple features, switch to WQSketch
    
    * Add tests for SparseCuts
    
    * Add external memory quantile tests, fix some existing tests
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    remove device shards (#4867)
    Move ellpack page construction into DMatrix (#4833)
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Set the appropriate device before freeing device memory... (#4566)
    
    * - set the appropriate device before freeing device memory...
       - pr #4532 added a global memory tracker/logger to keep track of number of (de)allocations
         and peak memory usage on a per device basis.
       - this pr adds the appropriate check to make sure that the (de)allocation counts and memory usages
         makes sense for the device. since verbosity is typically increased on debug/non-retail builds.
    * - pre-create cub allocators and reuse them
       - create them once and not resize them dynamically. we need to ensure that these allocators
         are created and destroyed exactly once so that the appropriate device id's are set
    Overload device memory allocation (#4532)
    
    * Group source files, include headers in source files
    
    * Overload device memory allocation
    - training with external memory part 1 of 2 (#4486)
    
    * - training with external memory part 1 of 2
       - this pr focuses on computing the quantiles using multiple gpus on a
         dataset that uses the external cache capabilities
       - there will a follow-up pr soon after this that will support creation
         of histogram indices on large dataset as well
       - both of these changes are required to support training with external memory
       - the sparse pages in dmatrix are taken in batches and the the cut matrices
         are incrementally built
       - also snuck in some (perf) changes related to sketches aggregation amongst multiple
         features across multiple sparse page batches. instead of aggregating the summary
         inside each device and merged later, it is aggregated in-place when the device
         is working on different rows but the same feature
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Remove various synchronisations from cuda API calls, instrument monitor (#4205)
    
    * Remove various synchronisations from cuda API calls, instrument monitor
    with nvtx profiler ranges.
    Single precision histograms on GPU (#3965)
    
    * Allow single precision histogram summation in gpu_hist
    
    * Add python test, reduce run-time of gpu_hist tests
    
    * Update documentation
    GPU performance logging/improvements (#3945)
    
    - Improved GPU performance logging
    
    - Only use one execute shards function
    
    - Revert performance regression on multi-GPU
    
    - Use threads to launch NCCL AllReduce
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Implement devices to devices reshard. (#3721)
    
    * Force clearing device memory before Reshard.
    * Remove calculating row_segments for gpu_hist and gpu_sketch.
    * Guard against changing device.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Fixed issue 3605. (#3628)
    
    * Fixed issue 3605.
    
    - https://github.com/dmlc/xgboost/issues/3605
    
    * Fixed the bug in a better way.
    
    * Added a test to catch the bug.
    
    * Fixed linter errors.
    Merge generic device helper functions into gpu set. (#3626)
    
    * Remove the use of old NDevices* functions.
    * Use GPUSet in timer.h.
    Added finding quantiles on GPU. (#3393)
    
    * Added finding quantiles on GPU.
    
    - this includes datasets where weights are assigned to data rows
    - as the quantiles found by the new algorithm are not the same
      as those found by the old one, test thresholds in
        tests/python-gpu/test_gpu_updaters.py have been adjusted.
    
    * Adjustments and improved testing for finding quantiles on the GPU.
    
    - added C++ tests for the DeviceSketch() function
    - reduced one of the thresholds in test_gpu_updaters.py
    - adjusted the cuts found by the find_cuts_k kernel
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Workaround `isnan` across different environments. (#4883)
    Complete cudf support. (#4850)
    
    
    * Handles missing value.
    * Accept all floating point and integer types.
    * Move to cudf 9.0 API.
    * Remove requirement on `null_count`.
    * Arbitrary column types support.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    Allow plug-ins to be built by cmake (#3752)
    
    * Remove references to AVX code.
    
    * Allow plugins to be built by cmake
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    AVX gradients (#2878)
    
    * AVX gradients
    
    * Add google test for AVX
    
    * Create fallback implementation, remove fma instruction
    
    * Improved accuracy of AVX exp function
    [R] make all customizations to meet strict standard of cran
    [R] enable R compile
    
    [R] Enable R build for windows and linux
    [METRIC] all metric move finished
    [OBJ] Add basic objective function and registry
    Reduce compile warnings (#6198)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Move thread local entry into Learner. (#5396)
    
    * Move thread local entry into Learner.
    
    This is an attempt to workaround CUDA context issue in static variable, where
    the CUDA context can be released before device vector.
    
    * Add PredictionEntry to thread local entry.
    
    This eliminates one copy of prediction vector.
    
    * Don't define CUDA C API in a namespace.
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Catch exception in transform function omp context. (#4960)
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    More explict sharding methods for device memory (#4396)
    
    * Rename the Reshard method to Shard
    
    * Add a new Reshard method for sharding a vector that's already sharded
    fix R-devel errors (#4251)
    Remove various synchronisations from cuda API calls, instrument monitor (#4205)
    
    * Remove various synchronisations from cuda API calls, instrument monitor
    with nvtx profiler ranges.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
    Fix CRAN check warnings/notes (#3988)
    
    * fix
    
    * reorder declaration to match initialization
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Fix Windows 2016 build. (#5902)
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Add categorical data support to GPU predictor. (#6165)
    Updates to GPUTreeShap (#6087)
    
    * Extract paths on device
    
    * Update GPUTreeShap
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Move feature names and types of DMatrix from Python to C++. (#5858)
    
    
    * Add thread local return entry for DMatrix.
    * Save feature name and feature type in binary file.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Implement extend method for meta info. (#5800)
    
    * Implement extend for host device vector.
    Implement `Empty` method for host device vector. (#5781)
    
    * Fix accessing nullptr.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Partial rewrite EllpackPage (#5352)
    Lazy initialization of device vector. (#5173)
    
    * Lazy initialization of device vector.
    
    * Fix #5162.
    
    * Disable copy constructor of HostDeviceVector.  Prevents implicit copying.
    
    * Fix CPU build.
    
    * Bring back move assignment operator.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Span: use `size_t' for index_type,  add `front' and `back'. (#4935)
    
    * Use `size_t' for index_type.  Add `front' and `back'.
    
    * Remove a batch of `static_cast'.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    further cleanup of single process multi-GPU code (#4810)
    
    
    * use subspan in gpu predictor instead of copying
    * Revise `HostDeviceVector`
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Set the appropriate device before freeing device memory... (#4566)
    
    * - set the appropriate device before freeing device memory...
       - pr #4532 added a global memory tracker/logger to keep track of number of (de)allocations
         and peak memory usage on a per device basis.
       - this pr adds the appropriate check to make sure that the (de)allocation counts and memory usages
         makes sense for the device. since verbosity is typically increased on debug/non-retail builds.
    * - pre-create cub allocators and reuse them
       - create them once and not resize them dynamically. we need to ensure that these allocators
         are created and destroyed exactly once so that the appropriate device id's are set
    Offload some configurations into GBM. (#4553)
    
    
    This is part 1 of refactoring configuration.
    
    * Move tree heuristic configurations.
    * Split up declarations and definitions for GBTree.
    * Implement UseGPU in gbm.
    Overload device memory allocation (#4532)
    
    * Group source files, include headers in source files
    
    * Overload device memory allocation
    More explict sharding methods for device memory (#4396)
    
    * Rename the Reshard method to Shard
    
    * Add a new Reshard method for sharding a vector that's already sharded
    Improve HostDeviceVector exception safety (#4301)
    
    
    * make the assignments of HostDeviceVector exception safe.
    * storing a dummy GPUDistribution instance in HDV for CPU based code.
    * change testxgboost binary location to build directory.
    jenkins build for cuda 10.0 (#4281)
    
    * jenkins build for cuda 10.0
    
    * yum install nccl2 for cuda 10.0
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Remove various synchronisations from cuda API calls, instrument monitor (#4205)
    
    * Remove various synchronisations from cuda API calls, instrument monitor
    with nvtx profiler ranges.
    support cuda 10.1 (#4223)
    
    * support cuda 10.1
    
    * add cuda 10.1 to jenkins build matrix
    Use nccl group calls to prevent from dead lock. (#4113)
    
    * launch all reduce sequentially.
    * Fix gpu_exact test memory leak.
    GPU performance logging/improvements (#3945)
    
    - Improved GPU performance logging
    
    - Only use one execute shards function
    
    - Revert performance regression on multi-GPU
    
    - Use threads to launch NCCL AllReduce
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Implement devices to devices reshard. (#3721)
    
    * Force clearing device memory before Reshard.
    * Remove calculating row_segments for gpu_hist and gpu_sketch.
    * Guard against changing device.
    Fix gpu devices. (#3693)
    
    * Fix gpu_set normalized and unnormalized.
    * Fix DeviceSpan.
    Fixed copy constructor for HostDeviceVectorImpl. (#3657)
    
    - previously, vec_ in DeviceShard wasn't updated on copy; as a result,
      the shards continued to refer to the old HostDeviceVectorImpl object,
      which resulted in a dangling pointer once that object was deallocated
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Add google test for a column sampling, restore metainfo tests (#3637)
    
    * Add google test for a column sampling, restore metainfo tests
    
    * Update metainfo test for visual studio
    
    * Fix multi-GPU bug introduced in #3635
    GPU memory usage fixes + column sampling refactor (#3635)
    
    * Remove thrust copy calls
    
    * Fix  histogram memory usage
    
    * Cap extreme histogram memory usage
    
    * More efficient column sampling
    
    * Use column sampler across updaters
    
    * More efficient split evaluation on GPU with column sampling
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Multi-GPU HostDeviceVector. (#3287)
    
    * Multi-GPU HostDeviceVector.
    
    - HostDeviceVector instances can now span multiple devices, defined by GPUSet struct
    - the interface of HostDeviceVector has been modified accordingly
    - GPU objective functions are now multi-GPU
    - GPU predicting from cache is now multi-GPU
    - avoiding omp_set_num_threads() calls
    - other minor changes
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Feature weights (#5962)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Refactor the CLI. (#5574)
    
    
    * Enable parameter validation.
    * Enable JSON.
    * Catch `dmlc::Error`.
    * Show help message.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
     Combine thread launches into single launch per tree for gpu_hist (#4343)
    
    * Combine thread launches into single launch per tree for gpu_hist
    algorithm.
    
    * Address deprecation warning
    
    * Add manual column sampler constructor
    
    * Turn off omp dynamic to get a guaranteed number of threads
    
    * Enable openmp in cuda code
    Optimisations for gpu_hist. (#4248)
    
    * Optimisations for gpu_hist.
    
    * Use streams to overlap operations.
    
    * ColumnSampler now uses HostDeviceVector to prevent repeatedly copying feature vectors to the device.
    Improved multi-node multi-GPU random forests. (#4238)
    
    * Improved multi-node multi-GPU random forests.
    
    - removed rabit::Broadcast() from each invocation of column sampling
    - instead, syncing the PRNG seed when a ColumnSampler() object is constructed
    - this makes non-trivial column sampling significantly faster in the distributed case
    - refactored distributed GPU tests
    - added distributed random forests tests
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    Column sampling at individual nodes (splits). (#3971)
    
    * Column sampling at individual nodes (splits).
    
    * Documented colsample_bynode parameter.
    
    - also updated documentation for colsample_by* parameters
    
    * Updated documentation.
    
    * GetFeatureSet() returns shared pointer to std::vector.
    
    * Sync sampled columns across multiple processes.
    Fix #3523: Fix CustomGlobalRandomEngine for R (#3781)
    
    **Symptom** Apple Clang's implementation of `std::shuffle` expects doesn't work
    correctly when it is run with the random bit generator for R package:
    ```cpp
    CustomGlobalRandomEngine::result_type
    CustomGlobalRandomEngine::operator()() {
      return static_cast<result_type>(
          std::floor(unif_rand() * CustomGlobalRandomEngine::max()));
    }
    ```
    
    Minimial reproduction of failure (compile using Apple Clang 10.0):
    ```cpp
    std::vector<int> feature_set(100);
    std::iota(feature_set.begin(), feature_set.end(), 0);
        // initialize with 0, 1, 2, 3, ..., 99
    std::shuffle(feature_set.begin(), feature_set.end(), common::GlobalRandom());
        // This returns 0, 1, 2, ..., 99, so content didn't get shuffled at all!!!
    ```
    
    Note that this bug is platform-dependent; it does not appear when GCC or
    upstream LLVM Clang is used.
    
    **Diagnosis** Apple Clang's `std::shuffle` expects 32-bit integer
    inputs, whereas `CustomGlobalRandomEngine::operator()` produces 64-bit
    integers.
    
    **Fix** Have `CustomGlobalRandomEngine::operator()` produce 32-bit integers.
    
    Closes #3523.
    GPU memory usage fixes + column sampling refactor (#3635)
    
    * Remove thrust copy calls
    
    * Fix  histogram memory usage
    
    * Cap extreme histogram memory usage
    
    * More efficient column sampling
    
    * Use column sampler across updaters
    
    * More efficient split evaluation on GPU with column sampling
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    [R] make all customizations to meet strict standard of cran
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [TREE] Move colmaker
    [OBJ] Add basic objective function and registry
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Prevent copying SimpleDMatrix. (#5453)
    
    * Set default dtor for SimpleDMatrix to initialize default copy ctor, which is
    deleted due to unique ptr.
    
    * Remove commented code.
    * Remove warning for calling host function (std::max).
    * Remove warning for initialization order.
    * Remove warning for unused variables.
    Device dmatrix (#5420)
    Force compressed buffer to be 4 bytes aligned. (#5441)
    Partial rewrite EllpackPage (#5352)
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Add CMake option to run Undefined Behavior Sanitizer (UBSan) (#5211)
    
    * Fix related errors.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Added finding quantiles on GPU. (#3393)
    
    * Added finding quantiles on GPU.
    
    - this includes datasets where weights are assigned to data rows
    - as the quantiles found by the new algorithm are not the same
      as those found by the old one, test thresholds in
        tests/python-gpu/test_gpu_updaters.py have been adjusted.
    
    * Adjustments and improved testing for finding quantiles on the GPU.
    
    - added C++ tests for the DeviceSketch() function
    - reduced one of the thresholds in test_gpu_updaters.py
    - adjusted the cuts found by the find_cuts_k kernel
    GPU binning and compression. (#3319)
    
    * GPU binning and compression.
    
    - binning and index compression are done inside the DeviceShard constructor
    - in case of a DMatrix with multiple row batches, it is first converted into a single row batch
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    [GPU-Plugin] Fix gpu_hist to allow matrices with more than just 2^{32} elements. Also fixed CPU hist algorithm. (#2518)
    [GPU-Plugin] Add load balancing search to gpu_hist. Add compressed iterator. (#2504)
    Work around a compiler bug in MacOS AppleClang 11 (#6103)
    
    * Workaround a compiler bug in MacOS AppleClang
    
    * [CI] Run C++ test with MacOS Catalina + AppleClang 11.0.3
    
    * [CI] Migrate cmake_test on MacOS from Travis CI to GitHub Actions
    
    * Install OpenMP runtime
    
    * [CI] Use CMake to locate lz4 lib
    Unify CPU hist sketching (#5880)
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Fix non-openmp build. (#5566)
    
    
    * Add test to Jenkins.
    * Fix threading utils tests.
    * Require thread library.
    Optimized ApplySplit, BuildHist and UpdatePredictCache functions on CPU (#5244)
    
    * Split up sparse and dense build hist kernels.
    * Add `PartitionBuilder`.
    Optimized BuildHist function (#5156)
    Optimized EvaluateSplut function (#5138)
    
    
    * Add block based threading utilities.
    C++14 for xgboost (#5664)
    skip missing lookup if nothing is missing in CPU hist partition kernel. (#5644)
    
    * [xgboost] skip missing lookup if nothing is missing
    Resolve vector<bool>::iterator crash (#5642)
    Fix non-openmp build. (#5566)
    
    
    * Add test to Jenkins.
    * Fix threading utils tests.
    * Require thread library.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Reducing memory consumption for 'hist' method on CPU (#5334)
    Optimized ApplySplit, BuildHist and UpdatePredictCache functions on CPU (#5244)
    
    * Split up sparse and dense build hist kernels.
    * Add `PartitionBuilder`.
    Revert #4529 (#5008)
    
    * Revert " Optimize ‘hist’ for multi-core CPU (#4529)"
    
    This reverts commit 4d6590be3c9a043d44d9e4fe0a456a9f8179ec72.
    
    * Fix build
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
     Optimize ‘hist’ for multi-core CPU (#4529)
    
    * Initial performance optimizations for xgboost
    
    * remove includes
    
    * revert float->double
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * Check existence of _mm_prefetch and __builtin_prefetch
    
    * Fix lint
    
    * optimizations for CPU
    
    * appling comments in review
    
    * add some comments, code refactoring
    
    * fixing issues in CI
    
    * adding runtime checks
    
    * remove 1 extra check
    
    * remove extra checks in BuildHist
    
    * remove checks
    
    * add debug info
    
    * added debug info
    
    * revert changes
    
    * added comments
    
    * Apply suggestions from code review
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * apply review comments
    
    * Remove unused function CreateNewNodes()
    
    * Add descriptive comment on node_idx variable in QuantileHistMaker::Builder::BuildHistsBatch()
    Optimizations of pre-processing for 'hist' tree method (#4310)
    
    * oprimizations for pre-processing
    
    * code cleaning
    
    * code cleaning
    
    * code cleaning after review
    
    * Apply suggestions from code review
    
    Co-Authored-By: SmirnovEgorRu <egor.smirnov@intel.com>
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Address #4042: Prevent out-of-range access in column matrix (#4231)
    Refactor parts of fast histogram utilities (#3564)
    
    * Refactor parts of fast histogram utilities
    
    * Removed byte packing from column matrix
    Updates for GPU CI tests (#3467)
    
    * Fail GPU CI after test failure
    
    * Fix GPU linear tests
    
    * Reduced number of GPU tests to speed up CI
    
    * Remove static allocations of device memory
    
    * Resolve illegal memory access for updater_fast_hist.cc
    
    * Fix broken r tests dependency
    
    * Update python install documentation for GPU
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    [GPU-Plugin] Fix gpu_hist to allow matrices with more than just 2^{32} elements. Also fixed CPU hist algorithm. (#2518)
    Patch to improve multithreaded performance scaling (#2493)
    
    * Patch to improve multithreaded performance scaling
    
    Change parallel strategy for histogram construction.
    Instead of partitioning data rows among multiple threads, partition feature
    columns instead. Useful heuristics for assigning partitions have been adopted
    from LightGBM project.
    
    * Add missing header to satisfy MSVC
    
    * Restore max_bin and related parameters to TrainParam
    
    * Fix lint error
    
    * inline functions do not require static keyword
    
    * Feature grouping algorithm accepting FastHistParam
    
    Feature grouping algorithm accepts many parameters (3+), and it gets annoying to
    pass them one by one. Instead, simply pass the reference to FastHistParam. The
    definition of FastHistParam has been moved to a separate header file to
    accomodate this change.
    Improve multi-threaded performance (#2104)
    
    * Add UpdatePredictionCache() option to updaters
    
    Some updaters (e.g. fast_hist) has enough information to quickly compute
    prediction cache for the training data. Each updater may override
    UpdaterPredictionCache() method to update the prediction cache. Note: this
    trick does not apply to validation data.
    
    * Respond to code review
    
    * Disable some debug messages by default
    * Document UpdatePredictionCache() interface
    * Remove base_margin logic from UpdatePredictionCache() implementation
    * Do not take pointer to cfg, as reference may get stale
    
    * Improve multi-threaded performance
    
    * Use columnwise accessor to accelerate ApplySplit() step,
      with support for a compressed representation
    * Parallel sort for evaluation step
    * Inline BuildHist() function
    * Cache gradient pairs when building histograms in BuildHist()
    
    * Add missing #if macro
    
    * Respond to code review
    
    * Use wrapper to enable parallel sort on Linux
    
    * Fix C++ compatibility issues
    
    * MSVC doesn't support unsigned in OpenMP loops
    * gcc 4.6 doesn't support using keyword
    
    * Fix lint issues
    
    * Respond to code review
    
    * Fix bug in ApplySplitSparseData()
    
    * Attempting to read beyond the end of a sparse column
    * Mishandling the case where an entire range of rows have missing values
    
    * Fix training continuation bug
    
    Disable UpdatePredictionCache() in the first iteration. This way, we can
    accomodate the scenario where we build off of an existing (nonempty) ensemble.
    
    * Add regression test for fast_hist
    
    * Respond to code review
    
    * Add back old version of ApplySplitSparseData
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Enhance nvtx support. (#5636)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Remove use of std::cout from R package (#5261)
    Support FreeBSD (#5233)
    
    * Fix build on FreeBSD
    
    * Use __linux__ macro
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    monitor for distributed envorinment. (#4829)
    
    * Collect statistics from other ranks in monitor.
    
    * Workaround old GCC bug.
    Reduce compile warnings (#6198)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Implement robust regularization in 'survival:aft' objective (#5473)
    
    * Robust regularization of AFT gradient and hessian
    
    * Fix AFT doc; expose it to tutorial TOC
    
    * Apply robust regularization to uncensored case too
    
    * Revise unit test slightly
    
    * Fix lint
    
    * Update test_survival.py
    
    * Use GradientPairPrecise
    
    * Remove unused variables
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    GPU multiclass metrics (#4368)
    
    * Port multi classes metrics to CUDA.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [METRIC] change metric accumulator to double
    [LIBXGBOOST] pass demo running.
    [TREE] Enable updater registry
    [METRIC] all metric move finished
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Fix non-openmp build. (#5566)
    
    
    * Add test to Jenkins.
    * Fix threading utils tests.
    * Require thread library.
    Ranking metric acceleration on the gpu (#5398)
    - create a gpu metrics (internal) registry (#5387)
    
    * - create a gpu metrics (internal) registry
      - the objective is to separate the cpu and gpu implementations such that they evolve
        indepedently. to that end, this approach will:
        - preserve the same metrics configuration (from the end user perspective)
        - internally delegate the responsibility to the gpu metrics builder when there is a
          valid device present
        - decouple the gpu metrics builder from the cpu ones to prevent misuse
        - move away from including the cuda file from within the cc file and segregate the code
          via ifdef's
    Fixes and changes to the ranking metrics computed on cpu (#5380)
    
    * - fixes and changes to the ranking metrics computed on cpu
      - auc/aucpr ranking metric accelerated on cpu
      - fixes to the auc/aucpr metrics
    - ndcg ltr implementation on gpu (#5004)
    
    * - ndcg ltr implementation on gpu
      - this is a follow-up to the pairwise ltr implementation
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix auc error in distributed mode (#4798)
    
    
    Need more work for a complete fix.  See #4663 .
    fix auc error in distributed mode caused by unbalanced dataset (#4645)
    Support ndcg- and map- (#4635)
    In AUC and AUCPR metrics, detect whether weights are per-instance or per-group (#4216)
    
    * In AUC and AUCPR metrics, detect whether weights are per-instance or per-group
    
    * Fix C++ style check
    
    * Add a test for weighted AUC
    Make AUCPR work with multiple query groups (#4436)
    
    * Make AUCPR work with multiple query groups
    
    * Check AUCPR <= 1.0 in distributed setting
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    rank_metric: add AUC-PR (#3172)
    
    * rank_metric: add AUC-PR
    
    Implementation of the AUC-PR calculation for weighted data, proposed by Keilwagen, Grosse and Grau (https://doi.org/10.1371/journal.pone.0092209)
    
    * rank_metric: fix lint warnings
    
    * Implement tests for AUC-PR and fix implementation
    
    * add aucpr to documentation for other languages
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    Improve multi-threaded performance (#2104)
    
    * Add UpdatePredictionCache() option to updaters
    
    Some updaters (e.g. fast_hist) has enough information to quickly compute
    prediction cache for the training data. Each updater may override
    UpdaterPredictionCache() method to update the prediction cache. Note: this
    trick does not apply to validation data.
    
    * Respond to code review
    
    * Disable some debug messages by default
    * Document UpdatePredictionCache() interface
    * Remove base_margin logic from UpdatePredictionCache() implementation
    * Do not take pointer to cfg, as reference may get stale
    
    * Improve multi-threaded performance
    
    * Use columnwise accessor to accelerate ApplySplit() step,
      with support for a compressed representation
    * Parallel sort for evaluation step
    * Inline BuildHist() function
    * Cache gradient pairs when building histograms in BuildHist()
    
    * Add missing #if macro
    
    * Respond to code review
    
    * Use wrapper to enable parallel sort on Linux
    
    * Fix C++ compatibility issues
    
    * MSVC doesn't support unsigned in OpenMP loops
    * gcc 4.6 doesn't support using keyword
    
    * Fix lint issues
    
    * Respond to code review
    
    * Fix bug in ApplySplitSparseData()
    
    * Attempting to read beyond the end of a sparse column
    * Mishandling the case where an entire range of rows have missing values
    
    * Fix training continuation bug
    
    Disable UpdatePredictionCache() in the first iteration. This way, we can
    accomodate the scenario where we build off of an existing (nonempty) ensemble.
    
    * Add regression test for fast_hist
    
    * Respond to code review
    
    * Add back old version of ApplySplitSparseData
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    rank_metric.cc: Use GetWeight in EvalAMS
    
    The GetWeight is a wrapper which sets the correct weight
    if the weights vector is not provided. Hence accessing the default
    weights vector is not recommended.
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    correct CalcDCG in rank_metric.cc and rank_obj.cc (#1642)
    
    * correct CalcDCG in rank_metric.cc
    
    DCG use log base-2, however `std::log` returns log base-e.
    
    * correct CalcDCG in rank_obj.cc
    
    DCG use log base-2, however `std::log` returns log base-e.
    
    * use std::log2 instead of std::log
    
     make it more elegant
    
    * use std::log2 instead of std::log
    
    make it more elegant
    the fix of missing value assignment for name_ variable in EvalRankList method (#1558)
    no exception throwing within omp parallel; set nthread in Learner (#1421)
    [LIBXGBOOST] pass demo running.
    [TREE] Enable updater registry
    [METRIC] all metric move finished
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Fix compilation on Mac OSX High Sierra (10.13) (#5597)
    
    * Fix compilation on Mac OSX High Sierra
    
    * [CI] Build Mac OSX binary wheel using Travis CI
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Ranking metric acceleration on the gpu (#5398)
    - create a gpu metrics (internal) registry (#5387)
    
    * - create a gpu metrics (internal) registry
      - the objective is to separate the cpu and gpu implementations such that they evolve
        indepedently. to that end, this approach will:
        - preserve the same metrics configuration (from the end user perspective)
        - internally delegate the responsibility to the gpu metrics builder when there is a
          valid device present
        - decouple the gpu metrics builder from the cpu ones to prevent misuse
        - move away from including the cuda file from within the cc file and segregate the code
          via ifdef's
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Support ndcg- and map- (#4635)
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    GPU multiclass metrics (#4368)
    
    * Port multi classes metrics to CUDA.
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    [LIBXGBOOST] pass demo running.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    - create a gpu metrics (internal) registry (#5387)
    
    * - create a gpu metrics (internal) registry
      - the objective is to separate the cpu and gpu implementations such that they evolve
        indepedently. to that end, this approach will:
        - preserve the same metrics configuration (from the end user perspective)
        - internally delegate the responsibility to the gpu metrics builder when there is a
          valid device present
        - decouple the gpu metrics builder from the cpu ones to prevent misuse
        - move away from including the cuda file from within the cc file and segregate the code
          via ifdef's
    Fixes and changes to the ranking metrics computed on cpu (#5380)
    
    * - fixes and changes to the ranking metrics computed on cpu
      - auc/aucpr ranking metric accelerated on cpu
      - fixes to the auc/aucpr metrics
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    GPU multiclass metrics (#4368)
    
    * Port multi classes metrics to CUDA.
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Use caching allocator from RMM, when RMM is enabled (#6131)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Ranking metric acceleration on the gpu (#5398)
    Add MAPE metric (#6119)
    Move warning about empty dataset. (#5998)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Pseudo-huber loss metric added (#5647)
    
    
    - Add pseudo huber loss objective.
    - Add pseudo huber loss metric.
    
    Co-authored-by: Reetz <s02reetz@iavgroup.local>
    gpu_hist performance fixes (#5558)
    
    * Remove unnecessary cuda API calls
    
    * Fix histogram memory growth
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    further cleanup of single process multi-GPU code (#4810)
    
    
    * use subspan in gpu predictor instead of copying
    * Revise `HostDeviceVector`
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Set the appropriate device before freeing device memory... (#4566)
    
    * - set the appropriate device before freeing device memory...
       - pr #4532 added a global memory tracker/logger to keep track of number of (de)allocations
         and peak memory usage on a per device basis.
       - this pr adds the appropriate check to make sure that the (de)allocation counts and memory usages
         makes sense for the device. since verbosity is typically increased on debug/non-retail builds.
    * - pre-create cub allocators and reuse them
       - create them once and not resize them dynamically. we need to ensure that these allocators
         are created and destroyed exactly once so that the appropriate device id's are set
    Add `rmsle` metric and `reg:squaredlogerror` objective (#4541)
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    More explict sharding methods for device memory (#4396)
    
    * Rename the Reshard method to Shard
    
    * Add a new Reshard method for sharding a vector that's already sharded
    GPU multiclass metrics (#4368)
    
    * Port multi classes metrics to CUDA.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    [trivial] Fix typo in Poisson metric name. (#2026)
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Tweedie Regression Post-Rebase (#1737)
    
    * add support for tweedie regression
    
    * added back readme line that was accidentally deleted
    
    * fixed linting errors
    
    * add support for tweedie regression
    
    * added back readme line that was accidentally deleted
    
    * fixed linting errors
    
    * rebased with upstream master and added R example
    
    * changed parameter name to tweedie_variance_power
    
    * linting error fix
    
    * refactored tweedie-nloglik metric to be more like the other parameterized metrics
    
    * added upper and lower bound check to tweedie metric
    
    * add support for tweedie regression
    
    * added back readme line that was accidentally deleted
    
    * fixed linting errors
    
    * added upper and lower bound check to tweedie metric
    
    * added back readme line that was accidentally deleted
    
    * rebased with upstream master and added R example
    
    * rebased again on top of upstream master
    
    * linting error fix
    
    * added upper and lower bound check to tweedie metric
    
    * rebased with master
    
    * lint fix
    
    * removed whitespace at end of line 186 - elementwise_metric.cc
    Metrics for gamma regression (#1369)
    
    * Add deviance metric for gamma regression
    
    * Simplify the computation of nloglik for gamma regression
    
    * Add a description for gamma-deviance
    
    * Minor fix
    Add support for Gamma regression (#1258)
    
    * Add support for Gamma regression
    
    * Use base_score to replace the lp_bias
    
    * Remove the lp_bias config block
    
    * Add a demo for running gamma regression in Python
    
    * Typo fix
    
    * Revise the description for objective
    
    * Add a script to generate the autoclaims dataset
    ability to specify threshold for the error metric
    create mae
    [FIX] change evaluation to more precision
    [LIBXGBOOST] pass demo running.
    [TREE] Enable updater registry
    [METRIC] all metric move finished
    Dask device dmatrix (#5901)
    
    
    * Fix softprob with empty dmatrix.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    gpu_hist performance fixes (#5558)
    
    * Remove unnecessary cuda API calls
    
    * Fix histogram memory growth
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    further cleanup of single process multi-GPU code (#4810)
    
    
    * use subspan in gpu predictor instead of copying
    * Revise `HostDeviceVector`
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Set the appropriate device before freeing device memory... (#4566)
    
    * - set the appropriate device before freeing device memory...
       - pr #4532 added a global memory tracker/logger to keep track of number of (de)allocations
         and peak memory usage on a per device basis.
       - this pr adds the appropriate check to make sure that the (de)allocation counts and memory usages
         makes sense for the device. since verbosity is typically increased on debug/non-retail builds.
    * - pre-create cub allocators and reuse them
       - create them once and not resize them dynamically. we need to ensure that these allocators
         are created and destroyed exactly once so that the appropriate device id's are set
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    More explict sharding methods for device memory (#4396)
    
    * Rename the Reshard method to Shard
    
    * Add a new Reshard method for sharding a vector that's already sharded
    GPU multiclass metrics (#4368)
    
    * Port multi classes metrics to CUDA.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Config for linear updaters. (#5222)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Merge duplicated linear updater parameters. (#4013)
    
    * Merge duplicated linear updater parameters.
    
    * Split up coordinate descent parameter.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Prevent multiclass Hessian approaching 0 (#3304)
    
    * Prevent Hessian in multiclass objective becoming zero
    
    * Set default learning rate to 0.5 for "coord_descent" linear updater
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Merge duplicated linear updater parameters. (#4013)
    
    * Merge duplicated linear updater parameters.
    
    * Split up coordinate descent parameter.
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Config for linear updaters. (#5222)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Merge duplicated linear updater parameters. (#4013)
    
    * Merge duplicated linear updater parameters.
    
    * Split up coordinate descent parameter.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Remove accidental SparsePage copies (#3583)
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    gpu_hist performance fixes (#5558)
    
    * Remove unnecessary cuda API calls
    
    * Fix histogram memory growth
    Purge device_helpers.cuh (#5534)
    
    * Simplifications with caching_device_vector
    
    * Purge device helpers
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Remove unnecessary DMatrix methods (#5324)
    Config for linear updaters. (#5222)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    remove device shards (#4867)
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Set the appropriate device before freeing device memory... (#4566)
    
    * - set the appropriate device before freeing device memory...
       - pr #4532 added a global memory tracker/logger to keep track of number of (de)allocations
         and peak memory usage on a per device basis.
       - this pr adds the appropriate check to make sure that the (de)allocation counts and memory usages
         makes sense for the device. since verbosity is typically increased on debug/non-retail builds.
    * - pre-create cub allocators and reuse them
       - create them once and not resize them dynamically. we need to ensure that these allocators
         are created and destroyed exactly once so that the appropriate device id's are set
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Retire DVec class in favour of c++20 style span for device memory. (#4293)
    Fix multi-GPU test failures (#4259)
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Use Span in gpu coordinate. (#4029)
    
    * Use Span in gpu coordinate.
    
    * Use Span in device code.
    * Fix shard size calculation.
      - Use lower_bound instead of upper_bound.
    * Check empty devices.
    Merge duplicated linear updater parameters. (#4013)
    
    * Merge duplicated linear updater parameters.
    
    * Split up coordinate descent parameter.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    GPU performance logging/improvements (#3945)
    
    - Improved GPU performance logging
    
    - Only use one execute shards function
    
    - Revert performance regression on multi-GPU
    
    - Use threads to launch NCCL AllReduce
    Use consistent type for sharding GPU data in GPU coordinate updater (#3917)
    
    * Use consistent type for sharding GPU data in GPU coordinate updater
    
    * Use fast integer ceiling trick
    Fix gpu coordinate running on multi-gpu. (#3893)
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Fix gpu devices. (#3693)
    
    * Fix gpu_set normalized and unnormalized.
    * Fix DeviceSpan.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Merge generic device helper functions into gpu set. (#3626)
    
    * Remove the use of old NDevices* functions.
    * Use GPUSet in timer.h.
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Remove accidental SparsePage copies (#3583)
    Resolve GPU bug on large files (#3472)
    
    Remove calls to thrust copy, fix indexing bug
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Reduce compile warnings (#6198)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Merge duplicated linear updater parameters. (#4013)
    
    * Merge duplicated linear updater parameters.
    
    * Split up coordinate descent parameter.
    Document gblinear parameters: feature_selector and top_k (#3780)
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Remove accidental SparsePage copies (#3583)
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Merge duplicated linear updater parameters. (#4013)
    
    * Merge duplicated linear updater parameters.
    
    * Split up coordinate descent parameter.
    Validate weights are positive values. (#6115)
    Expand categorical node. (#6028)
    
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Optimize DMatrix build time. (#5877)
    
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Feature weights (#5962)
    [BLOCKING] Handle empty rows in data iterators correctly (#5929)
    
    * [jvm-packages] Handle empty rows in data iterators correctly
    
    * Fix clang-tidy error
    
    * last empty row
    
    * Add comments [skip ci]
    
    Co-authored-by: Nan Zhu <nanzhu@uber.com>
    Add explicit template specialization for portability (#5921)
    
    * Add explicit template specializations
    
    * Adding Specialization for FileAdapterBatch
    Dask device dmatrix (#5901)
    
    
    * Fix softprob with empty dmatrix.
    Move feature names and types of DMatrix from Python to C++. (#5858)
    
    
    * Add thread local return entry for DMatrix.
    * Save feature name and feature type in binary file.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Implement iterative DMatrix. (#5837)
    Implement extend method for meta info. (#5800)
    
    * Implement extend for host device vector.
    Avoid including `c_api.h` in header files. (#5782)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Fix build on big endian CPUs (#5617)
    
    * Fix build on big endian CPUs
    
    * Clang-tidy
    Set device in device dmatrix. (#5596)
    Group aware GPU sketching. (#5551)
    
    * Group aware GPU weighted sketching.
    
    * Distribute group weights to each data point.
    * Relax the test.
    * Validate input meta info.
    * Fix metainfo copy ctor.
    Fix slice and get info. (#5552)
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Prevent copying SimpleDMatrix. (#5453)
    
    * Set default dtor for SimpleDMatrix to initialize default copy ctor, which is
    deleted due to unique ptr.
    
    * Remove commented code.
    * Remove warning for calling host function (std::max).
    * Remove warning for initialization order.
    * Remove warning for unused variables.
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Add number of columns to native data iterator. (#5202)
    
    * Change native data iter into an adapter.
    Remove SimpleCSRSource (#5315)
    Extensible binary serialization format for DMatrix::MetaInfo (#5187)
    
    * Turn xgboost::DataType into C++11 enum class
    
    * New binary serialization format for DMatrix::MetaInfo
    
    * Fix clang-tidy
    
    * Fix c++ test
    
    * Implement new format proposal
    
    * Move helper functions to anonymous namespace; remove unneeded field
    
    * Fix lint
    
    * Add shape.
    
    * Keep only roundtrip test.
    
    * Fix test.
    
    * various fixes
    
    * Update data.cc
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Implement slice via adapters (#5198)
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    Use adapters for SparsePageDMatrix (#5092)
    [Breaking] Remove num roots. (#5059)
    External data adapters (#5044)
    
    * Use external data adapters as lightweight intermediate layer between external data and DMatrix
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Use configure_file() to configure version only (#4974)
    
    * Avoid writing build_config.h
    
    * Remove build_config.h all together.
    
    * Lint.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Fix data loading (#4862)
    
    * Fix loading text data.
    * Fix config regex.
    * Try to explain the error better in exception.
    * Update doc.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    remove the qids_ field in MetaInfo (#4744)
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix external memory for get column batches. (#4622)
    
    * Fix external memory for get column batches.
    
    This fixes two bugs:
    
    * Use PushCSC for get column batches.
    * Don't remove the created temporary directory before finishing test.
    
    * Check all pages.
    Fix crash with approx tree method on cpu (#4510)
    add a test for cpu predictor using external memory (#4308)
    
    * add a test for cpu predictor using external memory
    
    * allow different page size for testing
    Add PushCSC for SparsePage. (#4193)
    
    * Add PushCSC for SparsePage.
    
    * Move Push* definitions into cc file.
    * Add std:: prefix to `size_t` make clang++ happy.
    * Address monitor count == 0.
    Fix #4163: always copy sliced data (#4165)
    
    * Revert "Accept numpy array view. (#4147)"
    
    This reverts commit a985a99cf0dacb26a5d734835473d492d3c2a0df.
    
    * Fix #4163: always copy sliced data
    
    * Remove print() from the test; check shape equality
    
    * Check if 'base' attribute exists
    
    * Fix lint
    
    * Address reviewer comment
    
    * Fix lint
    Accept numpy array view. (#4147)
    
    * Accept array view (slice) in metainfo.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Add qid like ranklib format (#2749)
    
    * add qid for https://github.com/dmlc/xgboost/issues/2748
    
    * change names
    
    * change spaces
    
    * change qid to bst_uint type
    
    * change qid type to size_t
    
    * change qid first to SIZE_MAX
    
    * change qid type from size_t to uint64_t
    
    * update dmlc-core
    
    * fix qids name error
    
    * fix group_ptr_ error
    
    * Style fix
    
    * Add qid handling logic to SparsePage
    
    * New MetaInfo format + backward compatibility fix
    
    Old MetaInfo format (1.0) doesn't contain qid field. We still want to be able
    to read from MetaInfo files saved in old format. Also, define a new format
    (2.0) that contains the qid field. This way, we can distinguish files that
    contain qid and those that do not.
    
    * Update MetaInfo test
    
    * Simply group assignment logic
    
    * Explicitly set qid=nullptr in NativeDataIter
    
    NativeDataIter's callback does not support qid field. Users of NativeDataIter
    will need to call setGroup() function separately to set group information.
    
    * Save qids_ in SaveBinary()
    
    * Upgrade dmlc-core submodule
    
    * Add a test for reading qid
    
    * Add contributor
    
    * Check the size of qids_
    
    * Document qid format
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Sync number of features after loaded matrix in different workers. (#2722)
    Use int32_t explicitly when serializing version (#2389)
    
    Use int32_t explicitly when serializing version field of dmatrix in binary
    format. On ILP64 architectures, although very little, size of int is 64 bits.
    data.cc: Remove redundant ftype variable
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [DATA] fix instance weights loading
    [PYTHON-DIST] Distributed xgboost python training API.
    Update dmlc-core
    [APPROX] Make global proposal default, add group ptr solution
    [TREE] Enable global proposal for faster speed
    increase shard
    [DIST] fix distirbuted setting
    [PLUGIN] Add densify parser
    [TRAVIS] cleanup travis script
    [FIX] change evaluation to more precision
    [TEST] add partial load option
    [LZ4] enable 16 bit index
    [PLUGIN] Add plugin system
    [DATA] Isolate the format of page file
    [R] enable R compile
    
    [R] Enable R build for windows and linux
    [DATA] Make it fully compatible with rank
    [IO] Enable external memory
    [LIBXGBOOST] pass demo running.
    [OBJ] Add basic objective function and registry
    [DATA] basic data refactor done, basic version of csr source.
    latest data
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Partial rewrite EllpackPage (#5352)
    Remove SimpleCSRSource (#5315)
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Partial rewrite EllpackPage (#5352)
    Use DART tree weights when computing SHAPs (#5050)
    
    This PR fixes tree weights in dart being ignored when computing contributions.
    
    * Fix ellpack page source link.
    * Add tree weights to compute contribution.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Validate weights are positive values. (#6115)
    Feature weights (#5962)
    Dask device dmatrix (#5901)
    
    
    * Fix softprob with empty dmatrix.
    Accept string for ArrayInterface constructor. (#5799)
    Revert "Accept string for ArrayInterface constructor."
    
    This reverts commit e8ecafb8dc628f45b75b4c2844a236d27e0a6d98.
    Accept string for ArrayInterface constructor.
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    Avoid including `c_api.h` in header files. (#5782)
    Device dmatrix (#5420)
    Support dmatrix construction from cupy array (#5206)
    Implement cudf construction with adapters. (#5189)
    Use dynamic types for array interface columns instead of templates (#5108)
    Support feature names/types for cudf. (#4902)
    
    * Implement most of the pandas procedure for cudf except for type conversion.
    * Requires an array of interfaces in metainfo.
    Complete cudf support. (#4850)
    
    
    * Handles missing value.
    * Accept all floating point and integer types.
    * Move to cudf 9.0 API.
    * Remove requirement on `null_count`.
    * Arbitrary column types support.
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Simplify the data backends. (#5893)
    Implement a DMatrix Proxy. (#5803)
    Dask device dmatrix (#5901)
    
    
    * Fix softprob with empty dmatrix.
    Accept string for ArrayInterface constructor. (#5799)
    Revert "Accept string for ArrayInterface constructor."
    
    This reverts commit e8ecafb8dc628f45b75b4c2844a236d27e0a6d98.
    Accept string for ArrayInterface constructor.
    Don't use mask in array interface. (#5730)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Support dmatrix construction from cupy array (#5206)
    Implement cudf construction with adapters. (#5189)
    Use dynamic types for array interface columns instead of templates (#5108)
    Span: use `size_t' for index_type,  add `front' and `back'. (#4935)
    
    * Use `size_t' for index_type.  Add `front' and `back'.
    
    * Remove a batch of `static_cast'.
    Ignore columnar alignment requirement. (#4928)
    
    * Better error message for wrong type.
    * Fix stride size.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Complete cudf support. (#4850)
    
    
    * Handles missing value.
    * Accept all floating point and integer types.
    * Move to cudf 9.0 API.
    * Remove requirement on `null_count`.
    * Arbitrary column types support.
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Move device dmatrix construction code into ellpack. (#5623)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor SparsePageSource, delete cache files after use (#5321)
    
    * Refactor sparse page source
    
    * Delete temporary cache files
    
    * Log fatal if cache exists
    
    * Log fatal if multiple threads used with prefetcher
    Implement slice via adapters (#5198)
    Fix wrapping GPU ID and prevent data copying. (#5160)
    
    * Removed some data copying.
    
    * Make sure gpu_id is valid before any configuration is carried out.
    Use adapters for SparsePageDMatrix (#5092)
    Fix external memory race in colmaker. (#4980)
    
    
    * Move `GetColDensity` out of omp parallel block.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    remove the qids_ field in MetaInfo (#4744)
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Initial support for external memory in gpu_predictor (#4284)
    add a test for cpu predictor using external memory (#4308)
    
    * add a test for cpu predictor using external memory
    
    * allow different page size for testing
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    [DISK] Add shard option to disk
    [DATA] Isolate the format of page file
    [IO] Enable external memory
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    [PLUGIN] Add plugin system
    [DATA] Isolate the format of page file
    Loop over `thrust::reduce`. (#6229)
    
    * Check input chunk size of dqdm.
    * Add doc for current limitation.
    Dask device dmatrix (#5901)
    
    
    * Fix softprob with empty dmatrix.
    Accept iterator in device dmatrix. (#5783)
    
    
    * Remove Device DMatrix.
    Implement weighted sketching for adapter. (#5760)
    
    
    * Bounded memory tests.
    * Fixed memory estimation.
    Remove column major specialization. (#5755)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Don't use mask in array interface. (#5730)
    Move device dmatrix construction code into ellpack. (#5623)
    Use thrust functions instead of custom functions (#5544)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Device dmatrix (#5420)
    Support dmatrix construction from cupy array (#5206)
    Remove old cudf constructor code (#5194)
    Implement cudf construction with adapters. (#5189)
    Implement iterative DMatrix. (#5837)
    Avoid including `c_api.h` in header files. (#5782)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Fix slice and get info. (#5552)
    Sketching from adapters (#5365)
    
    * Sketching from adapters
    
    * Add weights test
    Add number of columns to native data iterator. (#5202)
    
    * Change native data iter into an adapter.
    Cleanup warnings. (#5247)
    
    From clang-tidy-9 and gcc-7: Invalid case style, narrowing definition, wrong
    initialization order, unused variables.
    Implement slice via adapters (#5198)
    Implement cudf construction with adapters. (#5189)
    Use adapters for SparsePageDMatrix (#5092)
    External data adapters (#5044)
    
    * Use external data adapters as lightweight intermediate layer between external data and DMatrix
    Handle duplicated values in sketching. (#6178)
    
    
    * Accumulate weights in duplicated values.
    * Fix device id in iterative dmatrix.
    Support categorical data in GPU sketching. (#6137)
    Dask device dmatrix (#5901)
    
    
    * Fix softprob with empty dmatrix.
    Cleanup on device sketch. (#5874)
    
    * Remove old functions.
    
    * Merge weighted and un-weighted into a common interface.
    Accept iterator in device dmatrix. (#5783)
    
    
    * Remove Device DMatrix.
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Implement iterative DMatrix. (#5837)
    Loop over copy_if (#6201)
    
    * Loop over copy_if
    
    * Catch OOM.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Remove column major specialization. (#5755)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Set device in device dmatrix. (#5596)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Device dmatrix (#5420)
    Remove SimpleCSRSource (#5315)
    Support dmatrix construction from cupy array (#5206)
    Implement cudf construction with adapters. (#5189)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Use adapters for SparsePageDMatrix (#5092)
    Fix external memory race in colmaker. (#4980)
    
    
    * Move `GetColDensity` out of omp parallel block.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Fix slice and get info. (#5552)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Prevent copying SimpleDMatrix. (#5453)
    
    * Set default dtor for SimpleDMatrix to initialize default copy ctor, which is
    deleted due to unique ptr.
    
    * Remove commented code.
    * Remove warning for calling host function (std::max).
    * Remove warning for initialization order.
    * Remove warning for unused variables.
    Remove unnecessary DMatrix methods (#5324)
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Remove SimpleCSRSource (#5315)
    Move SimpleDMatrix constructor to .cc file (#5188)
    Use adapters for SparsePageDMatrix (#5092)
    Group builder modified for incremental building (#5098)
    External data adapters (#5044)
    
    * Use external data adapters as lightweight intermediate layer between external data and DMatrix
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Move ellpack page construction into DMatrix (#4833)
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix external memory for get column batches. (#4622)
    
    * Fix external memory for get column batches.
    
    This fixes two bugs:
    
    * Use PushCSC for get column batches.
    * Don't remove the created temporary directory before finishing test.
    
    * Check all pages.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [MEM] Add rowset struct to save memory with billion level rows
    [LIBXGBOOST] pass demo running.
    Move ellpack page construction into DMatrix (#4833)
    Partial rewrite EllpackPage (#5352)
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Use DART tree weights when computing SHAPs (#5050)
    
    This PR fixes tree weights in dart being ignored when computing contributions.
    
    * Fix ellpack page source link.
    * Add tree weights to compute contribution.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Partial rewrite EllpackPage (#5352)
    Remove unnecessary DMatrix methods (#5324)
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Refactor SparsePageSource, delete cache files after use (#5321)
    
    * Refactor sparse page source
    
    * Delete temporary cache files
    
    * Log fatal if cache exists
    
    * Log fatal if multiple threads used with prefetcher
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Fix external memory race in colmaker. (#4980)
    
    
    * Move `GetColDensity` out of omp parallel block.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Move ellpack page construction into DMatrix (#4833)
    Enable natural copies of the batch iterators without the need of the clone method (#4748)
    
    - the synthesized copy constructor should do the appropriate job
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Initial support for external memory in gpu_predictor (#4284)
    Get rid of a few trivial compiler warnings. (#4312)
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Remove accidental SparsePage copies (#3583)
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Changing omp_get_num_threads to omp_get_max_threads (#1831)
    
    * Updating dmlc-core
    
    * Changing omp_get_num_threads to omp_get_max_threads
    [DATA] fix async data writing
    [MEM] Add rowset struct to save memory with billion level rows
    [DISK] Add shard option to disk
    [R] make all customizations to meet strict standard of cran
    [LZ4] enable 16 bit index
    [DATA] Isolate the format of page file
    [IO] Enable external memory
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Fix slice and get info. (#5552)
    Prevent copying SimpleDMatrix. (#5453)
    
    * Set default dtor for SimpleDMatrix to initialize default copy ctor, which is
    deleted due to unique ptr.
    
    * Remove commented code.
    * Remove warning for calling host function (std::max).
    * Remove warning for initialization order.
    * Remove warning for unused variables.
    Remove unnecessary DMatrix methods (#5324)
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Refactor SparsePageSource, delete cache files after use (#5321)
    
    * Refactor sparse page source
    
    * Delete temporary cache files
    
    * Log fatal if cache exists
    
    * Log fatal if multiple threads used with prefetcher
    Use adapters for SparsePageDMatrix (#5092)
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Move ellpack page construction into DMatrix (#4833)
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix external memory for get column batches. (#4622)
    
    * Fix external memory for get column batches.
    
    This fixes two bugs:
    
    * Use PushCSC for get column batches.
    * Don't remove the created temporary directory before finishing test.
    
    * Check all pages.
    Mark SparsePageDmatrix destructor default. (#4568)
    add a test for cpu predictor using external memory (#4308)
    
    * add a test for cpu predictor using external memory
    
    * allow different page size for testing
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Update sparse_page_dmatrix.h (#2139)
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [MEM] Add rowset struct to save memory with billion level rows
    [DISK] Add shard option to disk
    [PLUGIN] Add plugin system
    [DATA] Isolate the format of page file
    [R] enable R compile
    
    [R] Enable R build for windows and linux
    [IO] Enable external memory
    Support categorical data in ellpack. (#6140)
    Make binary bin search reusable. (#6058)
    
    * Move binary search row to hist util.
    * Remove dead code.
    Cleanup on device sketch. (#5874)
    
    * Remove old functions.
    
    * Merge weighted and un-weighted into a common interface.
    Implement iterative DMatrix. (#5837)
    Move device dmatrix construction code into ellpack. (#5623)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Device dmatrix (#5420)
    Partial rewrite EllpackPage (#5352)
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Fix external memory race in colmaker. (#4980)
    
    
    * Move `GetColDensity` out of omp parallel block.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Move ellpack page construction into DMatrix (#4833)
    Handle duplicated values in sketching. (#6178)
    
    
    * Accumulate weights in duplicated values.
    * Fix device id in iterative dmatrix.
    Implement a DMatrix Proxy. (#5803)
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [BLOCKING] Handle empty rows in data iterators correctly (#5929)
    
    * [jvm-packages] Handle empty rows in data iterators correctly
    
    * Fix clang-tidy error
    
    * last empty row
    
    * Add comments [skip ci]
    
    Co-authored-by: Nan Zhu <nanzhu@uber.com>
    Avoid including `c_api.h` in header files. (#5782)
    Fix slice and get info. (#5552)
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add number of columns to native data iterator. (#5202)
    
    * Change native data iter into an adapter.
    Remove unnecessary DMatrix methods (#5324)
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Remove SimpleCSRSource (#5315)
    Implement slice via adapters (#5198)
    Move SimpleDMatrix constructor to .cc file (#5188)
    Fix wrapping GPU ID and prevent data copying. (#5160)
    
    * Removed some data copying.
    
    * Make sure gpu_id is valid before any configuration is carried out.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Move ellpack page construction into DMatrix (#4833)
    Fix calling GPU predictor (#4836)
    
    * Fix calling GPU predictor
    Enable natural copies of the batch iterators without the need of the clone method (#4748)
    
    - the synthesized copy constructor should do the appropriate job
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Initial support for external memory in gpu_predictor (#4284)
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Remove accidental SparsePage copies (#3583)
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Changing omp_get_num_threads to omp_get_max_threads (#1831)
    
    * Updating dmlc-core
    
    * Changing omp_get_num_threads to omp_get_max_threads
    [MEM] Add rowset struct to save memory with billion level rows
    [LIBXGBOOST] pass demo running.
    Device dmatrix (#5420)
    Partial rewrite EllpackPage (#5352)
    Use DART tree weights when computing SHAPs (#5050)
    
    This PR fixes tree weights in dart being ignored when computing contributions.
    
    * Fix ellpack page source link.
    * Add tree weights to compute contribution.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Move ellpack page construction into DMatrix (#4833)
    Loop over `thrust::reduce`. (#6229)
    
    * Check input chunk size of dqdm.
    * Add doc for current limitation.
    Support categorical data in ellpack. (#6140)
    Cleanup on device sketch. (#5874)
    
    * Remove old functions.
    
    * Merge weighted and un-weighted into a common interface.
    Implement iterative DMatrix. (#5837)
    Remove column major specialization. (#5755)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Expose device sketching in header. (#5747)
    Enhance nvtx support. (#5636)
    Move device dmatrix construction code into ellpack. (#5623)
    Use thrust functions instead of custom functions (#5544)
    Device dmatrix (#5420)
    Fix memory usage of device sketching (#5407)
    Partial rewrite EllpackPage (#5352)
    Sketching from adapters (#5365)
    
    * Sketching from adapters
    
    * Add weights test
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Fix external memory race in colmaker. (#4980)
    
    
    * Move `GetColDensity` out of omp parallel block.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    remove device shards (#4867)
    Move ellpack page construction into DMatrix (#4833)
    Support categorical data in ellpack. (#6140)
    Expose device sketching in header. (#5747)
    Enhance nvtx support. (#5636)
    Move device dmatrix construction code into ellpack. (#5623)
    Fix memory usage of device sketching (#5407)
    Partial rewrite EllpackPage (#5352)
    Sketching from adapters (#5365)
    
    * Sketching from adapters
    
    * Add weights test
    Refactor SparsePageSource, delete cache files after use (#5321)
    
    * Refactor sparse page source
    
    * Delete temporary cache files
    
    * Log fatal if cache exists
    
    * Log fatal if multiple threads used with prefetcher
    Use DART tree weights when computing SHAPs (#5050)
    
    This PR fixes tree weights in dart being ignored when computing contributions.
    
    * Fix ellpack page source link.
    * Add tree weights to compute contribution.
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Avoid including `c_api.h` in header files. (#5782)
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Remove xgboost's thread_local and switch to dmlc::ThreadLocalStore (#2121)
    
    * Remove xgboost's own version of thread_local and switch to dmlc::ThreadLocalStore (#2109)
    
    * Update dmlc-core
    [LIBXGBOOST] pass demo running.
    Accept iterator in device dmatrix. (#5783)
    
    
    * Remove Device DMatrix.
    Remove column major specialization. (#5755)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Device dmatrix (#5420)
    Move thread local entry into Learner. (#5396)
    
    * Move thread local entry into Learner.
    
    This is an attempt to workaround CUDA context issue in static variable, where
    the CUDA context can be released before device vector.
    
    * Add PredictionEntry to thread local entry.
    
    This eliminates one copy of prediction vector.
    
    * Don't define CUDA C API in a namespace.
    Remove SimpleCSRSource (#5315)
    Support dmatrix construction from cupy array (#5206)
    Implement cudf construction with adapters. (#5189)
    Catch all standard exceptions in C API. (#6220)
    
    
    * `std::bad_alloc` is not guaranteed to be caught.
    Add CMake flag to log C API invocations, to aid debugging (#5925)
    
    * Add CMake flag to log C API invocations, to aid debugging
    
    * Remove unnecessary parentheses
    Avoid including `c_api.h` in header files. (#5782)
    Convert handle == nullptr from SegFault to user-friendly error. (#3021)
    
    * Convert SegFault to user-friendly error.
    
    * Apply the change to DMatrix API as well
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [LIBXGBOOST] pass demo running.
    Feature weights (#5962)
    Fix NDK Build. (#5886)
    
    * Explicit cast for slice.
    Add XGBoosterGetNumFeature (#5856)
    
    - add GetNumFeature to Learner
    - add XGBoosterGetNumFeature to C API
    - update c-api-demo accordingly
    Move feature names and types of DMatrix from Python to C++. (#5858)
    
    
    * Add thread local return entry for DMatrix.
    * Save feature name and feature type in binary file.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Implement iterative DMatrix. (#5837)
    Avoid including `c_api.h` in header files. (#5782)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Remove column major specialization. (#5755)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Clarify meaning of `training` parameter in XGBoosterPredict() (#5604)
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Fix slice and get info. (#5552)
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Fix dump model. (#5485)
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Move thread local entry into Learner. (#5396)
    
    * Move thread local entry into Learner.
    
    This is an attempt to workaround CUDA context issue in static variable, where
    the CUDA context can be released before device vector.
    
    * Add PredictionEntry to thread local entry.
    
    This eliminates one copy of prediction vector.
    
    * Don't define CUDA C API in a namespace.
    Add number of columns to native data iterator. (#5202)
    
    * Change native data iter into an adapter.
    Remove SimpleCSRSource (#5315)
    Pass shared pointer instead of raw pointer to Learner. (#5302)
    
    Extracted from https://github.com/dmlc/xgboost/pull/5220 .
    Cleanup warnings. (#5247)
    
    From clang-tidy-9 and gcc-7: Invalid case style, narrowing definition, wrong
    initialization order, unused variables.
    Fix compilation error due to 64-bit integer narrowing to size_t (#5250)
    Extensible binary serialization format for DMatrix::MetaInfo (#5187)
    
    * Turn xgboost::DataType into C++11 enum class
    
    * New binary serialization format for DMatrix::MetaInfo
    
    * Fix clang-tidy
    
    * Fix c++ test
    
    * Implement new format proposal
    
    * Move helper functions to anonymous namespace; remove unneeded field
    
    * Fix lint
    
    * Add shape.
    
    * Keep only roundtrip test.
    
    * Fix test.
    
    * various fixes
    
    * Update data.cc
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Support dmatrix construction from cupy array (#5206)
    Implement slice via adapters (#5198)
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Implement cudf construction with adapters. (#5189)
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Model IO in JSON. (#5110)
    Use adapters for SparsePageDMatrix (#5092)
    [Breaking] Remove num roots. (#5059)
    External data adapters (#5044)
    
    * Use external data adapters as lightweight intermediate layer between external data and DMatrix
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Use configure_file() to configure version only (#4974)
    
    * Avoid writing build_config.h
    
    * Remove build_config.h all together.
    
    * Lint.
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Deprecate set group (#4864)
    
    
    
    * Convert jvm package and R package.
    
    * Restore for compatibility.
    Complete cudf support. (#4850)
    
    
    * Handles missing value.
    * Accept all floating point and integer types.
    * Move to cudf 9.0 API.
    * Remove requirement on `null_count`.
    * Arbitrary column types support.
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Implement tree model dump with code generator. (#4602)
    
    * Implement tree model dump with a code generator.
    
    * Split up generators.
    * Implement graphviz generator.
    * Use pattern matching.
    
    * [Breaking] Return a Source in `to_graphviz` instead of Digraph in Python package.
    
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add support for cross-validation using query ID (#4474)
    
    * adding support for matrix slicing with query ID for cross-validation
    
    * hail mary test of unrar installation for windows tests
    
    * trying to modify tests to run in Github CI
    
    * Remove dependency on wget and unrar
    
    * Save error log from R test
    
    * Relax assertion in test_training
    
    * Use int instead of bool in C function interface
    
    * Revise R interface
    
    * Add XGDMatrixSliceDMatrixEx and keep old XGDMatrixSliceDMatrix for API compatibility
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Get rid of a few trivial compiler warnings. (#4312)
    Remove deprecated C APIs. (#4266)
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Fix #4163: always copy sliced data (#4165)
    
    * Revert "Accept numpy array view. (#4147)"
    
    This reverts commit a985a99cf0dacb26a5d734835473d492d3c2a0df.
    
    * Fix #4163: always copy sliced data
    
    * Remove print() from the test; check shape equality
    
    * Check if 'base' attribute exists
    
    * Fix lint
    
    * Address reviewer comment
    
    * Fix lint
    Accept numpy array view. (#4147)
    
    * Accept array view (slice) in metainfo.
    Fix #3342 and h2oai/h2o4gpu#625: Save predictor parameters in model file (#3856)
    
    * Fix #3342 and h2oai/h2o4gpu#625: Save predictor parameters in model file
    
    This allows pickled models to retain predictor attributes, such as
    'predictor' (whether to use CPU or GPU) and 'n_gpu' (number of GPUs
    to use). Related: h2oai/h2o4gpu#625
    
    Closes #3342.
    
    TODO. Write a test.
    
    * Fix lint
    
    * Do not load GPU predictor into CPU-only XGBoost
    
    * Add a test for pickling GPU predictors
    
    * Make sample data big enough to pass multi GPU test
    
    * Update test_gpu_predictor.cu
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Fix #3545: XGDMatrixCreateFromCSCEx silently discards empty trailing rows (#3553)
    
    * Fix #3545: XGDMatrixCreateFromCSCEx silently discards empty trailing rows
    
    Description: The bug is triggered when
    
    1. The data matrix has empty rows at the bottom. More precisely, the rows
       `n-k+1`, `n-k+2`, ..., `n` of the matrix have missing values in all
       dimensions (`n` number of instances, `k` number of trailing rows)
    2. The data matrix is given as Compressed Sparse Column (CSC) format.
    
    Diagnosis: When the CSC matrix is converted to Compressed Sparse Row (CSR)
    format (this is common format used for DMatrix), the trailing empty rows
    are silently ignored. More specifically, the row pointer (`offset`) of the
    newly created CSR matrix does not take account of these rows.
    
    Fix: Modify the row pointer.
    
    * Add regression test
    fix DMatrix load_row_split bug (#3431)
    Updates for GPU CI tests (#3467)
    
    * Fail GPU CI after test failure
    
    * Fix GPU linear tests
    
    * Reduced number of GPU tests to speed up CI
    
    * Remove static allocations of device memory
    
    * Resolve illegal memory access for updater_fast_hist.cc
    
    * Fix broken r tests dependency
    
    * Update python install documentation for GPU
    Add callback interface to re-direct console output (#3438)
    
    * Add callback interface to re-direct console output
    
    * Exempt TrackerLogger from custom logging
    
    * Fix lint
    Add qid like ranklib format (#2749)
    
    * add qid for https://github.com/dmlc/xgboost/issues/2748
    
    * change names
    
    * change spaces
    
    * change qid to bst_uint type
    
    * change qid type to size_t
    
    * change qid first to SIZE_MAX
    
    * change qid type from size_t to uint64_t
    
    * update dmlc-core
    
    * fix qids name error
    
    * fix group_ptr_ error
    
    * Style fix
    
    * Add qid handling logic to SparsePage
    
    * New MetaInfo format + backward compatibility fix
    
    Old MetaInfo format (1.0) doesn't contain qid field. We still want to be able
    to read from MetaInfo files saved in old format. Also, define a new format
    (2.0) that contains the qid field. This way, we can distinguish files that
    contain qid and those that do not.
    
    * Update MetaInfo test
    
    * Simply group assignment logic
    
    * Explicitly set qid=nullptr in NativeDataIter
    
    NativeDataIter's callback does not support qid field. Users of NativeDataIter
    will need to call setGroup() function separately to set group information.
    
    * Save qids_ in SaveBinary()
    
    * Upgrade dmlc-core submodule
    
    * Add a test for reading qid
    
    * Add contributor
    
    * Check the size of qids_
    
    * Document qid format
    Convert handle == nullptr from SegFault to user-friendly error. (#3021)
    
    * Convert SegFault to user-friendly error.
    
    * Apply the change to DMatrix API as well
    Allow import via python datatable. (#3272)
    
    * Allow import via python datatable.
    
    * Write unit tests
    
    * Refactor dt API functions
    
    * Refactor python code
    
    * Lint fixes
    
    * Address review comments
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Fix memory leak in XGDMatrixCreateFromMat_omp() (#3182)
    
    * Fix memory leak in XGDMatrixCreateFromMat_omp()
    
    This replaces the array allocated by new with a std::vector.
    
    Fixes #3161
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    c_api.cc variable declared inapproiate (#3044)
    
    In line 461, the "size_t offset = 0;" should be declared before any calculation, otherwise will cause compilation error.
    
    ```
    I:\Libraries\xgboost\src\c_api\c_api.cc(416): error C2146: Missing ";" before "offset" [I:\Libraries\xgboost\build\objxgboost.vcxproj]
    ```
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    MinGW: shared library prefix and appveyor CI (#2539)
    
    * for MinGW, drop the 'lib' prefix from shared library name
    
    * fix defines for 'g++ 4.8 or higher' to include g++ >= 5
    
    * fix compile warnings
    
    * [Appveyor] add MinGW with python; remove redundant jobs
    
    * [Appveyor] also do python build for one of msvc jobs
    Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation (#2530)
    
    * Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation from numpy arrays for python interface.
    Add prediction of feature contributions (#2003)
    
    * Add prediction of feature contributions
    
    This implements the idea described at http://blog.datadive.net/interpreting-random-forests/
    which tries to give insight in how a prediction is composed of its feature contributions
    and a bias.
    
    * Support multi-class models
    
    * Calculate learning_rate per-tree instead of using the one from the first tree
    
    * Do not rely on node.base_weight * learning_rate having the same value as the node mean value (aka leaf value, if it were a leaf); instead calculate them (lazily) on-the-fly
    
    * Add simple test for contributions feature
    
    * Check against param.num_nodes instead of checking for non-zero length
    
    * Loop over all roots instead of only the first
    Removes extraneous log (#2186)
    
    This log appears to fire every time I ask the python package to make a prediction. It's the only log that fires from XGBoost. When we're getting predictions on millions of items a day in production, this log seems out of place.
    Remove xgboost's thread_local and switch to dmlc::ThreadLocalStore (#2121)
    
    * Remove xgboost's own version of thread_local and switch to dmlc::ThreadLocalStore (#2109)
    
    * Update dmlc-core
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    adding a copy of base_margin to slice, fixes a bug where base_margin was notcopied during cross-validation (#2007)
    Automatically remove nan from input data when it is sparse. (#2062)
    
    * [DATALoad] Automatically remove Nan when load from sparse matrix
    
    * add log
    Changing omp_get_num_threads to omp_get_max_threads (#1831)
    
    * Updating dmlc-core
    
    * Changing omp_get_num_threads to omp_get_max_threads
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    c_api.cc: Bring back silent argument (#1794)
    
    In ecb3a271bed151252fb048528ce5a90ad75bb68f the silent argument
    in XGDMatrixCreateFromFile of c_api.cc was always overridden to
    be false. This disabled the functionality to hide log messages.
    
    This commit reverts that part to enable the hiding of log messages.
    fix build in MSVC 2013 (#1757)
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    fix for VX (#1614)
    More robust DMatrix creation from a sparse matrix (#1606)
    
    * [CORE] DMatrix from sparse w/ explicit #col #row; safer arg types
    
    * [python-package] c-api change for _init_from_csr _init_from_csc
    
    * fix spaces
    
    * [R-package] adopt the new XGDMatrixCreateFromCSCEx interface
    
    * [CORE] redirect old sparse creators to new ones
    [CORE] Refactor cache mechanism (#1540)
    Fix warnings from g++5 or higher (#1510)
    cmake build system (#1314)
    
    * Changed c api to compile under MSVC
    
    * Include functional.h header for MSVC
    
    * Add cmake build
    Fixes for multiple and default metric (#1239)
    
    * fix multiple evaluation metrics
    
    * create DefaultEvalMetric only when really necessary
    
    * py test for #1239
    
    * make travis happy
    fix multiple evaluation metrics
    methods to delete an attribute and get names of available attributes
    obey the lint
    avoid collecting duplicate parameters in Booster::cfg_
    XGBoosterCreate api unified to use const DMatrix[] argument
    [JVM] Add Iterator loading API
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [R] enable R compile
    
    [R] Enable R build for windows and linux
    [LIBXGBOOST] pass demo running.
    Thread-safe prediction by making the prediction cache thread-local. (#5853)
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    [WIP] Extract prediction into separate interface (#2531)
    
    * [WIP] Extract prediction into separate interface
    
    * Add copyright, fix linter errors
    
    * Add predictor to amalgamation
    
    * Fix documentation
    
    * Move prediction cache into predictor, add GBTreeModel
    
    * Updated predictor doc comments
    CPU predict performance improvement (#6127)
    
    Co-authored-by: ShvetsKS <kirill.shvets@intel.com>
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
    Unify CPU hist sketching (#5880)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Remove column major specialization. (#5755)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    [Breaking] Remove num roots. (#5059)
    Use DART tree weights when computing SHAPs (#5050)
    
    This PR fixes tree weights in dart being ignored when computing contributions.
    
    * Fix ellpack page source link.
    * Add tree weights to compute contribution.
    Improve operation efficiency for single predict (#5016)
    
    * Improve operation efficiency for single predict
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix calling GPU predictor (#4836)
    
    * Fix calling GPU predictor
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Remove accidental SparsePage copies (#3583)
    Fix #3505: Prevent undefined behavior due to incorrectly sized base_margin (#3555)
    
    The base margin will need to have length `[num_class] * [number of data points]`.
    Otherwise, the array holding prediction results will be only partially
    initialized, causing undefined behavior.
    
    Fix: check the length of the base margin. If the length is not correct,
    use the global bias (`base_score`) instead. Warn the user about the
    substitution.
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    [core] fix slow predict-caching with many classes (#3109)
    
    * fix prediction caching inefficiency for multiclass
    
    * silence some warnings
    
    * redundant if
    
    * workaround for R v3.4.3 bug; fixes #3081
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    [WIP] Extract prediction into separate interface (#2531)
    
    * [WIP] Extract prediction into separate interface
    
    * Add copyright, fix linter errors
    
    * Add predictor to amalgamation
    
    * Fix documentation
    
    * Move prediction cache into predictor, add GBTreeModel
    
    * Updated predictor doc comments
    Add categorical data support to GPU predictor. (#6165)
    Fix error message. (#6176)
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
    Upgrade gputreeshap. (#6099)
    
    * Upgrade gputreeshap.
    
    Co-authored-by: Rory Mitchell <r.a.mitchell.nz@gmail.com>
    Updates to GPUTreeShap (#6087)
    
    * Extract paths on device
    
    * Update GPUTreeShap
    Make binary bin search reusable. (#6058)
    
    * Move binary search row to hist util.
    * Remove dead code.
    GPUTreeShap (#6038)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Remove column major specialization. (#5755)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Partial rewrite EllpackPage (#5352)
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Lazy initialization of device vector. (#5173)
    
    * Lazy initialization of device vector.
    
    * Fix #5162.
    
    * Disable copy constructor of HostDeviceVector.  Prevents implicit copying.
    
    * Fix CPU build.
    
    * Bring back move assignment operator.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    [Breaking] Remove num roots. (#5059)
    Use DART tree weights when computing SHAPs (#5050)
    
    This PR fixes tree weights in dart being ignored when computing contributions.
    
    * Fix ellpack page source link.
    * Add tree weights to compute contribution.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    remove device shards (#4867)
    further cleanup of single process multi-GPU code (#4810)
    
    
    * use subspan in gpu predictor instead of copying
    * Revise `HostDeviceVector`
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Modify caching allocator/vector and fix issues relating to inability to train large datasets (#4615)
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    fix gpu predictor when dmatrix is mismatched with model (#4613)
    Overload device memory allocation (#4532)
    
    * Group source files, include headers in source files
    
    * Overload device memory allocation
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    only copy the model once when predicting multiple batches (#4457)
    mgpu predictor using explicit offsets (#4438)
    
    * mgpu prediction using explicit sharding
    Initial support for external memory in gpu_predictor (#4284)
    More explict sharding methods for device memory (#4396)
    
    * Rename the Reshard method to Shard
    
    * Add a new Reshard method for sharding a vector that's already sharded
     Combine thread launches into single launch per tree for gpu_hist (#4343)
    
    * Combine thread launches into single launch per tree for gpu_hist
    algorithm.
    
    * Address deprecation warning
    
    * Add manual column sampler constructor
    
    * Turn off omp dynamic to get a guaranteed number of threads
    
    * Enable openmp in cuda code
    Further optimisations for gpu_hist. (#4283)
    
    - Fuse final update position functions into a single more efficient kernel
    
    - Refactor gpu_hist with a more explicit ellpack  matrix representation
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Remove various synchronisations from cuda API calls, instrument monitor (#4205)
    
    * Remove various synchronisations from cuda API calls, instrument monitor
    with nvtx profiler ranges.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    GPU performance logging/improvements (#3945)
    
    - Improved GPU performance logging
    
    - Only use one execute shards function
    
    - Revert performance regression on multi-GPU
    
    - Use threads to launch NCCL AllReduce
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Multi-GPU support in GPUPredictor. (#3738)
    
    * Multi-GPU support in GPUPredictor.
    
    - GPUPredictor is multi-GPU
    - removed DeviceMatrix, as it has been made obsolete by using HostDeviceVector in DMatrix
    
    * Replaced pointers with spans in GPUPredictor.
    
    * Added a multi-GPU predictor test.
    
    * Fix multi-gpu test.
    
    * Fix n_rows < n_gpus.
    
    * Reinitialize shards when GPUSet is changed.
    * Tests range of data.
    
    * Remove commented code.
    
    * Remove commented code.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Fix gpu devices. (#3693)
    
    * Fix gpu_set normalized and unnormalized.
    * Fix DeviceSpan.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Merge generic device helper functions into gpu set. (#3626)
    
    * Remove the use of old NDevices* functions.
    * Use GPUSet in timer.h.
    Remove accidental SparsePage copies (#3583)
    Resolve GPU bug on large files (#3472)
    
    Remove calls to thrust copy, fix indexing bug
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Multi-GPU HostDeviceVector. (#3287)
    
    * Multi-GPU HostDeviceVector.
    
    - HostDeviceVector instances can now span multiple devices, defined by GPUSet struct
    - the interface of HostDeviceVector has been modified accordingly
    - GPU objective functions are now multi-GPU
    - GPU predicting from cache is now multi-GPU
    - avoiding omp_set_num_threads() calls
    - other minor changes
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Fix logic in GPU predictor cache lookup (#3217)
    
    * Fix logic in GPU predictor cache lookup
    
    * Add sklearn test for GPU prediction
    Fix bug with gpu_predictor caching behaviour (#3177)
    
    * Fixes #3162
    Fixed performance bug (#3171)
    
    Minor performance improvements to gpu predictor
    Added back UpdatePredictionCache() in updater_gpu_hist.cu. (#3120)
    
    * Added back UpdatePredictionCache() in updater_gpu_hist.cu.
    
    - it had been there before, but wasn't ported to the new version
      of updater_gpu_hist.cu
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Avoid repeated cuda API call in GPU predictor and only synchronize used GPUs (#2936)
    Fix several GPU bugs (#2916)
    
    * Fix #2905
    
    * Fix gpu_exact test failures
    
    * Fix bug in GPU prediction where multiple calls to batch prediction can produce incorrect results
    
    * Fix GPU documentation formatting
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix num_roots to be 1. (#5165)
    Model IO in JSON. (#5110)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    [Breaking] Remove num roots. (#5059)
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Several fixes (#2572)
    
    * repared serialization after update process; fixes #2545
    
    * non-stratified folds in python could omit some data instances
    
    * Makefile: fixes for older makes on windows; clean R-package too
    
    * make cub to be a shallow submodule
    
    * improve $(MAKE) recovery
    [GPU-Plugin] Various fixes (#2579)
    
    * Fix test large
    
    * Add check for max_depth 0
    
    * Update readme
    
    * Add LBS specialisation for dense data
    
    * Add bst_gpair_precise
    
    * Temporarily disable accuracy tests on test_large.py
    
    * Solve unused variable compiler warning
    
    * Fix max_bin > 1024 error
    [WIP] Extract prediction into separate interface (#2531)
    
    * [WIP] Extract prediction into separate interface
    
    * Add copyright, fix linter errors
    
    * Add predictor to amalgamation
    
    * Fix documentation
    
    * Move prediction cache into predictor, add GBTreeModel
    
    * Updated predictor doc comments
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [CORE] Refactor cache mechanism (#1540)
    [LIBXGBOOST] pass demo running.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Model IO in JSON. (#5110)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Model IO in JSON. (#5110)
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Pass shared pointer instead of raw pointer to Learner. (#5302)
    
    Extracted from https://github.com/dmlc/xgboost/pull/5220 .
    Config for linear updaters. (#5222)
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Model IO in JSON. (#5110)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    [Breaking] Remove num roots. (#5059)
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Restrict access to `cfg_` in gbm. (#4801)
    
    * Restrict access to `cfg_` in gbm.
    
    * Verify having correct updaters.
    
    * Remove `grow_global_histmaker`
    
    This updater is the same as `grow_histmaker`.  The former is not in our
    document so we just remove it.
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Offload some configurations into GBM. (#4553)
    
    
    This is part 1 of refactoring configuration.
    
    * Move tree heuristic configurations.
    * Split up declarations and definitions for GBTree.
    * Implement UseGPU in gbm.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Remove accidental SparsePage copies (#3583)
    Revert "Fix #3485, #3540: Don't use dropout for predicting test sets" (#3563)
    
    * Revert "Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)"
    
    This reverts commit 44811f233071c5805d70c287abd22b155b732727.
    
    * Document behavior of predict() for DART booster
    
    * Add notice to parameter.rst
    Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)
    
    * Fix #3485, #3540: Don't use dropout for predicting test sets
    
    Dropout (for DART) should only be used at training time.
    
    * Add regression test
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    Integer gradient summation for GPU histogram algorithm. (#2681)
    [WIP] Extract prediction into separate interface (#2531)
    
    * [WIP] Extract prediction into separate interface
    
    * Add copyright, fix linter errors
    
    * Add predictor to amalgamation
    
    * Fix documentation
    
    * Move prediction cache into predictor, add GBTreeModel
    
    * Updated predictor doc comments
    adding feature contributions to R and gblinear (#2295)
    
    * [gblinear] add features contribution prediction; fix DumpModel bug
    
    * [gbtree] minor changes to PredContrib
    
    * [R] add feature contribution prediction to R
    
    * [R] bump up version; update NEWS
    
    * [gblinear] fix the base_margin issue; fixes #1969
    
    * [R] list of matrices as output of multiclass feature contributions
    
    * [gblinear] make order of DumpModel coefficients consistent: group index changes the fastest
    Add prediction of feature contributions (#2003)
    
    * Add prediction of feature contributions
    
    This implements the idea described at http://blog.datadive.net/interpreting-random-forests/
    which tries to give insight in how a prediction is composed of its feature contributions
    and a bias.
    
    * Support multi-class models
    
    * Calculate learning_rate per-tree instead of using the one from the first tree
    
    * Do not rely on node.base_weight * learning_rate having the same value as the node mean value (aka leaf value, if it were a leaf); instead calculate them (lazily) on-the-fly
    
    * Add simple test for contributions feature
    
    * Check against param.num_nodes instead of checking for non-zero length
    
    * Loop over all roots instead of only the first
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    [CORE] Refactor cache mechanism (#1540)
    fix duplicate loop over output_group when predict (#1342)
    
    * fix sparse page source meta info empty when load from dmatrix
    
    * fix duplicate loop over output_group when predict
    [MEM] Add rowset struct to save memory with billion level rows
    [LIBXGBOOST] pass demo running.
    [GBM] remove need to explicit InitModel, rename save/load
    [LEARNER] Init learner interface
    [GBM] Finish migrate all gbms
    [Update] remove rabit subtree, use submodule, move code
    [REFACTOR] cleanup structure
    lint half way
    try fix memleak when test data have more features than training
    add single instance prediction
    linear text dump model
    add dump to linear model
    add rabit checkpoint to xgb
    add predict leaf indices
    new change for mpi
    remove using std from cpp
    add ntree limit
    complete refactor data.h, now replies on iterator to access column
    rename SparseBatch to RowBatch
    rename findex->index
    change omp loop var to bst_omp_uint, add XGB_DLL to wrapper
    chg
    fix line from auto spacing by msvc
    clean up warnings from msvc
    check in linear model
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
    Introducing DPC++-based plugin (predictor, objective function) supporting oneAPI programming model (#5825)
    
    * Added plugin with DPC++-based predictor and objective function
    
    * Update CMakeLists.txt
    
    * Update regression_obj_oneapi.cc
    
    * Added README.md for OneAPI plugin
    
    * Added OneAPI predictor support to gbtree
    
    * Update README.md
    
    * Merged kernels in gradient computation. Enabled multiple loss functions with DPC++ backend
    
    * Aligned plugin CMake files with latest master changes. Fixed whitespace typos
    
    * Removed debug output
    
    * [CI] Make oneapi_plugin a CMake target
    
    * Added tests for OneAPI plugin for predictor and obj. functions
    
    * Temporarily switched to default selector for device dispacthing in OneAPI plugin to enable execution in environments without gpus
    
    * Updated readme file.
    
    * Fixed USM usage in predictor
    
    * Removed workaround with explicit templated names for DPC++ kernels
    
    * Fixed warnings in plugin tests
    
    * Fix CMake build of gtest
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Better message when no GPU is found. (#5594)
    [Breaking] Set output margin to True for custom objective. (#5564)
    
    * Set output margin to True for custom objective in Python and R.
    
    * Add a demo for writing multi-class custom objective function.
    
    * Run tests on selected demos.
    Serialise booster after training to reset state (#5484)
    
    * Serialise booster after training to reset state
    
    * Prevent process_type being set on load
    
    * Check for correct updater sequence
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Device dmatrix (#5420)
    Better error message for updating. (#5418)
    Check whether current updater can modify a tree. (#5406)
    
    * Check whether current updater can modify a tree.
    
    * Fix tree model JSON IO for pruned trees.
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Fix R dart prediction. (#5204)
    
    * Fix R dart prediction and add test.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    Parameter validation (#5157)
    
    
    
    * Unused code.
    
    * Split up old colmaker parameters from train param.
    
    * Fix dart.
    
    * Better name.
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Model IO in JSON. (#5110)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    [Breaking] Remove num roots. (#5059)
    Use DART tree weights when computing SHAPs (#5050)
    
    This PR fixes tree weights in dart being ignored when computing contributions.
    
    * Fix ellpack page source link.
    * Add tree weights to compute contribution.
    Improve operation efficiency for single predict (#5016)
    
    * Improve operation efficiency for single predict
    Fix dart usegpu. (#4984)
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Restrict access to `cfg_` in gbm. (#4801)
    
    * Restrict access to `cfg_` in gbm.
    
    * Verify having correct updaters.
    
    * Remove `grow_global_histmaker`
    
    This updater is the same as `grow_histmaker`.  The former is not in our
    document so we just remove it.
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Prevent copying data to host. (#4795)
    Remove gpu_exact tree method (#4742)
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Choose the appropriate tree method *only* when the tree method is auto (#4571)
    
    * Remove redundant checks.
    Offload some configurations into GBM. (#4553)
    
    
    This is part 1 of refactoring configuration.
    
    * Move tree heuristic configurations.
    * Split up declarations and definitions for GBTree.
    * Implement UseGPU in gbm.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Document num_parallel_tree. (#4022)
    Combine TreeModel and RegTree (#3995)
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Multi-GPU support in GPUPredictor. (#3738)
    
    * Multi-GPU support in GPUPredictor.
    
    - GPUPredictor is multi-GPU
    - removed DeviceMatrix, as it has been made obsolete by using HostDeviceVector in DMatrix
    
    * Replaced pointers with spans in GPUPredictor.
    
    * Added a multi-GPU predictor test.
    
    * Fix multi-gpu test.
    
    * Fix n_rows < n_gpus.
    
    * Reinitialize shards when GPUSet is changed.
    * Tests range of data.
    
    * Remove commented code.
    
    * Remove commented code.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Remove accidental SparsePage copies (#3583)
    Revert "Fix #3485, #3540: Don't use dropout for predicting test sets" (#3563)
    
    * Revert "Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)"
    
    This reverts commit 44811f233071c5805d70c287abd22b155b732727.
    
    * Document behavior of predict() for DART booster
    
    * Add notice to parameter.rst
    Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)
    
    * Fix #3485, #3540: Don't use dropout for predicting test sets
    
    Dropout (for DART) should only be used at training time.
    
    * Add regression test
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Multi-GPU HostDeviceVector. (#3287)
    
    * Multi-GPU HostDeviceVector.
    
    - HostDeviceVector instances can now span multiple devices, defined by GPUSet struct
    - the interface of HostDeviceVector has been modified accordingly
    - GPU objective functions are now multi-GPU
    - GPU predicting from cache is now multi-GPU
    - avoiding omp_set_num_threads() calls
    - other minor changes
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    [core] fix slow predict-caching with many classes (#3109)
    
    * fix prediction caching inefficiency for multiclass
    
    * silence some warnings
    
    * redundant if
    
    * workaround for R v3.4.3 bug; fixes #3081
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    [WIP] Extract prediction into separate interface (#2531)
    
    * [WIP] Extract prediction into separate interface
    
    * Add copyright, fix linter errors
    
    * Add predictor to amalgamation
    
    * Fix documentation
    
    * Move prediction cache into predictor, add GBTreeModel
    
    * Updated predictor doc comments
    [gbtree] fix update process to work with multiclass and multitree; fixes #2315 (#2332)
    adding feature contributions to R and gblinear (#2295)
    
    * [gblinear] add features contribution prediction; fix DumpModel bug
    
    * [gbtree] minor changes to PredContrib
    
    * [R] add feature contribution prediction to R
    
    * [R] bump up version; update NEWS
    
    * [gblinear] fix the base_margin issue; fixes #1969
    
    * [R] list of matrices as output of multiclass feature contributions
    
    * [gblinear] make order of DumpModel coefficients consistent: group index changes the fastest
    Add prediction of feature contributions (#2003)
    
    * Add prediction of feature contributions
    
    This implements the idea described at http://blog.datadive.net/interpreting-random-forests/
    which tries to give insight in how a prediction is composed of its feature contributions
    and a bias.
    
    * Support multi-class models
    
    * Calculate learning_rate per-tree instead of using the one from the first tree
    
    * Do not rely on node.base_weight * learning_rate having the same value as the node mean value (aka leaf value, if it were a leaf); instead calculate them (lazily) on-the-fly
    
    * Add simple test for contributions feature
    
    * Check against param.num_nodes instead of checking for non-zero length
    
    * Loop over all roots instead of only the first
    Improve multi-threaded performance (#2104)
    
    * Add UpdatePredictionCache() option to updaters
    
    Some updaters (e.g. fast_hist) has enough information to quickly compute
    prediction cache for the training data. Each updater may override
    UpdaterPredictionCache() method to update the prediction cache. Note: this
    trick does not apply to validation data.
    
    * Respond to code review
    
    * Disable some debug messages by default
    * Document UpdatePredictionCache() interface
    * Remove base_margin logic from UpdatePredictionCache() implementation
    * Do not take pointer to cfg, as reference may get stale
    
    * Improve multi-threaded performance
    
    * Use columnwise accessor to accelerate ApplySplit() step,
      with support for a compressed representation
    * Parallel sort for evaluation step
    * Inline BuildHist() function
    * Cache gradient pairs when building histograms in BuildHist()
    
    * Add missing #if macro
    
    * Respond to code review
    
    * Use wrapper to enable parallel sort on Linux
    
    * Fix C++ compatibility issues
    
    * MSVC doesn't support unsigned in OpenMP loops
    * gcc 4.6 doesn't support using keyword
    
    * Fix lint issues
    
    * Respond to code review
    
    * Fix bug in ApplySplitSparseData()
    
    * Attempting to read beyond the end of a sparse column
    * Mishandling the case where an entire range of rows have missing values
    
    * Fix training continuation bug
    
    Disable UpdatePredictionCache() in the first iteration. This way, we can
    accomodate the scenario where we build off of an existing (nonempty) ensemble.
    
    * Add regression test for fast_hist
    
    * Respond to code review
    
    * Add back old version of ApplySplitSparseData
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Histogram Optimized Tree Grower (#1940)
    
    * Support histogram-based algorithm + multiple tree growing strategy
    
    * Add a brand new updater to support histogram-based algorithm, which buckets
      continuous features into discrete bins to speed up training. To use it, set
      `tree_method = fast_hist` to configuration.
    * Support multiple tree growing strategies. For now, two policies are supported:
      * `grow_policy=depthwise` (default):  favor splitting at nodes closest to the
        root, i.e. grow depth-wise.
      * `grow_policy=lossguide`: favor splitting at nodes with highest loss change
    * Improve single-threaded performance
      * Unroll critical loops
      * Introduce specialized code for dense data (i.e. no missing values)
    * Additional training parameters: `max_leaves`, `max_bin`, `grow_policy`, `verbose`
    
    * Adding a small test for hist method
    
    * Fix memory error in row_set.h
    
    When std::vector is resized, a reference to one of its element may become
    stale. Any such reference must be updated as well.
    
    * Resolve cross-platform compilation issues
    
    * Versions of g++ older than 4.8 lacks support for a few C++11 features, e.g.
      alignas(*) and new initializer syntax. To support g++ 4.6, use pre-C++11
      initializer and remove alignas(*).
    * Versions of MSVC older than 2015 does not support alignas(*). To support
      MSVC 2012, remove alignas(*).
    * For g++ 4.8 and newer, alignas(*) is enabled for performance benefits.
    * Some old compilers (MSVC 2012, g++ 4.6) do not support template aliases
      (which uses `using` to declate type aliases). So always use `typedef`.
    
    * Fix a host of CI issues
    
    * Remove dependency for libz on osx
    * Fix heading for hist_util
    * Fix minor style issues
    * Add missing #include
    * Remove extraneous logging
    
    * Enable tree_method=hist in R
    
    * Renaming HistMaker to GHistBuilder to avoid confusion
    
    * Fix R integration
    
    * Respond to style comments
    
    * Consistent tie-breaking for priority queue using timestamps
    
    * Last-minute style fixes
    
    * Fix issuecomment-271977647
    
    The way we quantize data is broken. The agaricus data consists of all
    categorical values. When NAs are converted into 0's,
    `HistCutMatrix::Init` assign both 0's and 1's to the same single bin.
    
    Why? gmat only the smallest value (0) and an upper bound (2), which is twice
    the maximum value (1). Add the maximum value itself to gmat to fix the issue.
    
    * Fix issuecomment-272266358
    
    * Remove padding from cut values for the continuous case
    * For categorical/ordinal values, use midpoints as bin boundaries to be safe
    
    * Fix CI issue -- do not use xrange(*)
    
    * Fix corner case in quantile sketch
    
    Signed-off-by: Philip Cho <chohyu01@cs.washington.edu>
    
    * Adding a test for an edge case in quantile sketcher
    
    max_bin=2 used to cause an exception.
    
    * Fix fast_hist test
    
    The test used to require a strictly increasing Test AUC for all examples.
    One of them exhibits a small blip in Test AUC before achieving a Test AUC
    of 1. (See bottom.)
    
    Solution: do not require monotonic increase for this particular example.
    
    [0] train-auc:0.99989 test-auc:0.999497
    [1] train-auc:1 test-auc:0.999749
    [2] train-auc:1 test-auc:0.999749
    [3] train-auc:1 test-auc:0.999749
    [4] train-auc:1 test-auc:0.999749
    [5] train-auc:1 test-auc:0.999497
    [6] train-auc:1 test-auc:1
    [7] train-auc:1 test-auc:1
    [8] train-auc:1 test-auc:1
    [9] train-auc:1 test-auc:1
    An option for doing binomial+1 or epsilon-dropout from DART paper (#1922)
    
    * An option for doing binomial+1 or epsilon-dropout from DART paper
    
    * use callback-based discrete_distribution to make MSVC2013 happy
    fix dart bug (#1882)
    Changing omp_get_num_threads to omp_get_max_threads (#1831)
    
    * Updating dmlc-core
    
    * Changing omp_get_num_threads to omp_get_max_threads
    [CORE] The update process for a tree model, and its application to feature importance (#1670)
    
    * [CORE] allow updating trees in an existing model
    
    * [CORE] in refresh updater, allow keeping old leaf values and update stats only
    
    * [R-package] xgb.train mod to allow updating trees in an existing model
    
    * [R-package] added check for nrounds when is_update
    
    * [CORE] merge parameter declaration changes; unify their code style
    
    * [CORE] move the update-process trees initialization to Configure; rename default process_type to 'default'; fix the trees and trees_to_update sizes comparison check
    
    * [R-package] unit tests for the update process type
    
    * [DOC] documentation for process_type parameter; improved docs for updater, Gamma and Tweedie; added some parameter aliases; metrics indentation and some were non-documented
    
    * fix my sloppy merge conflict resolutions
    
    * [CORE] add a TreeProcessType enum
    
    * whitespace fix
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    [TREE] Experimental version of monotone constraint (#1516)
    
    * [TREE] Experimental version of monotone constraint
    
    * Allow default detection of montone option
    
    * loose the condition of strict check
    
    * Update gbtree.cc
    [CORE] Refactor cache mechanism (#1540)
    no exception throwing within omp parallel; set nthread in Learner (#1421)
    fix Dart::NormalizeTrees (#1265)
    add Dart booster (#1220)
    Fix continue training in CLI
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    [MEM] Add rowset struct to save memory with billion level rows
    [LIBXGBOOST] pass demo running.
    [LEARNER] refactor learner
    [GBM] remove need to explicit InitModel, rename save/load
    [LEARNER] Init learner interface
    [GBM] Finish migrate all gbms
    [Update] remove rabit subtree, use submodule, move code
    [REFACTOR] cleanup structure
    spelling, wording, and doc fixes in c++ code
    
    I was reading through the code and fixing some things in the comments.
    Only a few trivial actual code changes were made to make things more
    readable.
    style fix
    GBTree::Predict performance fix: removed excess thread_temp initialization
    GBTree::Predict performance fix: removed excess thread_temp initialization
    lint half way
    add with pbuffer info to model, allow xgb model to be saved in a more memory compact way
    fix platform dependent thing
    quick fix
    fix
    some potential fix
    add single instance prediction
    change makefile to lazy checkpt, fix col splt code
    pas mock, need to fix rabit lib for not initialization
    add rabit checkpoint to xgb
    remove warning from MSVC need another round of check
    add predict leaf indices
    checkin continue training
    new change for mpi
    fix
    try predpath
    remove using std from cpp
    add ntree limit
    complete refactor data.h, now replies on iterator to access column
    rename SparseBatch to RowBatch
    change omp loop var to bst_omp_uint, add XGB_DLL to wrapper
    clean up warnings from msvc
    initial correction for vec tree
    add changes
    check in linear model
    chg root index to booster info, need review
    change dense fvec logic to tree
    add base_margin
    fix base score, and print message
    fix omp
    first version that reproduce binary classification demo
    pass fmatrix as const
    mv code into src
    start unity refactor
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
    GPUTreeShap (#6038)
    Introducing DPC++-based plugin (predictor, objective function) supporting oneAPI programming model (#5825)
    
    * Added plugin with DPC++-based predictor and objective function
    
    * Update CMakeLists.txt
    
    * Update regression_obj_oneapi.cc
    
    * Added README.md for OneAPI plugin
    
    * Added OneAPI predictor support to gbtree
    
    * Update README.md
    
    * Merged kernels in gradient computation. Enabled multiple loss functions with DPC++ backend
    
    * Aligned plugin CMake files with latest master changes. Fixed whitespace typos
    
    * Removed debug output
    
    * [CI] Make oneapi_plugin a CMake target
    
    * Added tests for OneAPI plugin for predictor and obj. functions
    
    * Temporarily switched to default selector for device dispacthing in OneAPI plugin to enable execution in environments without gpus
    
    * Updated readme file.
    
    * Fixed USM usage in predictor
    
    * Removed workaround with explicit templated names for DPC++ kernels
    
    * Fixed warnings in plugin tests
    
    * Fix CMake build of gtest
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Remove distcol updater. (#5507)
    
    Closes #5498.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Model IO in JSON. (#5110)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    [Breaking] Remove num roots. (#5059)
    Use DART tree weights when computing SHAPs (#5050)
    
    This PR fixes tree weights in dart being ignored when computing contributions.
    
    * Fix ellpack page source link.
    * Add tree weights to compute contribution.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix calling GPU predictor (#4836)
    
    * Fix calling GPU predictor
    Restrict access to `cfg_` in gbm. (#4801)
    
    * Restrict access to `cfg_` in gbm.
    
    * Verify having correct updaters.
    
    * Remove `grow_global_histmaker`
    
    This updater is the same as `grow_histmaker`.  The former is not in our
    document so we just remove it.
    Remove gpu_exact tree method (#4742)
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Offload some configurations into GBM. (#4553)
    
    
    This is part 1 of refactoring configuration.
    
    * Move tree heuristic configurations.
    * Split up declarations and definitions for GBTree.
    * Implement UseGPU in gbm.
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Enforce tree order in JSON. (#5974)
    
    
    * Make JSON model IO more future proof by using tree id in model loading.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Small refinements for JSON model. (#5112)
    
    * Naming consistency.
    
    * Remove duplicated test.
    Model IO in JSON. (#5110)
    [R] Fix CRAN error for Mac OS X (#4672)
    
    * fix cran error for mac os x
    
    * ignore float on windows check for now
    Bump up version number, add cleanup script (#1886)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    
    * fix cran check
    
    * add cleanup and bump up version number
    
    * use clean in build
    
    * Update Makefile
    For CRAN submission, remove all #pragma's that suppress compiler warnings (#3329)
    
    * For CRAN submission, remove all #pragma's that suppress compiler warnings
    
    A few headers in dmlc-core contain #pragma's that disable compiler warnings,
    which is against the CRAN submission policy. Fix the problem by removing
    the offending #pragma's as part of the command `make Rbuild`.
    
    This addresses issue #3322.
    
    * Fix script to improve Cygwin/MSYS compatibility
    
    We need this to pass rmingw CI test
    
    * Remove remove_warning_suppression_pragma.sh from packaged tarball
    fix namespace and desc
    Roxygen update
    add Rbuildignore to avoid compile .o files
    modify licence and desc to standard format
    fix numpy convert
    chg license, README
    update license
    update license
    Initial commit
    Add option to enable all compiler warnings in GCC/Clang (#5897)
    
    * Add option to enable all compiler warnings in GCC/Clang
    
    * Fix -Wall for CUDA sources
    
    * Make -Wall private req for xgboost-r
    Fix R package build with CMake 3.13 (#5895)
    
    * Fix R package build with CMake 3.13
    
    * Require OpenMP for xgboost-r target
    [CI] Simplify CMake build with modern CMake techniques (#5871)
    
    * [CI] Simplify CMake build
    
    * Make sure that plugins can be built
    
    * [CI] Install lz4 on Mac
    [R] Remove dependency on gendef for Visual Studio builds (fixes #5608) (#5764)
    
    * [R-package] Remove dependency on gendef for Visual Studio builds (fixes #5608)
    
    * clarify docs
    
    * removed debugging print statement
    
    * Make R CMake install more robust
    
    * Fix doc format; add ToC
    
    * Update build.rst
    
    * Fix AppVeyor
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    C++14 for xgboost (#5664)
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Specify version macro in CMake. (#4730)
    
    * Specify version macro in CMake.
    
    * Use `XGBOOST_DEFINITIONS` instead.
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Update R contribute link. (#4236)
    fix typo in README (#3263)
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    Fix typo in R-package README.md (#2190)
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    simplify installation of R pkg devel version (#1653)
    Updated obsolete installation instructions
    
    Fixed local compilation, and installation for R package and Python
    package. Modified the according documents.
    Update README.md
    [Doc] documents update:
    
    (1) install_github is not support due to the usage of submodule
    
    (2) remove part of the markdown which is not displayed correctly, see
    https://xgboost.readthedocs.org/en/latest/R-package/discoverYourData.html
    Update installation instructions for R package
    [DOC] Update R doc
    [R] make all customizations to meet strict standard of cran
    Update README.md
    Document refactor
    
    change badge
    quick fix of solaris problem in cranc check
    Adding workaround for install the R-package
    
    I was facing this issue and this workaround worked for me. Maybe this should be moved to know issues section.
    Update README.md
    ref in README
    Update README.md
    Update README.md
    space
    Update README.md
    Username parameter is deprecated in install_function (see doc of the package for more information).
    add
    ok
    Update README.md
    Update README.md
    Update README.md
    Update README.md
    refine readme.md
    final revision before CRAN
    modify readme in R-package
    export fewer functions to user and optimize parameter setting
    import package methods in desc
    initial trial package
    [R] remove warning in configure.ac (fixes #6151) (#6152)
    
    * [R] remove warning in configure.ac (fixes #6151)
    
    * update configure
    [R] Fix duplicated libomp.dylib error on Mac OSX (#5701)
    [R-package] Reduce duplication in configure.ac (#5693)
    
    
    * updated configure
    [R] Enable OpenMP with AppleClang in XGBoost R package (#5240)
    
    * [R] Enable OpenMP with AppleClang in XGBoost R package
    
    * Dramatically simplify install doc
    [R] Robust endian detection in CRAN xgboost build (#5232)
    
    * [R] Robust endian detection in CRAN xgboost build
    
    * Check for external backtrace() lib
    
    * Update Makevars.win
    Fix CRAN check warnings/notes (#3988)
    
    * fix
    
    * reorder declaration to match initialization
    fix #2228 (#2238)
    disable openmp on solaris (#1912)
    [R Package] Use the C++ 11 compiler to test OpenMP flags (#1881)
    
    * fix segfault when gctorture() is enabled
    
    * use the C++ 11 compiler to test OpenMP flags
    
    * auto-generated configure script
    autoconf for solaris (#1880)
    [core] fix slow predict-caching with many classes (#3109)
    
    * fix prediction caching inefficiency for multiclass
    
    * silence some warnings
    
    * redundant if
    
    * workaround for R v3.4.3 bug; fixes #3081
    [R] remove warning in configure.ac (fixes #6151) (#6152)
    
    * [R] remove warning in configure.ac (fixes #6151)
    
    * update configure
    Enable building rabit on Windows (#6105)
    [R] Fix duplicated libomp.dylib error on Mac OSX (#5701)
    [R-package] Reduce duplication in configure.ac (#5693)
    
    
    * updated configure
    [R] Enable OpenMP with AppleClang in XGBoost R package (#5240)
    
    * [R] Enable OpenMP with AppleClang in XGBoost R package
    
    * Dramatically simplify install doc
    [R] Robust endian detection in CRAN xgboost build (#5232)
    
    * [R] Robust endian detection in CRAN xgboost build
    
    * Check for external backtrace() lib
    
    * Update Makevars.win
    Fix CRAN check warnings/notes (#3988)
    
    * fix
    
    * reorder declaration to match initialization
    fix #2228 (#2238)
    disable openmp on solaris (#1912)
    [R Package] Use the C++ 11 compiler to test OpenMP flags (#1881)
    
    * fix segfault when gctorture() is enabled
    
    * use the C++ 11 compiler to test OpenMP flags
    
    * auto-generated configure script
    autoconf for solaris (#1880)
    [R] Remove stringi dependency (#6109)
    
    * [R] Fix empty empty tests and a test warnings
    
    * [R] Remove stringi dependency (fix #5905)
    
    * Fix R lint check
    
    * [R] Fix automatic conversion to factor in R < 4.0.0 in xgb.model.dt.tree
    
    * Add `R` Makefile variable
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    Bump version to 1.3.0 snapshot in master (#6052)
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    Cache dependencies on Github Action. (#5928)
    Bump version to 1.2.0 snapshot in master (#5733)
    C++14 for xgboost (#5664)
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    Update R doc by roxygen2. (#5201)
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    [R] Fix CRAN error for Mac OS X (#4672)
    
    * fix cran error for mac os x
    
    * ignore float on windows check for now
    upgrade version num (#4670)
    
    * upgrade version num
    
    * missign changes
    
    * fix version script
    
    * change versions
    
    * rm files
    
    * Update CMakeLists.txt
    [R] Use built-in label when xgb.DMatrix is given to xgb.cv() (#4631)
    
    * Use built-in label when xgb.DMatrix is given to xgb.cv()
    
    * Add a test
    
    * Fix test
    
    * Bump version number
    fix R-devel errors (#4251)
    [R] R-interface for SHAP interactions (#3636)
    
    * add R-interface for SHAP interactions
    
    * update docs for new roxygen version
    Release version 0.80 (#3541)
    
    * Up versions
    
    * Write release note for 0.80
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    CRAN Submission for 0.71.1 (#3311)
    
    * fix for CRAN manual checks
    
    * fix for CRAN manual checks
    
    * pass local check
    
    * fix variable naming style
    
    * Adding Philip's record
    Release version 0.71 (#3200)
    Change DESCRIPTION to more modern look (#3179)
    
    So other things can be added in comment field, such as ORCID.
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    A fix for CRAN submission of version 0.7-0 (#3061)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    [R] fix for the 32 bit windows issue (#2994)
    
    * [R] disable thred_local for 32bit windows
    
    * [R] require C++11 and GNU make in DESCRIPTION
    
    * [R] enable 32+64 build and check in appveyor
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    [R] xgb.importance: fix for multiclass gblinear, new 'trees' parameter (#2388)
    adding feature contributions to R and gblinear (#2295)
    
    * [gblinear] add features contribution prediction; fix DumpModel bug
    
    * [gbtree] minor changes to PredContrib
    
    * [R] add feature contribution prediction to R
    
    * [R] bump up version; update NEWS
    
    * [gblinear] fix the base_margin issue; fixes #1969
    
    * [R] list of matrices as output of multiclass feature contributions
    
    * [gblinear] make order of DumpModel coefficients consistent: group index changes the fastest
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    0.6-4 submission (#1935)
    [R] Increase the version number, date and required R version (#1920)
    
    * remove unnecessary line
    Bump up version number, add cleanup script (#1886)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    
    * fix cran check
    
    * add cleanup and bump up version number
    
    * use clean in build
    
    * Update Makefile
    Bump up the date of R package (#1813)
    Fix the "No visible binding" CRAN checks (#1504)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    Tag version 0.6 (#1422)
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    Update DESCRIPTION (#1348)
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    added name to DESCRIPTION
    fix cran, update version to 0.4-3
    fix for Travis
    fix for Travis
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    add test module in R
    Document refactor
    
    change badge
    modify desc
    fix namespace and desc
    update date
    update to 0.4
    upgrade DiagrammeR to fix a bug in v 0.5
    moved the external graphing packages to Suggested in order to trim the dependencies
    moved the external graphing packages to Suggested in order to trim the dependencies
    update links dmlc
    change version of the package
    change order of sentences
    
    Dear Prof. Ripley said that "The Description field should not start with the package name, 'This package' or similar."
    import vcd to eliminate note
    Simplified my name :-)
    improve function documentation.
    
    Switch xgboost detailed parameters with xgb.tain function.
    Update CK.means version
    version stringr
    small change in package version
    Update DESCRIPTION
    Update DESCRIPTION
    new plot feature importance function
    change in Description
    fix mermaid
    fix some issues from the cran check
    resolving not-CRAN issues
    Add `vcd` to the dependencies
    add new dependency on DiagrammeR
    new dependency over stringr
    change version number + date
    Add new dependency
    Update DESCRIPTION
    improvement for reducing warnings
    Update DESCRIPTION
    Update DESCRIPTION
    Update DESCRIPTION
    fix doc with redirection to inst/examples
    Update DESCRIPTION
    final revision before CRAN
    move demo to inst/examples
    Update DESCRIPTION
    Update DESCRIPTION
    check in description
    add license name
    fix print problem, fix Tong's email format
    Update DESCRIPTION
    add import methods in NAMESPACE
    add back import of methdos
    eliminate warnings and notes from R CMD check
    modify licence and desc to standard format
    Update DESCRIPTION
    Update DESCRIPTION
    modify xgb.getinfo to getinfo
    import package methods in desc
    initial trial package
    [R] Remove stringi dependency (#6109)
    
    * [R] Fix empty empty tests and a test warnings
    
    * [R] Remove stringi dependency (fix #5905)
    
    * Fix R lint check
    
    * [R] Fix automatic conversion to factor in R < 4.0.0 in xgb.model.dt.tree
    
    * Add `R` Makefile variable
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Add configuration to R interface. (#5217)
    
    * Save and load internal parameter configuration as JSON.
    Replace cBind by cbind (#3203)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    
    * change Matrix cBind to cbind
    
    * fix
    
    * fix error in demo
    
    * fix examples
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    Fix the "No visible binding" CRAN checks (#1504)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] consolidate importFrom-s; parameter style
    R-callbacks docs
    xgb.model.dt.tree up to x100 faster
    [R] more attribute handling functionality
    R accessors for model attributes
    convert S4 to S3; add some extra methods to DMatrix
    Generate new features based on tree leafs
    Fix missing dependencies
    Fix missing dependencies
    add exclusion of global variables + generate Roxygen doc
    add exclusion of global variables + generate Roxygen doc
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Remove DiagrammeR dependency to make travis happy...
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    moved the external graphing packages to Suggested in order to trim the dependencies
    moved the external graphing packages to Suggested in order to trim the dependencies
    new nrow function for xgb.DMatrix
    add vcd back
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    import vcd to eliminate note
    missing feature management
    better co occurence function
    fix bug
    documentation simplification
    new included feature in dt.tree function
    add saveload to raw
    doc
    new plot feature importance function
    fix mermaid
    edit document
    fix plenty of small bugs
    small change in import lib
    refactor dump function to adapt to the new possibilities of exporting a String
    add new function to read model and use it in the plot function
    Add stat indicators in plot
    fix import issue
    new documentation
    new import
    Add a new verbose parameter to print progress during the process (set to true by default to not change behavior of existing code) + source code refactoring
    parse history first line to guess which columns are required
    fix some missing imports
    return history as data.table for cross validation + documentation
    new dependency over stringr
    generated documentation with ROxygen2
    Update Namespace with new function
    refine doc, with Rd
    add boost from prediction
    add cross validation
    add import methods in NAMESPACE
    fix NAMESPACE with import classes
    checkin slice
    fix NAMESPACE
    compile Rd files, i.e. R documents
    hide xgb.Boost
    change higgs script, remove R wrapper
    modify xgb.getinfo to getinfo
    refinement of R package
    major change in the design of R interface
    fix NAMESPACE with export method predict
    export fewer functions to user and optimize parameter setting
    import package methods in desc
    runnable
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    Remove gpu_exact tree method (#4742)
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    add poisson demo
    [R] fix uses of 1:length(x) and other small things (#5992)
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [TREE] add interaction constraints (#3466)
    
    * add interaction constraints
    
    * enable both interaction and monotonic constraints at the same time
    
    * fix lint
    
    * add R test, fix lint, update demo
    
    * Use dmlc::JSONReader to express interaction constraints as nested lists; Use sparse arrays for bookkeeping
    
    * Add Python test for interaction constraints
    
    * make R interaction constraints parameter based on feature index instead of column names, fix R coding style
    
    * Fix lint
    
    * Add BlueTea88 to CONTRIBUTORS.md
    
    * Short circuit when no constraint is specified; address review comments
    
    * Add tutorial for feature interaction constraints
    
    * allow interaction constraints to be passed as string, remove redundant column_names argument
    
    * Fix typo
    
    * Address review comments
    
    * Add comments to Python test
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    [R] adopt demos and vignettes to a more consistent parameter style
    Fixing duplicate params in demo
    
    Issue in "demo(package="xgboost", custom_objective)"
    
    > bst <- xgb.train(param, dtrain, num_round, watchlist,
    +                  objective=logregobj, eval_metric=evalerror)
    Error in xgb.train(param, dtrain, num_round, watchlist, objective = logregobj,  :
      Duplicated term in parameters. Please check your list of params.
    modify script to use objective and eval_metric
    change doc and demo for new obj feval interface
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    add example with additional attr
     change max depth
    add customize objective
    custom eval
    in the middle of guide-r
    move python example
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    Remove silent from R demos. (#5675)
    
    * Remove silent from R demos.
    
    * Vignettes.
    replace nround with nrounds to match actual parameter (#3592)
    [R] adopt demos and vignettes to a more consistent parameter style
    Cleaning of demo
    Cleaning of demo
    modify script to use objective and eval_metric
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    Fix bug in Cross Validation when showsd = FALSE
    improve demo of cv in R
     change max depth
    ok
    fix R check warning (#3728)
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    DOC: Added new demo to index
    rename demo of early stopping
    add poisson demo
    fix logic
    resolving not-CRAN issues
    fix demo index
    Add new demo
    remove runall
    ok
    move demo to inst/examples
    eliminate warnings and notes from R CMD check
    add 00Index
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Tweedie Regression Post-Rebase (#1737)
    
    * add support for tweedie regression
    
    * added back readme line that was accidentally deleted
    
    * fixed linting errors
    
    * add support for tweedie regression
    
    * added back readme line that was accidentally deleted
    
    * fixed linting errors
    
    * rebased with upstream master and added R example
    
    * changed parameter name to tweedie_variance_power
    
    * linting error fix
    
    * refactored tweedie-nloglik metric to be more like the other parameterized metrics
    
    * added upper and lower bound check to tweedie metric
    
    * add support for tweedie regression
    
    * added back readme line that was accidentally deleted
    
    * fixed linting errors
    
    * added upper and lower bound check to tweedie metric
    
    * added back readme line that was accidentally deleted
    
    * rebased with upstream master and added R example
    
    * rebased again on top of upstream master
    
    * linting error fix
    
    * added upper and lower bound check to tweedie metric
    
    * rebased with master
    
    * lint fix
    
    * removed whitespace at end of line 186 - elementwise_metric.cc
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    DOC: Added caret_wrapper.R link into demo/README.md
    Spell
    Add new demo
    remove unneeded text...
    Spell
    R demo code README
    Update README.md
    ok
    add boost from prediction
    add glm
    add cv for python
    fix doc
    ok
    push python examples in
    move python example
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    ok
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [R] adopt demos and vignettes to a more consistent parameter style
    Cleaning of demo
    Cleaning of demo
    fixed some typos in demos comments
    need to load vcd if it was freshly installed
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    resolving not-CRAN issues
    rewording
    text change
    Small text improvement
    Improve explanation, add new concepts.
    change assignation sign
    Add new demo
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    CRAN Submission for 0.71.1 (#3311)
    
    * fix for CRAN manual checks
    
    * fix for CRAN manual checks
    
    * pass local check
    
    * fix variable naming style
    
    * Adding Philip's record
    [R] adopt demos and vignettes to a more consistent parameter style
    Fixed typos.
    Cleaning of demo
    Cleaning of demo
    Implement #431 PR
    Implement #431 PR
    fixed typos in basic_walkthrough demo
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    add saveload to raw
    add saveload to raw
    Update wlkthrough R demo code to include variable importance.
    ok
    add basic walkthrough
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    Remove silent from R demos. (#5675)
    
    * Remove silent from R demos.
    
    * Vignettes.
    replace nround with nrounds to match actual parameter (#3592)
    [R] adopt demos and vignettes to a more consistent parameter style
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
     change max depth
    ok
    change data from iris back to mushroom
    custom eval
    in the middle of guide-r
    move python example
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    Remove silent from R demos. (#5675)
    
    * Remove silent from R demos.
    
    * Vignettes.
    [R] adopt demos and vignettes to a more consistent parameter style
    Cleaning of demo
    Cleaning of demo
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
     change max depth
    add boost from prediction
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    in `caret` settings, if you want do 10*10 cross validation, you need to set repeats=10, number=10 and method=repeatedcv, (#2061)
    
    if you set method=cv, actually just one 10-fold cross validation will be run; fixes #2055
    ENH: More comments and explanation on demo using xgboost from caret
    ENH/DOC: Added R package demo using caret library to train xgbTree model
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    Remove silent from R demos. (#5675)
    
    * Remove silent from R demos.
    
    * Vignettes.
    Make the train and test input with same colnames. (#4329)
    
    Fix the bug report of https://github.com/dmlc/xgboost/issues/4328.
    I am the beginner of the Git so just try my best to follows the guide, https://xgboost.readthedocs.io/en/latest/contribute.html#r-package.
    I find there is no `dev`  branch, so I pull this fix from my master branch to the original master branch.
    replace nround with nrounds to match actual parameter (#3592)
    Replace cBind by cbind (#3203)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    
    * change Matrix cBind to cbind
    
    * fix
    
    * fix error in demo
    
    * fix examples
    Fixed loop bound in create.new.tree.features (#2328)
    
    for loop in create.new.tree.features was referencing length(trees) as the upper bound of the loop. trees is a base R dataset and not the model that the code is generating. Changed loop boundary to model$niter which should be the number of trees.
    [R] adopt demos and vignettes to a more consistent parameter style
    Generate new features based on tree leafs
    remove intersect column in sparse Matrix
    Add code to demo of leaf (show imprmt in accuracy)
    Cleaning of demo
    Cleaning of demo
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    fix bug in format of input
    add leaf example in R
     change max depth
    ok
    change data from iris back to mushroom
    custom eval
    in the middle of guide-r
    move python example
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    [R] adopt demos and vignettes to a more consistent parameter style
    modify script to use objective and eval_metric
    rename demo of early stopping
    fix early stopping
    add early stopping to xgb.cv
    add demo for early_stopping in R
    fix logic
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    add example with additional attr
     change max depth
    add customize objective
    custom eval
    in the middle of guide-r
    move python example
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    DOC: Added new demo to index
    rename demo of early stopping
    add poisson demo
    Update runall.R
    improve runall.R
    ok
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Add configuration to R interface. (#5217)
    
    * Save and load internal parameter configuration as JSON.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Fix typo in xgboost_R.h (#4432)
    [R-package] JSON dump format and a couple of bugfixes (#1855)
    
    * [R-package] JSON tree dump interface
    
    * [R-package] precision bugfix in xgb.attributes
    
    * [R-package] bugfix for cb.early.stop called from xgb.cv
    
    * [R-package] a bit more clarity on labels checking in xgb.cv
    
    * [R-package] test JSON dump for gblinear as well
    
    * whitespace lint
    More robust DMatrix creation from a sparse matrix (#1606)
    
    * [CORE] DMatrix from sparse w/ explicit #col #row; safer arg types
    
    * [python-package] c-api change for _init_from_csr _init_from_csc
    
    * fix spaces
    
    * [R-package] adopt the new XGDMatrixCreateFromCSCEx interface
    
    * [CORE] redirect old sparse creators to new ones
    [R] more attribute handling functionality
    fix attribute accessors C-interface for R
    learner attribute setter & getter for R interface
    .Call-interface functions need to return SEXP
    added XGDMatrixNumCol_R function
    fix cran, update version to 0.4-3
    [R] enable R compile
    
    [R] Enable R build for windows and linux
    fix all cpp lint
    add poisson regression
    OK
    some initial try of cachefiles
    bugfix booster.check
    add saveload to raw
    add proptype of predleaf in R, fix bug in lambda rank
    C part export a model dump string
    add dump statistics
    add cross validation
    add ntree limit
    allow standalone random
    checkin slice
    fix
    initial trial package
    seems ok
    complete R example
    chg
    finish dump
    workable R wrapper
    try add R wrapper
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Add configuration to R interface. (#5217)
    
    * Save and load internal parameter configuration as JSON.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    A fix for CRAN submission of version 0.7-0 (#3061)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    Enable building rabit on Windows (#6105)
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    [R] make all customizations to meet strict standard of cran
    [R] Add a compatibility layer to load Booster object from an old RDS file (#5940)
    
    * [R] Add a compatibility layer to load Booster from an old RDS
    * Modify QuantileHistMaker::LoadConfig() to be backward compatible with 1.1.x
    * Add a big warning about compatibility in QuantileHistMaker::LoadConfig()
    * Add testing suite
    * Discourage use of saveRDS() in CRAN doc
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Add configuration to R interface. (#5217)
    
    * Save and load internal parameter configuration as JSON.
    Fix R dart prediction. (#5204)
    
    * Fix R dart prediction and add test.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Deprecate set group (#4864)
    
    
    
    * Convert jvm package and R package.
    
    * Restore for compatibility.
    Add support for cross-validation using query ID (#4474)
    
    * adding support for matrix slicing with query ID for cross-validation
    
    * hail mary test of unrar installation for windows tests
    
    * trying to modify tests to run in Github CI
    
    * Remove dependency on wget and unrar
    
    * Save error log from R test
    
    * Relax assertion in test_training
    
    * Use int instead of bool in C function interface
    
    * Revise R interface
    
    * Add XGDMatrixSliceDMatrixEx and keep old XGDMatrixSliceDMatrix for API compatibility
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R-package] JSON dump format and a couple of bugfixes (#1855)
    
    * [R-package] JSON tree dump interface
    
    * [R-package] precision bugfix in xgb.attributes
    
    * [R-package] bugfix for cb.early.stop called from xgb.cv
    
    * [R-package] a bit more clarity on labels checking in xgb.cv
    
    * [R-package] test JSON dump for gblinear as well
    
    * whitespace lint
    More robust DMatrix creation from a sparse matrix (#1606)
    
    * [CORE] DMatrix from sparse w/ explicit #col #row; safer arg types
    
    * [python-package] c-api change for _init_from_csr _init_from_csc
    
    * fix spaces
    
    * [R-package] adopt the new XGDMatrixCreateFromCSCEx interface
    
    * [CORE] redirect old sparse creators to new ones
    fix segfault when gctorture() is enabled (#1489)
    [R] more attribute handling functionality
    fix attribute accessors C-interface for R
    learner attribute setter & getter for R interface
    .Call-interface functions need to return SEXP
    added XGDMatrixNumCol_R function
    [R] make all customizations to meet strict standard of cran
    [R] enable R compile
    
    [R] Enable R build for windows and linux
    [R] make all customizations to meet strict standard of cran
    [R] enable R compile
    
    [R] Enable R build for windows and linux
    fix all cpp lint
    move sprintf into std
    make R package strict c99
    Enable building rabit on Windows (#6105)
    C++14 for xgboost (#5664)
    Fix CRAN check warnings/notes (#3988)
    
    * fix
    
    * reorder declaration to match initialization
    Fix CRAN check by removing reference to std::cerr (#3660)
    
    * Fix CRAN check by removing reference to std::cerr
    
    * Mask tests that fail on 32-bit Windows R
    Refactor of FastHistMaker to allow for custom regularisation methods (#3335)
    
    * Refactor to allow for custom regularisation methods
    
    * Implement compositional SplitEvaluator framework
    
    * Fixed segfault when no monotone_constraints are supplied.
    
    * Change pid to parentID
    
    * test_monotone_constraints.py now passes
    
    * Refactor ColMaker and DistColMaker to use SplitEvaluator
    
    * Performance optimisation when no monotone_constraints specified
    
    * Fix linter messages
    
    * Fix a few more linter errors
    
    * Update the amalgamation
    
    * Add bounds check
    
    * Add check for leaf node
    
    * Fix linter error in param.h
    
    * Fix clang-tidy errors on CI
    
    * Fix incorrect function name
    
    * Fix clang-tidy error in updater_fast_hist.cc
    
    * Enable SSE2 for Win32 R MinGW
    
    Addresses https://github.com/dmlc/xgboost/pull/3335#issuecomment-400535752
    
    * Add contributor
    [R] fix for the 32 bit windows issue (#2994)
    
    * [R] disable thred_local for 32bit windows
    
    * [R] require C++11 and GNU make in DESCRIPTION
    
    * [R] enable 32+64 build and check in appveyor
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    [PYTHON] Simplify training logic, update rabit lib
    [R] make all customizations to meet strict standard of cran
    [R] enable R compile
    
    [R] Enable R build for windows and linux
    some initial try of cachefiles
    compile with dmlc
    ok
    fix win compile
    windows changes
    change R build script
    ok
    make R package strict c99
    cleaner makevar
    fix win
    more strict makefile
    remove GNUism
    change windows
    change flagname to pass check
    remove useless flag
    more clean makefile
    fix print problem, fix Tong's email format
    fix some windows type conversion warning
    fix makefile in win
    fix windows
    chg makefile
    windows check
    checkin makefile
    make it packable
    do things
    adapt R package
    improve makefile
    add package parameter to all calls, test pass in mac
    add openmp flags
    add win make
    initial trial package
    Enable building rabit on Windows (#6105)
    C++14 for xgboost (#5664)
    [R] Enable OpenMP with AppleClang in XGBoost R package (#5240)
    
    * [R] Enable OpenMP with AppleClang in XGBoost R package
    
    * Dramatically simplify install doc
    [R] Robust endian detection in CRAN xgboost build (#5232)
    
    * [R] Robust endian detection in CRAN xgboost build
    
    * Check for external backtrace() lib
    
    * Update Makevars.win
    Fix CRAN check warnings/notes (#3988)
    
    * fix
    
    * reorder declaration to match initialization
    Fix CRAN check by removing reference to std::cerr (#3660)
    
    * Fix CRAN check by removing reference to std::cerr
    
    * Mask tests that fail on 32-bit Windows R
    Refactor of FastHistMaker to allow for custom regularisation methods (#3335)
    
    * Refactor to allow for custom regularisation methods
    
    * Implement compositional SplitEvaluator framework
    
    * Fixed segfault when no monotone_constraints are supplied.
    
    * Change pid to parentID
    
    * test_monotone_constraints.py now passes
    
    * Refactor ColMaker and DistColMaker to use SplitEvaluator
    
    * Performance optimisation when no monotone_constraints specified
    
    * Fix linter messages
    
    * Fix a few more linter errors
    
    * Update the amalgamation
    
    * Add bounds check
    
    * Add check for leaf node
    
    * Fix linter error in param.h
    
    * Fix clang-tidy errors on CI
    
    * Fix incorrect function name
    
    * Fix clang-tidy error in updater_fast_hist.cc
    
    * Enable SSE2 for Win32 R MinGW
    
    Addresses https://github.com/dmlc/xgboost/pull/3335#issuecomment-400535752
    
    * Add contributor
    [R] fix for the 32 bit windows issue (#2994)
    
    * [R] disable thred_local for 32bit windows
    
    * [R] require C++11 and GNU make in DESCRIPTION
    
    * [R] enable 32+64 build and check in appveyor
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    autoconf for solaris (#1880)
    [PYTHON] Simplify training logic, update rabit lib
    [R] make all customizations to meet strict standard of cran
    [R] enable R compile
    
    [R] Enable R build for windows and linux
    some initial try of cachefiles
    compile with dmlc
    change R build script
    ok
    make R package strict c99
    message
    change warning to pragma message
    cleaner makevar
    allow standalone random
    more strict makefile
    remove GNUism
    change flagname to pass check
    remove useless flag
    more clean makefile
    fix print problem, fix Tong's email format
    chg makefile
    windows check
    checkin makefile
    make it packable
    do things
    adapt R package
    add package parameter to all calls, test pass in mac
    add openmp flags
    initial trial package
    [R] Add a compatibility layer to load Booster object from an old RDS file (#5940)
    
    * [R] Add a compatibility layer to load Booster from an old RDS
    * Modify QuantileHistMaker::LoadConfig() to be backward compatible with 1.1.x
    * Add a big warning about compatibility in QuantileHistMaker::LoadConfig()
    * Add testing suite
    * Discourage use of saveRDS() in CRAN doc
    add test module in R
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Expand categorical node. (#6028)
    
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    Enable parameter validation for R. (#5569)
    
    * Enable parameter validation for R.
    
    * Add test.
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Remove silent parameter. (#5476)
    Add configuration to R interface. (#5217)
    
    * Save and load internal parameter configuration as JSON.
    Fix R dart prediction. (#5204)
    
    * Fix R dart prediction and add test.
    [R] Use built-in label when xgb.DMatrix is given to xgb.cv() (#4631)
    
    * Use built-in label when xgb.DMatrix is given to xgb.cv()
    
    * Add a test
    
    * Fix test
    
    * Bump version number
    fix R-devel errors (#4251)
    Fix #3523: Fix CustomGlobalRandomEngine for R (#3781)
    
    **Symptom** Apple Clang's implementation of `std::shuffle` expects doesn't work
    correctly when it is run with the random bit generator for R package:
    ```cpp
    CustomGlobalRandomEngine::result_type
    CustomGlobalRandomEngine::operator()() {
      return static_cast<result_type>(
          std::floor(unif_rand() * CustomGlobalRandomEngine::max()));
    }
    ```
    
    Minimial reproduction of failure (compile using Apple Clang 10.0):
    ```cpp
    std::vector<int> feature_set(100);
    std::iota(feature_set.begin(), feature_set.end(), 0);
        // initialize with 0, 1, 2, 3, ..., 99
    std::shuffle(feature_set.begin(), feature_set.end(), common::GlobalRandom());
        // This returns 0, 1, 2, ..., 99, so content didn't get shuffled at all!!!
    ```
    
    Note that this bug is platform-dependent; it does not appear when GCC or
    upstream LLVM Clang is used.
    
    **Diagnosis** Apple Clang's `std::shuffle` expects 32-bit integer
    inputs, whereas `CustomGlobalRandomEngine::operator()` produces 64-bit
    integers.
    
    **Fix** Have `CustomGlobalRandomEngine::operator()` produce 32-bit integers.
    
    Closes #3523.
    Add the missing max_delta_step (#3668)
    
    * add max_delta_step to SplitEvaluator
    
    * test for max_delta_step
    
    * missing x2 factor for L1 term
    
    * remove gamma from ElasticNet
    A fix for CRAN submission of version 0.7-0 (#3061)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    0.6-4 submission (#1935)
    [R] disable for now some of the RF tests that fail in travis
    [R] additional and modified tests
    R-callbacks tests + other tests brushup
    Minor addition to R unit tests
    Lint fix on infix operators
    Lint fix on consistent assignment
    fix test
    add test module in R
    [R] Remove stringi dependency (#6109)
    
    * [R] Fix empty empty tests and a test warnings
    
    * [R] Remove stringi dependency (fix #5905)
    
    * Fix R lint check
    
    * [R] Fix automatic conversion to factor in R < 4.0.0 in xgb.model.dt.tree
    
    * Add `R` Makefile variable
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    replace nround with nrounds to match actual parameter (#3592)
    Add unittest for garbage collection's safety in R (#1490)
    
    * Add test for garbage collection safety
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    Fix r interaction constraints (#5543)
    
    * Unify the parsing code.
    
    * Cleanup.
    [TREE] add interaction constraints (#3466)
    
    * add interaction constraints
    
    * enable both interaction and monotonic constraints at the same time
    
    * fix lint
    
    * add R test, fix lint, update demo
    
    * Use dmlc::JSONReader to express interaction constraints as nested lists; Use sparse arrays for bookkeeping
    
    * Add Python test for interaction constraints
    
    * make R interaction constraints parameter based on feature index instead of column names, fix R coding style
    
    * Fix lint
    
    * Add BlueTea88 to CONTRIBUTORS.md
    
    * Short circuit when no constraint is specified; address review comments
    
    * Add tutorial for feature interaction constraints
    
    * allow interaction constraints to be passed as string, remove redundant column_names argument
    
    * Fix typo
    
    * Address review comments
    
    * Add comments to Python test
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    Enable parameter validation for R. (#5569)
    
    * Enable parameter validation for R.
    
    * Add test.
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    fixed unit test in R
    TST: Added R unit test for glm
    Validate weights are positive values. (#6115)
    [R] fix uses of 1:length(x) and other small things (#5992)
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    Fix slice and get info. (#5552)
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Fix #3545: XGDMatrixCreateFromCSCEx silently discards empty trailing rows (#3553)
    
    * Fix #3545: XGDMatrixCreateFromCSCEx silently discards empty trailing rows
    
    Description: The bug is triggered when
    
    1. The data matrix has empty rows at the bottom. More precisely, the rows
       `n-k+1`, `n-k+2`, ..., `n` of the matrix have missing values in all
       dimensions (`n` number of instances, `k` number of trailing rows)
    2. The data matrix is given as Compressed Sparse Column (CSC) format.
    
    Diagnosis: When the CSC matrix is converted to Compressed Sparse Row (CSR)
    format (this is common format used for DMatrix), the trailing empty rows
    are silently ignored. More specifically, the row pointer (`offset`) of the
    newly created CSR matrix does not take account of these rows.
    
    Fix: Modify the row pointer.
    
    * Add regression test
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    More robust DMatrix creation from a sparse matrix (#1606)
    
    * [CORE] DMatrix from sparse w/ explicit #col #row; safer arg types
    
    * [python-package] c-api change for _init_from_csr _init_from_csc
    
    * fix spaces
    
    * [R-package] adopt the new XGDMatrixCreateFromCSCEx interface
    
    * [CORE] redirect old sparse creators to new ones
    added unit tests for xgb.DMatrix
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    Disable flaky tests in R-package/tests/testthat/test_update.R (#3723)
    [gbtree] fix update process to work with multiclass and multitree; fixes #2315 (#2332)
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [CORE] The update process for a tree model, and its application to feature importance (#1670)
    
    * [CORE] allow updating trees in an existing model
    
    * [CORE] in refresh updater, allow keeping old leaf values and update stats only
    
    * [R-package] xgb.train mod to allow updating trees in an existing model
    
    * [R-package] added check for nrounds when is_update
    
    * [CORE] merge parameter declaration changes; unify their code style
    
    * [CORE] move the update-process trees initialization to Configure; rename default process_type to 'default'; fix the trees and trees_to_update sizes comparison check
    
    * [R-package] unit tests for the update process type
    
    * [DOC] documentation for process_type parameter; improved docs for updater, Gamma and Tweedie; added some parameter aliases; metrics indentation and some were non-documented
    
    * fix my sloppy merge conflict resolutions
    
    * [CORE] add a TreeProcessType enum
    
    * whitespace fix
    [R] Remove stringi dependency (#6109)
    
    * [R] Fix empty empty tests and a test warnings
    
    * [R] Remove stringi dependency (fix #5905)
    
    * Fix R lint check
    
    * [R] Fix automatic conversion to factor in R < 4.0.0 in xgb.model.dt.tree
    
    * Add `R` Makefile variable
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [Breaking] Fix custom metric for multi output. (#5954)
    
    
    * Set output margin to true for custom metric.  This fixes only R and Python.
    Fix r early stop with custom objective. (#5923)
    
    * Specify `ntreelimit`.
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    [Breaking] Set output margin to True for custom objective. (#5564)
    
    * Set output margin to True for custom objective in Python and R.
    
    * Add a demo for writing multi-class custom objective function.
    
    * Run tests on selected demos.
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Change obj name to `reg:squarederror` in learner. (#4427)
    
    
    * Change memory dump size in R test.
    [R] additional and modified tests
    R-callbacks tests + other tests brushup
    Fixed bug in eta decay (+2 squashed commits)
    Squashed commits:
    [b67caf2] Fix build
    [365ceaa] Fixed bug in eta decay
    Added test for eta decay (+3 squashed commits)
    Squashed commits:
    [9109887] Added test for eta decay(+1 squashed commit)
    Squashed commits:
    [1336bd4] Added tests for eta decay (+2 squashed commit)
    Squashed commits:
    [91aac2d] Added tests for eta decay (+1 squashed commit)
    Squashed commits:
    [3ff48e7] Added test for eta decay
    [6bb1eed] Rewrote Rd files
    [bf0dec4] Added learning_rates for diff eta in each boosting round
    Fixed most of the lint issues
    Lint fix on infix operators
    TST: Added more checks for testing custom objective
    TST: Added test for models with custom objective
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    R-callbacks tests + other tests brushup
    [TRAVIS] cleanup travis script
    Minor addition to R unit tests
    Fixed most of the lint issues
    Lint fix on infix operators
    TST: Added test for poisson regression
    [R] Enable weighted learning to rank (#5945)
    
    * [R] enable weighted learning to rank
    
    * Add R unit test for ranking
    
    * Fix lint
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    [R] additional and modified tests
    R-callbacks tests + other tests brushup
    Add test that model paramaters are accessible within R
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R] Monotonic Constraints in Tree Construction (#1557)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    [R] Check warnings explicitly for model compatibility tests (#6114)
    
    * [R] Check warnings explicitly for model compatibility tests
    
    * Address reviewer's feedback
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    [R] Add a compatibility layer to load Booster object from an old RDS file (#5940)
    
    * [R] Add a compatibility layer to load Booster from an old RDS
    * Modify QuantileHistMaker::LoadConfig() to be backward compatible with 1.1.x
    * Add a big warning about compatibility in QuantileHistMaker::LoadConfig()
    * Add testing suite
    * Discourage use of saveRDS() in CRAN doc
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Avoid generating NaNs in UnwoundPathSum (#3943)
    
    * Avoid generating NaNs in UnwoundPathSum.
    
    Kudos to Jakub Zakrzewski for tracking down the bug.
    
    * Add a test
    [R] R-interface for SHAP interactions (#3636)
    
    * add R-interface for SHAP interactions
    
    * update docs for new roxygen version
    [R] Remove stringi dependency (#6109)
    
    * [R] Fix empty empty tests and a test warnings
    
    * [R] Remove stringi dependency (fix #5905)
    
    * Fix R lint check
    
    * [R] Fix automatic conversion to factor in R < 4.0.0 in xgb.model.dt.tree
    
    * Add `R` Makefile variable
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add SHAP summary plot using ggplot2 (#5882)
    
    * add SHAP summary plot using ggplot2
    
    * Update xgb.plot.shap
    
    * Update example in xgb.plot.shap documentation
    
    * update logic, add tests
    
    * whitespace fixes
    
    * whitespace fixes for test_helpers
    
    * namespace for sd function
    
    * explicitly declare variables that are automatically evaluated by data.table
    
    * Fix R lint
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [R] fix uses of 1:length(x) and other small things (#5992)
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Port R compatibility patches from 1.0.0 release branch (#5577)
    
    * Don't use memset to set struct when compiling for R
    
    * Support 32-bit Solaris target for R package
    Fix R dart prediction. (#5204)
    
    * Fix R dart prediction and add test.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Use DART tree weights when computing SHAPs (#5050)
    
    This PR fixes tree weights in dart being ignored when computing contributions.
    
    * Fix ellpack page source link.
    * Add tree weights to compute contribution.
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    Fix CRAN check by removing reference to std::cerr (#3660)
    
    * Fix CRAN check by removing reference to std::cerr
    
    * Mask tests that fail on 32-bit Windows R
    CRAN Submission for 0.71.1 (#3311)
    
    * fix for CRAN manual checks
    
    * fix for CRAN manual checks
    
    * pass local check
    
    * fix variable naming style
    
    * Adding Philip's record
    A fix for CRAN submission of version 0.7-0 (#3061)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    [R] xgb.importance: fix for multiclass gblinear, new 'trees' parameter (#2388)
    adding feature contributions to R and gblinear (#2295)
    
    * [gblinear] add features contribution prediction; fix DumpModel bug
    
    * [gbtree] minor changes to PredContrib
    
    * [R] add feature contribution prediction to R
    
    * [R] bump up version; update NEWS
    
    * [gblinear] fix the base_margin issue; fixes #1969
    
    * [R] list of matrices as output of multiclass feature contributions
    
    * [gblinear] make order of DumpModel coefficients consistent: group index changes the fastest
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    0.6-4 submission (#1935)
    [R-package] JSON dump format and a couple of bugfixes (#1855)
    
    * [R-package] JSON tree dump interface
    
    * [R-package] precision bugfix in xgb.attributes
    
    * [R-package] bugfix for cb.early.stop called from xgb.cv
    
    * [R-package] a bit more clarity on labels checking in xgb.cv
    
    * [R-package] test JSON dump for gblinear as well
    
    * whitespace lint
    [R-package] store numeric attributes with higher precision (#1628)
    [R-package] a few fixes for R (#1485)
    
    * [R] fix #1465
    
    * [R] add sanity check to fix #1434
    
    * [R] some clean-ups for custom obj&eval; require maximize only for early stopping
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R] additional and modified tests
    R-callbacks tests + other tests brushup
    xgb.model.dt.tree up to x100 faster
    [R] more attribute handling functionality
    R accessors for model attributes
    Increase cover of tests #Rstat
    Increase cover of tests #Rstat
    Improve feature importance on GLM model
    Improve feature importance on GLM model
    Add new tests for helper functions
    Add new tests for helper functions
    some fixes for Travis #Rstat
    some fixes for Travis #Rstat
    Add new tests for new functions
    Add new tests for new functions
    Update test to take care of API change
    Update test to take care of API change
    Frequence to Frequency
    Fix Travis build
    Minor addition to R unit tests
    Fix travis build (+1 squashed commit)
    Squashed commits:
    [9240d5f] Fix Travis build
    Lint fix on infix operators
    Lint fix on consistent assignment
    TST: Added one minor check for xgb.importance
    TST: Added test for xgb.plot.tree
    TST: Added test for xgb.importance
    TST: Added test for dump
    Disable JSON full serialization for now. (#6248)
    
    * Disable JSON serialization for now.
    
    * Multi-class classification is checkpointing for each iteration.
    This brings significant overhead.
    
    Revert: 90355b4f007ae
    
    * Set R tests to use binary.
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Expand categorical node. (#6028)
    
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    Restore attributes in complete. (#5573)
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    [R] Fix CRAN error for Mac OS X (#4672)
    
    * fix cran error for mac os x
    
    * ignore float on windows check for now
    [R] Use built-in label when xgb.DMatrix is given to xgb.cv() (#4631)
    
    * Use built-in label when xgb.DMatrix is given to xgb.cv()
    
    * Add a test
    
    * Fix test
    
    * Bump version number
    fix R-devel errors (#4251)
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R-package] JSON dump format and a couple of bugfixes (#1855)
    
    * [R-package] JSON tree dump interface
    
    * [R-package] precision bugfix in xgb.attributes
    
    * [R-package] bugfix for cb.early.stop called from xgb.cv
    
    * [R-package] a bit more clarity on labels checking in xgb.cv
    
    * [R-package] test JSON dump for gblinear as well
    
    * whitespace lint
    [R-package] a few fixes for R (#1485)
    
    * [R] fix #1465
    
    * [R] add sanity check to fix #1434
    
    * [R] some clean-ups for custom obj&eval; require maximize only for early stopping
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R] additional and modified tests
    R-callbacks tests + other tests brushup
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    [R] fix uses of 1:length(x) and other small things (#5992)
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    [R] Add a compatibility layer to load Booster object from an old RDS file (#5940)
    
    * [R] Add a compatibility layer to load Booster from an old RDS
    * Modify QuantileHistMaker::LoadConfig() to be backward compatible with 1.1.x
    * Add a big warning about compatibility in QuantileHistMaker::LoadConfig()
    * Add testing suite
    * Discourage use of saveRDS() in CRAN doc
    [R] Fix CRAN error for Mac OS X (#4672)
    
    * fix cran error for mac os x
    
    * ignore float on windows check for now
    added JSON vignette (#4439)
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    Fix typo (#2264)
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    Update discoverYourData.Rmd (#1482)
    
    * Fixed some typos
    * RF is not experimental anymore
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] adopt demos and vignettes to a more consistent parameter style
    fixed typo
    
    panda -> Pandas
    [DOC] Update R doc
    Cleaning in documentation
    Cleaning in documentation
    Frequence to Frequency
    Update vignette
    Add Random Forest parameter (num_parallel_tree) in function doc + example in Vignette.
    update links dmlc
    code simplification
    Vignette text
    Vignette text
    df spell
    trademark RF
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    Update discoverYourData.Rmd
    Vignette text
    Vignette text
    Vignette text
    Vignette text
    Vignette text
    Fix Vignette bug!
    Vignette text
    text
    splell
    vignette text
    refix
    vignette text
    text refactor
    vignette text
    text vignette
    Vignette text
    refactor vignette
    Vignettes
    Vignette text
    Vignettes improvement
    text change
    improved vignette text
    modif CSS
    improve text of the Vignette
    Vignette, 1st version
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [R] Address warnings to comply with CRAN submission policy (#5600)
    
    * [R] Address warnings to comply with CRAN submission policy
    
    * Include <xgboost/logging.h>
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    [R] adopt demos and vignettes to a more consistent parameter style
    [R] update doc; add drat repo
    [DOC] Update R doc
    Fix typo
    
    "Until Know" to "Until Now"
    Cleaning in documentation
    Cleaning in documentation
    spelling changes
    Document refactor
    
    change badge
    Adding examples on xgb.importance, xgb.plot.importance and xgb.plot tree
    Update xgboostPresentation.Rmd
    
    Edited to note unavailability of stable version of this package on CRAN.
    
    http://cran.r-project.org/web/packages/xgboost/index.html
    update links dmlc
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    Add ref.
    possible polishments
    Update xgboostPresentation.Rmd
    Vignette text
    text vignette
    Vignette text
    text vignette
    Vignette txt
    CSS improvement
    add linear boosting part
    clean temp
    vignette text
    Vignette text
    vignette text
    text vignette
    add bibliography
    Vignette text
    add introduction paragraph from PDF file
    refactor vignette
    Vignettes
    Vignette text
    Vignette text
    text vignette
    Vignettes improvement
    text change
    add saveload to raw
    add saveload to raw
    Update wlkthrough R demo code to include variable importance.
    ok
    add basic walkthrough
    Remove silent from R demos. (#5675)
    
    * Remove silent from R demos.
    
    * Vignettes.
    [R] Increase the version number, date and required R version (#1920)
    
    * remove unnecessary line
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] adopt demos and vignettes to a more consistent parameter style
    [R] update doc; add drat repo
    Update xgboost.Rnw
    clean temp
    fix temp file created by PDF
    Vignettes
    refine style with max.depth
    remove incorrect link to old folders
    remove inst/, improve vignette
    refine doc, with Rd
    fix iris multiclass problem
    Update xgboost.Rnw
    final revision before CRAN
    remove logo
    change location and template of vignette
    edit the doc
    refine vignette
    refine vignette
    add vignette
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    refine doc, with Rd
    change location and template of vignette
    Vignette text
    CSS: Add slight line after Header 1
    CSS improvement, more space, change in style titles
    Vignette text
    Vignette text
    text vignette
    refix
    better CSS
    text refactor
    css improvement
    CSS improvement
    justified text in CSS
    vignette text
    fix some CSS
    refactor vignette
    Vignettes
    improved vignette text
    CSS
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R] xgb.plot.tree fixes (#1939)
    
    * [R] a few fixes and improvements to xgb.plot.tree
    
    * [R] deprecate n_first_tree replace with trees; fix types in xgb.model.dt.tree
    0.6-4 submission (#1935)
    [R] parameter style consistency
    New documentation rewording
    fix relative to examples #Rstat
    fix relative to examples #Rstat
    some fixes for Travis #Rstat
    some fixes for Travis #Rstat
    parameter names change in R function
    parameter names change in R function
    Fix some bug + improve display + code clean
    Change the way functions are called
    Remove DiagrammeR dependency to make travis happy...
    Rewrite tree plot function
    Replace Mermaid by GraphViz
    Fixed most of the lint issues
    moved the external graphing packages to Suggested in order to trim the dependencies
    moved the external graphing packages to Suggested in order to trim the dependencies
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    fix mermaid
    fix a bug introduced in previous commit
    Avoid some Cran check error messages
    documentation update
    resolving not-CRAN issues
    add new parameters model to avoid the use of dump file for functions plot, dt.tree, importance
    add new size parameter for plot function
    add new parameters to several functions avoid the need of a text dump
    fix plenty of small bugs
    add new function to read model and use it in the plot function
    refactoring for perf
    add style option
    Change in aesthetic
    Improve documentation
    Add stat indicators in plot
    add limit number of trees option
    fix import issue
    Change code to look like a function
    refactor plot function
    use style CSS class instead of q style per item
    plot all trees
    basis to plot
    build data.table from raw model data
    [R] Remove stringi dependency (#6109)
    
    * [R] Fix empty empty tests and a test warnings
    
    * [R] Remove stringi dependency (fix #5905)
    
    * Fix R lint check
    
    * [R] Fix automatic conversion to factor in R < 4.0.0 in xgb.model.dt.tree
    
    * Add `R` Makefile variable
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    [R] many minor changes to increase the robustness of the R code (#2404)
    
    * many minor changes to increase robustness of R code
    
    * fixing which mistake in xgb.model.dt.tree.R and a few cosmetics
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R] xgb.plot.tree fixes (#1939)
    
    * [R] a few fixes and improvements to xgb.plot.tree
    
    * [R] deprecate n_first_tree replace with trees; fix types in xgb.model.dt.tree
    [R-package] a few fixes for R (#1485)
    
    * [R] fix #1465
    
    * [R] add sanity check to fix #1434
    
    * [R] some clean-ups for custom obj&eval; require maximize only for early stopping
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R] parameter style consistency
    make travis happy
    some more xgb.model.dt.tree improvements
    xgb.model.dt.tree up to x100 faster
    model dt tree function documentation improvement
    some fixes for Travis #Rstat
    some fixes for Travis #Rstat
    Add new tests for new functions
    Add new tests for new functions
    Polishing API + wording in function description #Rstat
    Polishing API + wording in function description #Rstat
    Fix missing dependencies
    Fix missing dependencies
    Frequence to Frequency
    Fixed most of the lint issues
    Lint fix on infix operators
    Lint fix on consistent assignment
    Code: Lint fixes on trailing spaces
    issue #368, data.table problems
    multi trees
    fixing parsing of any numbers
    fixed parsing of negative reals, integers and scientific notation which
    can occur in model dump
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    remove not required dependency
    remove buggy feature
    simplidied included column computation
    wording
    wording
    new included feature in dt.tree function
    Documentation: no need to save model in txt...
    forget one variable
    fix a bug introduced in previous commit
    Avoid some Cran check error messages
    documentation update
    documentation update
    resolving not-CRAN issues
    fix error
    avoid error when a tree is just made of one leaf
    add new parameters model to avoid the use of dump file for functions plot, dt.tree, importance
    add new size parameter for plot function
    fix plenty of small bugs
    refactoring of importance function
    add new function to read model and use it in the plot function
    refactoring for perf
    add style option
    Change in aesthetic
    Improve documentation
    Add stat indicators in plot
    add limit number of trees option
    fix import issue
    Change code to look like a function
    refactor plot function
    use style CSS class instead of q style per item
    plot all trees
    basis to plot
    build data.table from raw model data
    [R] fix uses of 1:length(x) and other small things (#5992)
    [R] Enable weighted learning to rank (#5945)
    
    * [R] enable weighted learning to rank
    
    * Add R unit test for ranking
    
    * Fix lint
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    Fix slice and get info. (#5552)
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    Fix matrix attributes not sliced (#4311)
    [R] many minor changes to increase the robustness of the R code (#2404)
    
    * many minor changes to increase robustness of R code
    
    * fixing which mistake in xgb.model.dt.tree.R and a few cosmetics
    Improve setinfo documentation on R package (#2357)
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    Typo Issue (#2100)
    
    Contruct to Construct
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    More robust DMatrix creation from a sparse matrix (#1606)
    
    * [CORE] DMatrix from sparse w/ explicit #col #row; safer arg types
    
    * [python-package] c-api change for _init_from_csr _init_from_csc
    
    * fix spaces
    
    * [R-package] adopt the new XGDMatrixCreateFromCSCEx interface
    
    * [CORE] redirect old sparse creators to new ones
    Class function returns more than one value (#1417)
    
    Fix to a bug when the class function returns more than one value. In that case, the code will fail.
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    protection against returning 0-length vector
    make travis happy
    fix print.xgb.DMatrix doc
    convert S4 to S3; add some extra methods to DMatrix
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    Code: Lint fixes on trailing spaces
    Switch missing values from 0 to NA in R package
    update document
    replace iris in docs
    fix iris multiclass problem
    fix NAMESPACE
    get a pass in function docstring
    add documentation notes
    modify xgb.getinfo to getinfo
    refinement of R package
    major change in the design of R interface
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    [R] Remove stringi dependency (#6109)
    
    * [R] Fix empty empty tests and a test warnings
    
    * [R] Remove stringi dependency (fix #5905)
    
    * Fix R lint check
    
    * [R] Fix automatic conversion to factor in R < 4.0.0 in xgb.model.dt.tree
    
    * Add `R` Makefile variable
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix R dart prediction. (#5204)
    
    * Fix R dart prediction and add test.
    Replace cBind by cbind (#3203)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    
    * change Matrix cBind to cbind
    
    * fix
    
    * fix error in demo
    
    * fix examples
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    Fix the "No visible binding" CRAN checks (#1504)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] consolidate importFrom-s; parameter style
    R-callbacks refactor
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    Fixed most of the lint issues
    Switch missing values from 0 to NA in R package
    add error for data.frame, add weight to xgboost
    add save_period
    rename arguments to be dot-seperated
    new parameter in xgboost() and xgb.train() to print every N-th progress message
    support both early stop name
    add early stopping to R
    update links dmlc
    small changes in doc
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    improve function documentation.
    
    Switch xgboost detailed parameters with xgb.tain function.
    add web address
    documentation simplification
    small doc fix
    fix documentation
    new doc
    update document
    Update xgboost.R
    Missing parameter documentation
    Fix data documentation
    Update xgboost.R
    
    add parameter missing
    refine style with max.depth
    remove incorrect link to old folders
    add doc for agaricus.test
    improvement for reducing warnings
    add documentation for datasets
    add basic walkthrough
    style cleanup, incomplete CV
    fix iris multiclass problem
    fix doc with redirection to inst/examples
    speed test for R, and refinement of item list in doc
    fix NAMESPACE
    compile Rd files, i.e. R documents
    OK
    chg
    get a pass in function docstring
    add documentation notes
    remove default value for nrounds
    add files back
    styling of else in R
    fix a tiny bug in xgboost
    modify xgb.getinfo to getinfo
    refinement of R package
    major change in the design of R interface
    export fewer functions to user and optimize parameter setting
    add package parameter to all calls, test pass in mac
    initial trial package
    fix compilation on mac
    seems ok
    complete R example
    chg
    [R] Remove stringi dependency (#6109)
    
    * [R] Fix empty empty tests and a test warnings
    
    * [R] Remove stringi dependency (fix #5905)
    
    * Fix R lint check
    
    * [R] Fix automatic conversion to factor in R < 4.0.0 in xgb.model.dt.tree
    
    * Add `R` Makefile variable
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    [Breaking] Fix custom metric for multi output. (#5954)
    
    
    * Set output margin to true for custom metric.  This fixes only R and Python.
    [R] Add a compatibility layer to load Booster object from an old RDS file (#5940)
    
    * [R] Add a compatibility layer to load Booster from an old RDS
    * Modify QuantileHistMaker::LoadConfig() to be backward compatible with 1.1.x
    * Add a big warning about compatibility in QuantileHistMaker::LoadConfig()
    * Add testing suite
    * Discourage use of saveRDS() in CRAN doc
    Fix r early stop with custom objective. (#5923)
    
    * Specify `ntreelimit`.
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    [Breaking] Set output margin to True for custom objective. (#5564)
    
    * Set output margin to True for custom objective in Python and R.
    
    * Add a demo for writing multi-class custom objective function.
    
    * Run tests on selected demos.
    [R-package] fixed uses of class() (#5426)
    
    Thank you a lot. Good catch!
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Amend xgb.createFolds to handle classes of a single element. (#3630)
    
    * Amend xgb.createFolds to handle classes of a single element.
    
    * Fix variable name
    [TREE] add interaction constraints (#3466)
    
    * add interaction constraints
    
    * enable both interaction and monotonic constraints at the same time
    
    * fix lint
    
    * add R test, fix lint, update demo
    
    * Use dmlc::JSONReader to express interaction constraints as nested lists; Use sparse arrays for bookkeeping
    
    * Add Python test for interaction constraints
    
    * make R interaction constraints parameter based on feature index instead of column names, fix R coding style
    
    * Fix lint
    
    * Add BlueTea88 to CONTRIBUTORS.md
    
    * Short circuit when no constraint is specified; address review comments
    
    * Add tutorial for feature interaction constraints
    
    * allow interaction constraints to be passed as string, remove redundant column_names argument
    
    * Fix typo
    
    * Address review comments
    
    * Add comments to Python test
    [R] many minor changes to increase the robustness of the R code (#2404)
    
    * many minor changes to increase robustness of R code
    
    * fixing which mistake in xgb.model.dt.tree.R and a few cosmetics
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] xgb.plot.tree fixes (#1939)
    
    * [R] a few fixes and improvements to xgb.plot.tree
    
    * [R] deprecate n_first_tree replace with trees; fix types in xgb.model.dt.tree
    [R] Monotonic Constraints in Tree Construction (#1557)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    [R-package] a few fixes for R (#1485)
    
    * [R] fix #1465
    
    * [R] add sanity check to fix #1434
    
    * [R] some clean-ups for custom obj&eval; require maximize only for early stopping
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R] add parameter deprecation related utilities; code style
    R-callbacks refactor
    convert S4 to S3; add some extra methods to DMatrix
    Update utils.R
    Fix travis build (+1 squashed commit)
    Squashed commits:
    [9240d5f] Fix Travis build
    Fixed most of the lint issues
    Lint fix on infix operators
    Lint fix on consistent assignment
    Code: Some Lint fixes
    Update utils.R
    Switch missing values from 0 to NA in R package
    add error for data.frame, add weight to xgboost
    fix namespace and desc
    check whether objective is character
    cleanup
    a safeguard against someone using automatic folds creation with ranking
    Improved logic in stratified CV to guess class/regr
    
    Somewhat more robust and clear logic in stratified CV to guess classification/regression settings. Allows to accomodate custom objectives (classification is assumed when number of unique values in labels <= 5).
    Fix length check in utils.R
    make it possible to use a list of pre-defined CV folds in xgb.cv
    fix some wording
    added an option for stratified CV to xgb.cv
    allow xgb.load re-use raw information if necessary
    bugfix booster.check
    Update utils.R
    Update utils.R
    Update utils.R
    add length check
    fix segfault and add two function for handle and booster
    fix with new predict
    fix bugs
    add handle and raw structure to xgb.Booster
    add saveload to raw
    enable returning prediction in cv
    Update utils.R
    
    add parameter missing
    remove deprecate
    ok
    add customize objective
    chg cv
    add cross validation
    style cleanup, incomplete CV
    fix the zero length vector
    allow standalone random
    add import methods in NAMESPACE
    fix NAMESPACE with import classes
    modify xgb.getinfo to getinfo
    major change in the design of R interface
    export fewer functions to user and optimize parameter setting
    [R] fix uses of 1:length(x) and other small things (#5992)
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R] parameter style consistency
    add support of GLM model in importance plot function
    add support of GLM model in importance plot function
    fix example
    fix example
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Fixed most of the lint issues
    Lint fix on infix operators
    fix crash
    fix namespace and desc
    Suppress a Note in Cran check
    moved the external graphing packages to Suggested in order to trim the dependencies
    moved the external graphing packages to Suggested in order to trim the dependencies
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    small fixes
    ...
    fix error message during check
    new plot feature importance function
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] simplified the code; parameter style consistency
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    Code: Lint fixes on trailing spaces
    replace iris in docs
    Update xgb.DMatrix.save.R
    Update xgb.DMatrix.save.R
    fix iris multiclass problem
    improve document format
    fix NAMESPACE
    add documentation notes
    modify xgb.getinfo to getinfo
    refinement of R package
    major change in the design of R interface
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    [R] many minor changes to increase the robustness of the R code (#2404)
    
    * many minor changes to increase robustness of R code
    
    * fixing which mistake in xgb.model.dt.tree.R and a few cosmetics
    [R] xgb.importance: fix for multiclass gblinear, new 'trees' parameter (#2388)
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    Fixing a few typos (#1771)
    
    * Fixing a few typos
    
    * Fixing a few typos
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] parameter style consistency
    wording fix
    Wording improvement
    Generate new features based on tree leafs
    Small rewording function xgb.importance
    Small rewording function xgb.importance
    Improve feature importance on GLM model
    Improve feature importance on GLM model
    fix example
    fix example
    fix relative to examples #Rstat
    fix relative to examples #Rstat
    Add new tests for new functions
    Add new tests for new functions
    Fix Rstat
    Fix Rstat
    Polishing API + wording in function description #Rstat
    Polishing API + wording in function description #Rstat
    Frequence to Frequency
    Fix travis build (+1 squashed commit)
    Squashed commits:
    [9240d5f] Fix Travis build
    Fixed most of the lint issues
    Lint fix on infix operators
    Lint fix on consistent assignment
    Code: Lint fixes on trailing spaces
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    fix warning
    memory optimization
    fix
    small fixes
    avoid error message
    Documentation feature importance
    Add comments
    missing feature management
    better co occurence function
    Bug + documentation
    fixed bug
    Add new co-occurence computation capacity to importance feature function + related documentation
    remove buggy feature
    add importance feature sign
    fix a bug introduced in previous commit
    Avoid some Cran check error messages
    documentation update
    resolving not-CRAN issues
    refactoring
    add new parameters model to avoid the use of dump file for functions plot, dt.tree, importance
    add new size parameter for plot function
    add new parameters to several functions avoid the need of a text dump
    fix plenty of small bugs
    refactoring of importance function
    fix arg check
    small documentation change
    add new Gain and Weight columns.
    documentation updated.
    Add a new Weight and Gain column.
    Update documentation.
    wording
    Take gain into account to discover most important variables
    Add variable type checks
    Documentation of the function
    fix small bug introduced in refactoring
    refactoring of validation to improve source code readability.
    new function cv.importance + documentation
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    Restore attributes in complete. (#5573)
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Add configuration to R interface. (#5217)
    
    * Save and load internal parameter configuration as JSON.
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    fix #4764 (#4800)
    
    check all classes in xgb.get.handle
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    [R] R-interface for SHAP interactions (#3636)
    
    * add R-interface for SHAP interactions
    
    * update docs for new roxygen version
    Fix print.xgb.Booster for R (#3338)
    
    * Fix print.xgb.Booster
    
    valid_handle should be TRUE when x$handle is NOT null
    
    * Update xgb.Booster.R
    
    Modify is.null.handle to return TRUE for NULL handle
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    [R] many minor changes to increase the robustness of the R code (#2404)
    
    * many minor changes to increase robustness of R code
    
    * fixing which mistake in xgb.model.dt.tree.R and a few cosmetics
    adding feature contributions to R and gblinear (#2295)
    
    * [gblinear] add features contribution prediction; fix DumpModel bug
    
    * [gbtree] minor changes to PredContrib
    
    * [R] add feature contribution prediction to R
    
    * [R] bump up version; update NEWS
    
    * [gblinear] fix the base_margin issue; fixes #1969
    
    * [R] list of matrices as output of multiclass feature contributions
    
    * [gblinear] make order of DumpModel coefficients consistent: group index changes the fastest
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R] fix #1903 (#1929)
    [R-package] JSON dump format and a couple of bugfixes (#1855)
    
    * [R-package] JSON tree dump interface
    
    * [R-package] precision bugfix in xgb.attributes
    
    * [R-package] bugfix for cb.early.stop called from xgb.cv
    
    * [R-package] a bit more clarity on labels checking in xgb.cv
    
    * [R-package] test JSON dump for gblinear as well
    
    * whitespace lint
    Fixing a few typos (#1771)
    
    * Fixing a few typos
    
    * Fixing a few typos
    [R-package] store numeric attributes with higher precision (#1628)
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] in predict: doc, examples, reshape parameter
    print method; construct from initial xgb.Booster
    [R] more attribute handling functionality
    use short-circuiting scalar &&
    R accessors for model attributes
    convert S4 to S3; add some extra methods to DMatrix
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    Add SHAP summary plot using ggplot2 (#5882)
    
    * add SHAP summary plot using ggplot2
    
    * Update xgb.plot.shap
    
    * Update example in xgb.plot.shap documentation
    
    * update logic, add tests
    
    * whitespace fixes
    
    * whitespace fixes for test_helpers
    
    * namespace for sd function
    
    * explicitly declare variables that are automatically evaluated by data.table
    
    * Fix R lint
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [R] fix uses of 1:length(x) and other small things (#5992)
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    Issue warning when requesting bivariate plotting (#3516)
    [R] AppVeyor CI for R package (#2954)
    
    * [R] fix finding R.exe with cmake on WIN when it is in PATH
    
    * [R] appveyor config for R package
    
    * [R] wrap the lines to make R check happier
    
    * [R] install only binary dep-packages in appveyor
    
    * [R] for MSVC appveyor, also build a binary for R package and keep as an artifact
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    Fix CRAN check warnings/notes (#3988)
    
    * fix
    
    * reorder declaration to match initialization
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    Fix the "No visible binding" CRAN checks (#1504)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] parameter style consistency
    New documentation rewording
    Wording improvement
    fix relative to examples #Rstat
    fix relative to examples #Rstat
    some fixes for Travis #Rstat
    some fixes for Travis #Rstat
    Polishing API + wording in function description #Rstat
    Polishing API + wording in function description #Rstat
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Add SHAP summary plot using ggplot2 (#5882)
    
    * add SHAP summary plot using ggplot2
    
    * Update xgb.plot.shap
    
    * Update example in xgb.plot.shap documentation
    
    * update logic, add tests
    
    * whitespace fixes
    
    * whitespace fixes for test_helpers
    
    * namespace for sd function
    
    * explicitly declare variables that are automatically evaluated by data.table
    
    * Fix R lint
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    Update R-package/R/xgb.ggplot.R (#3820)
    
    Changed width parameter of var important ggplot from 0.05 to 0.5 to make it more visible when displaying more variables.
    Fix the "No visible binding" CRAN checks (#1504)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R] Add a compatibility layer to load Booster object from an old RDS file (#5940)
    
    * [R] Add a compatibility layer to load Booster from an old RDS
    * Modify QuantileHistMaker::LoadConfig() to be backward compatible with 1.1.x
    * Add a big warning about compatibility in QuantileHistMaker::LoadConfig()
    * Add testing suite
    * Discourage use of saveRDS() in CRAN doc
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Restore attributes in complete. (#5573)
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    Fixing a few typos (#1771)
    
    * Fixing a few typos
    
    * Fixing a few typos
    [R] parameter style consistency
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    Code: Lint fixes on trailing spaces
    allow xgb.load re-use raw information if necessary
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    fix segfault and add two function for handle and booster
    fix with new predict
    add handle and raw structure to xgb.Booster
    replace iris in docs
    fix iris multiclass problem
    fix NAMESPACE
    add documentation notes
    modify xgb.getinfo to getinfo
    major change in the design of R interface
    [R] Remove stringi dependency (#6109)
    
    * [R] Fix empty empty tests and a test warnings
    
    * [R] Remove stringi dependency (fix #5905)
    
    * Fix R lint check
    
    * [R] Fix automatic conversion to factor in R < 4.0.0 in xgb.model.dt.tree
    
    * Add `R` Makefile variable
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    [R] xgb.plot.tree fixes (#1939)
    
    * [R] a few fixes and improvements to xgb.plot.tree
    
    * [R] deprecate n_first_tree replace with trees; fix types in xgb.model.dt.tree
    0.6-4 submission (#1935)
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] parameter style consistency
    New documentation rewording
    some fixes for Travis #Rstat
    some fixes for Travis #Rstat
    Add new tests for new functions
    Add new tests for new functions
    Polishing API + wording in function description #Rstat
    Polishing API + wording in function description #Rstat
    Fix missing dependencies
    Fix missing dependencies
    add exclusion of global variables + generate Roxygen doc
    add exclusion of global variables + generate Roxygen doc
    Improve description wording
    Improve description wording
    Add new multi.tree function to R package
    Add new multi.tree function to R package
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    replace nround with nrounds to match actual parameter (#3592)
    Replace cBind by cbind (#3203)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    
    * change Matrix cBind to cbind
    
    * fix
    
    * fix error in demo
    
    * fix examples
    [R] Fix for cran submission of xgboost 0.6 (#1875)
    
    fix cran check
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] parameter style consistency
    Wording improvement
    Generate new features based on tree leafs
    [R] Remove stringi dependency (#6109)
    
    * [R] Fix empty empty tests and a test warnings
    
    * [R] Remove stringi dependency (fix #5905)
    
    * Fix R lint check
    
    * [R] Fix automatic conversion to factor in R < 4.0.0 in xgb.model.dt.tree
    
    * Add `R` Makefile variable
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    CRAN Submission for 0.71.1 (#3311)
    
    * fix for CRAN manual checks
    
    * fix for CRAN manual checks
    
    * pass local check
    
    * fix variable naming style
    
    * Adding Philip's record
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R] Fix for cran submission of xgboost 0.6 (#1875)
    
    fix cran check
    [R-package] JSON dump format and a couple of bugfixes (#1855)
    
    * [R-package] JSON tree dump interface
    
    * [R-package] precision bugfix in xgb.attributes
    
    * [R-package] bugfix for cb.early.stop called from xgb.cv
    
    * [R-package] a bit more clarity on labels checking in xgb.cv
    
    * [R-package] test JSON dump for gblinear as well
    
    * whitespace lint
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R] simplified the code; parameter style consistency
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    Code: Lint fixes on trailing spaces
    update links dmlc
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    fix segfault and add two function for handle and booster
    add handle and raw structure to xgb.Booster
    fix some warning in Cran check
    fix bug
    Optimization in dump function (replaced some regular R function by data.table)
    Avoid some Cran check error messages
    resolving not-CRAN issues
    add new parameters to several functions avoid the need of a text dump
    documentation example change
    fix plenty of small bugs
    better doc of dump function
    small change in import lib
    only replace tabulation which begins a line (avoid wrong replacement in feature name)
    refactor dump function to adapt to the new possibilities of exporting a String
    C part export a model dump string
    add dump statistics
    remove incorrect link to old folders
    replace iris in docs
    fix iris multiclass problem
    attemp to fix line breaking issue of doc
    fix doc with redirection to inst/examples
    fix NAMESPACE
    get a pass in function docstring
    add documentation notes
    modify xgb.getinfo to getinfo
    major change in the design of R interface
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    [R] simplified the code; parameter style consistency
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    update doc, resolve warnings
    fix save.raw
    add handle and raw structure to xgb.Booster
    Update xgb.save.raw.R
    add saveload to raw
    replace iris in docs
    fix iris multiclass problem
    refine vignette
    fix NAMESPACE
    add documentation notes
    modify xgb.getinfo to getinfo
    refinement of R package
    major change in the design of R interface
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    replace nround with nrounds to match actual parameter (#3592)
    CRAN Submission for 0.71.1 (#3311)
    
    * fix for CRAN manual checks
    
    * fix for CRAN manual checks
    
    * pass local check
    
    * fix variable naming style
    
    * Adding Philip's record
    Replace cBind by cbind (#3203)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    
    * change Matrix cBind to cbind
    
    * fix
    
    * fix error in demo
    
    * fix examples
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    [R] many minor changes to increase the robustness of the R code (#2404)
    
    * many minor changes to increase robustness of R code
    
    * fixing which mistake in xgb.model.dt.tree.R and a few cosmetics
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R-package] JSON dump format and a couple of bugfixes (#1855)
    
    * [R-package] JSON tree dump interface
    
    * [R-package] precision bugfix in xgb.attributes
    
    * [R-package] bugfix for cb.early.stop called from xgb.cv
    
    * [R-package] a bit more clarity on labels checking in xgb.cv
    
    * [R-package] test JSON dump for gblinear as well
    
    * whitespace lint
    [R-package] a few fixes for R (#1485)
    
    * [R] fix #1465
    
    * [R] add sanity check to fix #1434
    
    * [R] some clean-ups for custom obj&eval; require maximize only for early stopping
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R] CB naming change; cv-prediction as CB; add.cb function to ensure proper CB order; docs; minor fixes + changes
    R-callbacks refactor
    Add MAPE metric (#6119)
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    Document more objective parameters in R package (#5682)
    corrected spelling of 'list' (#5482)
    Added new train_folds parameter (#5114)
    
    https://stackoverflow.com/questions/32433458/how-to-specify-train-and-test-indices-for-xgb-cv-in-r-package-xgboost/51412073
    [R] Use built-in label when xgb.DMatrix is given to xgb.cv() (#4631)
    
    * Use built-in label when xgb.DMatrix is given to xgb.cv()
    
    * Add a test
    
    * Fix test
    
    * Bump version number
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Fix typo (#3305)
    rank_metric: add AUC-PR (#3172)
    
    * rank_metric: add AUC-PR
    
    Implementation of the AUC-PR calculation for weighted data, proposed by Keilwagen, Grosse and Grau (https://doi.org/10.1371/journal.pone.0092209)
    
    * rank_metric: fix lint warnings
    
    * Implement tests for AUC-PR and fix implementation
    
    * add aucpr to documentation for other languages
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    [R] many minor changes to increase the robustness of the R code (#2404)
    
    * many minor changes to increase robustness of R code
    
    * fixing which mistake in xgb.model.dt.tree.R and a few cosmetics
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R-package] JSON dump format and a couple of bugfixes (#1855)
    
    * [R-package] JSON tree dump interface
    
    * [R-package] precision bugfix in xgb.attributes
    
    * [R-package] bugfix for cb.early.stop called from xgb.cv
    
    * [R-package] a bit more clarity on labels checking in xgb.cv
    
    * [R-package] test JSON dump for gblinear as well
    
    * whitespace lint
    Typo Problem (#1759)
    
    cross validation
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] CB naming change; cv-prediction as CB; add.cb function to ensure proper CB order; docs; minor fixes + changes
    add best_ntreelimit attribute
    print.xgb.cv fix
    R-callbacks refactor
    convert S4 to S3; add some extra methods to DMatrix
    Fix CV which was monitoring train-metric
    
    https://github.com/dmlc/xgboost/issues/807
    Fixed off by 1 bug in xgb.cv
    Add newline char to early.stop.round message
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    Fix travis build (+1 squashed commit)
    Squashed commits:
    [9240d5f] Fix Travis build
    Fixed most of the lint issues
    Lint fix on infix operators
    Lint fix on consistent assignment
    Code: Lint fixes on trailing spaces
    Code: Some Lint fixes
    Switch missing values from 0 to NA in R package
    fix namespace and desc
    Update xgb.cv.R
    fix early stopping and prediction
    check duplicated params
    Update xgb.cv.R
    Update xgb.cv.R
    rename arguments to be dot-seperated
    customized obj and feval interface
    xgb.csv(printEveryN) parameter to print every n-th progress message
    fix early stopping
    add early stopping to xgb.cv
    make it possible to use a list of pre-defined CV folds in xgb.cv
    force xgb.cv to return numeric performance values instead of character; update its docs
    added an option for stratified CV to xgb.cv
    fix multi cv pred
    optim pred in cv
    fix matrix form prediction
    Cross validation documentation improvement
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    Fix bug in Cross Validation when showsd = FALSE
    update document
    Avoid some Cran check error messages
    fix some issues from the cran check
    documentation update
    resolving not-CRAN issues
    improve demo of cv in R
    enable returning prediction in cv
    fix a small bug in CV function
    small fix in parsing
    Add a new verbose parameter to print progress during the process (set to true by default to not change behavior of existing code) + source code refactoring
    parse history first line to guess which columns are required
    fix some missing imports
    return history as data.table for cross validation + documentation
    Missing parameter documentation
    Update xgb.cv.R
    
    add parameter missing
    refine style with max.depth
    remove incorrect link to old folders
    replace iris in docs
    refine doc, with Rd
    ok
    chg cv
    add cross validation
    style cleanup, incomplete CV
    Add MAPE metric (#6119)
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    Document more objective parameters in R package (#5682)
    Enable parameter validation for R. (#5569)
    
    * Enable parameter validation for R.
    
    * Add test.
    Remove silent parameter. (#5476)
    Check against R seed. (#5125)
    
    
    
    * Handle it in R instead.
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    [TREE] add interaction constraints (#3466)
    
    * add interaction constraints
    
    * enable both interaction and monotonic constraints at the same time
    
    * fix lint
    
    * add R test, fix lint, update demo
    
    * Use dmlc::JSONReader to express interaction constraints as nested lists; Use sparse arrays for bookkeeping
    
    * Add Python test for interaction constraints
    
    * make R interaction constraints parameter based on feature index instead of column names, fix R coding style
    
    * Fix lint
    
    * Add BlueTea88 to CONTRIBUTORS.md
    
    * Short circuit when no constraint is specified; address review comments
    
    * Add tutorial for feature interaction constraints
    
    * allow interaction constraints to be passed as string, remove redundant column_names argument
    
    * Fix typo
    
    * Address review comments
    
    * Add comments to Python test
    replace nround with nrounds to match actual parameter (#3592)
    rank_metric: add AUC-PR (#3172)
    
    * rank_metric: add AUC-PR
    
    Implementation of the AUC-PR calculation for weighted data, proposed by Keilwagen, Grosse and Grau (https://doi.org/10.1371/journal.pone.0092209)
    
    * rank_metric: fix lint warnings
    
    * Implement tests for AUC-PR and fix implementation
    
    * add aucpr to documentation for other languages
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    A fix for CRAN submission of version 0.7-0 (#3061)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    [R] many minor changes to increase the robustness of the R code (#2404)
    
    * many minor changes to increase robustness of R code
    
    * fixing which mistake in xgb.model.dt.tree.R and a few cosmetics
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R] Fix for cran submission of xgboost 0.6 (#1875)
    
    fix cran check
    [CORE] The update process for a tree model, and its application to feature importance (#1670)
    
    * [CORE] allow updating trees in an existing model
    
    * [CORE] in refresh updater, allow keeping old leaf values and update stats only
    
    * [R-package] xgb.train mod to allow updating trees in an existing model
    
    * [R-package] added check for nrounds when is_update
    
    * [CORE] merge parameter declaration changes; unify their code style
    
    * [CORE] move the update-process trees initialization to Configure; rename default process_type to 'default'; fix the trees and trees_to_update sizes comparison check
    
    * [R-package] unit tests for the update process type
    
    * [DOC] documentation for process_type parameter; improved docs for updater, Gamma and Tweedie; added some parameter aliases; metrics indentation and some were non-documented
    
    * fix my sloppy merge conflict resolutions
    
    * [CORE] add a TreeProcessType enum
    
    * whitespace fix
    Fixing a few typos (#1771)
    
    * Fixing a few typos
    
    * Fixing a few typos
    [R] Monotonic Constraints in Tree Construction (#1557)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    [R-package] a few fixes for R (#1485)
    
    * [R] fix #1465
    
    * [R] add sanity check to fix #1434
    
    * [R] some clean-ups for custom obj&eval; require maximize only for early stopping
    [R] CB naming change; cv-prediction as CB; add.cb function to ensure proper CB order; docs; minor fixes + changes
    add best_ntreelimit attribute
    R-callbacks refactor
    Add newline char to early.stop.round message
    Expose model parameters to R
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    Fix travis build (+1 squashed commit)
    Squashed commits:
    [9240d5f] Fix Travis build
    Fixed most of the lint issues
    Lint fix on infix operators
    Lint fix on consistent assignment
    add save_period
    slight update in documentation
    check duplicated params
    rename arguments to be dot-seperated
    change doc and demo for new obj feval interface
    customized obj and feval interface
    Update xgb.train.R
    fix early stopping
    new parameter in xgboost() and xgb.train() to print every N-th progress message
    support both early stop name
    Regularization parameters documentation improvement
    add demo for early_stopping in R
    fix logic
    add early stopping to R
    small change in the documentation
    multiclass documentation
    Experimental parameter
    Add Random Forest parameter (num_parallel_tree) in function doc + example in Vignette.
    update links dmlc
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    improve function documentation.
    
    Switch xgboost detailed parameters with xgb.tain function.
    fix segfault and add two function for handle and booster
    add handle and raw structure to xgb.Booster
    refine style with max.depth
    remove incorrect link to old folders
    replace iris in docs
    add basic walkthrough
    style cleanup, incomplete CV
    fix iris multiclass problem
    attemp to fix line breaking issue of doc
    fix doc with redirection to inst/examples
    add vignette
    speed test for R, and refinement of item list in doc
    fix NAMESPACE
    compile Rd files, i.e. R documents
    OK
    chg
    get a pass in function docstring
    add documentation notes
    remove default value for nrounds
    modify xgb.getinfo to getinfo
    major change in the design of R interface
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    Add Github Action for R. (#5911)
    
    * Fix lintr errors.
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R] simplified the code; parameter style consistency
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    Fixed most of the lint issues
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    fix segfault and add two function for handle and booster
    add handle and raw structure to xgb.Booster
    replace iris in docs
    fix iris multiclass problem
    refine vignette
    fix NAMESPACE
    add documentation notes
    modify xgb.getinfo to getinfo
    refinement of R package
    major change in the design of R interface
    resolving not-CRAN issues
    fix data labels
    re-compress the data
    add mushroom data
    resolving not-CRAN issues
    fix data labels
    re-compress the data
    add mushroom data
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] docs update - callbacks and parameter style
    [R] more attribute handling functionality
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    Update R doc by roxygen2. (#5201)
    replace nround with nrounds to match actual parameter (#3592)
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] Fix for cran submission of xgboost 0.6 (#1875)
    
    fix cran check
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] docs update - callbacks and parameter style
    Wording improvement
    Generate new features based on tree leafs
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] docs update - callbacks and parameter style
    [R] more attribute handling functionality
    R accessors for model attributes
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    Update R doc by roxygen2. (#5201)
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R] xgb.plot.tree fixes (#1939)
    
    * [R] a few fixes and improvements to xgb.plot.tree
    
    * [R] deprecate n_first_tree replace with trees; fix types in xgb.model.dt.tree
    [R] docs update - callbacks and parameter style
    make travis happy
    some more xgb.model.dt.tree improvements
    xgb.model.dt.tree up to x100 faster
    model dt tree function documentation improvement
    some fixes for Travis #Rstat
    some fixes for Travis #Rstat
    Polishing API + wording in function description #Rstat
    Polishing API + wording in function description #Rstat
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    fix documentation
    wording
    wording
    new included feature in dt.tree function
    doc
    Documentation: no need to save model in txt...
    edit document
    documentation update
    documentation update
    add new parameters model to avoid the use of dump file for functions plot, dt.tree, importance
    add new size parameter for plot function
    refactoring of importance function
    add new function to read model and use it in the plot function
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] docs update - callbacks and parameter style
    R-callbacks docs
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    make travis happy
    fix print.xgb.DMatrix doc
    convert S4 to S3; add some extra methods to DMatrix
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    Update R doc by roxygen2. (#5201)
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] docs update - callbacks and parameter style
    R-callbacks docs
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Update R doc by roxygen2. (#5201)
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    [R] R-interface for SHAP interactions (#3636)
    
    * add R-interface for SHAP interactions
    
    * update docs for new roxygen version
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    adding feature contributions to R and gblinear (#2295)
    
    * [gblinear] add features contribution prediction; fix DumpModel bug
    
    * [gbtree] minor changes to PredContrib
    
    * [R] add feature contribution prediction to R
    
    * [R] bump up version; update NEWS
    
    * [gblinear] fix the base_margin issue; fixes #1969
    
    * [R] list of matrices as output of multiclass feature contributions
    
    * [gblinear] make order of DumpModel coefficients consistent: group index changes the fastest
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] docs update - callbacks and parameter style
    convert S4 to S3; add some extra methods to DMatrix
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R-package] a few fixes for R (#1485)
    
    * [R] fix #1465
    
    * [R] add sanity check to fix #1434
    
    * [R] some clean-ups for custom obj&eval; require maximize only for early stopping
    [R] docs update - callbacks and parameter style
    Replace cBind by cbind (#3203)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    
    * change Matrix cBind to cbind
    
    * fix
    
    * fix error in demo
    
    * fix examples
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] docs update - callbacks and parameter style
    R-callbacks docs
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Add configuration to R interface. (#5217)
    
    * Save and load internal parameter configuration as JSON.
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    convert S4 to S3; add some extra methods to DMatrix
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] docs update - callbacks and parameter style
    print.xgb.cv fix - Rd too
    R-callbacks docs
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Update R doc by roxygen2. (#5201)
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    [R] R-interface for SHAP interactions (#3636)
    
    * add R-interface for SHAP interactions
    
    * update docs for new roxygen version
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R] docs update - callbacks and parameter style
    add support of GLM model in importance plot function
    add support of GLM model in importance plot function
    fix example
    fix example
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    new plot feature importance function
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    convert S4 to S3; add some extra methods to DMatrix
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    doc
    edit document
    Documentation regenerated with fixes
    generated documentation with ROxygen2
    forced add doc for test
    add doc for agaricus.test
    add documentation for datasets
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    fix print.xgb.DMatrix doc
    convert S4 to S3; add some extra methods to DMatrix
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    Added test for eta decay (+3 squashed commits)
    Squashed commits:
    [9109887] Added test for eta decay(+1 squashed commit)
    Squashed commits:
    [1336bd4] Added tests for eta decay (+2 squashed commit)
    Squashed commits:
    [91aac2d] Added tests for eta decay (+1 squashed commit)
    Squashed commits:
    [3ff48e7] Added test for eta decay
    [6bb1eed] Rewrote Rd files
    [bf0dec4] Added learning_rates for diff eta in each boosting round
    update roxygen2
    add early stopping to R
    Roxygen update
    doc
    update document
    edit document
    generated documentation with ROxygen2
    replace iris in docs
    fix iris to Rd files
    compile Rd files, i.e. R documents
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] docs update - callbacks and parameter style
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    fix save.raw doc
    fix some warning in Cran check
    doc
    edit document
    generated documentation with ROxygen2
    replace iris in docs
    fix iris to Rd files
    compile Rd files, i.e. R documents
    Add MAPE metric (#6119)
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    Document more objective parameters in R package (#5682)
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Update R doc by roxygen2. (#5201)
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    [R] R-interface for SHAP interactions (#3636)
    
    * add R-interface for SHAP interactions
    
    * update docs for new roxygen version
    CRAN Submission for 0.71.1 (#3311)
    
    * fix for CRAN manual checks
    
    * fix for CRAN manual checks
    
    * pass local check
    
    * fix variable naming style
    
    * Adding Philip's record
    rank_metric: add AUC-PR (#3172)
    
    * rank_metric: add AUC-PR
    
    Implementation of the AUC-PR calculation for weighted data, proposed by Keilwagen, Grosse and Grau (https://doi.org/10.1371/journal.pone.0092209)
    
    * rank_metric: fix lint warnings
    
    * Implement tests for AUC-PR and fix implementation
    
    * add aucpr to documentation for other languages
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R-package] JSON dump format and a couple of bugfixes (#1855)
    
    * [R-package] JSON tree dump interface
    
    * [R-package] precision bugfix in xgb.attributes
    
    * [R-package] bugfix for cb.early.stop called from xgb.cv
    
    * [R-package] a bit more clarity on labels checking in xgb.cv
    
    * [R-package] test JSON dump for gblinear as well
    
    * whitespace lint
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    [R] docs update - callbacks and parameter style
    R-callbacks docs
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    Added test for eta decay (+3 squashed commits)
    Squashed commits:
    [9109887] Added test for eta decay(+1 squashed commit)
    Squashed commits:
    [1336bd4] Added tests for eta decay (+2 squashed commit)
    Squashed commits:
    [91aac2d] Added tests for eta decay (+1 squashed commit)
    Squashed commits:
    [3ff48e7] Added test for eta decay
    [6bb1eed] Rewrote Rd files
    [bf0dec4] Added learning_rates for diff eta in each boosting round
    rename arguments to be dot-seperated
    change doc and demo for new obj feval interface
    xgb.csv(printEveryN) parameter to print every n-th progress message
    add early stopping to xgb.cv
    update roxygen2
    add early stopping to R
    Roxygen update
    make it possible to use a list of pre-defined CV folds in xgb.cv
    force xgb.cv to return numeric performance values instead of character; update its docs
    update documentation for xgb.cv
    Cross validation documentation improvement
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    doc
    update document
    edit document
    documentation update
    Add a new verbose parameter to print progress during the process (set to true by default to not change behavior of existing code) + source code refactoring
    return history as data.table for cross validation + documentation
    Documentation regenerated with fixes
    generated documentation with ROxygen2
    refine style with max.depth
    remove incorrect link to old folders
    replace iris in docs
    add documentation for datasets
    refine doc, with Rd
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    Update R doc by roxygen2. (#5201)
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] xgb.plot.tree fixes (#1939)
    
    * [R] a few fixes and improvements to xgb.plot.tree
    
    * [R] deprecate n_first_tree replace with trees; fix types in xgb.model.dt.tree
    [R] docs update - callbacks and parameter style
    New documentation rewording
    fix relative to examples #Rstat
    fix relative to examples #Rstat
    some fixes for Travis #Rstat
    some fixes for Travis #Rstat
    add exclusion of global variables + generate Roxygen doc
    add exclusion of global variables + generate Roxygen doc
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    doc
    edit document
    documentation update
    add new parameters model to avoid the use of dump file for functions plot, dt.tree, importance
    add new size parameter for plot function
    add new parameters to several functions avoid the need of a text dump
    add new function to read model and use it in the plot function
    refactoring for perf
    add style option
    Change in aesthetic
    Improve documentation
    add limit number of trees option
    new documentation
    new import
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] docs update - callbacks and parameter style
    R-callbacks docs
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    convert S4 to S3; add some extra methods to DMatrix
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    Update R doc by roxygen2. (#5201)
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    [R] R-interface for SHAP interactions (#3636)
    
    * add R-interface for SHAP interactions
    
    * update docs for new roxygen version
    [R] AppVeyor CI for R package (#2954)
    
    * [R] fix finding R.exe with cmake on WIN when it is in PATH
    
    * [R] appveyor config for R package
    
    * [R] wrap the lines to make R check happier
    
    * [R] install only binary dep-packages in appveyor
    
    * [R] for MSVC appveyor, also build a binary for R package and keep as an artifact
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    Add MAPE metric (#6119)
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    Document more objective parameters in R package (#5682)
    Update R doc by roxygen2. (#5201)
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    [R] R-interface for SHAP interactions (#3636)
    
    * add R-interface for SHAP interactions
    
    * update docs for new roxygen version
    replace nround with nrounds to match actual parameter (#3592)
    rank_metric: add AUC-PR (#3172)
    
    * rank_metric: add AUC-PR
    
    Implementation of the AUC-PR calculation for weighted data, proposed by Keilwagen, Grosse and Grau (https://doi.org/10.1371/journal.pone.0092209)
    
    * rank_metric: fix lint warnings
    
    * Implement tests for AUC-PR and fix implementation
    
    * add aucpr to documentation for other languages
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    A fix for CRAN submission of version 0.7-0 (#3061)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R] Fix for cran submission of xgboost 0.6 (#1875)
    
    fix cran check
    Fixing a few typos (#1771)
    
    * Fixing a few typos
    
    * Fixing a few typos
    [R] Monotonic Constraints in Tree Construction (#1557)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    [R-package] a few fixes for R (#1485)
    
    * [R] fix #1465
    
    * [R] add sanity check to fix #1434
    
    * [R] some clean-ups for custom obj&eval; require maximize only for early stopping
    [R] docs update - callbacks and parameter style
    R-callbacks docs
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    add save_period
    rename arguments to be dot-seperated
    change doc and demo for new obj feval interface
    new parameter in xgboost() and xgb.train() to print every N-th progress message
    support both early stop name
    Regularization parameters documentation improvement
    update roxygen2
    add early stopping to R
    multiclass documentation
    Experimental parameter
    Add Random Forest parameter (num_parallel_tree) in function doc + example in Vignette.
    update links dmlc
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    improve function documentation.
    
    Switch xgboost detailed parameters with xgb.tain function.
    doc
    edit document
    generated documentation with ROxygen2
    refine style with max.depth
    remove incorrect link to old folders
    replace iris in docs
    refine doc, with Rd
    fix iris to Rd files
    attemp to fix line breaking issue of doc
    fix doc with redirection to inst/examples
    refinement of document
    speed test for R, and refinement of item list in doc
    compile Rd files, i.e. R documents
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    replace nround with nrounds to match actual parameter (#3592)
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] docs update - callbacks and parameter style
    R-callbacks docs
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] docs update - callbacks and parameter style
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    [R] docs update - callbacks and parameter style
    R-callbacks docs
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] docs update - callbacks and parameter style
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    doc
    edit document
    generated documentation with ROxygen2
    replace iris in docs
    add doc for agaricus.test
    fix iris to Rd files
    improve document format
    compile Rd files, i.e. R documents
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    Updates from 1.2.0 cran submission (#6077)
    
    * update for 1.2.0 cran submission
    
    * recover cmakelists
    
    * fix unittest from the shap PR
    
    * trigger CI
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    Update R doc by roxygen2. (#5201)
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    Fix CRAN check warnings/notes (#3988)
    
    * fix
    
    * reorder declaration to match initialization
    [R] R-interface for SHAP interactions (#3636)
    
    * add R-interface for SHAP interactions
    
    * update docs for new roxygen version
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [R] docs update - callbacks and parameter style
    New documentation rewording
    Wording improvement
    fix relative to examples #Rstat
    fix relative to examples #Rstat
    some fixes for Travis #Rstat
    some fixes for Travis #Rstat
    Polishing API + wording in function description #Rstat
    Polishing API + wording in function description #Rstat
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Plot model deepness
    
    New function to explore the model by ploting the way splits are done.
    Update R doc by roxygen2. (#5201)
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] xgb.plot.tree fixes (#1939)
    
    * [R] a few fixes and improvements to xgb.plot.tree
    
    * [R] deprecate n_first_tree replace with trees; fix types in xgb.model.dt.tree
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    [R] docs update - callbacks and parameter style
    New documentation rewording
    some fixes for Travis #Rstat
    some fixes for Travis #Rstat
    Add new tests for new functions
    Add new tests for new functions
    Polishing API + wording in function description #Rstat
    Polishing API + wording in function description #Rstat
    add exclusion of global variables + generate Roxygen doc
    add exclusion of global variables + generate Roxygen doc
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R] docs update - callbacks and parameter style
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    doc
    edit document
    generated documentation with ROxygen2
    replace iris in docs
    fix iris to Rd files
    compile Rd files, i.e. R documents
    [R] R raw serialization. (#5123)
    
    * Add bindings for serialization.
    * Change `xgb.save.raw' into full serialization instead of simple model.
    * Add `xgb.load.raw' for unserialization.
    * Run devtools.
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    doc
    edit document
    Documentation regenerated with fixes
    generated documentation with ROxygen2
    add doc for agaricus.test
    add documentation for datasets
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    Improve setinfo documentation on R package (#2357)
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    convert S4 to S3; add some extra methods to DMatrix
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    doc
    small changes in doc
    doc
    edit document
    generated documentation with ROxygen2
    replace iris in docs
    refine doc, with Rd
    fix iris to Rd files
    improve document format
    change getinfo Rd
    compile Rd files, i.e. R documents
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    Update R doc by roxygen2. (#5201)
    [R] xgb.importance: fix for multiclass gblinear, new 'trees' parameter (#2388)
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    Fix for CRAN Submission (#1826)
    
    * fix cran check
    
    * change required R version because of utils::globalVariables
    
    * temporary commit, monotone not working
    
    * fix test
    
    * fix doc
    
    * fix doc
    
    * fix cran note and warning
    
    * improve checks
    
    * fix urls
    [R] docs update - callbacks and parameter style
    wording fix
    Wording improvement
    Generate new features based on tree leafs
    Small rewording function xgb.importance
    Small rewording function xgb.importance
    fix example
    fix example
    fix relative to examples #Rstat
    fix relative to examples #Rstat
    Fix Rstat
    Fix Rstat
    Polishing API + wording in function description #Rstat
    Polishing API + wording in function description #Rstat
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    Documentation feature importance
    better co occurence function
    Bug + documentation
    Add new co-occurence computation capacity to importance feature function + related documentation
    doc
    edit document
    documentation update
    add new parameters to several functions avoid the need of a text dump
    refactor dump function to adapt to the new possibilities of exporting a String
    refactoring of importance function
    small documentation change
    documentation wording
    regeneration of documentation
    generated documentation with ROxygen2
    Improve setinfo documentation on R package (#2357)
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R-package] various fixes for R CMD check (#1328)
    
    * [R] fix xgb.create.features
    
    * [R] fixes for R CMD check
    convert S4 to S3; add some extra methods to DMatrix
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    doc
    small changes in doc
    doc
    edit document
    generated documentation with ROxygen2
    replace iris in docs
    refine doc, with Rd
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    Update R doc by roxygen2. (#5201)
    [R] R-interface for SHAP interactions (#3636)
    
    * add R-interface for SHAP interactions
    
    * update docs for new roxygen version
    CRAN Submission for 0.71.1 (#3311)
    
    * fix for CRAN manual checks
    
    * fix for CRAN manual checks
    
    * pass local check
    
    * fix variable naming style
    
    * Adding Philip's record
    [R] maintenance Apr 2017 (#2237)
    
    * [R] make sure things work for a single split model; fixes #2191
    
    * [R] add option use_int_id to xgb.model.dt.tree
    
    * [R] add example of exporting tree plot to a file
    
    * [R] set save_period = NULL as default in xgboost() to be the same as in xgb.train; fixes #2182
    
    * [R] it's a good practice after CRAN releases to bump up package version in dev
    
    * [R] allow xgb.DMatrix construction from integer dense matrices
    
    * [R] xgb.DMatrix: silent parameter; improve documentation
    
    * [R] xgb.model.dt.tree code style changes
    
    * [R] update NEWS with parameter changes
    
    * [R] code safety & style; handle non-strict matrix and inherited classes of input and model; fixes #2242
    
    * [R] change to x.y.z.p R-package versioning scheme and set version to 0.6.4.3
    
    * [R] add an R package versioning section to the contributors guide
    
    * [R] R-package/README.md: clean up the redundant old installation instructions, link the contributors guide
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R] Fix for cran submission of xgboost 0.6 (#1875)
    
    fix cran check
    [R-package] JSON dump format and a couple of bugfixes (#1855)
    
    * [R-package] JSON tree dump interface
    
    * [R-package] precision bugfix in xgb.attributes
    
    * [R-package] bugfix for cb.early.stop called from xgb.cv
    
    * [R-package] a bit more clarity on labels checking in xgb.cv
    
    * [R-package] test JSON dump for gblinear as well
    
    * whitespace lint
    [R] docs update - callbacks and parameter style
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    update links dmlc
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    doc
    edit document
    documentation update
    documentation example change
    better doc of dump function
    refactor dump function to adapt to the new possibilities of exporting a String
    regeneration of documentation
    generated documentation with ROxygen2
    remove incorrect link to old folders
    replace iris in docs
    fix iris to Rd files
    attemp to fix line breaking issue of doc
    fix doc with redirection to inst/examples
    compile Rd files, i.e. R documents
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] docs update - callbacks and parameter style
    R-callbacks docs
    fixed typos in R package docs (#4345)
    
    * fixed typos in R package docs
    
    * updated verbosity parameter in xgb.train docs
    CRAN Submission for 0.71.1 (#3311)
    
    * fix for CRAN manual checks
    
    * fix for CRAN manual checks
    
    * pass local check
    
    * fix variable naming style
    
    * Adding Philip's record
    Replace cBind by cbind (#3203)
    
    * modify test_helper.R
    
    * fix noLD
    
    * update desc
    
    * fix solaris test
    
    * fix desc
    
    * improve fix
    
    * fix url
    
    * change Matrix cBind to cbind
    
    * fix
    
    * fix error in demo
    
    * fix examples
    Remove silent from R demos. (#5675)
    
    * Remove silent from R demos.
    
    * Vignettes.
    fix additional files note (#4699)
    
    * fix additional files note
    
    * Trigger CI
    
    * Trigger CI
    R maintenance Feb2017 (#2045)
    
    * [R] better argument check in xgb.DMatrix; fixes #1480
    
    * [R] showsd was a dummy; fixes #2044
    
    * [R] better categorical encoding explanation in vignette; fixes #1989
    
    * [R] new roxygen version docs update
    [R] various R code maintenance (#1964)
    
    * [R] xgb.save must work when handle in nil but raw exists
    
    * [R] print.xgb.Booster should still print other info when handle is nil
    
    * [R] rename internal function xgb.Booster to xgb.Booster.handle to make its intent clear
    
    * [R] rename xgb.Booster.check to xgb.Booster.complete and make it visible; more docs
    
    * [R] storing evaluation_log should depend only on watchlist, not on verbose
    
    * [R] reduce the excessive chattiness of unit tests
    
    * [R] only disable some tests in windows when it's not 64-bit
    
    * [R] clean-up xgb.DMatrix
    
    * [R] test xgb.DMatrix loading from libsvm text file
    
    * [R] store feature_names in xgb.Booster, use them from utility functions
    
    * [R] remove non-functional co-occurence computation from xgb.importance
    
    * [R] verbose=0 is enough without a callback
    
    * [R] added forgotten xgb.Booster.complete.Rd; cran check fixes
    
    * [R] update installation instructions
    [R] docs update - callbacks and parameter style
    Update lib version dependencies (for DiagrammeR mainly)
    Fix @export tag in each R file (for Roxygen 5, otherwise it doesn't work anymore)
    Regerate Roxygen doc
    update roxygen2
    add early stopping to R
    Roxygen update
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    doc
    edit document
    generated documentation with ROxygen2
    replace iris in docs
    fix iris to Rd files
    compile Rd files, i.e. R documents
    [R] Remove dependency on gendef for Visual Studio builds (fixes #5608) (#5764)
    
    * [R-package] Remove dependency on gendef for Visual Studio builds (fixes #5608)
    
    * clarify docs
    
    * removed debugging print statement
    
    * Make R CMake install more robust
    
    * Fix doc format; add ToC
    
    * Update build.rst
    
    * Fix AppVeyor
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    DOC: Typo in README.md in tests folder
    last check
    [CI] Enforce daily budget in Jenkins CI (#5884)
    
    * [CI] Throttle Jenkins CI
    
    * Don't use Jenkins master instance
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Fix loading old model. (#5724)
    
    
    * Add test.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Fix data loading (#4862)
    
    * Fix loading text data.
    * Fix config regex.
    * Try to explain the error better in exception.
    * Update doc.
    Remove remaining reg:linear. (#4544)
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Feature weights (#5962)
    Fix prediction heuristic (#5955)
    
    
    * Relax check for prediction.
    * Relax test in spark test.
    * Add tests in C++.
    Update Python demos with tests. (#5651)
    
    * Remove GPU memory usage demo.
    * Add tests for demos.
    * Remove `silent`.
    * Remove shebang as it's not portable.
    [Breaking] Set output margin to True for custom objective. (#5564)
    
    * Set output margin to True for custom objective in Python and R.
    
    * Add a demo for writing multi-class custom objective function.
    
    * Run tests on selected demos.
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix plotting test. (#6040)
    
    Previously the test loads a model generated by `test_basic.py`, now we generate
    the model explicitly.
    
    * Cleanup saved files for basic tests.
    Update JSON schema. (#5982)
    
    
    * Update JSON schema for pseudo huber.
    * Update JSON model schema.
    Update Python custom objective demo. (#5981)
    Fix r early stop with custom objective. (#5923)
    
    * Specify `ntreelimit`.
    Add JSON schema to model dump. (#5660)
    Restore loading model from buffer. (#5360)
    Fix changing locale. (#5314)
    
    * Fix changing locale.
    
    * Don't use locale guard.
    
    As number parsing is implemented in house, we don't need locale.
    
    * Update doc.
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Save Scikit-Learn attributes into learner attributes. (#5245)
    
    * Remove the recommendation for pickle.
    
    * Save skl attributes in booster.attr
    
    * Test loading scikit-learn model with native booster.
    Add dart to JSON schema. (#5218)
    
    * Add dart to JSON schema.
    
    * Use spaces instead of tab.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    [Breaking] Remove `learning_rates` in Python. (#5155)
    
    * Remove `learning_rates`.
    
    It's been deprecated since we have callback.
    
    * Set `before_iteration` of `reset_learning_rate` to False to preserve
      the initial learning rate, and comply to the term "reset".
    
    Closes #4709.
    
    * Tests for various `tree_method`.
    Add base margin to sklearn interface. (#5151)
    Example JSON model parser and Schema. (#5137)
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Small refinements for JSON model. (#5112)
    
    * Naming consistency.
    
    * Remove duplicated test.
    Model IO in JSON. (#5110)
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Revert "Fix #3485, #3540: Don't use dropout for predicting test sets" (#3563)
    
    * Revert "Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)"
    
    This reverts commit 44811f233071c5805d70c287abd22b155b732727.
    
    * Document behavior of predict() for DART booster
    
    * Add notice to parameter.rst
    Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)
    
    * Fix #3485, #3540: Don't use dropout for predicting test sets
    
    Dropout (for DART) should only be used at training time.
    
    * Add regression test
    GLM test unit: make run deterministic (#2147)
    Bumped up err assert in glm test (#1792)
    add Dart booster (#1220)
    Fixes for multiple and default metric (#1239)
    
    * fix multiple evaluation metrics
    
    * create DefaultEvalMetric only when really necessary
    
    * py test for #1239
    
    * make travis happy
    make travis happy
    py test for #1239
    [py] eta decay bugfix
    Bug mixing DMatrix's with and without feature names
    Enable flake8
    Separate dependencies and lightweight test env for Python
    [PYTHON] Simplify training logic, update rabit lib
    Added test for maximize parameter
    Fixed bug in eta decay (+2 squashed commits)
    Squashed commits:
    [b67caf2] Fix build
    [365ceaa] Fixed bug in eta decay
    Added test for eta decay (+3 squashed commits)
    Squashed commits:
    [9109887] Added test for eta decay(+1 squashed commit)
    Squashed commits:
    [1336bd4] Added tests for eta decay (+2 squashed commit)
    Squashed commits:
    [91aac2d] Added tests for eta decay (+1 squashed commit)
    Squashed commits:
    [3ff48e7] Added test for eta decay
    [6bb1eed] Rewrote Rd files
    [bf0dec4] Added learning_rates for diff eta in each boosting round
    Added fixed random seed for tests (+1 squashed commit)
    Squashed commits:
    [76e3664] Added fixed random seed for tests
    Deleted redundant blank lines
    TST: Added tests for binary classification
    TST: Added test for custom_objective function in cv
    TST: Added test for show_stdv when using cv
    TST: Added test for fpreproc
    TST: More thorough checks for Python tests
    TST: Added Python test for custom objective functions
    TST: Added glm test for Python
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    Modin DF support (#6055)
    
    * Modin DF support
    
    * mode change
    
    * tests were added, ci env was extended
    
    * mode change
    
    * Remove redundant installation of modin
    
    * Add a pytest skip marker for modin
    
    * Install Modin[ray] from PyPI
    
    * fix interfering
    
    * avoid extra conversion
    
    * delete cv test for modin
    
    * revert cv function
    
    Co-authored-by: ShvetsKS <kirill.shvets@intel.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Unify evaluation functions. (#6037)
    GPUTreeShap (#6038)
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix prediction heuristic (#5955)
    
    
    * Relax check for prediction.
    * Relax test in spark test.
    * Add tests in C++.
    Further improvements and savings in Jenkins pipeline (#5904)
    
    * Publish artifacts only on the master and release branches
    
    * Build CUDA only for Compute Capability 7.5 when building PRs
    
    * Run all Windows jobs in a single worker image
    
    * Build nightly XGBoost4J SNAPSHOT JARs with Scala 2.12 only
    
    * Show skipped Python tests on Windows
    
    * Make Graphviz optional for Python tests
    
    * Add back C++ tests
    
    * Unstash xgboost_cpp_tests
    
    * Fix label to CUDA 10.1
    
    * Install cuPy for CUDA 10.1
    
    * Install jsonschema
    
    * Address reviewer's feedback
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    [dask] Return GPU Series when input is from cuDF. (#5710)
    
    
    * Refactor predict function.
    Define lazy isinstance for Python compat. (#5364)
    
    * Avoid importing datatable.
    * Fix #5363.
    Support dmatrix construction from cupy array (#5206)
    Example JSON model parser and Schema. (#5137)
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    [CI] Run cuDF tests in Jenkins CI server (#4927)
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Add native support for Dask (#4473)
    
    * Add native support for Dask
    
    * Add multi-GPU demo
    
    * Add sklearn example
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Allow import via python datatable. (#3272)
    
    * Allow import via python datatable.
    
    * Write unit tests
    
    * Refactor dt API functions
    
    * Refactor python code
    
    * Lint fixes
    
    * Address review comments
    [PYTHON] Refactor trainnig API to use callback
    DOC/TST: Fix Python sklearn dep
    [Dask] Asyncio support. (#5862)
    Setup github action. (#5917)
    Add Python binding for rabit ops. (#5743)
    [python package] include dmlc-tracker into xgb python pkg (#4731)
    Cleanup Python code. (#6223)
    
    
    * Remove pathlike as XGBoost 1.2 requires Python 3.6.
    * Move conditional import of dask/distributed into dask module.
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix plotting test. (#6040)
    
    Previously the test loads a model generated by `test_basic.py`, now we generate
    the model explicitly.
    
    * Cleanup saved files for basic tests.
    Simplify the data backends. (#5893)
    Define lazy isinstance for Python compat. (#5364)
    
    * Avoid importing datatable.
    * Fix #5363.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    [Breaking] Remove num roots. (#5059)
    External data adapters (#5044)
    
    * Use external data adapters as lightweight intermediate layer between external data and DMatrix
    Complete cudf support. (#4850)
    
    
    * Handles missing value.
    * Accept all floating point and integer types.
    * Move to cudf 9.0 API.
    * Remove requirement on `null_count`.
    * Arbitrary column types support.
    add os.PathLike support for file paths to DMatrix and Booster Python classes (#4757)
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Fix #4163: always copy sliced data (#4165)
    
    * Revert "Accept numpy array view. (#4147)"
    
    This reverts commit a985a99cf0dacb26a5d734835473d492d3c2a0df.
    
    * Fix #4163: always copy sliced data
    
    * Remove print() from the test; check shape equality
    
    * Check if 'base' attribute exists
    
    * Fix lint
    
    * Address reviewer comment
    
    * Fix lint
    Accept numpy array view. (#4147)
    
    * Accept array view (slice) in metainfo.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Fix #3714: preserve feature names when slicing DMatrix (#3766)
    
    * Fix #3714: preserve feature names when slicing DMatrix
    
    * Add test
    Fix get_uint_info() (#3442)
    
    
    
    * Add regression test
    allow arbitrary cross validation fold indices (#3353)
    
    * allow arbitrary cross validation fold indices
    
     - use training indices passed to `folds` parameter in `training.cv`
     - update doc string
    
    * add tests for arbitrary fold indices
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation (#2530)
    
    * Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation from numpy arrays for python interface.
    Add prediction of feature contributions (#2003)
    
    * Add prediction of feature contributions
    
    This implements the idea described at http://blog.datadive.net/interpreting-random-forests/
    which tries to give insight in how a prediction is composed of its feature contributions
    and a bias.
    
    * Support multi-class models
    
    * Calculate learning_rate per-tree instead of using the one from the first tree
    
    * Do not rely on node.base_weight * learning_rate having the same value as the node mean value (aka leaf value, if it were a leaf); instead calculate them (lazily) on-the-fly
    
    * Add simple test for contributions feature
    
    * Check against param.num_nodes instead of checking for non-zero length
    
    * Loop over all roots instead of only the first
    option to shuffle data in mknfolds (#1459)
    
    * option to shuffle data in mknfolds
    
    * removed possibility to run as stand alone test
    
    * split function def in 2 lines for lint
    
    * option to shuffle data in mknfolds
    
    * removed possibility to run as stand alone test
    
    * split function def in 2 lines for lint
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    [PYTHON] Refactor trainnig API to use callback
    Added other feature importances in python package (#1135)
    
    * added new function to calculate other feature importances
    
    * added capability to plot other feature importance measures
    
    * changed plotting default to fscore
    
    * added info on importance_type to boilerplate comment
    
    * updated text of error statement
    
    * added self module name to fix call
    
    * added unit test for feature importances
    
    * style fixes
    Bug mixing DMatrix's with and without feature names
    Enable flake8
    Fix multi-class loading
    Separate dependencies and lightweight test env for Python
    [TEST] Fix travis test when reading hdfs
    Fix testcase after update and allow hdfs load
    re-using the verbose-eval parameter in the cv and aggcv methods and tests adapted
    unittest for cv bugfixes added
    Cleanup pandas support
    Changed 4 tests
    
    Changed symbol test to give error on < sign, not on = sign
    Changed 3 other functions, so that float is used instead of q
    Python: adjusts plot_importance ylim
    Added fixed random seed for tests (+1 squashed commit)
    Squashed commits:
    [76e3664] Added fixed random seed for tests
    Allow plot function to handle XGBModel
    Support non-str column names
    CV returns ndarray or DataFrame
    python DMatrix now accepts pandas DataFrame
    Change to properties
    Add feature_types
    Fix numpy array check logic
    Cleanup str roundtrip using ctypes
    BUG: incorrect model_file results in segfault
    Mac build fix
    Use ctypes
    Fix for python 3
    ENH: Add visualization to python package
    last check
    Fix typo in dask interface. (#6240)
    [dask] Test for data initializaton. (#6226)
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Update base margin dask (#6155)
    
    * Add `base-margin`
    * Add `output_margin` to regressor.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    [dask] Support more meta data on functional interface. (#6132)
    
    
    * Add base_margin, label_(lower|upper)_bound.
    * Test survival training with dask.
    Allow kwargs in dask predict (#6117)
    Optimize cpu sketch allreduce for sparse data. (#6009)
    
    * Bypass RABIT serialization reducer and use custom allgather based merging.
    Unify CPU hist sketching (#5880)
    [Breaking] Fix .predict() method and add .predict_proba() in xgboost.dask.DaskXGBClassifier (#5986)
    Fix dask predict shape infer. (#5989)
    [Dask] Asyncio support. (#5862)
    Dask device dmatrix (#5901)
    
    
    * Fix softprob with empty dmatrix.
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    [dask] Fix missing value for scikit-learn interface. (#5435)
    [dask] Accept other inputs for prediction. (#5428)
    
    
    * Returns a series when input is dataframe.
    
    * Merge assert client.
    [dask] Enable gridsearching with skl. (#5417)
    [dask] Honor `nthreads` from dask worker. (#5414)
    [dask] Order the prediction result. (#5416)
    Avoid dask test fixtures. (#5270)
    
    * Fix Travis OSX timeout.
    
    * Fix classifier.
    Tests for empty dmatrix. (#5159)
    Support dask dataframe as y for classifier. (#5077)
    
    * Support dask dataframe as y for classifier.
    
    * Lint.
    Don't `set_params` at the end of `set_state`. (#4947)
    
    * Don't set_params at the end of set_state.
    
    * Also fix another issue found in dask prediction.
    
    * Add note about prediction.
    
    Don't support other prediction modes at the moment.
    Fix dask prediction. (#4941)
    
    * Fix dask prediction.
    
    * Add better error messages for wrong partition.
    Use `cudf.concat` explicitly. (#4918)
    
    * Use `cudf.concat` explicitly.
    
    * Add test.
    Rewrite Dask interface. (#4819)
    Complete cudf support. (#4850)
    
    
    * Handles missing value.
    * Accept all floating point and integer types.
    * Move to cudf 9.0 API.
    * Remove requirement on `null_count`.
    * Arbitrary column types support.
    Add native support for Dask (#4473)
    
    * Add native support for Dask
    
    * Add multi-GPU demo
    
    * Add sklearn example
    Relax linear test. (#5849)
    
    * Increased error in coordinate is mostly due to floating point error.
    * Shotgun uses Hogwild!, which is non-deterministic and can have even greater
    floating point error.
    Relax test for shotgun. (#5835)
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    Testing hist_util (#5251)
    
    * Rank tests
    
    * Remove categorical split specialisation
    
    * Extend tests to multiple features, switch to WQSketch
    
    * Add tests for SparseCuts
    
    * Add external memory quantile tests, fix some existing tests
    Clean up Python 2 compatibility code. (#5161)
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Fix test_gpu_coordinate. (#3974)
    
    * Fix test_gpu_coordinate.
    
    * Use `gpu_coord_descent` in test.
    * Reduce number of running rounds.
    
    * Remove nthread.
    
    * Use githubusercontent for r-appveyor.
    
    * Use githubusercontent in travis r tests.
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Added finding quantiles on GPU. (#3393)
    
    * Added finding quantiles on GPU.
    
    - this includes datasets where weights are assigned to data rows
    - as the quantiles found by the new algorithm are not the same
      as those found by the old one, test thresholds in
        tests/python-gpu/test_gpu_updaters.py have been adjusted.
    
    * Adjustments and improved testing for finding quantiles on the GPU.
    
    - added C++ tests for the DeviceSketch() function
    - reduced one of the thresholds in test_gpu_updaters.py
    - adjusted the cuts found by the find_cuts_k kernel
    Refactor python tests (#3410)
    
    * Add unit test utility
    
    * Refactor updater tests. Add coverage for histmaker.
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Skip related tests when sklearn is not installed. (#4791)
    Add additional Python tests to test training under constraints (#4426)
    more correct way to build node stats in distributed fast hist (#4140)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * more changes
    
    * temp
    
    * update
    
    * udpate rabit
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * broadcast subsampled feature correctly
    
    * init col
    
    * temp
    
    * col sampling
    
    * fix histmastrix init
    
    * fix col sampling
    
    * remove cout
    
    * fix out of bound access
    
    * fix core dump
    
    remove core dump file
    
    * update
    
    * add fid
    
    * update
    
    * revert some changes
    
    * temp
    
    * temp
    
    * pass all tests
    
    * bring back some tests
    
    * recover some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * recover column init part
    
    * more recovery
    
    * fix core dumps
    
    * code clean
    
    * revert some changes
    
    * fix test compilation issue
    
    * fix lint issue
    
    * resolve compilation issue
    
    * fix issues of lint caused by rebase
    
    * fix stylistic changes and change variable names
    
    * modularize depth width
    
    * address the comments
    
    * fix failed tests
    
    * wrap perf timers with class
    
    * temp
    
    * pass all lossguide
    
    * pass tests
    
    * add comments
    
    * more changes
    
    * use separate flow for single and tests
    
    * add test for lossguide hist
    
    * remove duplications
    
    * syncing stats for only once
    
    * recover more changes
    
    * recover more changes
    
    * fix root-stats
    
    * simplify code
    
    * remove outdated comments
    'hist': Montonic Constraints (#3085)
    
    * Extended monotonic constraints support to 'hist' tree method.
    
    * Added monotonic constraints tests.
    
    * Fix the signature of NoConstraint::CalcSplitGain()
    
    * Document monotonic constraint support in 'hist'
    
    * Update signature of Update to account for latest refactor
    Fix metainfo from DataFrame. (#5216)
    
    * Fix metainfo from DataFrame.
    
    * Unify helper functions for data and meta.
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Allow import via python datatable. (#3272)
    
    * Allow import via python datatable.
    
    * Write unit tests
    
    * Refactor dt API functions
    
    * Refactor python code
    
    * Lint fixes
    
    * Address review comments
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Add LASSO (#3429)
    
    * Allow multiple split constraints
    
    * Replace RidgePenalty with ElasticNet
    
    * Add test for checking Ridge, LASSO, and Elastic Net are implemented
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Added trees_to_df() method for Booster class (#4153)
    
    * add test_parse_tree.py to tests/python
    
    * Fix formatting
    
    * Fix pylint error
    
    * Ignore 'no member' error for Pandas dataframe
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    Add float32 histogram (#5624)
    
    * new single_precision_histogram param was added.
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Co-authored-by: fis <jm.yuan@outlook.com>
    Fix pruner. (#5335)
    
    
    * Honor the tree depth.
    * Prevent pruning pruned node.
    Complete cudf support. (#4850)
    
    
    * Handles missing value.
    * Accept all floating point and integer types.
    * Move to cudf 9.0 API.
    * Remove requirement on `null_count`.
    * Arbitrary column types support.
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Prevent empty quantiles in fast hist (#4155)
    
    * Prevent empty quantiles
    
    * Revise and improve unit tests for quantile hist
    
    * Remove unnecessary comment
    
    * Add #2943 as a test case
    
    * Skip test if no sklearn
    
    * Revise misleading comments
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Refactor python tests (#3410)
    
    * Add unit test utility
    
    * Refactor updater tests. Add coverage for histmaker.
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Don't `set_params` at the end of `set_state`. (#4947)
    
    * Don't set_params at the end of set_state.
    
    * Also fix another issue found in dask prediction.
    
    * Add note about prediction.
    
    Don't support other prediction modes at the moment.
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add back support for scipy.sparse.coo_matrix (#6162)
    Feature weights (#5962)
    Fix missing data warning. (#5969)
    
    * Fix data warning.
    
    * Add numpy/scipy test.
    Move feature names and types of DMatrix from Python to C++. (#5858)
    
    
    * Add thread local return entry for DMatrix.
    * Save feature name and feature type in binary file.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix slice and get info. (#5552)
    [Breaking] Remove num roots. (#5059)
    External data adapters (#5044)
    
    * Use external data adapters as lightweight intermediate layer between external data and DMatrix
    Fix plotting test. (#6040)
    
    Previously the test loads a model generated by `test_basic.py`, now we generate
    the model explicitly.
    
    * Cleanup saved files for basic tests.
    Further improvements and savings in Jenkins pipeline (#5904)
    
    * Publish artifacts only on the master and release branches
    
    * Build CUDA only for Compute Capability 7.5 when building PRs
    
    * Run all Windows jobs in a single worker image
    
    * Build nightly XGBoost4J SNAPSHOT JARs with Scala 2.12 only
    
    * Show skipped Python tests on Windows
    
    * Make Graphviz optional for Python tests
    
    * Add back C++ tests
    
    * Unstash xgboost_cpp_tests
    
    * Fix label to CUDA 10.1
    
    * Install cuPy for CUDA 10.1
    
    * Install jsonschema
    
    * Address reviewer's feedback
    Implement tree model dump with code generator. (#4602)
    
    * Implement tree model dump with a code generator.
    
    * Split up generators.
    * Implement graphviz generator.
    * Use pattern matching.
    
    * [Breaking] Return a Source in `to_graphviz` instead of Digraph in Python package.
    
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    [PYTHON] Refactor trainnig API to use callback
    DOC/TST: Fix Python sklearn dep
    Enable flake8
    Separate dependencies and lightweight test env for Python
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    fix DeprecationWarning on sklearn.cross_validation (#2075)
    
    * fix DeprecationWarning on sklearn.cross_validation
    
    * fix syntax
    
    * fix kfold n_split issue
    
    * fix mistype
    
    * fix n_splits multiple value issue
    
    * split should pass a iterable
    
    * use np.arange instead of xrange, py3 compatibility
    [PYTHON] Refactor trainnig API to use callback
    DOC/TST: Fix Python sklearn dep
    Enable flake8
    python: multiple eval_metrics changes
    
    - allows feval to return a list of tuples (name, error/score value)
    - changed behavior for multiple eval_metrics in conjunction with
    early_stopping: Instead of raising an error, the last passed evel_metric
    (or last entry in return value of feval) is used for early stopping
    - allows list of eval_metrics in dict-typed params
    - unittest for new features / behavior
    
    documentation updated
    
    - example for assigning a list to 'eval_metric'
    - note about early stopping on last passed eval metric
    
    - info msg for used eval metric added
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [Breaking] Fix custom metric for multi output. (#5954)
    
    
    * Set output margin to true for custom metric.  This fixes only R and Python.
    [Breaking] Remove Scikit-Learn default parameters (#5130)
    
    * Simplify Scikit-Learn parameter management.
    
    * Copy base class for removing duplicated parameter signatures.
    * Set all parameters to None.
    * Handle None in set_param.
    * Extract the doc.
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Fix early stopping in the Python package (#4638)
    
    * Fix #4630, #4421: Preserve correct ordering between metrics, and always use last metric for early stopping
    
    * Clarify semantics of early stopping in presence of multiple valid sets and metrics
    
    * Add a test
    
    * Fix lint
    pytest tests/python fails if no pandas installed (#4620)
    
    * _maybe_pandas_xxx should return their arguments unchanged if no pandas installed
    
    * Tests should not assume pandas is installed
    
    * Mark tests which require pandas as such
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    fix DeprecationWarning on sklearn.cross_validation (#2075)
    
    * fix DeprecationWarning on sklearn.cross_validation
    
    * fix syntax
    
    * fix kfold n_split issue
    
    * fix mistype
    
    * fix n_splits multiple value issue
    
    * split should pass a iterable
    
    * use np.arange instead of xrange, py3 compatibility
    [PYTHON] Refactor trainnig API to use callback
    DOC/TST: Fix Python sklearn dep
    Enable flake8
    Separate dependencies and lightweight test env for Python
    python: unittest for early stopping of cv
    Added more thorough test for early stopping (+1 squashed commit)
    Squashed commits:
    [4f78cc0] Added test for early stopping (+1 squashed commit)
    Added fixed random seed for tests (+1 squashed commit)
    Squashed commits:
    [76e3664] Added fixed random seed for tests
    Fix failed tests (+2 squashed commits)
    Squashed commits:
    [962e1e4] Fix failed tests
    [21ca3fb] Removed one unnecessary line
    DOC: Updated contributors.md
    TST: Added test for early stopping
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    [PYTHON] Refactor trainnig API to use callback
    DOC/TST: Fix Python sklearn dep
    Enable flake8
    unittest for 'num_class > 2' added
    best_ntree_limit attribute added
    
    - best_ntree_limit as new booster atrribute added
    - usage of bst.best_ntree_limit in python doc added
    - fixed wrong 'best_iteration' after training continuation
    added unittest for training continuation
    Feature weights (#5962)
    Disable feature validation on sklearn predict prob. (#5953)
    
    
    * Fix issue when scikit learn interface receives transformed inputs.
    Further improvements and savings in Jenkins pipeline (#5904)
    
    * Publish artifacts only on the master and release branches
    
    * Build CUDA only for Compute Capability 7.5 when building PRs
    
    * Run all Windows jobs in a single worker image
    
    * Build nightly XGBoost4J SNAPSHOT JARs with Scala 2.12 only
    
    * Show skipped Python tests on Windows
    
    * Make Graphviz optional for Python tests
    
    * Add back C++ tests
    
    * Unstash xgboost_cpp_tests
    
    * Fix label to CUDA 10.1
    
    * Install cuPy for CUDA 10.1
    
    * Install jsonschema
    
    * Address reviewer's feedback
    Add new skl model attribute for number of features (#5780)
    Assert matching length of evaluation inputs. (#5540)
    Fix skl nan tag. (#5538)
    Fix checking booster. (#5505)
    
    * Use `get_params()` instead of `getattr` intrinsic.
    Enable parameter validation for skl. (#5477)
    Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)
    Export JSON config in `get_params`. (#5256)
    Save Scikit-Learn attributes into learner attributes. (#5245)
    
    * Remove the recommendation for pickle.
    
    * Save skl attributes in booster.attr
    
    * Test loading scikit-learn model with native booster.
    Add constraint parameters to Scikit-Learn interface. (#5227)
    
    * Add document for constraints.
    
    * Fix a format error in doc for objective function.
    [Breaking] Remove Scikit-Learn default parameters (#5130)
    
    * Simplify Scikit-Learn parameter management.
    
    * Copy base class for removing duplicated parameter signatures.
    * Set all parameters to None.
    * Handle None in set_param.
    * Extract the doc.
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Fix metainfo from DataFrame. (#5216)
    
    * Fix metainfo from DataFrame.
    
    * Unify helper functions for data and meta.
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    Add base margin to sklearn interface. (#5151)
    Allow using RandomState object from Numpy in sklearn interface. (#5049)
    [Breaking] Update sklearn interface. (#4929)
    
    
    * Remove nthread, seed, silent. Add tree_method, gpu_id, num_parallel_tree. Fix #4909.
    * Check data shape. Fix #4896.
    * Check element of eval_set is tuple. Fix #4875
    *  Add doc for random_state with hogwild. Fixes #4919
    Complete cudf support. (#4850)
    
    
    * Handles missing value.
    * Accept all floating point and integer types.
    * Move to cudf 9.0 API.
    * Remove requirement on `null_count`.
    * Arbitrary column types support.
    Remove gpu_exact tree method (#4742)
    pytest tests/python fails if no pandas installed (#4620)
    
    * _maybe_pandas_xxx should return their arguments unchanged if no pandas installed
    
    * Tests should not assume pandas is installed
    
    * Mark tests which require pandas as such
    Implement tree model dump with code generator. (#4602)
    
    * Implement tree model dump with a code generator.
    
    * Split up generators.
    * Implement graphviz generator.
    * Use pattern matching.
    
    * [Breaking] Return a Source in `to_graphviz` instead of Digraph in Python package.
    
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Brought the silent parameter for the SKLearn-like API back, marked it deprecated. (#4255)
    
    * Brought the silent parameter for the SKLearn-like API back, marked it deprecated.
    
    - added deprecation notice and warning
    - removed silent from the tests for the SKLearn-like API
    Added SKLearn-like random forest Python API. (#4148)
    
    
    * Added SKLearn-like random forest Python API.
    
    - added XGBRFClassifier and XGBRFRegressor classes to SKL-like xgboost API
    - also added n_gpus and gpu_id parameters to SKL classes
    - added documentation describing how to use xgboost for random forests,
      as well as existing caveats
    enable xgb_model in scklearn XGBClassifier and test. (#4092)
    
    * Enable xgb_model parameter in XGClassifier scikit-learn API
    
    https://github.com/dmlc/xgboost/issues/3049
    
    * add test_XGBClassifier_resume():
    
    test for xgb_model parameter in XGBClassifier API.
    
    * Update test_with_sklearn.py
    
    * Fix lint
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    use gain for sklearn feature_importances_ (#3876)
    
    * use gain for sklearn feature_importances_
    
    `gain` is a better feature importance criteria than the currently used `weight`
    
    * added importance_type to class
    
    * fixed test
    
    * white space
    
    * fix variable name
    
    * fix deprecation warning
    
    * fix exp array
    
    * white spaces
    Fix coef_ and intercept_ signature to be compatible with sklearn.RFECV (#3873)
    
    * Fix coef_ and intercept_ signature to be compatible with sklearn.RFECV
    
    * Fix lint
    
    * Fix lint
    Allow sklearn grid search over parameters specified as kwargs (#3791)
    Fix #3730: scikit-learn 0.20 compatibility fix (#3731)
    
    * Fix #3730: scikit-learn 0.20 compatibility fix
    
    sklearn.cross_validation has been removed from scikit-learn 0.20,
    so replace it with sklearn.model_selection
    
    * Display test names for Python tests for clarity
    Fix #3648: XGBClassifier.predict() should return margin scores when output_margin=True (#3651)
    
    * Fix #3648: XGBClassifier.predict() should return margin scores when output_margin=True
    
    * Fix tests to reflect correct implementation of XGBClassifier.predict(output_margin=True)
    
    * Fix flaky test test_with_sklearn.test_sklearn_api_gblinear
    sklearn api for ranking (#3560)
    
    * added xgbranker
    
    * fixed predict method and ranking test
    
    * reformatted code in accordance with pep8
    
    * fixed lint error
    
    * fixed docstring and added checks on objective
    
    * added ranking demo for python
    
    * fixed suffix in rank.py
    Save and load model in sklearn API (#3192)
    
    * Add (load|save)_model to XGBModel
    
    * Add docstring
    
    * Fix docstring
    
    * Fix mixed use of space and tab
    
    * Add a test
    
    * Fix Flake8 style errors
    Sklearn: validation set weights (#2354)
    
    * Add option to use weights when evaluating metrics in validation sets
    
    * Add test for validation-set weights functionality
    
    * simplify case with no weights for test sets
    
    * fix lint issues
    Fix bug with gpu_predictor caching behaviour (#3177)
    
    * Fixes #3162
    python: follow the default warning filters of Python (#2666)
    
    * python: follow the default warning filters of Python
    
    https://docs.python.org/3/library/warnings.html#default-warning-filters
    
    * update tests
    
    * update tests
    [python-package] fix sklearn n_jobs/nthreads and seed/random_state bug  (#2378)
    
    * add a testcase causing RuntimeError
    
    * move seed/random_state/nthread/n_jobs check to get_xgb_params()
    
    * fix failed test
    Sklearn kwargs (#2338)
    
    * Added kwargs support for Sklearn API
    
    * Updated NEWS and CONTRIBUTORS
    
    * Fixed CONTRIBUTORS.md
    
    * Added clarification of **kwargs and test for proper usage
    
    * Fixed lint error
    
    * Fixed more lint errors and clf assigned but never used
    
    * Fixed more lint errors
    
    * Fixed more lint errors
    
    * Fixed issue with changes from different branch bleeding over
    
    * Fixed issue with changes from other branch bleeding over
    
    * Added note that kwargs may not be compatible with Sklearn
    
    * Fixed linting on kwargs note
    Sklearn convention update (#2323)
    
    * Added n_jobs and random_state to keep up to date with sklearn API.
    Deprecated nthread and seed.  Added tests for new params and
    deprecations.
    
    * Fixed docstring to reflect updates to n_jobs and random_state.
    
    * Fixed whitespace issues and removed nose import.
    
    * Added deprecation note for nthread and seed in docstring.
    
    * Attempted fix of deprecation tests.
    
    * Second attempted fix to tests.
    
    * Set n_jobs to 1.
    Add option to choose booster in scikit intreface (gbtree by default) (#2303)
    
    * Add option to choose booster in scikit intreface (gbtree by default)
    
    * Add option to choose booster in scikit intreface: complete docstring.
    
    * Fix XGBClassifier to work with booster option
    
    * Added test case for gblinear booster
    fix DeprecationWarning on sklearn.cross_validation (#2075)
    
    * fix DeprecationWarning on sklearn.cross_validation
    
    * fix syntax
    
    * fix kfold n_split issue
    
    * fix mistype
    
    * fix n_splits multiple value issue
    
    * split should pass a iterable
    
    * use np.arange instead of xrange, py3 compatibility
    Fix mknfold using new StratifiedKFold API (#1660)
    [PYTHON] Refactor trainnig API to use callback
    DOC/TST: Fix Python sklearn dep
    [py] split value histograms
    BUG: XGBClassifier.feature_importances_ raises ValueError if input is pandas DataFrame
    Enable flake8
    Separate dependencies and lightweight test env for Python
    - Added test cases for the use of custom objective functions
    - Made the indentation more consistent with pep8
    Added more thorough test for early stopping (+1 squashed commit)
    Squashed commits:
    [4f78cc0] Added test for early stopping (+1 squashed commit)
    Added tests for additional params in sklearn wrapper (+1 squashed commit)
    Squashed commits:
    [43892b9] Added tests for additional params in sklearn wrapper
    Fix failed tests (+2 squashed commits)
    Squashed commits:
    [962e1e4] Fix failed tests
    [21ca3fb] Removed one unnecessary line
    TST: Added test for parameter tuning using GridSearchCV
    TST: Added test for early stopping
    Added test for regression ysing Boston Housing dataset
    Added assertions for classification tests
    TST: Added tests for multi-class classification
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    [Breaking] Remove num roots. (#5059)
    Skip related tests when sklearn is not installed. (#4791)
    Add support for cross-validation using query ID (#4474)
    
    * adding support for matrix slicing with query ID for cross-validation
    
    * hail mary test of unrar installation for windows tests
    
    * trying to modify tests to run in Github CI
    
    * Remove dependency on wget and unrar
    
    * Save error log from R test
    
    * Relax assertion in test_training
    
    * Use int instead of bool in C function interface
    
    * Revise R interface
    
    * Add XGDMatrixSliceDMatrixEx and keep old XGDMatrixSliceDMatrix for API compatibility
    In AUC and AUCPR metrics, detect whether weights are per-instance or per-group (#4216)
    
    * In AUC and AUCPR metrics, detect whether weights are per-instance or per-group
    
    * Fix C++ style check
    
    * Add a test for weighted AUC
    Make AUCPR work with multiple query groups (#4436)
    
    * Make AUCPR work with multiple query groups
    
    * Check AUCPR <= 1.0 in distributed setting
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Skip related tests when sklearn is not installed. (#4791)
    Feature interaction for GPU Hist. (#4534)
    
    * GPU hist Interaction Constraints.
    * Duplicate related parameters.
    * Add tests for CPU interaction constraint.
    * Add better error reporting.
    * Thorough tests.
    Add additional Python tests to test training under constraints (#4426)
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    [TREE] add interaction constraints (#3466)
    
    * add interaction constraints
    
    * enable both interaction and monotonic constraints at the same time
    
    * fix lint
    
    * add R test, fix lint, update demo
    
    * Use dmlc::JSONReader to express interaction constraints as nested lists; Use sparse arrays for bookkeeping
    
    * Add Python test for interaction constraints
    
    * make R interaction constraints parameter based on feature index instead of column names, fix R coding style
    
    * Fix lint
    
    * Add BlueTea88 to CONTRIBUTORS.md
    
    * Short circuit when no constraint is specified; address review comments
    
    * Add tutorial for feature interaction constraints
    
    * allow interaction constraints to be passed as string, remove redundant column_names argument
    
    * Fix typo
    
    * Address review comments
    
    * Add comments to Python test
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Enable categorical data support on Python DMatrix. (#6166)
    
    
    * Only pandas is recognized.
    Validate weights are positive values. (#6115)
    Simplify the data backends. (#5893)
    Implement Python data handler. (#5689)
    
    
    * Define data handlers for DMatrix.
    * Throw ValueError in scikit learn interface.
    Support pandas SparseArray. (#5431)
    Fix metainfo from DataFrame. (#5216)
    
    * Fix metainfo from DataFrame.
    
    * Unify helper functions for data and meta.
    Fix feature_name crated from int64index dataframe. (#5081)
    fix broken python test (#4395)
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Fix issue 2670 (#2671)
    
    * fix issue 2670
    
    * add python<3.6 compatibility
    
    * fix Index
    
    * fix Index/MultiIndex
    
    * fix lint
    
    * fix W0622
    
    really nonsense
    
    * fix lambda
    
    * Trigger Travis
    
    * add test for MultiIndex
    
    * remove tailing whitespace
    Fix wrong expected feature types (#1646)
    [PYTHON] Refactor trainnig API to use callback
    DOC/TST: Fix Python sklearn dep
    Enable flake8
    Separate dependencies and lightweight test env for Python
    Disable JSON full serialization for now. (#6248)
    
    * Disable JSON serialization for now.
    
    * Multi-class classification is checkpointing for each iteration.
    This brings significant overhead.
    
    Revert: 90355b4f007ae
    
    * Set R tests to use binary.
    [CI] Port CI fixes from the 1.2.0 branch (#6050)
    
    * Fix a unit test on CLI, to handle RC versions
    
    * [CI] Use mgpu machine to run gpu hist unit tests
    
    * [CI] Build GPU-enabled JAR artifact and deploy to xgboost-maven-repo
    Make JSON the default full serialization format. (#6027)
    Fix prediction heuristic (#5955)
    
    
    * Relax check for prediction.
    * Relax test in spark test.
    * Add tests in C++.
    Refactor the CLI. (#5574)
    
    
    * Enable parameter validation.
    * Enable JSON.
    * Catch `dmlc::Error`.
    * Show help message.
    Don't set seed on CLI interface. (#5563)
    Fix CLI model IO. (#5535)
    
    
    * Add test for comparing Python and CLI training result.
    [dask] Support more meta data on functional interface. (#6132)
    
    
    * Add base_margin, label_(lower|upper)_bound.
    * Test survival training with dask.
    Implement robust regularization in 'survival:aft' objective (#5473)
    
    * Robust regularization of AFT gradient and hessian
    
    * Fix AFT doc; expose it to tutorial TOC
    
    * Apply robust regularization to uncensored case too
    
    * Revise unit test slightly
    
    * Fix lint
    
    * Update test_survival.py
    
    * Use GradientPairPrecise
    
    * Remove unused variables
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Make Python model compatibility test runnable locally (#5941)
    Fix loading old model. (#5724)
    
    
    * Add test.
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Enable categorical data support on Python DMatrix. (#6166)
    
    
    * Only pandas is recognized.
    Validate weights are positive values. (#6115)
    Modin DF support (#6055)
    
    * Modin DF support
    
    * mode change
    
    * tests were added, ci env was extended
    
    * mode change
    
    * Remove redundant installation of modin
    
    * Add a pytest skip marker for modin
    
    * Install Modin[ray] from PyPI
    
    * fix interfering
    
    * avoid extra conversion
    
    * delete cv test for modin
    
    * revert cv function
    
    Co-authored-by: ShvetsKS <kirill.shvets@intel.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [HOTFIX] distributed training with hist method  (#4716)
    
    * add parallel test for hist.EvalualiteSplit
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * fix OMP schedule policy
    
    * fix clang-tidy
    
    * add logging: total_num_bins
    
    * fix
    
    * fix
    
    * test
    
    * replace guided OPENMP policy with static in updater_quantile_hist.cc
    Fix bugs in multithreaded ApplySplitSparseData() (#2161)
    
    * Bugfix 1: Fix segfault in multithreaded ApplySplitSparseData()
    
    When there are more threads than rows in rowset, some threads end up
    with empty ranges, causing them to crash. (iend - 1 needs to be
    accessible as part of algorithm)
    
    Fix: run only those threads with nonempty ranges.
    
    * Add regression test for Bugfix 1
    
    * Moving python_omp_test to existing python test group
    
    Turns out you don't need to set "OMP_NUM_THREADS" to enable
    multithreading. Just add nthread parameter.
    
    * Bugfix 2: Fix corner case of ApplySplitSparseData() for categorical feature
    
    When split value is less than all cut points, split_cond is set
    incorrectly.
    
    Fix: set split_cond = -1 to indicate this scenario
    
    * Bugfix 3: Initialize data layout indicator before using it
    
    data_layout_ is accessed before being set; this variable determines
    whether feature 0 is included in feat_set.
    
    Fix: re-order code in InitData() to initialize data_layout_ first
    
    * Adding regression test for Bugfix 2
    
    Unfortunately, no regression test for Bugfix 3, as there is no
    way to deterministically assign value to an uninitialized variable.
    Implement tree model dump with code generator. (#4602)
    
    * Implement tree model dump with a code generator.
    
    * Split up generators.
    * Implement graphviz generator.
    * Use pattern matching.
    
    * [Breaking] Return a Source in `to_graphviz` instead of Digraph in Python package.
    
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Add total_gain and total_cover importance measures (#3498)
    
    Add `'total_gain'` and `'total_cover'` as possible `importance_type`
    arguments to `Booster.get_score` in the Python package.
    
    `get_score` already accepts a `'gain'` argument, which returns each
    feature's average gain over all of its splits.  `'total_gain'` does the
    same, but returns a total rather than an average.  This seems more
    intuitively meaningful, and also matches the behavior of the R package's
    `xgb.importance` function.
    
    I also added an analogous `'total_cover'` command for consistency.
    
    This should resolve #3484.
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    Remove silent from R demos. (#5675)
    
    * Remove silent from R demos.
    
    * Vignettes.
    Avoid rabit calls in learner configuration (#5581)
    Serialise booster after training to reset state (#5484)
    
    * Serialise booster after training to reset state
    
    * Prevent process_type being set on load
    
    * Check for correct updater sequence
    Remove unnecessary dependencies in distributed test (#3132)
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    [BREAKING] prevent multi-gpu usage (#4749)
    
    * prevent multi-gpu usage
    
    * fix distributed test
    
    * combine gpu predictor tests
    
    * set upper bound on n_gpus
     Combine thread launches into single launch per tree for gpu_hist (#4343)
    
    * Combine thread launches into single launch per tree for gpu_hist
    algorithm.
    
    * Address deprecation warning
    
    * Add manual column sampler constructor
    
    * Turn off omp dynamic to get a guaranteed number of threads
    
    * Enable openmp in cuda code
    Improved multi-node multi-GPU random forests. (#4238)
    
    * Improved multi-node multi-GPU random forests.
    
    - removed rabit::Broadcast() from each invocation of column sampling
    - instead, syncing the PRNG seed when a ColumnSampler() object is constructed
    - this makes non-trivial column sampling significantly faster in the distributed case
    - refactored distributed GPU tests
    - added distributed random forests tests
    Avoid rabit calls in learner configuration (#5581)
    Serialise booster after training to reset state (#5484)
    
    * Serialise booster after training to reset state
    
    * Prevent process_type being set on load
    
    * Check for correct updater sequence
    Complete cudf support. (#4850)
    
    
    * Handles missing value.
    * Accept all floating point and integer types.
    * Move to cudf 9.0 API.
    * Remove requirement on `null_count`.
    * Arbitrary column types support.
    Fix #3402: wrong fid crashes distributed algorithm (#3535)
    
    * Fix #3402: wrong fid crashes distributed algorithm
    
    The bug was introduced by the recent DMatrix refactor (#3301). It was partially
    fixed by #3408 but the example in #3402 was still failing. The example in #3402
    will succeed after this fix is applied.
    
    * Explicitly specify "this" to prevent compile error
    
    * Add regression test
    
    * Add distributed test to Travis matrix
    
    * Install kubernetes Python package as dependency of dmlc tracker
    
    * Add Python dependencies
    
    * Add compile step
    
    * Reduce size of regression test case
    
    * Further reduce size of test
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Fix Python environment for distributed unit tests (#3806)
    Fix #3402: wrong fid crashes distributed algorithm (#3535)
    
    * Fix #3402: wrong fid crashes distributed algorithm
    
    The bug was introduced by the recent DMatrix refactor (#3301). It was partially
    fixed by #3408 but the example in #3402 was still failing. The example in #3402
    will succeed after this fix is applied.
    
    * Explicitly specify "this" to prevent compile error
    
    * Add regression test
    
    * Add distributed test to Travis matrix
    
    * Install kubernetes Python package as dependency of dmlc tracker
    
    * Add Python dependencies
    
    * Add compile step
    
    * Reduce size of regression test case
    
    * Further reduce size of test
    [PYTHON-DIST] Distributed xgboost python training API.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    [BREAKING] prevent multi-gpu usage (#4749)
    
    * prevent multi-gpu usage
    
    * fix distributed test
    
    * combine gpu predictor tests
    
    * set upper bound on n_gpus
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Improved multi-node multi-GPU random forests. (#4238)
    
    * Improved multi-node multi-GPU random forests.
    
    - removed rabit::Broadcast() from each invocation of column sampling
    - instead, syncing the PRNG seed when a ColumnSampler() object is constructed
    - this makes non-trivial column sampling significantly faster in the distributed case
    - refactored distributed GPU tests
    - added distributed random forests tests
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [BLOCKING] Adding JVM doc build to Jenkins CI (#3567)
    
    * Adding Java/Scala doc build to Jenkins CI
    
    * Deploy built doc to S3 bucket
    
    * Build doc only for branches
    
    * Build doc first, to get doc faster for branch updates
    
    * Have ReadTheDocs download doc tarball from S3
    
    * Update JVM doc links
    
    * Put doc build commands in a script
    
    * Specify Spark 2.3+ requirement for XGBoost4J-Spark
    
    * Build GPU wheel without NCCL, to reduce binary size
    [BUILD] Dockerfile and Jenkinsfile revisited (#2514)
    
    Includes:
      - Dockerfile changes
        - Dockerfile clean up
        - Fix execution privileges of files used from Dockerfile.
        - New Dockerfile entrypoint to replace with_user script
        - Defined a placeholders for CPU testing (script and Dockerfile)
      - Jenkinsfile
        - Jenkins file milestone defined
        - Single source code checkout and propagation via stash/unstash
        - Bash needs to be explicitly used in launching make build, since we need
    access to environment
        - Jenkinsfile build factory for cmake and make style of jobs
        - Archivation of artifacts (*.so, *.whl, *.egg) produced by cmake build
    
    Missing:
      - CPU testing
      - Python3 env build and testing
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Add option to enable all compiler warnings in GCC/Clang (#5897)
    
    * Add option to enable all compiler warnings in GCC/Clang
    
    * Fix -Wall for CUDA sources
    
    * Make -Wall private req for xgboost-r
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Use bundled gtest (#4900)
    
    * Suggest to use gtest bundled with dmlc
    
    * Use dmlc bundled gtest in all CI scripts
    
    * Make clang-tidy to use dmlc embedded gtest
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Remove redundant FindGTest.cmake. (#3533)
    
    During removal of FindGTest.cmake, also
    
    * Fix gtest include dirs.
    * Remove some blanks and use PWD for gtest dir.
    Do not unzip google test archive if exists (#3416)
    Fix wget for google tests in tests (#3414)
    
    CI tests were failing because wget prompts "the user" for a response
    whenever the google test archive is already on the disk.
    
    Fix: Use `-nc` option to skip download when the archive already
    exists
    Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container. (#3309)
    
    * Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container.
    
    * Add Google test
    [BUILD] Dockerfile and Jenkinsfile revisited (#2514)
    
    Includes:
      - Dockerfile changes
        - Dockerfile clean up
        - Fix execution privileges of files used from Dockerfile.
        - New Dockerfile entrypoint to replace with_user script
        - Defined a placeholders for CPU testing (script and Dockerfile)
      - Jenkinsfile
        - Jenkins file milestone defined
        - Single source code checkout and propagation via stash/unstash
        - Bash needs to be explicitly used in launching make build, since we need
    access to environment
        - Jenkinsfile build factory for cmake and make style of jobs
        - Archivation of artifacts (*.so, *.whl, *.egg) produced by cmake build
    
    Missing:
      - CPU testing
      - Python3 env build and testing
    [GPU-Plugin] Add basic continuous integration for GPU plugin. (#2431)
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    [CI] Fix Docker build for CUDA 11 (#6202)
    [CI] Upgrade cuDF and RMM to 0.16 nightlies; upgrade to Ubuntu 18.04 (#6157)
    
    * [CI] Upgrade cuDF and RMM to 0.16 nightlies
    
    * Use Ubuntu 18.04 in RMM test, since RMM needs GCC 7+
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Support building XGBoost with CUDA 11 (#5808)
    
    * Change serialization test.
    * Add CUDA 11 tests on Linux CI.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)
    
    * [CI] Use Vault repository to re-gain access to devtoolset-4
    
    * Use manylinux2010 tag
    
    * Update Dockerfile.jvm
    
    * Fix rename_whl.py
    
    * Upgrade Pip, to handle manylinux2010 tag
    
    * Update insert_vcomp140.py
    
    * Update test_python.sh
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Ensure that configured dmlc/build_config.h is picked up by Rabit and XGBoost (#5514)
    
    * Ensure that configured header (build_config.h) from dmlc-core is picked up by Rabit and XGBoost
    
    * Check which Rabit target is being used
    
    * Use CMake 3.13 in all Jenkins tests
    
    * Upgrade CMake in Travis CI
    
    * Install CMake using Kitware installer
    
    * Remove existing CMake (3.12.4)
    [CI] Use devtoolset-6 because devtoolset-4 is EOL and no longer available (#5506)
    
    * Use devtoolset-6.
    
    * [CI] Use devtoolset-6 because devtoolset-4 is EOL and no longer available
    
    * CUDA 9.0 doesn't work with devtoolset-6; use devtoolset-4 for GPU build only
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    add cuda 10.1 support (#4468)
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    [CI] Upgrade to GCC 5.3.1, CMake 3.6.0 (#4306)
    
    * Upgrade to GCC 5.3.1, CMake 3.6.0
    
    * <regex> is now okay
    jenkins build for cuda 10.0 (#4281)
    
    * jenkins build for cuda 10.0
    
    * yum install nccl2 for cuda 10.0
    [REVIEW] Enable Multi-Node Multi-GPU functionality (#4095)
    
    * Initial commit to support multi-node multi-gpu xgboost using dask
    
    * Fixed NCCL initialization by not ignoring the opg parameter.
    
    - it now crashes on NCCL initialization, but at least we're attempting it properly
    
    * At the root node, perform a rabit::Allreduce to get initial sum_gradient across workers
    
    * Synchronizing in a couple of more places.
    
    - now the workers don't go down, but just hang
    - no more "wild" values of gradients
    - probably needs syncing in more places
    
    * Added another missing max-allreduce operation inside BuildHistLeftRight
    
    * Removed unnecessary collective operations.
    
    * Simplified rabit::Allreduce() sync of gradient sums.
    
    * Removed unnecessary rabit syncs around ncclAllReduce.
    
    - this improves performance _significantly_ (7x faster for overall training,
      20x faster for xgboost proper)
    
    * pulling in latest xgboost
    
    * removing changes to updater_quantile_hist.cc
    
    * changing use_nccl_opg initialization, removing unnecessary if statements
    
    * added definition for opaque ncclUniqueId struct to properly encapsulate GetUniqueId
    
    * placing struct defintion in guard to avoid duplicate code errors
    
    * addressing linting errors
    
    * removing
    
    * removing additional arguments to AllReduer initialization
    
    * removing distributed flag
    
    * making comm init symmetric
    
    * removing distributed flag
    
    * changing ncclCommInit to support multiple modalities
    
    * fix indenting
    
    * updating ncclCommInitRank block with necessary group calls
    
    * fix indenting
    
    * adding print statement, and updating accessor in vector
    
    * improving print statement to end-line
    
    * generalizing nccl_rank construction using rabit
    
    * assume device_ordinals is the same for every node
    
    * test, assume device_ordinals is identical for all nodes
    
    * test, assume device_ordinals is unique for all nodes
    
    * changing names of offset variable to be more descriptive, editing indenting
    
    * wrapping ncclUniqueId GetUniqueId() and aesthetic changes
    
    * adding synchronization, and tests for distributed
    
    * adding  to tests
    
    * fixing broken #endif
    
    * fixing initialization of gpu histograms, correcting errors in tests
    
    * adding to contributors list
    
    * adding distributed tests to jenkins
    
    * fixing bad path in distributed test
    
    * debugging
    
    * adding kubernetes for distributed tests
    
    * adding proper import for OrderedDict
    
    * adding urllib3==1.22 to address ordered_dict import error
    
    * added sleep to allow workers to save their models for comparison
    
    * adding name to GPU contributors under docs
    Speed up Jenkins by not compiling CMake (#4099)
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Test wheels on CUDA 10.0 container for compatibility (#3838)
    Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way (#3783)
    
    * Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way
    
    Also install git inside NVIDIA GPU container
    
    * Update dmlc-core
    Upgrading to NCCL2 (#3404)
    
    * Upgrading to NCCL2
    
    * Part - II of NCCL2 upgradation
    
     - Doc updates to build with nccl2
     - Dockerfile.gpu update for a correct CI build with nccl2
     - Updated FindNccl package to have env-var NCCL_ROOT to take precedence
    
    * Upgrading to v9.2 for CI workflow, since it has the nccl2 binaries available
    
    * Added NCCL2 license + copy the nccl binaries into /usr location for the FindNccl module to find
    
    * Set LD_LIBRARY_PATH variable to pick nccl2 binary at runtime
    
    * Need the nccl2 library download instructions inside Dockerfile.release as well
    
    * Use NCCL2 as a static library
    Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container. (#3309)
    
    * Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container.
    
    * Add Google test
    [BUILD] Dockerfile and Jenkinsfile revisited (#2514)
    
    Includes:
      - Dockerfile changes
        - Dockerfile clean up
        - Fix execution privileges of files used from Dockerfile.
        - New Dockerfile entrypoint to replace with_user script
        - Defined a placeholders for CPU testing (script and Dockerfile)
      - Jenkinsfile
        - Jenkins file milestone defined
        - Single source code checkout and propagation via stash/unstash
        - Bash needs to be explicitly used in launching make build, since we need
    access to environment
        - Jenkinsfile build factory for cmake and make style of jobs
        - Archivation of artifacts (*.so, *.whl, *.egg) produced by cmake build
    
    Missing:
      - CPU testing
      - Python3 env build and testing
    [GPU-Plugin] Add basic continuous integration for GPU plugin. (#2431)
    Fix compilation on Mac OSX High Sierra (10.13) (#5597)
    
    * Fix compilation on Mac OSX High Sierra
    
    * [CI] Build Mac OSX binary wheel using Travis CI
    [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)
    
    * [CI] Use Vault repository to re-gain access to devtoolset-4
    
    * Use manylinux2010 tag
    
    * Update Dockerfile.jvm
    
    * Fix rename_whl.py
    
    * Upgrade Pip, to handle manylinux2010 tag
    
    * Update insert_vcomp140.py
    
    * Update test_python.sh
    [CI] Upload nightly builds to S3 (#4976)
    
    * Do not store built artifacts in the Jenkins master
    
    * Add wheel renaming script
    
    * Upload wheels to S3 bucket
    
    * Use env.GIT_COMMIT
    
    * Capture git hash correctly
    
    * Add missing import in Jenkinsfile
    
    * Address reviewer's comments
    
    * Put artifacts for pull requests in separate directory
    
    * No wildcard expansion in Windows CMD
    [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)
    
    * [CI] Use Vault repository to re-gain access to devtoolset-4
    
    * Use manylinux2010 tag
    
    * Update Dockerfile.jvm
    
    * Fix rename_whl.py
    
    * Upgrade Pip, to handle manylinux2010 tag
    
    * Update insert_vcomp140.py
    
    * Update test_python.sh
    [CI] Locate vcomp140.dll from System32 directory (#5078)
    [CI] Add Python and C++ tests for Windows GPU target (#4469)
    
    * Add CMake option to use bundled gtest from dmlc-core, so that it is easy to build XGBoost with gtest on Windows
    
    * Consistently apply OpenMP flag to all targets. Force enable OpenMP when USE_CUDA is turned on.
    
    * Insert vcomp140.dll into Windows wheels
    
    * Add C++ and Python tests for CPU and GPU targets (CUDA 9.0, 10.0, 10.1)
    
    * Prevent spurious msbuild failure
    
    * Add GPU tests
    
    * Upgrade dmlc-core
    Ensure that configured dmlc/build_config.h is picked up by Rabit and XGBoost (#5514)
    
    * Ensure that configured header (build_config.h) from dmlc-core is picked up by Rabit and XGBoost
    
    * Check which Rabit target is being used
    
    * Use CMake 3.13 in all Jenkins tests
    
    * Upgrade CMake in Travis CI
    
    * Install CMake using Kitware installer
    
    * Remove existing CMake (3.12.4)
    Use long key id. (#4783)
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Enable building rabit on Windows (#6105)
    [CI] Improve R linter script (#5944)
    
    * [CI] Move lint to a separate script
    
    * [CI] Improved lintr launcher
    
    * Add lintr as a separate action
    
    * Add custom parsing logic to print out logs
    
    * Fix lintr issues in demos
    
    * Run R demos
    
    * Fix CRAN checks
    
    * Install XGBoost into R env before running lintr
    
    * Install devtools (needed to run demos)
    Setup github action. (#5917)
    [CI] Fix Docker build for CUDA 11 (#6202)
    Support building XGBoost with CUDA 11 (#5808)
    
    * Change serialization test.
    * Add CUDA 11 tests on Linux CI.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)
    
    * [CI] Use Vault repository to re-gain access to devtoolset-4
    
    * Use manylinux2010 tag
    
    * Update Dockerfile.jvm
    
    * Fix rename_whl.py
    
    * Upgrade Pip, to handle manylinux2010 tag
    
    * Update insert_vcomp140.py
    
    * Update test_python.sh
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Ensure that configured dmlc/build_config.h is picked up by Rabit and XGBoost (#5514)
    
    * Ensure that configured header (build_config.h) from dmlc-core is picked up by Rabit and XGBoost
    
    * Check which Rabit target is being used
    
    * Use CMake 3.13 in all Jenkins tests
    
    * Upgrade CMake in Travis CI
    
    * Install CMake using Kitware installer
    
    * Remove existing CMake (3.12.4)
    [CI] Use devtoolset-6 because devtoolset-4 is EOL and no longer available (#5506)
    
    * Use devtoolset-6.
    
    * [CI] Use devtoolset-6 because devtoolset-4 is EOL and no longer available
    
    * CUDA 9.0 doesn't work with devtoolset-6; use devtoolset-4 for GPU build only
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    add cuda 10.1 support (#4468)
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    [CI] Upgrade to GCC 5.3.1, CMake 3.6.0 (#4306)
    
    * Upgrade to GCC 5.3.1, CMake 3.6.0
    
    * <regex> is now okay
    jenkins build for cuda 10.0 (#4281)
    
    * jenkins build for cuda 10.0
    
    * yum install nccl2 for cuda 10.0
    [REVIEW] Enable Multi-Node Multi-GPU functionality (#4095)
    
    * Initial commit to support multi-node multi-gpu xgboost using dask
    
    * Fixed NCCL initialization by not ignoring the opg parameter.
    
    - it now crashes on NCCL initialization, but at least we're attempting it properly
    
    * At the root node, perform a rabit::Allreduce to get initial sum_gradient across workers
    
    * Synchronizing in a couple of more places.
    
    - now the workers don't go down, but just hang
    - no more "wild" values of gradients
    - probably needs syncing in more places
    
    * Added another missing max-allreduce operation inside BuildHistLeftRight
    
    * Removed unnecessary collective operations.
    
    * Simplified rabit::Allreduce() sync of gradient sums.
    
    * Removed unnecessary rabit syncs around ncclAllReduce.
    
    - this improves performance _significantly_ (7x faster for overall training,
      20x faster for xgboost proper)
    
    * pulling in latest xgboost
    
    * removing changes to updater_quantile_hist.cc
    
    * changing use_nccl_opg initialization, removing unnecessary if statements
    
    * added definition for opaque ncclUniqueId struct to properly encapsulate GetUniqueId
    
    * placing struct defintion in guard to avoid duplicate code errors
    
    * addressing linting errors
    
    * removing
    
    * removing additional arguments to AllReduer initialization
    
    * removing distributed flag
    
    * making comm init symmetric
    
    * removing distributed flag
    
    * changing ncclCommInit to support multiple modalities
    
    * fix indenting
    
    * updating ncclCommInitRank block with necessary group calls
    
    * fix indenting
    
    * adding print statement, and updating accessor in vector
    
    * improving print statement to end-line
    
    * generalizing nccl_rank construction using rabit
    
    * assume device_ordinals is the same for every node
    
    * test, assume device_ordinals is identical for all nodes
    
    * test, assume device_ordinals is unique for all nodes
    
    * changing names of offset variable to be more descriptive, editing indenting
    
    * wrapping ncclUniqueId GetUniqueId() and aesthetic changes
    
    * adding synchronization, and tests for distributed
    
    * adding  to tests
    
    * fixing broken #endif
    
    * fixing initialization of gpu histograms, correcting errors in tests
    
    * adding to contributors list
    
    * adding distributed tests to jenkins
    
    * fixing bad path in distributed test
    
    * debugging
    
    * adding kubernetes for distributed tests
    
    * adding proper import for OrderedDict
    
    * adding urllib3==1.22 to address ordered_dict import error
    
    * added sleep to allow workers to save their models for comparison
    
    * adding name to GPU contributors under docs
    Speed up Jenkins by not compiling CMake (#4099)
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Test wheels on CUDA 10.0 container for compatibility (#3838)
    Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way (#3783)
    
    * Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way
    
    Also install git inside NVIDIA GPU container
    
    * Update dmlc-core
    Upgrading to NCCL2 (#3404)
    
    * Upgrading to NCCL2
    
    * Part - II of NCCL2 upgradation
    
     - Doc updates to build with nccl2
     - Dockerfile.gpu update for a correct CI build with nccl2
     - Updated FindNccl package to have env-var NCCL_ROOT to take precedence
    
    * Upgrading to v9.2 for CI workflow, since it has the nccl2 binaries available
    
    * Added NCCL2 license + copy the nccl binaries into /usr location for the FindNccl module to find
    
    * Set LD_LIBRARY_PATH variable to pick nccl2 binary at runtime
    
    * Need the nccl2 library download instructions inside Dockerfile.release as well
    
    * Use NCCL2 as a static library
    Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container. (#3309)
    
    * Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container.
    
    * Add Google test
    [BUILD] Dockerfile and Jenkinsfile revisited (#2514)
    
    Includes:
      - Dockerfile changes
        - Dockerfile clean up
        - Fix execution privileges of files used from Dockerfile.
        - New Dockerfile entrypoint to replace with_user script
        - Defined a placeholders for CPU testing (script and Dockerfile)
      - Jenkinsfile
        - Jenkins file milestone defined
        - Single source code checkout and propagation via stash/unstash
        - Bash needs to be explicitly used in launching make build, since we need
    access to environment
        - Jenkinsfile build factory for cmake and make style of jobs
        - Archivation of artifacts (*.so, *.whl, *.egg) produced by cmake build
    
    Missing:
      - CPU testing
      - Python3 env build and testing
    [GPU-Plugin] Add basic continuous integration for GPU plugin. (#2431)
    [CI] update spark version to 3.0.0 (#5890)
    
    * [CI] update spark version to 3.0.0
    
    * Update Dockerfile.jvm_cross
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Use Ubuntu 18.04 LTS in JVM CI, because 19.04 is EOL (#5537)
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Repair download URL for Maven 3.6.1 (#5139)
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [BLOCKING][CI] Upgrade to Spark 2.4.3 (#4414)
    
    * [CI] Upgrade to Spark 2.4.2
    
    * Pass Spark version to build script
    
    * Allow multiple --build-arg in ci_build.sh
    
    * Fix syntax
    
    * Fix container name
    
    * Update pom.xml
    
    * Fix container name
    
    * Update Jenkinsfile
    
    * Update pom.xml
    
    * Update Dockerfile.jvm_cross
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Add multi-GPU unit test environment (#3741)
    
    * Add multi-GPU unit test environment
    
    * Better assertion message
    
    * Temporarily disable failing test
    
    * Distinguish between multi-GPU and single-GPU CPP tests
    
    * Consolidate Python tests. Use attributes to distinguish multi-GPU Python tests from single-CPU counterparts
    Separate out restricted and unrestricted tasks (#3736)
    [jvm-packages] Fix up build for xgboost4j-gpu, xgboost4j-spark-gpu (#6216)
    
    * [CI] Clean up build for JVM packages
    
    * Use correct path for saving native lib
    
    * Fix groupId of maven-surefire-plugin
    
    * Fix stashing of xgboost4j_jar_gpu
    
    * [CI] Don't run xgboost4j-tester with GPU, since it doesn't use gpu_hist
    [CI] Port CI fixes from the 1.2.0 branch (#6050)
    
    * Fix a unit test on CLI, to handle RC versions
    
    * [CI] Use mgpu machine to run gpu hist unit tests
    
    * [CI] Build GPU-enabled JAR artifact and deploy to xgboost-maven-repo
    Further improvements and savings in Jenkins pipeline (#5904)
    
    * Publish artifacts only on the master and release branches
    
    * Build CUDA only for Compute Capability 7.5 when building PRs
    
    * Run all Windows jobs in a single worker image
    
    * Build nightly XGBoost4J SNAPSHOT JARs with Scala 2.12 only
    
    * Show skipped Python tests on Windows
    
    * Make Graphviz optional for Python tests
    
    * Add back C++ tests
    
    * Unstash xgboost_cpp_tests
    
    * Fix label to CUDA 10.1
    
    * Install cuPy for CUDA 10.1
    
    * Install jsonschema
    
    * Address reviewer's feedback
    [jvm-packages] [CI] Publish XGBoost4J JARs with Scala 2.11 and 2.12 (#5539)
    [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [BLOCKING][CI] Upgrade to Spark 2.4.3 (#4414)
    
    * [CI] Upgrade to Spark 2.4.2
    
    * Pass Spark version to build script
    
    * Allow multiple --build-arg in ci_build.sh
    
    * Fix syntax
    
    * Fix container name
    
    * Update pom.xml
    
    * Fix container name
    
    * Update Jenkinsfile
    
    * Update pom.xml
    
    * Update Dockerfile.jvm_cross
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [CI] Fix Docker build for CUDA 11 (#6202)
    Ensure that configured dmlc/build_config.h is picked up by Rabit and XGBoost (#5514)
    
    * Ensure that configured header (build_config.h) from dmlc-core is picked up by Rabit and XGBoost
    
    * Check which Rabit target is being used
    
    * Use CMake 3.13 in all Jenkins tests
    
    * Upgrade CMake in Travis CI
    
    * Install CMake using Kitware installer
    
    * Remove existing CMake (3.12.4)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Perform clang-tidy on both cpp and cuda source. (#4034)
    
    * Basic script for using compilation database.
    
    * Add `GENERATE_COMPILATION_DATABASE' to CMake.
    * Rearrange CMakeLists.txt.
    * Add basic python clang-tidy script.
    * Remove modernize-use-auto.
    * Add clang-tidy to Jenkins
    * Refine logic for correct path detection
    
    In Jenkins, the project root is of form /home/ubuntu/workspace/xgboost_PR-XXXX
    
    * Run clang-tidy in CUDA 9.2 container
    * Use clang_tidy container
    Speed up Jenkins by not compiling CMake (#4099)
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Test wheels on CUDA 10.0 container for compatibility (#3838)
    Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way (#3783)
    
    * Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way
    
    Also install git inside NVIDIA GPU container
    
    * Update dmlc-core
    Upgrading to NCCL2 (#3404)
    
    * Upgrading to NCCL2
    
    * Part - II of NCCL2 upgradation
    
     - Doc updates to build with nccl2
     - Dockerfile.gpu update for a correct CI build with nccl2
     - Updated FindNccl package to have env-var NCCL_ROOT to take precedence
    
    * Upgrading to v9.2 for CI workflow, since it has the nccl2 binaries available
    
    * Added NCCL2 license + copy the nccl binaries into /usr location for the FindNccl module to find
    
    * Set LD_LIBRARY_PATH variable to pick nccl2 binary at runtime
    
    * Need the nccl2 library download instructions inside Dockerfile.release as well
    
    * Use NCCL2 as a static library
    Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container. (#3309)
    
    * Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container.
    
    * Add Google test
    [BUILD] Dockerfile and Jenkinsfile revisited (#2514)
    
    Includes:
      - Dockerfile changes
        - Dockerfile clean up
        - Fix execution privileges of files used from Dockerfile.
        - New Dockerfile entrypoint to replace with_user script
        - Defined a placeholders for CPU testing (script and Dockerfile)
      - Jenkinsfile
        - Jenkins file milestone defined
        - Single source code checkout and propagation via stash/unstash
        - Bash needs to be explicitly used in launching make build, since we need
    access to environment
        - Jenkinsfile build factory for cmake and make style of jobs
        - Archivation of artifacts (*.so, *.whl, *.egg) produced by cmake build
    
    Missing:
      - CPU testing
      - Python3 env build and testing
    [GPU-Plugin] Add basic continuous integration for GPU plugin. (#2431)
    [CI] Fix broken Docker container 'cpu' (#5956)
    Remove makefiles. (#5513)
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    [jvm-packages] Fix up build for xgboost4j-gpu, xgboost4j-spark-gpu (#6216)
    
    * [CI] Clean up build for JVM packages
    
    * Use correct path for saving native lib
    
    * Fix groupId of maven-surefire-plugin
    
    * Fix stashing of xgboost4j_jar_gpu
    
    * [CI] Don't run xgboost4j-tester with GPU, since it doesn't use gpu_hist
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [BLOCKING][CI] Upgrade to Spark 2.4.3 (#4414)
    
    * [CI] Upgrade to Spark 2.4.2
    
    * Pass Spark version to build script
    
    * Allow multiple --build-arg in ci_build.sh
    
    * Fix syntax
    
    * Fix container name
    
    * Update pom.xml
    
    * Fix container name
    
    * Update Jenkinsfile
    
    * Update pom.xml
    
    * Update Dockerfile.jvm_cross
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Time GPU tests on CI. (#6141)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    [CI] Fix broken Docker container 'cpu' (#5956)
    [CI] Fix cuDF install; merge 'gpu' and 'cudf' test suite (#5814)
    Require Python 3.6+; drop Python 3.5 from CI (#5715)
    [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)
    
    * [CI] Use Vault repository to re-gain access to devtoolset-4
    
    * Use manylinux2010 tag
    
    * Update Dockerfile.jvm
    
    * Fix rename_whl.py
    
    * Upgrade Pip, to handle manylinux2010 tag
    
    * Update insert_vcomp140.py
    
    * Update test_python.sh
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Device dmatrix (#5420)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    Support dmatrix construction from cupy array (#5206)
    Remove benchmark code in GPU test. (#5141)
    
    
    
    * Update Jenkins script.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    [CI] Run cuDF tests in Jenkins CI server (#4927)
    Add to documentation how to run tests locally (#4610)
    
    * Add to documentation how to build native unit tests
    
    * Add instructions to run Python tests and to use Docker container [skip ci]
    
    * Fix link to pytest chapter
    
    * Add link to Google Test [skip ci]
    
    * Set PYTHONPATH [skip ci]
    
    * Revise test_python.sh for running tests locally
    
    * Update test_python.sh
    
    * Place Docker recommendation notice in a prominent place [skip ci]
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [CI] Fix Docker build for CUDA 11 (#6202)
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] update spark version to 3.0.0 (#5890)
    
    * [CI] update spark version to 3.0.0
    
    * Update Dockerfile.jvm_cross
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Use Ubuntu 18.04 LTS in JVM CI, because 19.04 is EOL (#5537)
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Repair download URL for Maven 3.6.1 (#5139)
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [BLOCKING][CI] Upgrade to Spark 2.4.3 (#4414)
    
    * [CI] Upgrade to Spark 2.4.2
    
    * Pass Spark version to build script
    
    * Allow multiple --build-arg in ci_build.sh
    
    * Fix syntax
    
    * Fix container name
    
    * Update pom.xml
    
    * Fix container name
    
    * Update Jenkinsfile
    
    * Update pom.xml
    
    * Update Dockerfile.jvm_cross
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Support building XGBoost with CUDA 11 (#5808)
    
    * Change serialization test.
    * Add CUDA 11 tests on Linux CI.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Specify account ID when logging into ECR Docker registry (#4584)
    
    * [CI] Specify account ID when logging into ECR Docker registry
    
    * Do not display awscli login command
    [CI] Cache two R build Docker containers (#4458)
    [BLOCKING][CI] Upgrade to Spark 2.4.3 (#4414)
    
    * [CI] Upgrade to Spark 2.4.2
    
    * Pass Spark version to build script
    
    * Allow multiple --build-arg in ci_build.sh
    
    * Fix syntax
    
    * Fix container name
    
    * Update pom.xml
    
    * Fix container name
    
    * Update Jenkinsfile
    
    * Update pom.xml
    
    * Update Dockerfile.jvm_cross
    Make CMakeLists.txt compatible with CMake 3.3 (#4420)
    
    * Make CMakeLists.txt compatible with CMake 3.3; require CMake 3.11 for MSVC
    
    * Use CMake 3.12 when sanitizer is enabled
    
    * Disable funroll-loops for MSVC
    
    * Use cmake version in container name
    
    * Add missing arg
    
    * Fix egrep use in ci_build.sh
    
    * Display CMake version
    
    * Do not set OpenMP_CXX_LIBRARIES for MSVC
    
    * Use cmake_minimum_required()
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [CI] Optimize external Docker build cache (#4334)
    
    * When building pull requests, use Docker cache for master branch
    
    Docker build caches are per-branch, so new pull requests will initially
    have no build cache, causing the Docker containers to be built from
    scratch. New pull requests should use the cache associated with the
    master branch. This makes sense, since most pull requests do not modify
    the Dockerfile.
    
    * Add comments
    [CI] Add external Docker build cache (#4331)
    Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container. (#3309)
    
    * Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container.
    
    * Add Google test
    [BUILD] Dockerfile and Jenkinsfile revisited (#2514)
    
    Includes:
      - Dockerfile changes
        - Dockerfile clean up
        - Fix execution privileges of files used from Dockerfile.
        - New Dockerfile entrypoint to replace with_user script
        - Defined a placeholders for CPU testing (script and Dockerfile)
      - Jenkinsfile
        - Jenkins file milestone defined
        - Single source code checkout and propagation via stash/unstash
        - Bash needs to be explicitly used in launching make build, since we need
    access to environment
        - Jenkinsfile build factory for cmake and make style of jobs
        - Archivation of artifacts (*.so, *.whl, *.egg) produced by cmake build
    
    Missing:
      - CPU testing
      - Python3 env build and testing
    [GPU-Plugin] Add basic continuous integration for GPU plugin. (#2431)
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [CI] Fix Docker build for CUDA 11 (#6202)
    [CI] Upgrade cuDF and RMM to 0.16 nightlies; upgrade to Ubuntu 18.04 (#6157)
    
    * [CI] Upgrade cuDF and RMM to 0.16 nightlies
    
    * Use Ubuntu 18.04 in RMM test, since RMM needs GCC 7+
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    [CI] Fix cuDF install; merge 'gpu' and 'cudf' test suite (#5814)
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    [CI] Use latest dask (#4973)
    
    
    * Remove version spec, to use latest dask always
    [CI] Fix broken installation of Pandas (#4722)
    
    * [CI] Fix broken installation of Pandas
    
    * Update Dockerfile.gpu
    Support Dask 2.0 (#4617)
    Add native support for Dask (#4473)
    
    * Add native support for Dask
    
    * Add multi-GPU demo
    
    * Add sklearn example
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    [CI] Upgrade to GCC 5.3.1, CMake 3.6.0 (#4306)
    
    * Upgrade to GCC 5.3.1, CMake 3.6.0
    
    * <regex> is now okay
    jenkins build for cuda 10.0 (#4281)
    
    * jenkins build for cuda 10.0
    
    * yum install nccl2 for cuda 10.0
    [REVIEW] Enable Multi-Node Multi-GPU functionality (#4095)
    
    * Initial commit to support multi-node multi-gpu xgboost using dask
    
    * Fixed NCCL initialization by not ignoring the opg parameter.
    
    - it now crashes on NCCL initialization, but at least we're attempting it properly
    
    * At the root node, perform a rabit::Allreduce to get initial sum_gradient across workers
    
    * Synchronizing in a couple of more places.
    
    - now the workers don't go down, but just hang
    - no more "wild" values of gradients
    - probably needs syncing in more places
    
    * Added another missing max-allreduce operation inside BuildHistLeftRight
    
    * Removed unnecessary collective operations.
    
    * Simplified rabit::Allreduce() sync of gradient sums.
    
    * Removed unnecessary rabit syncs around ncclAllReduce.
    
    - this improves performance _significantly_ (7x faster for overall training,
      20x faster for xgboost proper)
    
    * pulling in latest xgboost
    
    * removing changes to updater_quantile_hist.cc
    
    * changing use_nccl_opg initialization, removing unnecessary if statements
    
    * added definition for opaque ncclUniqueId struct to properly encapsulate GetUniqueId
    
    * placing struct defintion in guard to avoid duplicate code errors
    
    * addressing linting errors
    
    * removing
    
    * removing additional arguments to AllReduer initialization
    
    * removing distributed flag
    
    * making comm init symmetric
    
    * removing distributed flag
    
    * changing ncclCommInit to support multiple modalities
    
    * fix indenting
    
    * updating ncclCommInitRank block with necessary group calls
    
    * fix indenting
    
    * adding print statement, and updating accessor in vector
    
    * improving print statement to end-line
    
    * generalizing nccl_rank construction using rabit
    
    * assume device_ordinals is the same for every node
    
    * test, assume device_ordinals is identical for all nodes
    
    * test, assume device_ordinals is unique for all nodes
    
    * changing names of offset variable to be more descriptive, editing indenting
    
    * wrapping ncclUniqueId GetUniqueId() and aesthetic changes
    
    * adding synchronization, and tests for distributed
    
    * adding  to tests
    
    * fixing broken #endif
    
    * fixing initialization of gpu histograms, correcting errors in tests
    
    * adding to contributors list
    
    * adding distributed tests to jenkins
    
    * fixing bad path in distributed test
    
    * debugging
    
    * adding kubernetes for distributed tests
    
    * adding proper import for OrderedDict
    
    * adding urllib3==1.22 to address ordered_dict import error
    
    * added sleep to allow workers to save their models for comparison
    
    * adding name to GPU contributors under docs
    Speed up Jenkins by not compiling CMake (#4099)
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Test wheels on CUDA 10.0 container for compatibility (#3838)
    Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way (#3783)
    
    * Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way
    
    Also install git inside NVIDIA GPU container
    
    * Update dmlc-core
    Upgrading to NCCL2 (#3404)
    
    * Upgrading to NCCL2
    
    * Part - II of NCCL2 upgradation
    
     - Doc updates to build with nccl2
     - Dockerfile.gpu update for a correct CI build with nccl2
     - Updated FindNccl package to have env-var NCCL_ROOT to take precedence
    
    * Upgrading to v9.2 for CI workflow, since it has the nccl2 binaries available
    
    * Added NCCL2 license + copy the nccl binaries into /usr location for the FindNccl module to find
    
    * Set LD_LIBRARY_PATH variable to pick nccl2 binary at runtime
    
    * Need the nccl2 library download instructions inside Dockerfile.release as well
    
    * Use NCCL2 as a static library
    Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container. (#3309)
    
    * Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container.
    
    * Add Google test
    [BUILD] Dockerfile and Jenkinsfile revisited (#2514)
    
    Includes:
      - Dockerfile changes
        - Dockerfile clean up
        - Fix execution privileges of files used from Dockerfile.
        - New Dockerfile entrypoint to replace with_user script
        - Defined a placeholders for CPU testing (script and Dockerfile)
      - Jenkinsfile
        - Jenkins file milestone defined
        - Single source code checkout and propagation via stash/unstash
        - Bash needs to be explicitly used in launching make build, since we need
    access to environment
        - Jenkinsfile build factory for cmake and make style of jobs
        - Archivation of artifacts (*.so, *.whl, *.egg) produced by cmake build
    
    Missing:
      - CPU testing
      - Python3 env build and testing
    [GPU-Plugin] Add basic continuous integration for GPU plugin. (#2431)
    [CI] Fix broken Docker container 'cpu' (#5956)
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    [CI] Simplify CMake build with modern CMake techniques (#5871)
    
    * [CI] Simplify CMake build
    
    * Make sure that plugins can be built
    
    * [CI] Install lz4 on Mac
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    Require Python 3.6+; drop Python 3.5 from CI (#5715)
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Ensure that configured dmlc/build_config.h is picked up by Rabit and XGBoost (#5514)
    
    * Ensure that configured header (build_config.h) from dmlc-core is picked up by Rabit and XGBoost
    
    * Check which Rabit target is being used
    
    * Use CMake 3.13 in all Jenkins tests
    
    * Upgrade CMake in Travis CI
    
    * Install CMake using Kitware installer
    
    * Remove existing CMake (3.12.4)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Enable OpenMP with Apple Clang (Mac default compiler) (#5146)
    
    * Add OpenMP as CMake target
    
    * Require CMake 3.12, to allow linking OpenMP target to objxgboost
    
    * Specify OpenMP compiler flag for CUDA host compiler
    
    * Require CMake 3.16+ if the OS is Mac OSX
    
    * Use AppleClang in Mac tests.
    
    * Update dmlc-core
    Example JSON model parser and Schema. (#5137)
    [CI] Use latest dask (#4973)
    
    
    * Remove version spec, to use latest dask always
    [CI] Fix broken installation of Pandas (#4704)
    Support Dask 2.0 (#4617)
    Add native support for Dask (#4473)
    
    * Add native support for Dask
    
    * Add multi-GPU demo
    
    * Add sklearn example
    Make CMakeLists.txt compatible with CMake 3.3 (#4420)
    
    * Make CMakeLists.txt compatible with CMake 3.3; require CMake 3.11 for MSVC
    
    * Use CMake 3.12 when sanitizer is enabled
    
    * Disable funroll-loops for MSVC
    
    * Use cmake version in container name
    
    * Add missing arg
    
    * Fix egrep use in ci_build.sh
    
    * Display CMake version
    
    * Do not set OpenMP_CXX_LIBRARIES for MSVC
    
    * Use cmake_minimum_required()
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container. (#3309)
    
    * Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container.
    
    * Add Google test
    [BUILD] Dockerfile and Jenkinsfile revisited (#2514)
    
    Includes:
      - Dockerfile changes
        - Dockerfile clean up
        - Fix execution privileges of files used from Dockerfile.
        - New Dockerfile entrypoint to replace with_user script
        - Defined a placeholders for CPU testing (script and Dockerfile)
      - Jenkinsfile
        - Jenkins file milestone defined
        - Single source code checkout and propagation via stash/unstash
        - Bash needs to be explicitly used in launching make build, since we need
    access to environment
        - Jenkinsfile build factory for cmake and make style of jobs
        - Archivation of artifacts (*.so, *.whl, *.egg) produced by cmake build
    
    Missing:
      - CPU testing
      - Python3 env build and testing
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Fix Docker build for CUDA 11 (#6202)
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Support building XGBoost with CUDA 11 (#5808)
    
    * Change serialization test.
    * Add CUDA 11 tests on Linux CI.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)
    
    * [CI] Use Vault repository to re-gain access to devtoolset-4
    
    * Use manylinux2010 tag
    
    * Update Dockerfile.jvm
    
    * Fix rename_whl.py
    
    * Upgrade Pip, to handle manylinux2010 tag
    
    * Update insert_vcomp140.py
    
    * Update test_python.sh
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Ensure that configured dmlc/build_config.h is picked up by Rabit and XGBoost (#5514)
    
    * Ensure that configured header (build_config.h) from dmlc-core is picked up by Rabit and XGBoost
    
    * Check which Rabit target is being used
    
    * Use CMake 3.13 in all Jenkins tests
    
    * Upgrade CMake in Travis CI
    
    * Install CMake using Kitware installer
    
    * Remove existing CMake (3.12.4)
    [CI] Use devtoolset-6 because devtoolset-4 is EOL and no longer available (#5506)
    
    * Use devtoolset-6.
    
    * [CI] Use devtoolset-6 because devtoolset-4 is EOL and no longer available
    
    * CUDA 9.0 doesn't work with devtoolset-6; use devtoolset-4 for GPU build only
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    add cuda 10.1 support (#4468)
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    [CI] Upgrade to GCC 5.3.1, CMake 3.6.0 (#4306)
    
    * Upgrade to GCC 5.3.1, CMake 3.6.0
    
    * <regex> is now okay
    jenkins build for cuda 10.0 (#4281)
    
    * jenkins build for cuda 10.0
    
    * yum install nccl2 for cuda 10.0
    [REVIEW] Enable Multi-Node Multi-GPU functionality (#4095)
    
    * Initial commit to support multi-node multi-gpu xgboost using dask
    
    * Fixed NCCL initialization by not ignoring the opg parameter.
    
    - it now crashes on NCCL initialization, but at least we're attempting it properly
    
    * At the root node, perform a rabit::Allreduce to get initial sum_gradient across workers
    
    * Synchronizing in a couple of more places.
    
    - now the workers don't go down, but just hang
    - no more "wild" values of gradients
    - probably needs syncing in more places
    
    * Added another missing max-allreduce operation inside BuildHistLeftRight
    
    * Removed unnecessary collective operations.
    
    * Simplified rabit::Allreduce() sync of gradient sums.
    
    * Removed unnecessary rabit syncs around ncclAllReduce.
    
    - this improves performance _significantly_ (7x faster for overall training,
      20x faster for xgboost proper)
    
    * pulling in latest xgboost
    
    * removing changes to updater_quantile_hist.cc
    
    * changing use_nccl_opg initialization, removing unnecessary if statements
    
    * added definition for opaque ncclUniqueId struct to properly encapsulate GetUniqueId
    
    * placing struct defintion in guard to avoid duplicate code errors
    
    * addressing linting errors
    
    * removing
    
    * removing additional arguments to AllReduer initialization
    
    * removing distributed flag
    
    * making comm init symmetric
    
    * removing distributed flag
    
    * changing ncclCommInit to support multiple modalities
    
    * fix indenting
    
    * updating ncclCommInitRank block with necessary group calls
    
    * fix indenting
    
    * adding print statement, and updating accessor in vector
    
    * improving print statement to end-line
    
    * generalizing nccl_rank construction using rabit
    
    * assume device_ordinals is the same for every node
    
    * test, assume device_ordinals is identical for all nodes
    
    * test, assume device_ordinals is unique for all nodes
    
    * changing names of offset variable to be more descriptive, editing indenting
    
    * wrapping ncclUniqueId GetUniqueId() and aesthetic changes
    
    * adding synchronization, and tests for distributed
    
    * adding  to tests
    
    * fixing broken #endif
    
    * fixing initialization of gpu histograms, correcting errors in tests
    
    * adding to contributors list
    
    * adding distributed tests to jenkins
    
    * fixing bad path in distributed test
    
    * debugging
    
    * adding kubernetes for distributed tests
    
    * adding proper import for OrderedDict
    
    * adding urllib3==1.22 to address ordered_dict import error
    
    * added sleep to allow workers to save their models for comparison
    
    * adding name to GPU contributors under docs
    Speed up Jenkins by not compiling CMake (#4099)
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Test wheels on CUDA 10.0 container for compatibility (#3838)
    Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way (#3783)
    
    * Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way
    
    Also install git inside NVIDIA GPU container
    
    * Update dmlc-core
    Upgrading to NCCL2 (#3404)
    
    * Upgrading to NCCL2
    
    * Part - II of NCCL2 upgradation
    
     - Doc updates to build with nccl2
     - Dockerfile.gpu update for a correct CI build with nccl2
     - Updated FindNccl package to have env-var NCCL_ROOT to take precedence
    
    * Upgrading to v9.2 for CI workflow, since it has the nccl2 binaries available
    
    * Added NCCL2 license + copy the nccl binaries into /usr location for the FindNccl module to find
    
    * Set LD_LIBRARY_PATH variable to pick nccl2 binary at runtime
    
    * Need the nccl2 library download instructions inside Dockerfile.release as well
    
    * Use NCCL2 as a static library
    Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container. (#3309)
    
    * Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container.
    
    * Add Google test
    [BUILD] Dockerfile and Jenkinsfile revisited (#2514)
    
    Includes:
      - Dockerfile changes
        - Dockerfile clean up
        - Fix execution privileges of files used from Dockerfile.
        - New Dockerfile entrypoint to replace with_user script
        - Defined a placeholders for CPU testing (script and Dockerfile)
      - Jenkinsfile
        - Jenkins file milestone defined
        - Single source code checkout and propagation via stash/unstash
        - Bash needs to be explicitly used in launching make build, since we need
    access to environment
        - Jenkinsfile build factory for cmake and make style of jobs
        - Archivation of artifacts (*.so, *.whl, *.egg) produced by cmake build
    
    Missing:
      - CPU testing
      - Python3 env build and testing
    [GPU-Plugin] Add basic continuous integration for GPU plugin. (#2431)
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    C++14 for xgboost (#5664)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Use bundled gtest (#4900)
    
    * Suggest to use gtest bundled with dmlc
    
    * Use dmlc bundled gtest in all CI scripts
    
    * Make clang-tidy to use dmlc embedded gtest
    Use yaml.safe_load. (#4537)
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Perform clang-tidy on both cpp and cuda source. (#4034)
    
    * Basic script for using compilation database.
    
    * Add `GENERATE_COMPILATION_DATABASE' to CMake.
    * Rearrange CMakeLists.txt.
    * Add basic python clang-tidy script.
    * Remove modernize-use-auto.
    * Add clang-tidy to Jenkins
    * Refine logic for correct path detection
    
    In Jenkins, the project root is of form /home/ubuntu/workspace/xgboost_PR-XXXX
    
    * Run clang-tidy in CUDA 9.2 container
    * Use clang_tidy container
    [CI] Fix Docker build for CUDA 11 (#6202)
    Support building XGBoost with CUDA 11 (#5808)
    
    * Change serialization test.
    * Add CUDA 11 tests on Linux CI.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)
    
    * [CI] Use Vault repository to re-gain access to devtoolset-4
    
    * Use manylinux2010 tag
    
    * Update Dockerfile.jvm
    
    * Fix rename_whl.py
    
    * Upgrade Pip, to handle manylinux2010 tag
    
    * Update insert_vcomp140.py
    
    * Update test_python.sh
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Ensure that configured dmlc/build_config.h is picked up by Rabit and XGBoost (#5514)
    
    * Ensure that configured header (build_config.h) from dmlc-core is picked up by Rabit and XGBoost
    
    * Check which Rabit target is being used
    
    * Use CMake 3.13 in all Jenkins tests
    
    * Upgrade CMake in Travis CI
    
    * Install CMake using Kitware installer
    
    * Remove existing CMake (3.12.4)
    [CI] Use devtoolset-6 because devtoolset-4 is EOL and no longer available (#5506)
    
    * Use devtoolset-6.
    
    * [CI] Use devtoolset-6 because devtoolset-4 is EOL and no longer available
    
    * CUDA 9.0 doesn't work with devtoolset-6; use devtoolset-4 for GPU build only
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    add cuda 10.1 support (#4468)
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    [CI] Upgrade to GCC 5.3.1, CMake 3.6.0 (#4306)
    
    * Upgrade to GCC 5.3.1, CMake 3.6.0
    
    * <regex> is now okay
    jenkins build for cuda 10.0 (#4281)
    
    * jenkins build for cuda 10.0
    
    * yum install nccl2 for cuda 10.0
    [REVIEW] Enable Multi-Node Multi-GPU functionality (#4095)
    
    * Initial commit to support multi-node multi-gpu xgboost using dask
    
    * Fixed NCCL initialization by not ignoring the opg parameter.
    
    - it now crashes on NCCL initialization, but at least we're attempting it properly
    
    * At the root node, perform a rabit::Allreduce to get initial sum_gradient across workers
    
    * Synchronizing in a couple of more places.
    
    - now the workers don't go down, but just hang
    - no more "wild" values of gradients
    - probably needs syncing in more places
    
    * Added another missing max-allreduce operation inside BuildHistLeftRight
    
    * Removed unnecessary collective operations.
    
    * Simplified rabit::Allreduce() sync of gradient sums.
    
    * Removed unnecessary rabit syncs around ncclAllReduce.
    
    - this improves performance _significantly_ (7x faster for overall training,
      20x faster for xgboost proper)
    
    * pulling in latest xgboost
    
    * removing changes to updater_quantile_hist.cc
    
    * changing use_nccl_opg initialization, removing unnecessary if statements
    
    * added definition for opaque ncclUniqueId struct to properly encapsulate GetUniqueId
    
    * placing struct defintion in guard to avoid duplicate code errors
    
    * addressing linting errors
    
    * removing
    
    * removing additional arguments to AllReduer initialization
    
    * removing distributed flag
    
    * making comm init symmetric
    
    * removing distributed flag
    
    * changing ncclCommInit to support multiple modalities
    
    * fix indenting
    
    * updating ncclCommInitRank block with necessary group calls
    
    * fix indenting
    
    * adding print statement, and updating accessor in vector
    
    * improving print statement to end-line
    
    * generalizing nccl_rank construction using rabit
    
    * assume device_ordinals is the same for every node
    
    * test, assume device_ordinals is identical for all nodes
    
    * test, assume device_ordinals is unique for all nodes
    
    * changing names of offset variable to be more descriptive, editing indenting
    
    * wrapping ncclUniqueId GetUniqueId() and aesthetic changes
    
    * adding synchronization, and tests for distributed
    
    * adding  to tests
    
    * fixing broken #endif
    
    * fixing initialization of gpu histograms, correcting errors in tests
    
    * adding to contributors list
    
    * adding distributed tests to jenkins
    
    * fixing bad path in distributed test
    
    * debugging
    
    * adding kubernetes for distributed tests
    
    * adding proper import for OrderedDict
    
    * adding urllib3==1.22 to address ordered_dict import error
    
    * added sleep to allow workers to save their models for comparison
    
    * adding name to GPU contributors under docs
    Speed up Jenkins by not compiling CMake (#4099)
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Test wheels on CUDA 10.0 container for compatibility (#3838)
    Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way (#3783)
    
    * Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way
    
    Also install git inside NVIDIA GPU container
    
    * Update dmlc-core
    Upgrading to NCCL2 (#3404)
    
    * Upgrading to NCCL2
    
    * Part - II of NCCL2 upgradation
    
     - Doc updates to build with nccl2
     - Dockerfile.gpu update for a correct CI build with nccl2
     - Updated FindNccl package to have env-var NCCL_ROOT to take precedence
    
    * Upgrading to v9.2 for CI workflow, since it has the nccl2 binaries available
    
    * Added NCCL2 license + copy the nccl binaries into /usr location for the FindNccl module to find
    
    * Set LD_LIBRARY_PATH variable to pick nccl2 binary at runtime
    
    * Need the nccl2 library download instructions inside Dockerfile.release as well
    
    * Use NCCL2 as a static library
    Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container. (#3309)
    
    * Add cuda 8/9.1 centos 6 builds, test GPU wheel on CPU only container.
    
    * Add Google test
    [BUILD] Dockerfile and Jenkinsfile revisited (#2514)
    
    Includes:
      - Dockerfile changes
        - Dockerfile clean up
        - Fix execution privileges of files used from Dockerfile.
        - New Dockerfile entrypoint to replace with_user script
        - Defined a placeholders for CPU testing (script and Dockerfile)
      - Jenkinsfile
        - Jenkins file milestone defined
        - Single source code checkout and propagation via stash/unstash
        - Bash needs to be explicitly used in launching make build, since we need
    access to environment
        - Jenkinsfile build factory for cmake and make style of jobs
        - Archivation of artifacts (*.so, *.whl, *.egg) produced by cmake build
    
    Missing:
      - CPU testing
      - Python3 env build and testing
    [GPU-Plugin] Add basic continuous integration for GPU plugin. (#2431)
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)
    
    * [CI] Use Vault repository to re-gain access to devtoolset-4
    
    * Use manylinux2010 tag
    
    * Update Dockerfile.jvm
    
    * Fix rename_whl.py
    
    * Upgrade Pip, to handle manylinux2010 tag
    
    * Update insert_vcomp140.py
    
    * Update test_python.sh
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Ensure that configured dmlc/build_config.h is picked up by Rabit and XGBoost (#5514)
    
    * Ensure that configured header (build_config.h) from dmlc-core is picked up by Rabit and XGBoost
    
    * Check which Rabit target is being used
    
    * Use CMake 3.13 in all Jenkins tests
    
    * Upgrade CMake in Travis CI
    
    * Install CMake using Kitware installer
    
    * Remove existing CMake (3.12.4)
    [CI] Use devtoolset-6 because devtoolset-4 is EOL and no longer available (#5506)
    
    * Use devtoolset-6.
    
    * [CI] Use devtoolset-6 because devtoolset-4 is EOL and no longer available
    
    * CUDA 9.0 doesn't work with devtoolset-6; use devtoolset-4 for GPU build only
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Repair download URL for Maven 3.6.1 (#5139)
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Modin DF support (#6055)
    
    * Modin DF support
    
    * mode change
    
    * tests were added, ci env was extended
    
    * mode change
    
    * Remove redundant installation of modin
    
    * Add a pytest skip marker for modin
    
    * Install Modin[ray] from PyPI
    
    * fix interfering
    
    * avoid extra conversion
    
    * delete cv test for modin
    
    * revert cv function
    
    Co-authored-by: ShvetsKS <kirill.shvets@intel.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Fix broken Docker container 'cpu' (#5956)
    Further improvements and savings in Jenkins pipeline (#5904)
    
    * Publish artifacts only on the master and release branches
    
    * Build CUDA only for Compute Capability 7.5 when building PRs
    
    * Run all Windows jobs in a single worker image
    
    * Build nightly XGBoost4J SNAPSHOT JARs with Scala 2.12 only
    
    * Show skipped Python tests on Windows
    
    * Make Graphviz optional for Python tests
    
    * Add back C++ tests
    
    * Unstash xgboost_cpp_tests
    
    * Fix label to CUDA 10.1
    
    * Install cuPy for CUDA 10.1
    
    * Install jsonschema
    
    * Address reviewer's feedback
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
    Modin DF support (#6055)
    
    * Modin DF support
    
    * mode change
    
    * tests were added, ci env was extended
    
    * mode change
    
    * Remove redundant installation of modin
    
    * Add a pytest skip marker for modin
    
    * Install Modin[ray] from PyPI
    
    * fix interfering
    
    * avoid extra conversion
    
    * delete cv test for modin
    
    * revert cv function
    
    Co-authored-by: ShvetsKS <kirill.shvets@intel.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Fix broken Docker container 'cpu' (#5956)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Unify CPU hist sketching (#5880)
    Add missing Pytest marks to AsyncIO unit test (#5968)
    [Dask] Asyncio support. (#5862)
    Dask device dmatrix (#5901)
    
    
    * Fix softprob with empty dmatrix.
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    [dask] Return GPU Series when input is from cuDF. (#5710)
    
    
    * Refactor predict function.
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Tests for empty dmatrix. (#5159)
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Don't `set_params` at the end of `set_state`. (#4947)
    
    * Don't set_params at the end of set_state.
    
    * Also fix another issue found in dask prediction.
    
    * Add note about prediction.
    
    Don't support other prediction modes at the moment.
    [CI] Run cuDF tests in Jenkins CI server (#4927)
    Use `cudf.concat` explicitly. (#4918)
    
    * Use `cudf.concat` explicitly.
    
    * Add test.
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    Better message when no GPU is found. (#5594)
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    Disable parameter validation for Scikit-Learn interface. (#5167)
    
    * Disable parameter validation for now.
    
    Scikit-Learn passes all parameters down to XGBoost, whether they are used or
    not.
    
    * Add option `validate_parameters`.
    Fix wrapping GPU ID and prevent data copying. (#5160)
    
    * Removed some data copying.
    
    * Make sure gpu_id is valid before any configuration is carried out.
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    [BREAKING] prevent multi-gpu usage (#4749)
    
    * prevent multi-gpu usage
    
    * fix distributed test
    
    * combine gpu predictor tests
    
    * set upper bound on n_gpus
    Fix prediction from loaded pickle. (#4516)
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    [CI] Fix Dask Pytest fixture (#6024)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    Speed up python test (#5752)
    
    * Speed up tests
    
    * Prevent DeviceQuantileDMatrix initialisation with numpy
    
    * Use joblib.memory
    
    * Use RandomState
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    [BREAKING] prevent multi-gpu usage (#4749)
    
    * prevent multi-gpu usage
    
    * fix distributed test
    
    * combine gpu predictor tests
    
    * set upper bound on n_gpus
    Fix test_gpu_coordinate. (#3974)
    
    * Fix test_gpu_coordinate.
    
    * Use `gpu_coord_descent` in test.
    * Reduce number of running rounds.
    
    * Remove nthread.
    
    * Use githubusercontent for r-appveyor.
    
    * Use githubusercontent in travis r tests.
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Added finding quantiles on GPU. (#3393)
    
    * Added finding quantiles on GPU.
    
    - this includes datasets where weights are assigned to data rows
    - as the quantiles found by the new algorithm are not the same
      as those found by the old one, test thresholds in
        tests/python-gpu/test_gpu_updaters.py have been adjusted.
    
    * Adjustments and improved testing for finding quantiles on the GPU.
    
    - added C++ tests for the DeviceSketch() function
    - reduced one of the thresholds in test_gpu_updaters.py
    - adjusted the cuts found by the find_cuts_k kernel
    Updates for GPU CI tests (#3467)
    
    * Fail GPU CI after test failure
    
    * Fix GPU linear tests
    
    * Reduced number of GPU tests to speed up CI
    
    * Remove static allocations of device memory
    
    * Resolve illegal memory access for updater_fast_hist.cc
    
    * Fix broken r tests dependency
    
    * Update python install documentation for GPU
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    More categorical tests and disable shap sparse test. (#6219)
    
    * Fix tree load with 32 category.
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
    Unify evaluation functions. (#6037)
    GPUTreeShap (#6038)
    Add cupy to Windows CI (#5797)
    
    * Add cupy to Windows CI
    
    * Update Jenkinsfile-win64
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Update Jenkinsfile-win64
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Update tests/python-gpu/test_gpu_prediction.py
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    Refactor gpu_hist split evaluation (#5610)
    
    * Refactor
    
    * Rewrite evaluate splits
    
    * Add more tests
    Set device in device dmatrix. (#5596)
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    Remove benchmark code in GPU test. (#5141)
    
    
    
    * Update Jenkins script.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Fix #3730: scikit-learn 0.20 compatibility fix (#3731)
    
    * Fix #3730: scikit-learn 0.20 compatibility fix
    
    sklearn.cross_validation has been removed from scikit-learn 0.20,
    so replace it with sklearn.model_selection
    
    * Display test names for Python tests for clarity
    Fix logic in GPU predictor cache lookup (#3217)
    
    * Fix logic in GPU predictor cache lookup
    
    * Add sklearn test for GPU prediction
    Fix several GPU bugs (#2916)
    
    * Fix #2905
    
    * Fix gpu_exact test failures
    
    * Fix bug in GPU prediction where multiple calls to batch prediction can produce incorrect results
    
    * Fix GPU documentation formatting
    Add GPU documentation (#2695)
    
    * Add GPU documentation
    
    * Update Python GPU tests
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    Feature weights (#5962)
    Simplify the data backends. (#5893)
    Speed up python test (#5752)
    
    * Speed up tests
    
    * Prevent DeviceQuantileDMatrix initialisation with numpy
    
    * Use joblib.memory
    
    * Use RandomState
    More categorical tests and disable shap sparse test. (#6219)
    
    * Fix tree load with 32 category.
    Add high level tests for categorical data. (#6179)
    
    * Fix unique.
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    Speed up python test (#5752)
    
    * Speed up tests
    
    * Prevent DeviceQuantileDMatrix initialisation with numpy
    
    * Use joblib.memory
    
    * Use RandomState
    Device dmatrix (#5420)
    Testing hist_util (#5251)
    
    * Rank tests
    
    * Remove categorical split specialisation
    
    * Extend tests to multiple features, switch to WQSketch
    
    * Add tests for SparseCuts
    
    * Add external memory quantile tests, fix some existing tests
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    [BREAKING] prevent multi-gpu usage (#4749)
    
    * prevent multi-gpu usage
    
    * fix distributed test
    
    * combine gpu predictor tests
    
    * set upper bound on n_gpus
    Remove gpu_exact tree method (#4742)
     Combine thread launches into single launch per tree for gpu_hist (#4343)
    
    * Combine thread launches into single launch per tree for gpu_hist
    algorithm.
    
    * Address deprecation warning
    
    * Add manual column sampler constructor
    
    * Turn off omp dynamic to get a guaranteed number of threads
    
    * Enable openmp in cuda code
    Single precision histograms on GPU (#3965)
    
    * Allow single precision histogram summation in gpu_hist
    
    * Add python test, reduce run-time of gpu_hist tests
    
    * Update documentation
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Address #2754, accuracy issues with gpu_hist (#3793)
    
    * Address windows compilation error
    
    * Do not allow divide by zero in weight calculation
    
    * Update tests
    Add multi-GPU unit test environment (#3741)
    
    * Add multi-GPU unit test environment
    
    * Better assertion message
    
    * Temporarily disable failing test
    
    * Distinguish between multi-GPU and single-GPU CPP tests
    
    * Consolidate Python tests. Use attributes to distinguish multi-GPU Python tests from single-CPU counterparts
    Fixed issue 3605. (#3628)
    
    * Fixed issue 3605.
    
    - https://github.com/dmlc/xgboost/issues/3605
    
    * Fixed the bug in a better way.
    
    * Added a test to catch the bug.
    
    * Fixed linter errors.
    Added finding quantiles on GPU. (#3393)
    
    * Added finding quantiles on GPU.
    
    - this includes datasets where weights are assigned to data rows
    - as the quantiles found by the new algorithm are not the same
      as those found by the old one, test thresholds in
        tests/python-gpu/test_gpu_updaters.py have been adjusted.
    
    * Adjustments and improved testing for finding quantiles on the GPU.
    
    - added C++ tests for the DeviceSketch() function
    - reduced one of the thresholds in test_gpu_updaters.py
    - adjusted the cuts found by the find_cuts_k kernel
    Updates for GPU CI tests (#3467)
    
    * Fail GPU CI after test failure
    
    * Fix GPU linear tests
    
    * Reduced number of GPU tests to speed up CI
    
    * Remove static allocations of device memory
    
    * Resolve illegal memory access for updater_fast_hist.cc
    
    * Fix broken r tests dependency
    
    * Update python install documentation for GPU
    Refactor python tests (#3410)
    
    * Add unit test utility
    
    * Refactor updater tests. Add coverage for histmaker.
    Monotone constraints for gpu_hist (#2904)
    Update gpu_hist algorithm (#2901)
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    Add GPU documentation (#2695)
    
    * Add GPU documentation
    
    * Update Python GPU tests
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    [GPU-Plugin] Add throw of asserts and added compute compatibility error check. (#2565)
    
    * [GPU-Plugin] Added compute compatibility error check, added verbose timing
    Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation (#2530)
    
    * Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation from numpy arrays for python interface.
    [GPU-Plugin] Change GPU plugin to use tree_method parameter, bump cmake version to 3.5 for GPU plugin, add compute architecture 3.5, remove unused cmake files (#2455)
    [GPU-Plugin] Multi-GPU for grow_gpu_hist histogram method using NVIDIA NCCL. (#2395)
    [GPU-Plugin] Integration of a faster version of grow_gpu plugin into mainstream (#2360)
    
    * Integrating a faster version of grow_gpu plugin
    1. Removed the older files to reduce duplication
    2. Moved all of the grow_gpu files under 'exact' folder
    3. All of them are inside 'exact' namespace to avoid any conflicts
    4. Fixed a bug in benchmark.py while running only 'grow_gpu' plugin
    5. Added cub and googletest submodules to ease integration and unit-testing
    6. Updates to CMakeLists.txt to directly build cuda objects into libxgboost
    
    * Added support for building gpu plugins through make flow
    1. updated makefile and config.mk to add right targets
    2. added unit-tests for gpu exact plugin code
    
    * 1. Added support for building gpu plugin using 'make' flow as well
    2. Updated instructions for building and testing gpu plugin
    
    * Fix travis-ci errors for PR#2360
    1. lint errors on unit-tests
    2. removed googletest, instead depended upon dmlc-core provide gtest cache
    
    * Some more fixes to travis-ci lint failures PR#2360
    
    * Added Rory's copyrights to the files containing code from both.
    
    * updated copyright statement as per Rory's request
    
    * moved the static datasets into a script to generate them at runtime
    
    * 1. memory usage print when silent=0
    2. tests/ and test/ folder organization
    3. removal of the dependency of googletest for just building xgboost
    4. coding style updates for .cuh as well
    
    * Fixes for compilation warnings
    
    * add cuda object files as well when JVM_BINDINGS=ON
    [GPU Plugin] Fast histogram speed improvements. Updated benchmarks. (#2258)
    [GPU-Plugin] (#2227)
    
    * Add fast histogram algorithm
    * Fix Linux build
    * Add 'gpu_id' parameter
    Simplify the data backends. (#5893)
    Accept iterator in device dmatrix. (#5783)
    
    
    * Remove Device DMatrix.
    Device dmatrix (#5420)
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    [Breaking] Update sklearn interface. (#4929)
    
    
    * Remove nthread, seed, silent. Add tree_method, gpu_id, num_parallel_tree. Fix #4909.
    * Check data shape. Fix #4896.
    * Check element of eval_set is tuple. Fix #4875
    *  Add doc for random_state with hogwild. Fixes #4919
    [Breaking] Remove `learning_rates` in Python. (#5155)
    
    * Remove `learning_rates`.
    
    It's been deprecated since we have callback.
    
    * Set `before_iteration` of `reset_learning_rate` to False to preserve
      the initial learning rate, and comply to the term "reset".
    
    Closes #4709.
    
    * Tests for various `tree_method`.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Feature interaction for GPU Hist. (#4534)
    
    * GPU hist Interaction Constraints.
    * Duplicate related parameters.
    * Add tests for CPU interaction constraint.
    * Add better error reporting.
    * Thorough tests.
    Unify evaluation functions. (#6037)
    [dask] dask cudf inplace prediction. (#5512)
    
    * Add inplace prediction for dask-cudf.
    
    * Remove Dockerfile.release, since it's not used anywhere
    
    * Use Conda exclusively in CUDF and GPU containers
    
    * Improve cupy memory copying.
    
    * Add skip marks to tests.
    
    * Add mgpu-cudf category on the CI to run all distributed tests.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Monotone constraints for gpu_hist (#2904)
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    Upgrade to CUDA 10.0 (#5649) (#5652)
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    [Breaking] Remove `learning_rates` in Python. (#5155)
    
    * Remove `learning_rates`.
    
    It's been deprecated since we have callback.
    
    * Set `before_iteration` of `reset_learning_rate` to False to preserve
      the initial learning rate, and comply to the term "reset".
    
    Closes #4709.
    
    * Tests for various `tree_method`.
    Better message when no GPU is found. (#5594)
    Fix wrapping GPU ID and prevent data copying. (#5160)
    
    * Removed some data copying.
    
    * Make sure gpu_id is valid before any configuration is carried out.
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Group aware GPU sketching. (#5551)
    
    * Group aware GPU weighted sketching.
    
    * Distribute group weights to each data point.
    * Relax the test.
    * Validate input meta info.
    * Fix metainfo copy ctor.
    implementation of map ranking algorithm on gpu (#5129)
    
    * - implementation of map ranking algorithm
      - also effected necessary suggestions mentioned in the earlier ranking pr's
      - made some performance improvements to the ndcg algo as well
    - ndcg ltr implementation on gpu (#5004)
    
    * - ndcg ltr implementation on gpu
      - this is a follow-up to the pairwise ltr implementation
    Simplify the data backends. (#5893)
    Implement Python data handler. (#5689)
    
    
    * Define data handlers for DMatrix.
    * Throw ValueError in scikit learn interface.
    Set device in device dmatrix. (#5596)
    Add support for dlpack, expose python docs for DeviceQuantileDMatrix (#5465)
    Device dmatrix (#5420)
    Make some GPU tests deterministic (#5229)
    Support dmatrix construction from cupy array (#5206)
    Add DaskDeviceQuantileDMatrix demo. (#6156)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Further improvements and savings in Jenkins pipeline (#5904)
    
    * Publish artifacts only on the master and release branches
    
    * Build CUDA only for Compute Capability 7.5 when building PRs
    
    * Run all Windows jobs in a single worker image
    
    * Build nightly XGBoost4J SNAPSHOT JARs with Scala 2.12 only
    
    * Show skipped Python tests on Windows
    
    * Make Graphviz optional for Python tests
    
    * Add back C++ tests
    
    * Unstash xgboost_cpp_tests
    
    * Fix label to CUDA 10.1
    
    * Install cuPy for CUDA 10.1
    
    * Install jsonschema
    
    * Address reviewer's feedback
    Accept iterator in device dmatrix. (#5783)
    
    
    * Remove Device DMatrix.
    Update dmlc-core. (#5466)
    
    * Copy dmlc travis script to XGBoost.
    [R] make all customizations to meet strict standard of cran
    [TRAVIS] cleanup travis script
    fix travis
    Update travis_after_failure.sh
    refactor and ci
    Added arm64 job in Travis-CI (#6200)
    
    Signed-off-by: odidev <odidev@puresoftware.com>
    Work around a compiler bug in MacOS AppleClang 11 (#6103)
    
    * Workaround a compiler bug in MacOS AppleClang
    
    * [CI] Run C++ test with MacOS Catalina + AppleClang 11.0.3
    
    * [CI] Migrate cmake_test on MacOS from Travis CI to GitHub Actions
    
    * Install OpenMP runtime
    
    * [CI] Use CMake to locate lz4 lib
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    [CI] Grant public read access to Mac OSX wheels (#5602)
    Fix compilation on Mac OSX High Sierra (10.13) (#5597)
    
    * Fix compilation on Mac OSX High Sierra
    
    * [CI] Build Mac OSX binary wheel using Travis CI
    Resolve travis failure. (#5445)
    
    * Install dependencies by pip.
    Rewrite setup.py. (#5271)
    
    The setup.py is rewritten.  This new script uses only Python code and provide customized
    implementation of setuptools commands.  This way users can run most of setuptools commands
    just like any other Python libraries.
    
    * Remove setup_pip.py
    * Remove soft links.
    * Define customized commands.
    * Remove shell script.
    * Remove makefile script.
    * Update the doc for building from source.
    Make `pip install xgboost*.tar.gz` work by fixing build-python.sh (#5241)
    
    * Make pip install xgboost*.tar.gz work by fixing build-python.sh
    
    * Simplify install doc
    
    * Add test
    
    * Install Miniconda for Linux target too
    
    * Build XGBoost only once in sdist
    
    * Try importing xgboost after installation
    
    * Don't set PYTHONPATH env var for sdist test
    Support dmatrix construction from cupy array (#5206)
    Enable OpenMP with Apple Clang (Mac default compiler) (#5146)
    
    * Add OpenMP as CMake target
    
    * Require CMake 3.12, to allow linking OpenMP target to objxgboost
    
    * Specify OpenMP compiler flag for CUDA host compiler
    
    * Require CMake 3.16+ if the OS is Mac OSX
    
    * Use AppleClang in Mac tests.
    
    * Update dmlc-core
    [CI] Fix Travis tests. (#5062)
    
    - Install wget explicitly to match openssl.
    - Install CMake explicitly.
    - Use newer miniconda link.
    - Reenable unittests.
    - gcc@9 + xcode@10 for osx due to missing <_stdio.h>.  Other versions of gcc should also work.  But as homebrew pour gcc@9 after update by default, so I just stick with latest version.
    - Disabled one external memory test for OSX.  Not sure about the thread implementation in there and fixing external memory is beyond the scope of this PR.
    - Use Python3 with conda in jvm package.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    Use bundled gtest (#4900)
    
    * Suggest to use gtest bundled with dmlc
    
    * Use dmlc bundled gtest in all CI scripts
    
    * Make clang-tidy to use dmlc embedded gtest
    Rewrite Dask interface. (#4819)
    Remove plugin, cuda related code in automake & autoconf files (#4789)
    
    * Build plugin example with CMake.
    
    * Remove plugin, cuda related code in automake & autoconf files.
    
    * Fix typo in GPU doc.
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Migrate pylint check to Python 3 (#4381)
    
    * Migrate lint to Python 3
    
    * Fix lint errors
    
    * Use Miniconda3 to use Python 3.7
    
    * Use latest pylint and astroid
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    [r-package] cut CI-time dependency on craigcitro/r-travis (fixes #4348) (#4353)
    
    * [r-package] cut CI-time dependency on craigcitro/r-travis (fixes #4348)
    
    * Install R
    
    * Install R on OSX
    
    * Remove gfortran symlink
    
    * Specify CRAN repo
    
    * added more R dependencies needed for testing
    
    * removed heavy R dependencies in CI
    
    * fixed bug in env var, removed unnecessary apt installs of R
    
    * fix to R installs
    Improve HostDeviceVector exception safety (#4301)
    
    
    * make the assignments of HostDeviceVector exception safe.
    * storing a dummy GPUDistribution instance in HDV for CPU based code.
    * change testxgboost binary location to build directory.
    [CI] Upgrade to GCC 5.3.1, CMake 3.6.0 (#4306)
    
    * Upgrade to GCC 5.3.1, CMake 3.6.0
    
    * <regex> is now okay
    Fix test_gpu_coordinate. (#3974)
    
    * Fix test_gpu_coordinate.
    
    * Use `gpu_coord_descent` in test.
    * Reduce number of running rounds.
    
    * Remove nthread.
    
    * Use githubusercontent for r-appveyor.
    
    * Use githubusercontent in travis r tests.
    Fix broken R test: Install Homebrew GCC (#4142)
    
    * Fix broken R test: Install Homebrew GCC
    
    Missing GCC Fortran causes installation failure of a dependency package
    (igraph)
    
    * Register gfortran system-wide
    
    * Use correct keg
    
    * Set env vars to change compiler choice
    
    * Do not break other Mac builds
    
    * Nuclear option: symlink gfortran
    
    * Use /usr/local/bin instead of /usr/bin
    
    * Symlink library path too
    
    * Update run_test.sh
    Fix failing Travis CI on Mac (#4086)
    
    * Fix failing Travis CI on Mac
    
    Use Homebrew Addon + latest Mac image
    
    * Use long command for pytest
    
    * Downgrade OSX image to xcode9.3, to use Java 8
    
    * Install pytest in Python 2 environment
    
    * Remove clang-tidy from Travis
    Add back python2 tests for Travis light weight tests. (#3901)
    Refactor Python tests. (#3897)
    
    * Deprecate nose tests.
    * Format python tests.
    Disallow std::regex since it's not supported by GCC 4.8.x (#3870)
    Fix Python environment for distributed unit tests (#3806)
    Allow plug-ins to be built by cmake (#3752)
    
    * Remove references to AVX code.
    
    * Allow plugins to be built by cmake
    Fix #3730: scikit-learn 0.20 compatibility fix (#3731)
    
    * Fix #3730: scikit-learn 0.20 compatibility fix
    
    sklearn.cross_validation has been removed from scikit-learn 0.20,
    so replace it with sklearn.model_selection
    
    * Display test names for Python tests for clarity
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Remove redundant FindGTest.cmake. (#3533)
    
    During removal of FindGTest.cmake, also
    
    * Fix gtest include dirs.
    * Remove some blanks and use PWD for gtest dir.
    Fix #3402: wrong fid crashes distributed algorithm (#3535)
    
    * Fix #3402: wrong fid crashes distributed algorithm
    
    The bug was introduced by the recent DMatrix refactor (#3301). It was partially
    fixed by #3408 but the example in #3402 was still failing. The example in #3402
    will succeed after this fix is applied.
    
    * Explicitly specify "this" to prevent compile error
    
    * Add regression test
    
    * Add distributed test to Travis matrix
    
    * Install kubernetes Python package as dependency of dmlc tracker
    
    * Add Python dependencies
    
    * Add compile step
    
    * Reduce size of regression test case
    
    * Further reduce size of test
    Do not unzip google test archive if exists (#3416)
    Fix wget for google tests in tests (#3414)
    
    CI tests were failing because wget prompts "the user" for a response
    whenever the google test archive is already on the disk.
    
    Fix: Use `-nc` option to skip download when the archive already
    exists
    Allow import via python datatable. (#3272)
    
    * Allow import via python datatable.
    
    * Write unit tests
    
    * Refactor dt API functions
    
    * Refactor python code
    
    * Lint fixes
    
    * Address review comments
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Update dmlc-core submodule (#3221)
    
    * Update dmlc-core submodule
    
    * Fix dense_parser to work with the latest dmlc-core
    
    * Specify location of Google Test
    
    * Add more source files in dmlc-minimum to get latest dmlc-core working
    
    * Update dmlc-core submodule
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    AVX gradients (#2878)
    
    * AVX gradients
    
    * Add google test for AVX
    
    * Create fallback implementation, remove fma instruction
    
    * Improved accuracy of AVX exp function
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    Add Coverage Report for Java and Python (#2667)
    
    * Add coverage report for java
    
    * Add coverage report for python
    
    * Increase memory for JVM unit tests
    
    * Increase memory for JVM unit tests
    [jvm-packages] Another pack of build/CI improvements (#2422)
    
    * [jvm-packages] Fixed compilation on Windows
    
    * [jvm-packages] Build the JNI bindings on Appveyor
    
    * [jvm-packages] Build & test on OS X
    
    * [jvm-packages] Re-applied the CMake build changes reverted by #2395
    
    * Fixed Appveyor JVM build
    
    * Muted Maven on Travis
    
    * Don't link with libawt
    
    * "linux2"->"linux"
    
    Python2.x and 3.X use slightly different values for ``sys.platform``.
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Histogram Optimized Tree Grower (#1940)
    
    * Support histogram-based algorithm + multiple tree growing strategy
    
    * Add a brand new updater to support histogram-based algorithm, which buckets
      continuous features into discrete bins to speed up training. To use it, set
      `tree_method = fast_hist` to configuration.
    * Support multiple tree growing strategies. For now, two policies are supported:
      * `grow_policy=depthwise` (default):  favor splitting at nodes closest to the
        root, i.e. grow depth-wise.
      * `grow_policy=lossguide`: favor splitting at nodes with highest loss change
    * Improve single-threaded performance
      * Unroll critical loops
      * Introduce specialized code for dense data (i.e. no missing values)
    * Additional training parameters: `max_leaves`, `max_bin`, `grow_policy`, `verbose`
    
    * Adding a small test for hist method
    
    * Fix memory error in row_set.h
    
    When std::vector is resized, a reference to one of its element may become
    stale. Any such reference must be updated as well.
    
    * Resolve cross-platform compilation issues
    
    * Versions of g++ older than 4.8 lacks support for a few C++11 features, e.g.
      alignas(*) and new initializer syntax. To support g++ 4.6, use pre-C++11
      initializer and remove alignas(*).
    * Versions of MSVC older than 2015 does not support alignas(*). To support
      MSVC 2012, remove alignas(*).
    * For g++ 4.8 and newer, alignas(*) is enabled for performance benefits.
    * Some old compilers (MSVC 2012, g++ 4.6) do not support template aliases
      (which uses `using` to declate type aliases). So always use `typedef`.
    
    * Fix a host of CI issues
    
    * Remove dependency for libz on osx
    * Fix heading for hist_util
    * Fix minor style issues
    * Add missing #include
    * Remove extraneous logging
    
    * Enable tree_method=hist in R
    
    * Renaming HistMaker to GHistBuilder to avoid confusion
    
    * Fix R integration
    
    * Respond to style comments
    
    * Consistent tie-breaking for priority queue using timestamps
    
    * Last-minute style fixes
    
    * Fix issuecomment-271977647
    
    The way we quantize data is broken. The agaricus data consists of all
    categorical values. When NAs are converted into 0's,
    `HistCutMatrix::Init` assign both 0's and 1's to the same single bin.
    
    Why? gmat only the smallest value (0) and an upper bound (2), which is twice
    the maximum value (1). Add the maximum value itself to gmat to fix the issue.
    
    * Fix issuecomment-272266358
    
    * Remove padding from cut values for the continuous case
    * For categorical/ordinal values, use midpoints as bin boundaries to be safe
    
    * Fix CI issue -- do not use xrange(*)
    
    * Fix corner case in quantile sketch
    
    Signed-off-by: Philip Cho <chohyu01@cs.washington.edu>
    
    * Adding a test for an edge case in quantile sketcher
    
    max_bin=2 used to cause an exception.
    
    * Fix fast_hist test
    
    The test used to require a strictly increasing Test AUC for all examples.
    One of them exhibits a small blip in Test AUC before achieving a Test AUC
    of 1. (See bottom.)
    
    Solution: do not require monotonic increase for this particular example.
    
    [0] train-auc:0.99989 test-auc:0.999497
    [1] train-auc:1 test-auc:0.999749
    [2] train-auc:1 test-auc:0.999749
    [3] train-auc:1 test-auc:0.999749
    [4] train-auc:1 test-auc:0.999749
    [5] train-auc:1 test-auc:0.999497
    [6] train-auc:1 test-auc:1
    [7] train-auc:1 test-auc:1
    [8] train-auc:1 test-auc:1
    [9] train-auc:1 test-auc:1
    travis: Add code coverage on success
    
    Update the code coverage of the project on codecov for easy viewing.
    
    Also the gcov on travis uses a different version which cannot
    find the directory of the given files, and it needs to be specified
    in the -o flag. Hence now we loop over the list of files and
    run them independently.
    travis: Run CPP tests
    cmake build system (#1314)
    
    * Changed c api to compile under MSVC
    
    * Include functional.h header for MSVC
    
    * Add cmake build
    DOC/TST: Fix Python sklearn dep
    Enable flake8
    run native lib building command from maven
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    Separate dependencies and lightweight test env for Python
    [TRAVIS] Fix script
    [TRAVIS] cleanup travis script
    Added arm64 job in Travis-CI (#6200)
    
    Signed-off-by: odidev <odidev@puresoftware.com>
    Work around a compiler bug in MacOS AppleClang 11 (#6103)
    
    * Workaround a compiler bug in MacOS AppleClang
    
    * [CI] Run C++ test with MacOS Catalina + AppleClang 11.0.3
    
    * [CI] Migrate cmake_test on MacOS from Travis CI to GitHub Actions
    
    * Install OpenMP runtime
    
    * [CI] Use CMake to locate lz4 lib
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Ensure that configured dmlc/build_config.h is picked up by Rabit and XGBoost (#5514)
    
    * Ensure that configured header (build_config.h) from dmlc-core is picked up by Rabit and XGBoost
    
    * Check which Rabit target is being used
    
    * Use CMake 3.13 in all Jenkins tests
    
    * Upgrade CMake in Travis CI
    
    * Install CMake using Kitware installer
    
    * Remove existing CMake (3.12.4)
    Make `pip install xgboost*.tar.gz` work by fixing build-python.sh (#5241)
    
    * Make pip install xgboost*.tar.gz work by fixing build-python.sh
    
    * Simplify install doc
    
    * Add test
    
    * Install Miniconda for Linux target too
    
    * Build XGBoost only once in sdist
    
    * Try importing xgboost after installation
    
    * Don't set PYTHONPATH env var for sdist test
    Enable OpenMP with Apple Clang (Mac default compiler) (#5146)
    
    * Add OpenMP as CMake target
    
    * Require CMake 3.12, to allow linking OpenMP target to objxgboost
    
    * Specify OpenMP compiler flag for CUDA host compiler
    
    * Require CMake 3.16+ if the OS is Mac OSX
    
    * Use AppleClang in Mac tests.
    
    * Update dmlc-core
    [CI] Fix Travis tests. (#5062)
    
    - Install wget explicitly to match openssl.
    - Install CMake explicitly.
    - Use newer miniconda link.
    - Reenable unittests.
    - gcc@9 + xcode@10 for osx due to missing <_stdio.h>.  Other versions of gcc should also work.  But as homebrew pour gcc@9 after update by default, so I just stick with latest version.
    - Disabled one external memory test for OSX.  Not sure about the thread implementation in there and fixing external memory is beyond the scope of this PR.
    - Use Python3 with conda in jvm package.
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Migrate pylint check to Python 3 (#4381)
    
    * Migrate lint to Python 3
    
    * Fix lint errors
    
    * Use Miniconda3 to use Python 3.7
    
    * Use latest pylint and astroid
    Fix failing Travis CI on Mac (#4086)
    
    * Fix failing Travis CI on Mac
    
    Use Homebrew Addon + latest Mac image
    
    * Use long command for pytest
    
    * Downgrade OSX image to xcode9.3, to use Java 8
    
    * Install pytest in Python 2 environment
    
    * Remove clang-tidy from Travis
    Fix Python environment for distributed unit tests (#3806)
    Fix #3402: wrong fid crashes distributed algorithm (#3535)
    
    * Fix #3402: wrong fid crashes distributed algorithm
    
    The bug was introduced by the recent DMatrix refactor (#3301). It was partially
    fixed by #3408 but the example in #3402 was still failing. The example in #3402
    will succeed after this fix is applied.
    
    * Explicitly specify "this" to prevent compile error
    
    * Add regression test
    
    * Add distributed test to Travis matrix
    
    * Install kubernetes Python package as dependency of dmlc tracker
    
    * Add Python dependencies
    
    * Add compile step
    
    * Reduce size of regression test case
    
    * Further reduce size of test
    Update setup.sh
    Update setup.sh
    Separate dependencies and lightweight test env for Python
    [TRAVIS] Fix script
    [TRAVIS] cleanup travis script
    Update dmlc-core. (#5466)
    
    * Copy dmlc travis script to XGBoost.
    [python-package] remove unused imports (#5776)
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation (#2530)
    
    * Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation from numpy arrays for python interface.
    [GPU-Plugin] Add load balancing search to gpu_hist. Add compressed iterator. (#2504)
    [GPU-Plugin] Multi-GPU gpu_id bug fixes for grow_gpu_hist and grow_gpu methods, and additional documentation for the gpu plugin. (#2463)
    [GPU-Plugin] Change GPU plugin to use tree_method parameter, bump cmake version to 3.5 for GPU plugin, add compute architecture 3.5, remove unused cmake files (#2455)
    [GPU-Plugin] Integration of a faster version of grow_gpu plugin into mainstream (#2360)
    
    * Integrating a faster version of grow_gpu plugin
    1. Removed the older files to reduce duplication
    2. Moved all of the grow_gpu files under 'exact' folder
    3. All of them are inside 'exact' namespace to avoid any conflicts
    4. Fixed a bug in benchmark.py while running only 'grow_gpu' plugin
    5. Added cub and googletest submodules to ease integration and unit-testing
    6. Updates to CMakeLists.txt to directly build cuda objects into libxgboost
    
    * Added support for building gpu plugins through make flow
    1. updated makefile and config.mk to add right targets
    2. added unit-tests for gpu exact plugin code
    
    * 1. Added support for building gpu plugin using 'make' flow as well
    2. Updated instructions for building and testing gpu plugin
    
    * Fix travis-ci errors for PR#2360
    1. lint errors on unit-tests
    2. removed googletest, instead depended upon dmlc-core provide gtest cache
    
    * Some more fixes to travis-ci lint failures PR#2360
    
    * Added Rory's copyrights to the files containing code from both.
    
    * updated copyright statement as per Rory's request
    
    * moved the static datasets into a script to generate them at runtime
    
    * 1. memory usage print when silent=0
    2. tests/ and test/ folder organization
    3. removal of the dependency of googletest for just building xgboost
    4. coding style updates for .cuh as well
    
    * Fixes for compilation warnings
    
    * add cuda object files as well when JVM_BINDINGS=ON
    [GPU Plugin] Fast histogram speed improvements. Updated benchmarks. (#2258)
    [GPU-Plugin] (#2227)
    
    * Add fast histogram algorithm
    * Fix Linux build
    * Add 'gpu_id' parameter
    fix benchmark_tree.py (#4593)
    Overload device memory allocation (#4532)
    
    * Group source files, include headers in source files
    
    * Overload device memory allocation
    Update python benchmarking script (#4164)
    
    * a few tweaks to speed up data generation
    
    * del variable to save memory
    
    * switch to random numpy arrays
    reformat benchmark_tree.py to get rid of lint errors (#4126)
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation (#2530)
    
    * Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation from numpy arrays for python interface.
    [GPU-Plugin] Add load balancing search to gpu_hist. Add compressed iterator. (#2504)
    [GPU-Plugin] Multi-GPU gpu_id bug fixes for grow_gpu_hist and grow_gpu methods, and additional documentation for the gpu plugin. (#2463)
    [GPU-Plugin] Change GPU plugin to use tree_method parameter, bump cmake version to 3.5 for GPU plugin, add compute architecture 3.5, remove unused cmake files (#2455)
    [GPU-Plugin] Integration of a faster version of grow_gpu plugin into mainstream (#2360)
    
    * Integrating a faster version of grow_gpu plugin
    1. Removed the older files to reduce duplication
    2. Moved all of the grow_gpu files under 'exact' folder
    3. All of them are inside 'exact' namespace to avoid any conflicts
    4. Fixed a bug in benchmark.py while running only 'grow_gpu' plugin
    5. Added cub and googletest submodules to ease integration and unit-testing
    6. Updates to CMakeLists.txt to directly build cuda objects into libxgboost
    
    * Added support for building gpu plugins through make flow
    1. updated makefile and config.mk to add right targets
    2. added unit-tests for gpu exact plugin code
    
    * 1. Added support for building gpu plugin using 'make' flow as well
    2. Updated instructions for building and testing gpu plugin
    
    * Fix travis-ci errors for PR#2360
    1. lint errors on unit-tests
    2. removed googletest, instead depended upon dmlc-core provide gtest cache
    
    * Some more fixes to travis-ci lint failures PR#2360
    
    * Added Rory's copyrights to the files containing code from both.
    
    * updated copyright statement as per Rory's request
    
    * moved the static datasets into a script to generate them at runtime
    
    * 1. memory usage print when silent=0
    2. tests/ and test/ folder organization
    3. removal of the dependency of googletest for just building xgboost
    4. coding style updates for .cuh as well
    
    * Fixes for compilation warnings
    
    * add cuda object files as well when JVM_BINDINGS=ON
    [GPU Plugin] Fast histogram speed improvements. Updated benchmarks. (#2258)
    [GPU-Plugin] (#2227)
    
    * Add fast histogram algorithm
    * Fix Linux build
    * Add 'gpu_id' parameter
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Implement iterative DMatrix. (#5837)
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix tweedie metric string. (#4543)
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Add make commands for tests
    
    This adds the make commands required to build and run tests.
    Disable JSON full serialization for now. (#6248)
    
    * Disable JSON serialization for now.
    
    * Multi-class classification is checkpointing for each iteration.
    This brings significant overhead.
    
    Revert: 90355b4f007ae
    
    * Set R tests to use binary.
    Unify evaluation functions. (#6037)
    Make JSON the default full serialization format. (#6027)
    [BLOCKING] Remove to_string. (#5934)
    Support building XGBoost with CUDA 11 (#5808)
    
    * Change serialization test.
    * Add CUDA 11 tests on Linux CI.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Reduce device synchronisation (#5631)
    
    * Reduce device synchronisation
    
    * Initialise pinned memory
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor tests with data generator. (#5439)
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Pass shared pointer instead of raw pointer to Learner. (#5302)
    
    Extracted from https://github.com/dmlc/xgboost/pull/5220 .
    Disable parameter validation for Scikit-Learn interface. (#5167)
    
    * Disable parameter validation for now.
    
    Scikit-Learn passes all parameters down to XGBoost, whether they are used or
    not.
    
    * Add option `validate_parameters`.
    Tests and documents for new JSON routines. (#5120)
    Option for generating device debug info. (#6168)
    
    * Supply `-G;-src-in-ptx` when `USE_DEVICE_DEBUG` is set and debug mode is selected.
    * Refactor CMake script to gather all CUDA configuration.
    * Use CMAKE_CUDA_ARCHITECTURES.  Close #6029.
    * Add compute 80.  Close #5999
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Introducing DPC++-based plugin (predictor, objective function) supporting oneAPI programming model (#5825)
    
    * Added plugin with DPC++-based predictor and objective function
    
    * Update CMakeLists.txt
    
    * Update regression_obj_oneapi.cc
    
    * Added README.md for OneAPI plugin
    
    * Added OneAPI predictor support to gbtree
    
    * Update README.md
    
    * Merged kernels in gradient computation. Enabled multiple loss functions with DPC++ backend
    
    * Aligned plugin CMake files with latest master changes. Fixed whitespace typos
    
    * Removed debug output
    
    * [CI] Make oneapi_plugin a CMake target
    
    * Added tests for OneAPI plugin for predictor and obj. functions
    
    * Temporarily switched to default selector for device dispacthing in OneAPI plugin to enable execution in environments without gpus
    
    * Updated readme file.
    
    * Fixed USM usage in predictor
    
    * Removed workaround with explicit templated names for DPC++ kernels
    
    * Fixed warnings in plugin tests
    
    * Fix CMake build of gtest
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add option to enable all compiler warnings in GCC/Clang (#5897)
    
    * Add option to enable all compiler warnings in GCC/Clang
    
    * Fix -Wall for CUDA sources
    
    * Make -Wall private req for xgboost-r
    Support building XGBoost with CUDA 11 (#5808)
    
    * Change serialization test.
    * Add CUDA 11 tests on Linux CI.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Simplify CMake build with modern CMake techniques (#5871)
    
    * [CI] Simplify CMake build
    
    * Make sure that plugins can be built
    
    * [CI] Install lz4 on Mac
    Define _CRT_SECURE_NO_WARNINGS to remove unneeded warnings in MSVC (#5434)
    C++14 for xgboost (#5664)
    Enhance nvtx support. (#5636)
    Fix non-openmp build. (#5566)
    
    
    * Add test to Jenkins.
    * Fix threading utils tests.
    * Require thread library.
    Explicitly use UTF-8 codepage when using MSVC (#5197)
    
    * Explicitly use UTF-8 codepage when using MSVC
    
    * Fix build with CUDA enabled
    Enable OpenMP with Apple Clang (Mac default compiler) (#5146)
    
    * Add OpenMP as CMake target
    
    * Require CMake 3.12, to allow linking OpenMP target to objxgboost
    
    * Specify OpenMP compiler flag for CUDA host compiler
    
    * Require CMake 3.16+ if the OS is Mac OSX
    
    * Use AppleClang in Mac tests.
    
    * Update dmlc-core
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Remove plugin, cuda related code in automake & autoconf files (#4789)
    
    * Build plugin example with CMake.
    
    * Remove plugin, cuda related code in automake & autoconf files.
    
    * Fix typo in GPU doc.
    Overload device memory allocation (#4532)
    
    * Group source files, include headers in source files
    
    * Overload device memory allocation
    [CI] Add Python and C++ tests for Windows GPU target (#4469)
    
    * Add CMake option to use bundled gtest from dmlc-core, so that it is easy to build XGBoost with gtest on Windows
    
    * Consistently apply OpenMP flag to all targets. Force enable OpenMP when USE_CUDA is turned on.
    
    * Insert vcomp140.dll into Windows wheels
    
    * Add C++ and Python tests for CPU and GPU targets (CUDA 9.0, 10.0, 10.1)
    
    * Prevent spurious msbuild failure
    
    * Add GPU tests
    
    * Upgrade dmlc-core
    Make CMakeLists.txt compatible with CMake 3.3 (#4420)
    
    * Make CMakeLists.txt compatible with CMake 3.3; require CMake 3.11 for MSVC
    
    * Use CMake 3.12 when sanitizer is enabled
    
    * Disable funroll-loops for MSVC
    
    * Use cmake version in container name
    
    * Add missing arg
    
    * Fix egrep use in ci_build.sh
    
    * Display CMake version
    
    * Do not set OpenMP_CXX_LIBRARIES for MSVC
    
    * Use cmake_minimum_required()
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Enforce tree order in JSON. (#5974)
    
    
    * Make JSON model IO more future proof by using tree id in model loading.
    Thread-safe prediction by making the prediction cache thread-local. (#5853)
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Support 64bit seed. (#5643)
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor tests with data generator. (#5439)
    Remove unnecessary DMatrix methods (#5324)
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Pass shared pointer instead of raw pointer to Learner. (#5302)
    
    Extracted from https://github.com/dmlc/xgboost/pull/5220 .
    Config for linear updaters. (#5222)
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    Disable parameter validation for Scikit-Learn interface. (#5167)
    
    * Disable parameter validation for now.
    
    Scikit-Learn passes all parameters down to XGBoost, whether they are used or
    not.
    
    * Add option `validate_parameters`.
    Parameter validation (#5157)
    
    
    
    * Unused code.
    
    * Split up old colmaker parameters from train param.
    
    * Fix dart.
    
    * Better name.
    Small refinements for JSON model. (#5112)
    
    * Naming consistency.
    
    * Remove duplicated test.
    Model IO in JSON. (#5110)
    Use configure_file() to configure version only (#4974)
    
    * Avoid writing build_config.h
    
    * Remove build_config.h all together.
    
    * Lint.
    Fix calling GPU predictor (#4836)
    
    * Fix calling GPU predictor
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    [BREAKING] prevent multi-gpu usage (#4749)
    
    * prevent multi-gpu usage
    
    * fix distributed test
    
    * combine gpu predictor tests
    
    * set upper bound on n_gpus
    Fix model parameter recovery (#4738)
    Remove gpu_exact tree method (#4742)
    Specify version macro in CMake. (#4730)
    
    * Specify version macro in CMake.
    
    * Use `XGBOOST_DEFINITIONS` instead.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Offload some configurations into GBM. (#4553)
    
    
    This is part 1 of refactoring configuration.
    
    * Move tree heuristic configurations.
    * Split up declarations and definitions for GBTree.
    * Implement UseGPU in gbm.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Add PushCSC for SparsePage. (#4193)
    
    * Add PushCSC for SparsePage.
    
    * Move Push* definitions into cc file.
    * Add std:: prefix to `size_t` make clang++ happy.
    * Address monitor count == 0.
    Make `HistCutMatrix::Init' be aware of groups. (#4115)
    
    
    * Add checks for group size.
    * Simple docs.
    * Search group index during hist cut matrix initialization.
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix ignoring dart in updater configuration. (#4024)
    
    * Fix ignoring dart in updater configuration.
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Fix #3342 and h2oai/h2o4gpu#625: Save predictor parameters in model file (#3856)
    
    * Fix #3342 and h2oai/h2o4gpu#625: Save predictor parameters in model file
    
    This allows pickled models to retain predictor attributes, such as
    'predictor' (whether to use CPU or GPU) and 'n_gpu' (number of GPUs
    to use). Related: h2oai/h2o4gpu#625
    
    Closes #3342.
    
    TODO. Write a test.
    
    * Fix lint
    
    * Do not load GPU predictor into CPU-only XGBoost
    
    * Add a test for pickling GPU predictors
    
    * Make sample data big enough to pass multi GPU test
    
    * Update test_gpu_predictor.cu
    [Blocking] Fix #3840: Clean up logic for parsing tree_method parameter (#3849)
    
    * Clean up logic for converting tree_method to updater sequence
    
    * Use C++11 enum class for extra safety
    
    Compiler will give warnings if switch statements don't handle all
    possible values of C++11 enum class.
    
    Also allow enum class to be used as DMLC parameter.
    
    * Fix compiler error + lint
    
    * Address reviewer comment
    
    * Better docstring for DECLARE_FIELD_ENUM_CLASS
    
    * Fix lint
    
    * Add C++ test to see if tree_method is recognized
    
    * Fix clang-tidy error
    
    * Add test_learner.h to R package
    
    * Update comments
    
    * Fix lint error
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    Support categorical data in ellpack. (#6140)
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Split up test helpers header. (#5455)
    Support categorical data in ellpack. (#6140)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Accept string for ArrayInterface constructor. (#5799)
    Revert "Accept string for ArrayInterface constructor."
    
    This reverts commit e8ecafb8dc628f45b75b4c2844a236d27e0a6d98.
    Accept string for ArrayInterface constructor.
    Add helper for generating batches of data. (#5756)
    
    * Add helper for generating batches of data.
    
    * VC keyword clash.
    
    * Another clash.
    Fix non-openmp build. (#5566)
    
    
    * Add test to Jenkins.
    * Fix threading utils tests.
    * Require thread library.
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Refactor tests with data generator. (#5439)
    Fixes and changes to the ranking metrics computed on cpu (#5380)
    
    * - fixes and changes to the ranking metrics computed on cpu
      - auc/aucpr ranking metric accelerated on cpu
      - fixes to the auc/aucpr metrics
    Remove SimpleCSRSource (#5315)
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Testing hist_util (#5251)
    
    * Rank tests
    
    * Remove categorical split specialisation
    
    * Extend tests to multiple features, switch to WQSketch
    
    * Add tests for SparseCuts
    
    * Add external memory quantile tests, fix some existing tests
    Fix wrapping GPU ID and prevent data copying. (#5160)
    
    * Removed some data copying.
    
    * Make sure gpu_id is valid before any configuration is carried out.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix external memory for get column batches. (#4622)
    
    * Fix external memory for get column batches.
    
    This fixes two bugs:
    
    * Use PushCSC for get column batches.
    * Don't remove the created temporary directory before finishing test.
    
    * Check all pages.
    - training with external memory - part 2 of 2 (#4526)
    
    * - training with external memory - part 2 of 2
       - when external memory support is enabled, building of histogram indices are
         done incrementally for every sparse page
       - the entire set of input data is divided across multiple gpu's and the relative
         row positions within each device is tracked when building the compressed histogram buffer
       - this was tested using a mortgage dataset containing ~ 670m rows before 4xt4's could be
         saturated
    Initial support for external memory in gpu_predictor (#4284)
    refactor tests to get rid of duplication (#4358)
    
    * refactor tests to get rid of duplication
    
    * address review comments
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    Make C++ unit tests run and pass on Windows (#3869)
    
    * Make C++ unit tests run and pass on Windows
    
    * Fix logic for external memory. The letter ':' is part of drive letter,
    so remove the drive letter before splitting on ':'.
    * Cosmetic syntax changes to keep MSVC happy.
    
    * Fix lint
    
    * Add Windows guard
    Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way (#3783)
    
    * Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way
    
    Also install git inside NVIDIA GPU container
    
    * Update dmlc-core
    Add basic unittests for gpu-hist method. (#3785)
    
    * Split building histogram into separated class.
    * Extract `InitCompressedRow` definition.
    * Basic tests for gpu-hist.
    * Document the code more verbosely.
    * Removed `HistCutUnit`.
    * Removed some duplicated copies in `GPUHistMaker`.
    * Implement LCG and use it in tests.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    added code for instance based weighing for rank objectives (#3379)
    
    * added code for instance based weighing for rank objectives
    
    * Fix lint
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    GPU binning and compression. (#3319)
    
    * GPU binning and compression.
    
    - binning and index compression are done inside the DeviceShard constructor
    - in case of a DMatrix with multiple row batches, it is first converted into a single row batch
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Integer gradient summation for GPU histogram algorithm. (#2681)
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    [GPU-Plugin] Integration of a faster version of grow_gpu plugin into mainstream (#2360)
    
    * Integrating a faster version of grow_gpu plugin
    1. Removed the older files to reduce duplication
    2. Moved all of the grow_gpu files under 'exact' folder
    3. All of them are inside 'exact' namespace to avoid any conflicts
    4. Fixed a bug in benchmark.py while running only 'grow_gpu' plugin
    5. Added cub and googletest submodules to ease integration and unit-testing
    6. Updates to CMakeLists.txt to directly build cuda objects into libxgboost
    
    * Added support for building gpu plugins through make flow
    1. updated makefile and config.mk to add right targets
    2. added unit-tests for gpu exact plugin code
    
    * 1. Added support for building gpu plugin using 'make' flow as well
    2. Updated instructions for building and testing gpu plugin
    
    * Fix travis-ci errors for PR#2360
    1. lint errors on unit-tests
    2. removed googletest, instead depended upon dmlc-core provide gtest cache
    
    * Some more fixes to travis-ci lint failures PR#2360
    
    * Added Rory's copyrights to the files containing code from both.
    
    * updated copyright statement as per Rory's request
    
    * moved the static datasets into a script to generate them at runtime
    
    * 1. memory usage print when silent=0
    2. tests/ and test/ folder organization
    3. removal of the dependency of googletest for just building xgboost
    4. coding style updates for .cuh as well
    
    * Fixes for compilation warnings
    
    * add cuda object files as well when JVM_BINDINGS=ON
    tests/cpp: Add tests for Metric RMSE
    tests/cpp: Add tests for regression_obj.cc
    
    Test the objective functions in regression_obj.cc
    
    tests/cpp: Add tests for objective.cc and RegLossObj
    tests/cpp: Add tests for SparsePageDMatrix
    
    The SparsePageDMatrix or external memory DMatrix reads data from the
    file IO rather than load it into RAM.
    tests/cpp: Add tests for SimpleDMatrix
    tests/cpp/test_metainfo: Add tests to save and load
    Remove silent from R demos. (#5675)
    
    * Remove silent from R demos.
    
    * Vignettes.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix and optimize logger (#4002)
    
    * Fix logging switch statement.
    
    * Remove debug_verbose_ in AllReducer.
    
    * Don't construct the stream when not needed.
    
    * Make default constructor deleted.
    
    * Remove redundant IsVerbose.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Add helper for generating batches of data. (#5756)
    
    * Add helper for generating batches of data.
    
    * VC keyword clash.
    
    * Another clash.
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Refactor tests with data generator. (#5439)
    Support categorical data in ellpack. (#6140)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Implement iterative DMatrix. (#5837)
    Add helper for generating batches of data. (#5756)
    
    * Add helper for generating batches of data.
    
    * VC keyword clash.
    
    * Another clash.
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Split up test helpers header. (#5455)
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Refactor tests with data generator. (#5439)
    Partial rewrite EllpackPage (#5352)
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Fixes and changes to the ranking metrics computed on cpu (#5380)
    
    * - fixes and changes to the ranking metrics computed on cpu
      - auc/aucpr ranking metric accelerated on cpu
      - fixes to the auc/aucpr metrics
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Move ellpack page construction into DMatrix (#4833)
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    fixed year to 2019 in conf.py, helpers.h and LICENSE (#4661)
    Fix external memory for get column batches. (#4622)
    
    * Fix external memory for get column batches.
    
    This fixes two bugs:
    
    * Use PushCSC for get column batches.
    * Don't remove the created temporary directory before finishing test.
    
    * Check all pages.
    - training with external memory - part 2 of 2 (#4526)
    
    * - training with external memory - part 2 of 2
       - when external memory support is enabled, building of histogram indices are
         done incrementally for every sparse page
       - the entire set of input data is divided across multiple gpu's and the relative
         row positions within each device is tracked when building the compressed histogram buffer
       - this was tested using a mortgage dataset containing ~ 670m rows before 4xt4's could be
         saturated
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Initial support for external memory in gpu_predictor (#4284)
    refactor tests to get rid of duplication (#4358)
    
    * refactor tests to get rid of duplication
    
    * address review comments
    Further optimisations for gpu_hist. (#4283)
    
    - Fuse final update position functions into a single more efficient kernel
    
    - Refactor gpu_hist with a more explicit ellpack  matrix representation
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    Make C++ unit tests run and pass on Windows (#3869)
    
    * Make C++ unit tests run and pass on Windows
    
    * Fix logic for external memory. The letter ':' is part of drive letter,
    so remove the drive letter before splitting on ':'.
    * Cosmetic syntax changes to keep MSVC happy.
    
    * Fix lint
    
    * Add Windows guard
    Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way (#3783)
    
    * Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way
    
    Also install git inside NVIDIA GPU container
    
    * Update dmlc-core
    Add basic unittests for gpu-hist method. (#3785)
    
    * Split building histogram into separated class.
    * Extract `InitCompressedRow` definition.
    * Basic tests for gpu-hist.
    * Document the code more verbosely.
    * Removed `HistCutUnit`.
    * Removed some duplicated copies in `GPUHistMaker`.
    * Implement LCG and use it in tests.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    added code for instance based weighing for rank objectives (#3379)
    
    * added code for instance based weighing for rank objectives
    
    * Fix lint
    GPU binning and compression. (#3319)
    
    * GPU binning and compression.
    
    - binning and index compression are done inside the DeviceShard constructor
    - in case of a DMatrix with multiple row batches, it is first converted into a single row batch
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    tests/cpp: Add tests for Metric RMSE
    tests/cpp: Add tests for regression_obj.cc
    
    Test the objective functions in regression_obj.cc
    
    tests/cpp: Add tests for objective.cc and RegLossObj
    tests/cpp: Add tests for SparsePageDMatrix
    
    The SparsePageDMatrix or external memory DMatrix reads data from the
    file IO rather than load it into RAM.
    tests/cpp: Add tests for SimpleDMatrix
    tests/cpp/test_metainfo: Add tests to save and load
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    tests/cpp: Add tests for regression_obj.cc
    
    Test the objective functions in regression_obj.cc
    
    tests/cpp: Add tests for objective.cc and RegLossObj
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Pseudo-huber loss metric added (#5647)
    
    
    - Add pseudo huber loss objective.
    - Add pseudo huber loss metric.
    
    Co-authored-by: Reetz <s02reetz@iavgroup.local>
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Refactor tests with data generator. (#5439)
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Add `rmsle` metric and `reg:squaredlogerror` objective (#4541)
    Fix tweedie metric string. (#4543)
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Fix tweedie handling of base_score (#3295)
    
    * fix tweedie margin calculations
    
    * add entry to contributors
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    Cmake improvements (#2487)
    
    * Cmake improvements
    * Add google test to cmake
    [GPU-Plugin] Integration of a faster version of grow_gpu plugin into mainstream (#2360)
    
    * Integrating a faster version of grow_gpu plugin
    1. Removed the older files to reduce duplication
    2. Moved all of the grow_gpu files under 'exact' folder
    3. All of them are inside 'exact' namespace to avoid any conflicts
    4. Fixed a bug in benchmark.py while running only 'grow_gpu' plugin
    5. Added cub and googletest submodules to ease integration and unit-testing
    6. Updates to CMakeLists.txt to directly build cuda objects into libxgboost
    
    * Added support for building gpu plugins through make flow
    1. updated makefile and config.mk to add right targets
    2. added unit-tests for gpu exact plugin code
    
    * 1. Added support for building gpu plugin using 'make' flow as well
    2. Updated instructions for building and testing gpu plugin
    
    * Fix travis-ci errors for PR#2360
    1. lint errors on unit-tests
    2. removed googletest, instead depended upon dmlc-core provide gtest cache
    
    * Some more fixes to travis-ci lint failures PR#2360
    
    * Added Rory's copyrights to the files containing code from both.
    
    * updated copyright statement as per Rory's request
    
    * moved the static datasets into a script to generate them at runtime
    
    * 1. memory usage print when silent=0
    2. tests/ and test/ folder organization
    3. removal of the dependency of googletest for just building xgboost
    4. coding style updates for .cuh as well
    
    * Fixes for compilation warnings
    
    * add cuda object files as well when JVM_BINDINGS=ON
    tests/cpp: Add tests for TweedieRegression
    tests/cpp: Add tests for GammaRegression
    tests/cpp: Add tests for PoissonRegression
    tests/cpp: Add tests for regression_obj.cc
    
    Test the objective functions in regression_obj.cc
    
    tests/cpp: Add tests for objective.cc and RegLossObj
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Config for linear updaters. (#5222)
    implementation of map ranking algorithm on gpu (#5129)
    
    * - implementation of map ranking algorithm
      - also effected necessary suggestions mentioned in the earlier ranking pr's
      - made some performance improvements to the ndcg algo as well
    - ndcg ltr implementation on gpu (#5004)
    
    * - ndcg ltr implementation on gpu
      - this is a follow-up to the pairwise ltr implementation
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Pairwise ranking objective implementation on gpu (#4873)
    
    * - pairwise ranking objective implementation on gpu
       - there are couple of more algorithms (ndcg and map) for which support will be added
         as follow-up pr's
       - with no label groups defined, get gradient is 90x faster on gpu (120m instance
         mortgage dataset)
       - it can perform by an order of magnitude faster with ~ 10 groups (and adequate cores
         for the cpu implementation)
    
    * Add JSON config to rank obj.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    added code for instance based weighing for rank objectives (#3379)
    
    * added code for instance based weighing for rank objectives
    
    * Fix lint
    Add check for length of weights. (#4872)
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Make C++ unit tests run and pass on Windows (#3869)
    
    * Make C++ unit tests run and pass on Windows
    
    * Fix logic for external memory. The letter ':' is part of drive letter,
    so remove the drive letter before splitting on ':'.
    * Cosmetic syntax changes to keep MSVC happy.
    
    * Fix lint
    
    * Add Windows guard
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Implement robust regularization in 'survival:aft' objective (#5473)
    
    * Robust regularization of AFT gradient and hessian
    
    * Fix AFT doc; expose it to tutorial TOC
    
    * Apply robust regularization to uncensored case too
    
    * Revise unit test slightly
    
    * Fix lint
    
    * Update test_survival.py
    
    * Use GradientPairPrecise
    
    * Remove unused variables
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Implementation of hinge loss for binary classification (#3477)
    Move segment sorter to common (#5378)
    
    - move segment sorter to common
    - this is the first of a handful of pr's that splits the larger pr #5326
    - it moves this facility to common (from ranking objective class), so that it can be
        used for metric computation
    - it also wraps all the bald device pointers into span.
    implementation of map ranking algorithm on gpu (#5129)
    
    * - implementation of map ranking algorithm
      - also effected necessary suggestions mentioned in the earlier ranking pr's
      - made some performance improvements to the ndcg algo as well
    - ndcg ltr implementation on gpu (#5004)
    
    * - ndcg ltr implementation on gpu
      - this is a follow-up to the pairwise ltr implementation
    Pairwise ranking objective implementation on gpu (#4873)
    
    * - pairwise ranking objective implementation on gpu
       - there are couple of more algorithms (ndcg and map) for which support will be added
         as follow-up pr's
       - with no label groups defined, get gradient is 90x faster on gpu (120m instance
         mortgage dataset)
       - it can perform by an order of magnitude faster with ~ 10 groups (and adequate cores
         for the cpu implementation)
    
    * Add JSON config to rank obj.
    Introducing DPC++-based plugin (predictor, objective function) supporting oneAPI programming model (#5825)
    
    * Added plugin with DPC++-based predictor and objective function
    
    * Update CMakeLists.txt
    
    * Update regression_obj_oneapi.cc
    
    * Added README.md for OneAPI plugin
    
    * Added OneAPI predictor support to gbtree
    
    * Update README.md
    
    * Merged kernels in gradient computation. Enabled multiple loss functions with DPC++ backend
    
    * Aligned plugin CMake files with latest master changes. Fixed whitespace typos
    
    * Removed debug output
    
    * [CI] Make oneapi_plugin a CMake target
    
    * Added tests for OneAPI plugin for predictor and obj. functions
    
    * Temporarily switched to default selector for device dispacthing in OneAPI plugin to enable execution in environments without gpus
    
    * Updated readme file.
    
    * Fixed USM usage in predictor
    
    * Removed workaround with explicit templated names for DPC++ kernels
    
    * Fixed warnings in plugin tests
    
    * Fix CMake build of gtest
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Remove plugin, cuda related code in automake & autoconf files (#4789)
    
    * Build plugin example with CMake.
    
    * Remove plugin, cuda related code in automake & autoconf files.
    
    * Fix typo in GPU doc.
    Introducing DPC++-based plugin (predictor, objective function) supporting oneAPI programming model (#5825)
    
    * Added plugin with DPC++-based predictor and objective function
    
    * Update CMakeLists.txt
    
    * Update regression_obj_oneapi.cc
    
    * Added README.md for OneAPI plugin
    
    * Added OneAPI predictor support to gbtree
    
    * Update README.md
    
    * Merged kernels in gradient computation. Enabled multiple loss functions with DPC++ backend
    
    * Aligned plugin CMake files with latest master changes. Fixed whitespace typos
    
    * Removed debug output
    
    * [CI] Make oneapi_plugin a CMake target
    
    * Added tests for OneAPI plugin for predictor and obj. functions
    
    * Temporarily switched to default selector for device dispacthing in OneAPI plugin to enable execution in environments without gpus
    
    * Updated readme file.
    
    * Fixed USM usage in predictor
    
    * Removed workaround with explicit templated names for DPC++ kernels
    
    * Fixed warnings in plugin tests
    
    * Fix CMake build of gtest
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix prediction heuristic (#5955)
    
    
    * Relax check for prediction.
    * Relax test in spark test.
    * Add tests in C++.
    Remove column major specialization. (#5755)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Refactor tests with data generator. (#5439)
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Fix external memory for get column batches. (#4622)
    
    * Fix external memory for get column batches.
    
    This fixes two bugs:
    
    * Use PushCSC for get column batches.
    * Don't remove the created temporary directory before finishing test.
    
    * Check all pages.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Initial support for external memory in gpu_predictor (#4284)
    refactor tests to get rid of duplication (#4358)
    
    * refactor tests to get rid of duplication
    
    * address review comments
    add a test for cpu predictor using external memory (#4308)
    
    * add a test for cpu predictor using external memory
    
    * allow different page size for testing
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Combine TreeModel and RegTree (#3995)
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Refactor tests with data generator. (#5439)
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Unify evaluation functions. (#6037)
    Add float32 histogram (#5624)
    
    * new single_precision_histogram param was added.
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Co-authored-by: fis <jm.yuan@outlook.com>
    Distributed optimizations for 'hist' method with CPUs (#5557)
    
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    skip missing lookup if nothing is missing in CPU hist partition kernel. (#5644)
    
    * [xgboost] skip missing lookup if nothing is missing
    Optimizations for RNG in InitData kernel (#5522)
    
    * optimizations for subsampling in InitData
    
    * optimizations for subsampling in InitData
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor tests with data generator. (#5439)
    Optimized ApplySplit, BuildHist and UpdatePredictCache functions on CPU (#5244)
    
    * Split up sparse and dense build hist kernels.
    * Add `PartitionBuilder`.
    Optimized BuildHist function (#5156)
    Optimized EvaluateSplut function (#5138)
    
    
    * Add block based threading utilities.
    Quick fix for memory leak in CPU Hist. (#5153)
    
    
    
    Closes https://github.com/dmlc/xgboost/issues/3579 .
    
    * Don't use map.
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Revert #4529 (#5008)
    
    * Revert " Optimize ‘hist’ for multi-core CPU (#4529)"
    
    This reverts commit 4d6590be3c9a043d44d9e4fe0a456a9f8179ec72.
    
    * Fix build
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Span: use `size_t' for index_type,  add `front' and `back'. (#4935)
    
    * Use `size_t' for index_type.  Add `front' and `back'.
    
    * Remove a batch of `static_cast'.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [HOTFIX] distributed training with hist method  (#4716)
    
    * add parallel test for hist.EvalualiteSplit
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * update test_openmp.py
    
    * fix OMP schedule policy
    
    * fix clang-tidy
    
    * add logging: total_num_bins
    
    * fix
    
    * fix
    
    * test
    
    * replace guided OPENMP policy with static in updater_quantile_hist.cc
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
     Optimize ‘hist’ for multi-core CPU (#4529)
    
    * Initial performance optimizations for xgboost
    
    * remove includes
    
    * revert float->double
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * Check existence of _mm_prefetch and __builtin_prefetch
    
    * Fix lint
    
    * optimizations for CPU
    
    * appling comments in review
    
    * add some comments, code refactoring
    
    * fixing issues in CI
    
    * adding runtime checks
    
    * remove 1 extra check
    
    * remove extra checks in BuildHist
    
    * remove checks
    
    * add debug info
    
    * added debug info
    
    * revert changes
    
    * added comments
    
    * Apply suggestions from code review
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * apply review comments
    
    * Remove unused function CreateNewNodes()
    
    * Add descriptive comment on node_idx variable in QuantileHistMaker::Builder::BuildHistsBatch()
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Prevent empty quantiles in fast hist (#4155)
    
    * Prevent empty quantiles
    
    * Revise and improve unit tests for quantile hist
    
    * Remove unnecessary comment
    
    * Add #2943 as a test case
    
    * Skip test if no sklearn
    
    * Revise misleading comments
    Separate Depthwidth and Lossguide growing policy in fast histogram (#4102)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * more changes
    
    * temp
    
    * update
    
    * udpate rabit
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * broadcast subsampled feature correctly
    
    * init col
    
    * temp
    
    * col sampling
    
    * fix histmastrix init
    
    * fix col sampling
    
    * remove cout
    
    * fix out of bound access
    
    * fix core dump
    
    remove core dump file
    
    * disbale test temporarily
    
    * update
    
    * add fid
    
    * print perf data
    
    * update
    
    * revert some changes
    
    * temp
    
    * temp
    
    * pass all tests
    
    * bring back some tests
    
    * recover some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * recover column init part
    
    * more recovery
    
    * fix core dumps
    
    * code clean
    
    * revert some changes
    
    * fix test compilation issue
    
    * fix lint issue
    
    * resolve compilation issue
    
    * fix issues of lint caused by rebase
    
    * fix stylistic changes and change variable names
    
    * use regtree internal function
    
    * modularize depth width
    
    * address the comments
    
    * fix failed tests
    
    * wrap perf timers with class
    
    * fix lint
    
    * fix num_leaves count
    
    * fix indention
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * merge
    
    * fix compilation
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
    Combine TreeModel and RegTree (#3995)
    Remove leaf vector, add tree serialisation test, fix Windows tests (#3989)
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Add categorical data support to GPU Hist. (#6164)
    Unify evaluation functions. (#6037)
    Limit tree depth for GPU hist. (#6045)
    Feature weights (#5962)
    Refactor gpu_hist split evaluation (#5610)
    
    * Refactor
    
    * Rewrite evaluate splits
    
    * Add more tests
    For histograms, opting into maximum shared memory available per block. (#5491)
    Purge device_helpers.cuh (#5534)
    
    * Simplifications with caching_device_vector
    
    * Purge device helpers
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Split up test helpers header. (#5455)
    Refactor tests with data generator. (#5439)
    Fix memory usage of device sketching (#5407)
    Partial rewrite EllpackPage (#5352)
    Sketching from adapters (#5365)
    
    * Sketching from adapters
    
    * Add weights test
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Testing hist_util (#5251)
    
    * Rank tests
    
    * Remove categorical split specialisation
    
    * Extend tests to multiple features, switch to WQSketch
    
    * Add tests for SparseCuts
    
    * Add external memory quantile tests, fix some existing tests
    Pass shared pointer instead of raw pointer to Learner. (#5302)
    
    Extracted from https://github.com/dmlc/xgboost/pull/5220 .
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Small refinements for JSON model. (#5112)
    
    * Naming consistency.
    
    * Remove duplicated test.
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    remove device shards (#4867)
    Support gamma in GPU_Hist. (#4874)
    
    
    * Just prevent building the tree instead of using an explicit pruner.
    Move ellpack page construction into DMatrix (#4833)
    further cleanup of single process multi-GPU code (#4810)
    
    
    * use subspan in gpu predictor instead of copying
    * Revise `HostDeviceVector`
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    [BREAKING] prevent multi-gpu usage (#4749)
    
    * prevent multi-gpu usage
    
    * fix distributed test
    
    * combine gpu predictor tests
    
    * set upper bound on n_gpus
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    fix compiler warning (#4588)
    Refactor out row partitioning logic from gpu_hist, introduce caching device vectors (#4554)
    Feature interaction for GPU Hist. (#4534)
    
    * GPU hist Interaction Constraints.
    * Duplicate related parameters.
    * Add tests for CPU interaction constraint.
    * Add better error reporting.
    * Thorough tests.
    Optimizations for quantisation on device (#4572)
    
    * - do not create device vectors for the entire sparse page while computing histograms...
       - while creating the compressed histogram indices, the row vector is created for the entire
         sparse page batch. this is needless as we only process chunks at a time based on a slice
         of the total gpu memory
       - this pr will allocate only as much as required to store the ppropriate row indices and the entries
    
    * - do not dereference row_ptrs once the device_vector has been created to elide host copies of those counts
       - instead, grab the entry counts directly from the sparsepage
    - training with external memory - part 2 of 2 (#4526)
    
    * - training with external memory - part 2 of 2
       - when external memory support is enabled, building of histogram indices are
         done incrementally for every sparse page
       - the entire set of input data is divided across multiple gpu's and the relative
         row positions within each device is tracked when building the compressed histogram buffer
       - this was tested using a mortgage dataset containing ~ 670m rows before 4xt4's could be
         saturated
    Overload device memory allocation (#4532)
    
    * Group source files, include headers in source files
    
    * Overload device memory allocation
    Refactor histogram building code for gpu_hist (#4528)
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
     Combine thread launches into single launch per tree for gpu_hist (#4343)
    
    * Combine thread launches into single launch per tree for gpu_hist
    algorithm.
    
    * Address deprecation warning
    
    * Add manual column sampler constructor
    
    * Turn off omp dynamic to get a guaranteed number of threads
    
    * Enable openmp in cuda code
    Fix Histogram allocation. (#4347)
    
    * Fix Histogram allocation.
    
    nidx_map is cleared after `Reset`, but histogram data size isn't changed hence
    histogram recycling is used in later iterations.  After a reset(building new
    tree), newly allocated node will start from 0, while recycling always choose
    the node with smallest index, which happens to be our newly allocated node 0.
    Retire DVec class in favour of c++20 style span for device memory. (#4293)
    Further optimisations for gpu_hist. (#4283)
    
    - Fuse final update position functions into a single more efficient kernel
    
    - Refactor gpu_hist with a more explicit ellpack  matrix representation
    Optimisations for gpu_hist. (#4248)
    
    * Optimisations for gpu_hist.
    
    * Use streams to overlap operations.
    
    * ColumnSampler now uses HostDeviceVector to prevent repeatedly copying feature vectors to the device.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Fix gpu_hist apply_split test. (#4158)
    Combine TreeModel and RegTree (#3995)
    Column sampling at individual nodes (splits). (#3971)
    
    * Column sampling at individual nodes (splits).
    
    * Documented colsample_bynode parameter.
    
    - also updated documentation for colsample_by* parameters
    
    * Updated documentation.
    
    * GetFeatureSet() returns shared pointer to std::vector.
    
    * Sync sampled columns across multiple processes.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Single precision histograms on GPU (#3965)
    
    * Allow single precision histogram summation in gpu_hist
    
    * Add python test, reduce run-time of gpu_hist tests
    
    * Update documentation
    GPU performance logging/improvements (#3945)
    
    - Improved GPU performance logging
    
    - Only use one execute shards function
    
    - Revert performance regression on multi-GPU
    
    - Use threads to launch NCCL AllReduce
    Improve update position function for gpu_hist (#3895)
    Minor refactor of split evaluation in gpu_hist (#3889)
    
    * Refactor evaluate split into shard
    
    * Use span in evaluate split
    
    * Update google tests
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Make C++ unit tests run and pass on Windows (#3869)
    
    * Make C++ unit tests run and pass on Windows
    
    * Fix logic for external memory. The letter ':' is part of drive letter,
    so remove the drive letter before splitting on ':'.
    * Cosmetic syntax changes to keep MSVC happy.
    
    * Fix lint
    
    * Add Windows guard
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Add basic unittests for gpu-hist method. (#3785)
    
    * Split building histogram into separated class.
    * Extract `InitCompressedRow` definition.
    * Basic tests for gpu-hist.
    * Document the code more verbosely.
    * Removed `HistCutUnit`.
    * Removed some duplicated copies in `GPUHistMaker`.
    * Implement LCG and use it in tests.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Add multi-GPU unit test environment (#3741)
    
    * Add multi-GPU unit test environment
    
    * Better assertion message
    
    * Temporarily disable failing test
    
    * Distinguish between multi-GPU and single-GPU CPP tests
    
    * Consolidate Python tests. Use attributes to distinguish multi-GPU Python tests from single-CPU counterparts
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Refactor parts of fast histogram utilities (#3564)
    
    * Refactor parts of fast histogram utilities
    
    * Removed byte packing from column matrix
    Added finding quantiles on GPU. (#3393)
    
    * Added finding quantiles on GPU.
    
    - this includes datasets where weights are assigned to data rows
    - as the quantiles found by the new algorithm are not the same
      as those found by the old one, test thresholds in
        tests/python-gpu/test_gpu_updaters.py have been adjusted.
    
    * Adjustments and improved testing for finding quantiles on the GPU.
    
    - added C++ tests for the DeviceSketch() function
    - reduced one of the thresholds in test_gpu_updaters.py
    - adjusted the cuts found by the find_cuts_k kernel
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    GPU binning and compression. (#3319)
    
    * GPU binning and compression.
    
    - binning and index compression are done inside the DeviceShard constructor
    - in case of a DMatrix with multiple row batches, it is first converted into a single row batch
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Fix Google test warnings and error (#2957)
    Update gpu_hist algorithm (#2901)
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Fix parsing empty vector in parameter. (#5087)
     Require leaf statistics when expanding tree (#4015)
    
    * Cache left and right gradient sums
    
    * Require leaf statistics when expanding tree
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Fix test_param.cc header path (#3317)
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    tests/cpp: Add tests for SplitEntry
    tests/cpp: Add tests for param.h
    Requires setting leaf stat when expanding tree. (#5501)
    
    * Fix GPU Hist feature importance.
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Refactor tests with data generator. (#5439)
    Fix pruner. (#5335)
    
    
    * Honor the tree depth.
    * Prevent pruning pruned node.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
     Require leaf statistics when expanding tree (#4015)
    
    * Cache left and right gradient sums
    
    * Require leaf statistics when expanding tree
    Reduce tree expand boilerplate code (#4008)
    Combine TreeModel and RegTree (#3995)
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Requires setting leaf stat when expanding tree. (#5501)
    
    * Fix GPU Hist feature importance.
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Refactor tests with data generator. (#5439)
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
     Require leaf statistics when expanding tree (#4015)
    
    * Cache left and right gradient sums
    
    * Require leaf statistics when expanding tree
    Reduce tree expand boilerplate code (#4008)
    Combine TreeModel and RegTree (#3995)
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Reduce device synchronisation (#5631)
    
    * Reduce device synchronisation
    
    * Initialise pinned memory
    Fix github merge. (#5509)
    Requires setting leaf stat when expanding tree. (#5501)
    
    * Fix GPU Hist feature importance.
    Fix r interaction constraints (#5543)
    
    * Unify the parsing code.
    
    * Cleanup.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Feature interaction for GPU Hist. (#4534)
    
    * GPU hist Interaction Constraints.
    * Duplicate related parameters.
    * Add tests for CPU interaction constraint.
    * Add better error reporting.
    * Thorough tests.
    More categorical tests and disable shap sparse test. (#6219)
    
    * Fix tree load with 32 category.
    Expand categorical node. (#6028)
    
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add JSON schema to model dump. (#5660)
    Requires setting leaf stat when expanding tree. (#5501)
    
    * Fix GPU Hist feature importance.
    Check whether current updater can modify a tree. (#5406)
    
    * Check whether current updater can modify a tree.
    
    * Fix tree model JSON IO for pruned trees.
    Config for linear updaters. (#5222)
    Model IO in JSON. (#5110)
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Implement tree model dump with code generator. (#4602)
    
    * Implement tree model dump with a code generator.
    
    * Split up generators.
    * Implement graphviz generator.
    * Use pattern matching.
    
    * [Breaking] Return a Source in `to_graphviz` instead of Digraph in Python package.
    
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix node reuse. (#4404)
    
    * Reinitialize `_sindex` when reallocating a deleted node.
    Remove leaf vector, add tree serialisation test, fix Windows tests (#3989)
    Unify evaluation functions. (#6037)
    Reduce device synchronisation (#5631)
    
    * Reduce device synchronisation
    
    * Initialise pinned memory
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Modify caching allocator/vector and fix issues relating to inability to train large datasets (#4615)
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Refactor out row partitioning logic from gpu_hist, introduce caching device vectors (#4554)
    Add categorical data support to GPU Hist. (#6164)
    Unify evaluation functions. (#6037)
    Refactor gpu_hist split evaluation (#5610)
    
    * Refactor
    
    * Rewrite evaluate splits
    
    * Add more tests
    Support categorical data in ellpack. (#6140)
    Split Features into Groups to Compute Histograms in Shared Memory (#5795)
    For histograms, opting into maximum shared memory available per block. (#5491)
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Refactor tests with data generator. (#5439)
    Fix memory usage of device sketching (#5407)
    Partial rewrite EllpackPage (#5352)
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Refactor gpu_hist split evaluation (#5610)
    
    * Refactor
    
    * Rewrite evaluate splits
    
    * Add more tests
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix memory usage of device sketching (#5407)
    Partial rewrite EllpackPage (#5352)
    Move segment sorter to common (#5378)
    
    - move segment sorter to common
    - this is the first of a handful of pr's that splits the larger pr #5326
    - it moves this facility to common (from ranking objective class), so that it can be
        used for metric computation
    - it also wraps all the bald device pointers into span.
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Support categorical data in ellpack. (#6140)
    Optimize cpu sketch allreduce for sparse data. (#6009)
    
    * Bypass RABIT serialization reducer and use custom allgather based merging.
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Implement weighted sketching for adapter. (#5760)
    
    
    * Bounded memory tests.
    * Fixed memory estimation.
    Group aware GPU sketching. (#5551)
    
    * Group aware GPU weighted sketching.
    
    * Distribute group weights to each data point.
    * Relax the test.
    * Validate input meta info.
    * Fix metainfo copy ctor.
    Refactor tests with data generator. (#5439)
    Sketching from adapters (#5365)
    
    * Sketching from adapters
    
    * Add weights test
    Remove SimpleCSRSource (#5315)
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Testing hist_util (#5251)
    
    * Rank tests
    
    * Remove categorical split specialisation
    
    * Extend tests to multiple features, switch to WQSketch
    
    * Add tests for SparseCuts
    
    * Add external memory quantile tests, fix some existing tests
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Fix C++11 config parser (#4521)
    
    * Fix C++11 config parser
    * Use raw strings to improve readability of regex
    * Fix compilation for GCC 5.x
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Optimize cpu sketch allreduce for sparse data. (#6009)
    
    * Bypass RABIT serialization reducer and use custom allgather based merging.
    Unify CPU hist sketching (#5880)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Reduce span check overhead. (#5464)
    Catch exception in transform function omp context. (#4960)
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Make C++ unit tests run and pass on Windows (#3869)
    
    * Make C++ unit tests run and pass on Windows
    
    * Fix logic for external memory. The letter ':' is part of drive letter,
    so remove the drive letter before splitting on ':'.
    * Cosmetic syntax changes to keep MSVC happy.
    
    * Fix lint
    
    * Add Windows guard
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Group builder modified for incremental building (#5098)
    External data adapters (#5044)
    
    * Use external data adapters as lightweight intermediate layer between external data and DMatrix
    Disable parameter validation for Scikit-Learn interface. (#5167)
    
    * Disable parameter validation for now.
    
    Scikit-Learn passes all parameters down to XGBoost, whether they are used or
    not.
    
    * Add option `validate_parameters`.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix and optimize logger (#4002)
    
    * Fix logging switch statement.
    
    * Remove debug_verbose_ in AllReducer.
    
    * Don't construct the stream when not needed.
    
    * Make default constructor deleted.
    
    * Remove redundant IsVerbose.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Use bundled gtest (#4900)
    
    * Suggest to use gtest bundled with dmlc
    
    * Use dmlc bundled gtest in all CI scripts
    
    * Make clang-tidy to use dmlc embedded gtest
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    [Blocking] Fix #3840: Clean up logic for parsing tree_method parameter (#3849)
    
    * Clean up logic for converting tree_method to updater sequence
    
    * Use C++11 enum class for extra safety
    
    Compiler will give warnings if switch statements don't handle all
    possible values of C++11 enum class.
    
    Also allow enum class to be used as DMLC parameter.
    
    * Fix compiler error + lint
    
    * Address reviewer comment
    
    * Better docstring for DECLARE_FIELD_ENUM_CLASS
    
    * Fix lint
    
    * Add C++ test to see if tree_method is recognized
    
    * Fix clang-tidy error
    
    * Add test_learner.h to R package
    
    * Update comments
    
    * Fix lint error
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    Use configure_file() to configure version only (#4974)
    
    * Avoid writing build_config.h
    
    * Remove build_config.h all together.
    
    * Lint.
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Optimize cpu sketch allreduce for sparse data. (#6009)
    
    * Bypass RABIT serialization reducer and use custom allgather based merging.
    Unify CPU hist sketching (#5880)
    Cleanup on device sketch. (#5874)
    
    * Remove old functions.
    
    * Merge weighted and un-weighted into a common interface.
    Add float32 histogram (#5624)
    
    * new single_precision_histogram param was added.
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Co-authored-by: fis <jm.yuan@outlook.com>
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Prevent copying SimpleDMatrix. (#5453)
    
    * Set default dtor for SimpleDMatrix to initialize default copy ctor, which is
    deleted due to unique ptr.
    
    * Remove commented code.
    * Remove warning for calling host function (std::max).
    * Remove warning for initialization order.
    * Remove warning for unused variables.
    Reducing memory consumption for 'hist' method on CPU (#5334)
    Refactor tests with data generator. (#5439)
    Sketching from adapters (#5365)
    
    * Sketching from adapters
    
    * Add weights test
    Remove SimpleCSRSource (#5315)
    Testing hist_util (#5251)
    
    * Rank tests
    
    * Remove categorical split specialisation
    
    * Extend tests to multiple features, switch to WQSketch
    
    * Add tests for SparseCuts
    
    * Add external memory quantile tests, fix some existing tests
    Cleanup warnings. (#5247)
    
    From clang-tidy-9 and gcc-7: Invalid case style, narrowing definition, wrong
    initialization order, unused variables.
    Optimized BuildHist function (#5156)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Make `HistCutMatrix::Init' be aware of groups. (#4115)
    
    
    * Add checks for group size.
    * Simple docs.
    * Search group index during hist cut matrix initialization.
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Handle duplicated values in sketching. (#6178)
    
    
    * Accumulate weights in duplicated values.
    * Fix device id in iterative dmatrix.
    Add categorical data support to GPU Hist. (#6164)
    Support categorical data in GPU sketching. (#6137)
    Merge extract cuts into QuantileContainer. (#6125)
    
    
    * Use pruning for initial summary construction.
    Unify CPU hist sketching (#5880)
    Cleanup on device sketch. (#5874)
    
    * Remove old functions.
    
    * Merge weighted and un-weighted into a common interface.
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Feature weights (#5962)
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Fix gpu devices. (#3693)
    
    * Fix gpu_set normalized and unnormalized.
    * Fix DeviceSpan.
    Merge generic device helper functions into gpu set. (#3626)
    
    * Remove the use of old NDevices* functions.
    * Use GPUSet in timer.h.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Optimize cpu sketch allreduce for sparse data. (#6009)
    
    * Bypass RABIT serialization reducer and use custom allgather based merging.
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add an option to run brute-force test for JSON round-trip (#5804)
    
    * Add an option to run brute-force test for JSON round-trip
    
    * Apply reviewer's feedback
    
    * Remove unneeded objects
    
    * Parallel run.
    
    * Max.
    
    * Use signed 64-bit loop var, to support MSVC
    
    * Add exhaustive test to CI
    
    * Run JSON test in Win build worker
    
    * Revert "Run JSON test in Win build worker"
    
    This reverts commit c97b2c7dda37b3585b445d36961605b79552ca89.
    
    * Revert "Add exhaustive test to CI"
    
    This reverts commit c149c2ce9971a07a7289f9b9bc247818afd5a667.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Convenient methods for JSON integer. (#5089)
    
    * Fix parsing empty object.
    Fix parsing empty json object. (#4868)
    
    * Fix parsing empty json object.
    
    * Better error message.
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Add Json integer, remove specialization. (#4739)
    A simple Json implementation for future use. (#4708)
    
    * A simple Json implementation for future use.
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    [GPU-Plugin] Fix gpu_hist to allow matrices with more than just 2^{32} elements. Also fixed CPU hist algorithm. (#2518)
    [GPU-Plugin] Add load balancing search to gpu_hist. Add compressed iterator. (#2504)
    Fixed single-GPU tests. (#4053)
    
    - ./testxgboost (without filters) failed if run on a multi-GPU machine because
      the memory was allocated on the current device, but device 0
      was always passed into LaunchN
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    GPU binning and compression. (#3319)
    
    * GPU binning and compression.
    
    - binning and index compression are done inside the DeviceShard constructor
    - in case of a DMatrix with multiple row batches, it is first converted into a single row batch
    Implement intrusive ptr (#6129)
    
    
    * Use intrusive ptr for JSON.
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Span: use `size_t' for index_type,  add `front' and `back'. (#4935)
    
    * Use `size_t' for index_type.  Add `front' and `back'.
    
    * Remove a batch of `static_cast'.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Fix empty subspan. (#4151)
    
    * Silent the death tests.
    Fixed single-GPU tests. (#4053)
    
    - ./testxgboost (without filters) failed if run on a multi-GPU machine because
      the memory was allocated on the current device, but device 0
      was always passed into LaunchN
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Ignore columnar alignment requirement. (#4928)
    
    * Better error message for wrong type.
    * Fix stride size.
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Move bitfield into common. (#4737)
    
    
    * Prepare for columnar format support.
    Feature interaction for GPU Hist. (#4534)
    
    * GPU hist Interaction Constraints.
    * Duplicate related parameters.
    * Add tests for CPU interaction constraint.
    * Add better error reporting.
    * Thorough tests.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    More explict sharding methods for device memory (#4396)
    
    * Rename the Reshard method to Shard
    
    * Add a new Reshard method for sharding a vector that's already sharded
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Implement `Empty` method for host device vector. (#5781)
    
    * Fix accessing nullptr.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Implement host span. (#5459)
    Lazy initialization of device vector. (#5173)
    
    * Lazy initialization of device vector.
    
    * Fix #5162.
    
    * Disable copy constructor of HostDeviceVector.  Prevents implicit copying.
    
    * Fix CPU build.
    
    * Bring back move assignment operator.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    further cleanup of single process multi-GPU code (#4810)
    
    
    * use subspan in gpu predictor instead of copying
    * Revise `HostDeviceVector`
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Set the appropriate device before freeing device memory... (#4566)
    
    * - set the appropriate device before freeing device memory...
       - pr #4532 added a global memory tracker/logger to keep track of number of (de)allocations
         and peak memory usage on a per device basis.
       - this pr adds the appropriate check to make sure that the (de)allocation counts and memory usages
         makes sense for the device. since verbosity is typically increased on debug/non-retail builds.
    * - pre-create cub allocators and reuse them
       - create them once and not resize them dynamically. we need to ensure that these allocators
         are created and destroyed exactly once so that the appropriate device id's are set
    More explict sharding methods for device memory (#4396)
    
    * Rename the Reshard method to Shard
    
    * Add a new Reshard method for sharding a vector that's already sharded
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Implement devices to devices reshard. (#3721)
    
    * Force clearing device memory before Reshard.
    * Remove calculating row_segments for gpu_hist and gpu_sketch.
    * Guard against changing device.
    Fixed copy constructor for HostDeviceVectorImpl. (#3657)
    
    - previously, vec_ in DeviceShard wasn't updated on copy; as a result,
      the shards continued to refer to the old HostDeviceVectorImpl object,
      which resulted in a dangling pointer once that object was deallocated
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Feature weights (#5962)
    Lazy initialization of device vector. (#5173)
    
    * Lazy initialization of device vector.
    
    * Fix #5162.
    
    * Disable copy constructor of HostDeviceVector.  Prevents implicit copying.
    
    * Fix CPU build.
    
    * Bring back move assignment operator.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
     Combine thread launches into single launch per tree for gpu_hist (#4343)
    
    * Combine thread launches into single launch per tree for gpu_hist
    algorithm.
    
    * Address deprecation warning
    
    * Add manual column sampler constructor
    
    * Turn off omp dynamic to get a guaranteed number of threads
    
    * Enable openmp in cuda code
    Optimisations for gpu_hist. (#4248)
    
    * Optimisations for gpu_hist.
    
    * Use streams to overlap operations.
    
    * ColumnSampler now uses HostDeviceVector to prevent repeatedly copying feature vectors to the device.
    Column sampling at individual nodes (splits). (#3971)
    
    * Column sampling at individual nodes (splits).
    
    * Documented colsample_bynode parameter.
    
    - also updated documentation for colsample_by* parameters
    
    * Updated documentation.
    
    * GetFeatureSet() returns shared pointer to std::vector.
    
    * Sync sampled columns across multiple processes.
    Add google test for a column sampling, restore metainfo tests (#3637)
    
    * Add google test for a column sampling, restore metainfo tests
    
    * Update metainfo test for visual studio
    
    * Fix multi-GPU bug introduced in #3635
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Fix non-openmp build. (#5566)
    
    
    * Add test to Jenkins.
    * Fix threading utils tests.
    * Require thread library.
    Cleanup warnings. (#5247)
    
    From clang-tidy-9 and gcc-7: Invalid case style, narrowing definition, wrong
    initialization order, unused variables.
    Optimized BuildHist function (#5156)
    Optimized EvaluateSplut function (#5138)
    
    
    * Add block based threading utilities.
    Remove rabit dependency on public headers. (#6005)
    RMM integration plugin (#5873)
    
    * [CI] Add RMM as an optional dependency
    
    * Replace caching allocator with pool allocator from RMM
    
    * Revert "Replace caching allocator with pool allocator from RMM"
    
    This reverts commit e15845d4e72e890c2babe31a988b26503a7d9038.
    
    * Use rmm::mr::get_default_resource()
    
    * Try setting default resource (doesn't work yet)
    
    * Allocate pool_mr in the heap
    
    * Prevent leaking pool_mr handle
    
    * Separate EXPECT_DEATH() in separate test suite suffixed DeathTest
    
    * Turn off death tests for RMM
    
    * Address reviewer's feedback
    
    * Prevent leaking of cuda_mr
    
    * Fix Jenkinsfile syntax
    
    * Remove unnecessary function in Jenkinsfile
    
    * [CI] Install NCCL into RMM container
    
    * Run Python tests
    
    * Try building with RMM, CUDA 10.0
    
    * Do not use RMM for CUDA 10.0 target
    
    * Actually test for test_rmm flag
    
    * Fix TestPythonGPU
    
    * Use CNMeM allocator, since pool allocator doesn't yet support multiGPU
    
    * Use 10.0 container to build RMM-enabled XGBoost
    
    * Revert "Use 10.0 container to build RMM-enabled XGBoost"
    
    This reverts commit 789021fa31112e25b683aef39fff375403060141.
    
    * Fix Jenkinsfile
    
    * [CI] Assign larger /dev/shm to NCCL
    
    * Use 10.2 artifact to run multi-GPU Python tests
    
    * Add CUDA 10.0 -> 11.0 cross-version test; remove CUDA 10.0 target
    
    * Rename Conda env rmm_test -> gpu_test
    
    * Use env var to opt into CNMeM pool for C++ tests
    
    * Use identical CUDA version for RMM builds and tests
    
    * Use Pytest fixtures to enable RMM pool in Python tests
    
    * Move RMM to plugin/CMakeLists.txt; use PLUGIN_RMM
    
    * Use per-device MR; use command arg in gtest
    
    * Set CMake prefix path to use Conda env
    
    * Use 0.15 nightly version of RMM
    
    * Remove unnecessary header
    
    * Fix a unit test when cudf is missing
    
    * Add RMM demos
    
    * Remove print()
    
    * Use HostDeviceVector in GPU predictor
    
    * Simplify pytest setup; use LocalCUDACluster fixture
    
    * Address reviewers' commments
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.wasshington.edu>
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Reduce span check overhead. (#5464)
    Span: use `size_t' for index_type,  add `front' and `back'. (#4935)
    
    * Use `size_t' for index_type.  Add `front' and `back'.
    
    * Remove a batch of `static_cast'.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Fix empty subspan. (#4151)
    
    * Silent the death tests.
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Loop over `thrust::reduce`. (#6229)
    
    * Check input chunk size of dqdm.
    * Add doc for current limitation.
    Loop over copy_if (#6201)
    
    * Loop over copy_if
    
    * Catch OOM.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Merge extract cuts into QuantileContainer. (#6125)
    
    
    * Use pruning for initial summary construction.
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    gpu_hist performance fixes (#5558)
    
    * Remove unnecessary cuda API calls
    
    * Fix histogram memory growth
    Use thrust functions instead of custom functions (#5544)
    Purge device_helpers.cuh (#5534)
    
    * Simplifications with caching_device_vector
    
    * Purge device helpers
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    implementation of map ranking algorithm on gpu (#5129)
    
    * - implementation of map ranking algorithm
      - also effected necessary suggestions mentioned in the earlier ranking pr's
      - made some performance improvements to the ndcg algo as well
    Set correct file permission. (#4964)
    Use heuristic to select histogram node, avoid rabit call (#4951)
    further cleanup of single process multi-GPU code (#4810)
    
    
    * use subspan in gpu predictor instead of copying
    * Revise `HostDeviceVector`
    Refactor out row partitioning logic from gpu_hist, introduce caching device vectors (#4554)
     Smarter choice of histogram construction for distributed gpu_hist (#4519)
    
    * Smarter choice of histogram construction for distributed gpu_hist
    
    * Limit omp team size in ExecuteShards
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Retire DVec class in favour of c++20 style span for device memory. (#4293)
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Implement devices to devices reshard. (#3721)
    
    * Force clearing device memory before Reshard.
    * Remove calculating row_segments for gpu_hist and gpu_sketch.
    * Guard against changing device.
    Add cuda forwards compatibility (#3316)
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Change reduce operation from thrust to cub. Fix for cuda 9.1 error (#3218)
    
    * Change reduce operation from thrust to cub. Fix for cuda 9.1 runtime error
    
    * Unit test sum reduce
    Improved gpu_hist_experimental algorithm (#2866)
    
    - Implement colsampling, subsampling for gpu_hist_experimental
    
     - Optimised multi-GPU implementation for gpu_hist_experimental
    
     - Make nccl optional
    
     - Add Volta architecture flag
    
     - Optimise RegLossObj
    
     - Add timing utilities for debug verbose mode
    
     - Bump required cuda version to 8.0
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    [GPU-Plugin] Improved load balancing search (#2521)
    [GPU-Plugin] Add load balancing search to gpu_hist. Add compressed iterator. (#2504)
    Use dmlc stream when URI protocol is not local file. (#5857)
    Add IO utilities. (#5091)
    
    
    * Add fixed size stream for reading model stream.
    * Add file extension.
    Handle duplicated values in sketching. (#6178)
    
    
    * Accumulate weights in duplicated values.
    * Fix device id in iterative dmatrix.
    Support categorical data in GPU sketching. (#6137)
    Unify CPU hist sketching (#5880)
    Fix sketch size calculation. (#5898)
    Cleanup on device sketch. (#5874)
    
    * Remove old functions.
    
    * Merge weighted and un-weighted into a common interface.
    fix device sketch with weights in external memory mode (#5870)
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Implement weighted sketching for adapter. (#5760)
    
    
    * Bounded memory tests.
    * Fixed memory estimation.
    Expose device sketching in header. (#5747)
    Group aware GPU sketching. (#5551)
    
    * Group aware GPU weighted sketching.
    
    * Distribute group weights to each data point.
    * Relax the test.
    * Validate input meta info.
    * Fix metainfo copy ctor.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Device dmatrix (#5420)
    Fix memory usage of device sketching (#5407)
    Sketching from adapters (#5365)
    
    * Sketching from adapters
    
    * Add weights test
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Implement robust regularization in 'survival:aft' objective (#5473)
    
    * Robust regularization of AFT gradient and hessian
    
    * Fix AFT doc; expose it to tutorial TOC
    
    * Apply robust regularization to uncensored case too
    
    * Revise unit test slightly
    
    * Fix lint
    
    * Update test_survival.py
    
    * Use GradientPairPrecise
    
    * Remove unused variables
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Optimized ApplySplit, BuildHist and UpdatePredictCache functions on CPU (#5244)
    
    * Split up sparse and dense build hist kernels.
    * Add `PartitionBuilder`.
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Reducing memory consumption for 'hist' method on CPU (#5334)
    Refactor tests with data generator. (#5439)
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Fix external memory for get column batches. (#4622)
    
    * Fix external memory for get column batches.
    
    This fixes two bugs:
    
    * Use PushCSC for get column batches.
    * Don't remove the created temporary directory before finishing test.
    
    * Check all pages.
    - fix issues with training with external memory on cpu (#4487)
    
    * - fix issues with training with external memory on cpu
       - use the batch size to determine the correct number of rows in a batch
       - use the right number of threads in omp parallalization if the batch size
         is less than the default omp max threads (applicable for the last batch)
    
    * - handle scenarios where last batch size is < available number of threads
    - augment tests such that we can test all scenarios (batch size <, >, = number of threads)
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Refactor parts of fast histogram utilities (#3564)
    
    * Refactor parts of fast histogram utilities
    
    * Removed byte packing from column matrix
    Optimize cpu sketch allreduce for sparse data. (#6009)
    
    * Bypass RABIT serialization reducer and use custom allgather based merging.
    Unify CPU hist sketching (#5880)
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    GPU multiclass metrics (#4368)
    
    * Port multi classes metrics to CUDA.
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    added code for instance based weighing for rank objectives (#3379)
    
    * added code for instance based weighing for rank objectives
    
    * Fix lint
    Cmake improvements (#2487)
    
    * Cmake improvements
    * Add google test to cmake
    tests/cpp: Add tests for multiclass_metric.cc
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    GPU multiclass metrics (#4368)
    
    * Port multi classes metrics to CUDA.
    Ranking metric acceleration on the gpu (#5398)
    GPU multiclass metrics (#4368)
    
    * Port multi classes metrics to CUDA.
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Ranking metric acceleration on the gpu (#5398)
    Fixes and changes to the ranking metrics computed on cpu (#5380)
    
    * - fixes and changes to the ranking metrics computed on cpu
      - auc/aucpr ranking metric accelerated on cpu
      - fixes to the auc/aucpr metrics
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Support ndcg- and map- (#4635)
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Change reduce operation from thrust to cub. Fix for cuda 9.1 error (#3218)
    
    * Change reduce operation from thrust to cub. Fix for cuda 9.1 runtime error
    
    * Unit test sum reduce
    rank_metric: add AUC-PR (#3172)
    
    * rank_metric: add AUC-PR
    
    Implementation of the AUC-PR calculation for weighted data, proposed by Keilwagen, Grosse and Grau (https://doi.org/10.1371/journal.pone.0092209)
    
    * rank_metric: fix lint warnings
    
    * Implement tests for AUC-PR and fix implementation
    
    * add aucpr to documentation for other languages
    Cmake improvements (#2487)
    
    * Cmake improvements
    * Add google test to cmake
    tests/cpp: Add tests for rank_metrics.cc
    Add MAPE metric (#6119)
    Pseudo-huber loss metric added (#5647)
    
    
    - Add pseudo huber loss objective.
    - Add pseudo huber loss metric.
    
    Co-authored-by: Reetz <s02reetz@iavgroup.local>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    [BREAKING] prevent multi-gpu usage (#4749)
    
    * prevent multi-gpu usage
    
    * fix distributed test
    
    * combine gpu predictor tests
    
    * set upper bound on n_gpus
    Add `rmsle` metric and `reg:squaredlogerror` objective (#4541)
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Cmake improvements (#2487)
    
    * Cmake improvements
    * Add google test to cmake
    tests/cpp: Add test for elementwise_metric.cc
    tests/cpp: Add tests for Metric RMSE
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Cmake improvements (#2487)
    
    * Cmake improvements
    * Add google test to cmake
    tests/cpp: Add tests for metric.cc
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor tests with data generator. (#5439)
    Config for linear updaters. (#5222)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    [BREAKING] prevent multi-gpu usage (#4749)
    
    * prevent multi-gpu usage
    
    * fix distributed test
    
    * combine gpu predictor tests
    
    * set upper bound on n_gpus
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix gpu coordinate running on multi-gpu. (#3893)
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor tests with data generator. (#5439)
    Config for linear updaters. (#5222)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Use Span in gpu coordinate. (#4029)
    
    * Use Span in gpu coordinate.
    
    * Use Span in device code.
    * Fix shard size calculation.
      - Use lower_bound instead of upper_bound.
    * Check empty devices.
    Merge duplicated linear updater parameters. (#4013)
    
    * Merge duplicated linear updater parameters.
    
    * Split up coordinate descent parameter.
    Add basic unittests for gpu-hist method. (#3785)
    
    * Split building histogram into separated class.
    * Extract `InitCompressedRow` definition.
    * Basic tests for gpu-hist.
    * Document the code more verbosely.
    * Removed `HistCutUnit`.
    * Removed some duplicated copies in `GPUHistMaker`.
    * Implement LCG and use it in tests.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Prevent multiclass Hessian approaching 0 (#3304)
    
    * Prevent Hessian in multiclass objective becoming zero
    
    * Set default learning rate to 0.5 for "coord_descent" linear updater
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Additional improvements for gblinear (#3134)
    
    * fix rebase conflict
    
    * [core] additional gblinear improvements
    
    * [R] callback for gblinear coefficients history
    
    * force eta=1 for gblinear python tests
    
    * add top_k to GreedyFeatureSelector
    
    * set eta=1 in shotgun test
    
    * [core] fix SparsePage processing in gblinear; col-wise multithreading in greedy updater
    
    * set sorted flag within TryInitColData
    
    * gblinear tests: use scale, add external memory test
    
    * fix multiclass for greedy updater
    
    * fix whitespace
    
    * fix typo
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Config for linear updaters. (#5222)
    Accept string for ArrayInterface constructor. (#5799)
    Revert "Accept string for ArrayInterface constructor."
    
    This reverts commit e8ecafb8dc628f45b75b4c2844a236d27e0a6d98.
    Accept string for ArrayInterface constructor.
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Support dmatrix construction from cupy array (#5206)
    Implement cudf construction with adapters. (#5189)
    Move feature names and types of DMatrix from Python to C++. (#5858)
    
    
    * Add thread local return entry for DMatrix.
    * Save feature name and feature type in binary file.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Implement extend method for meta info. (#5800)
    
    * Implement extend for host device vector.
    Set device in device dmatrix. (#5596)
    Group aware GPU sketching. (#5551)
    
    * Group aware GPU weighted sketching.
    
    * Distribute group weights to each data point.
    * Relax the test.
    * Validate input meta info.
    * Fix metainfo copy ctor.
    Remove SimpleCSRSource (#5315)
    Extensible binary serialization format for DMatrix::MetaInfo (#5187)
    
    * Turn xgboost::DataType into C++11 enum class
    
    * New binary serialization format for DMatrix::MetaInfo
    
    * Fix clang-tidy
    
    * Fix c++ test
    
    * Implement new format proposal
    
    * Move helper functions to anonymous namespace; remove unneeded field
    
    * Fix lint
    
    * Add shape.
    
    * Keep only roundtrip test.
    
    * Fix test.
    
    * various fixes
    
    * Update data.cc
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    [Breaking] Remove num roots. (#5059)
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    remove the qids_ field in MetaInfo (#4744)
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way (#3783)
    
    * Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way
    
    Also install git inside NVIDIA GPU container
    
    * Update dmlc-core
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Add google test for a column sampling, restore metainfo tests (#3637)
    
    * Add google test for a column sampling, restore metainfo tests
    
    * Update metainfo test for visual studio
    
    * Fix multi-GPU bug introduced in #3635
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Refactor parts of fast histogram utilities (#3564)
    
    * Refactor parts of fast histogram utilities
    
    * Removed byte packing from column matrix
    Add qid like ranklib format (#2749)
    
    * add qid for https://github.com/dmlc/xgboost/issues/2748
    
    * change names
    
    * change spaces
    
    * change qid to bst_uint type
    
    * change qid type to size_t
    
    * change qid first to SIZE_MAX
    
    * change qid type from size_t to uint64_t
    
    * update dmlc-core
    
    * fix qids name error
    
    * fix group_ptr_ error
    
    * Style fix
    
    * Add qid handling logic to SparsePage
    
    * New MetaInfo format + backward compatibility fix
    
    Old MetaInfo format (1.0) doesn't contain qid field. We still want to be able
    to read from MetaInfo files saved in old format. Also, define a new format
    (2.0) that contains the qid field. This way, we can distinguish files that
    contain qid and those that do not.
    
    * Update MetaInfo test
    
    * Simply group assignment logic
    
    * Explicitly set qid=nullptr in NativeDataIter
    
    NativeDataIter's callback does not support qid field. Users of NativeDataIter
    will need to call setGroup() function separately to set group information.
    
    * Save qids_ in SaveBinary()
    
    * Upgrade dmlc-core submodule
    
    * Add a test for reading qid
    
    * Add contributor
    
    * Check the size of qids_
    
    * Document qid format
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    tests/cpp/test_metainfo: Add tests to save and load
    Add make commands for tests
    
    This adds the make commands required to build and run tests.
    Implement extend method for meta info. (#5800)
    
    * Implement extend for host device vector.
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Use hypothesis (#5759)
    
    * Use hypothesis
    
    * Allow int64 array interface for groups
    
    * Add packages to Windows CI
    
    * Add to travis
    
    * Make sure device index is set correctly
    
    * Fix dask-cudf test
    
    * appveyor
    Purge device_helpers.cuh (#5534)
    
    * Simplifications with caching_device_vector
    
    * Purge device helpers
    Support dmatrix construction from cupy array (#5206)
    Support feature names/types for cudf. (#4902)
    
    * Implement most of the pandas procedure for cudf except for type conversion.
    * Requires an array of interfaces in metainfo.
    Complete cudf support. (#4850)
    
    
    * Handles missing value.
    * Accept all floating point and integer types.
    * Move to cudf 9.0 API.
    * Remove requirement on `null_count`.
    * Arbitrary column types support.
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Purge device_helpers.cuh (#5534)
    
    * Simplifications with caching_device_vector
    
    * Purge device helpers
    Remove SimpleCSRSource (#5315)
    Support dmatrix construction from cupy array (#5206)
    Implement cudf construction with adapters. (#5189)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix memory usage of device sketching (#5407)
    Partial rewrite EllpackPage (#5352)
    Sketching from adapters (#5365)
    
    * Sketching from adapters
    
    * Add weights test
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Implement iterative DMatrix. (#5837)
    Enhance nvtx support. (#5636)
    Add number of columns to native data iterator. (#5202)
    
    * Change native data iter into an adapter.
    Remove unnecessary DMatrix methods (#5324)
    Refactor SparsePageSource, delete cache files after use (#5321)
    
    * Refactor sparse page source
    
    * Delete temporary cache files
    
    * Log fatal if cache exists
    
    * Log fatal if multiple threads used with prefetcher
    Cleanup warnings. (#5247)
    
    From clang-tidy-9 and gcc-7: Invalid case style, narrowing definition, wrong
    initialization order, unused variables.
    Fix wrapping GPU ID and prevent data copying. (#5160)
    
    * Removed some data copying.
    
    * Make sure gpu_id is valid before any configuration is carried out.
    Use adapters for SparsePageDMatrix (#5092)
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix external memory for get column batches. (#4622)
    
    * Fix external memory for get column batches.
    
    This fixes two bugs:
    
    * Use PushCSC for get column batches.
    * Don't remove the created temporary directory before finishing test.
    
    * Check all pages.
    Initial support for external memory in gpu_predictor (#4284)
    refactor tests to get rid of duplication (#4358)
    
    * refactor tests to get rid of duplication
    
    * address review comments
    add a test for cpu predictor using external memory (#4308)
    
    * add a test for cpu predictor using external memory
    
    * allow different page size for testing
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way (#3783)
    
    * Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way
    
    Also install git inside NVIDIA GPU container
    
    * Update dmlc-core
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    tests/cpp: Add tests for SparsePageDMatrix
    
    The SparsePageDMatrix or external memory DMatrix reads data from the
    file IO rather than load it into RAM.
    [BLOCKING] Handle empty rows in data iterators correctly (#5929)
    
    * [jvm-packages] Handle empty rows in data iterators correctly
    
    * Fix clang-tidy error
    
    * last empty row
    
    * Add comments [skip ci]
    
    Co-authored-by: Nan Zhu <nanzhu@uber.com>
    Avoid including `c_api.h` in header files. (#5782)
    Fix slice and get info. (#5552)
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor tests with data generator. (#5439)
    Add number of columns to native data iterator. (#5202)
    
    * Change native data iter into an adapter.
    Cleanup warnings. (#5247)
    
    From clang-tidy-9 and gcc-7: Invalid case style, narrowing definition, wrong
    initialization order, unused variables.
    Implement slice via adapters (#5198)
    Implement cudf construction with adapters. (#5189)
    Use adapters for SparsePageDMatrix (#5092)
    Group builder modified for incremental building (#5098)
    External data adapters (#5044)
    
    * Use external data adapters as lightweight intermediate layer between external data and DMatrix
    Fix slice and get info. (#5552)
    External data adapters (#5044)
    
    * Use external data adapters as lightweight intermediate layer between external data and DMatrix
    [CI] Fix Travis tests. (#5062)
    
    - Install wget explicitly to match openssl.
    - Install CMake explicitly.
    - Use newer miniconda link.
    - Reenable unittests.
    - gcc@9 + xcode@10 for osx due to missing <_stdio.h>.  Other versions of gcc should also work.  But as homebrew pour gcc@9 after update by default, so I just stick with latest version.
    - Disabled one external memory test for OSX.  Not sure about the thread implementation in there and fixing external memory is beyond the scope of this PR.
    - Use Python3 with conda in jvm package.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Fix data loading (#4862)
    
    * Fix loading text data.
    * Fix config regex.
    * Try to explain the error better in exception.
    * Update doc.
    Span: use `size_t' for index_type,  add `front' and `back'. (#4935)
    
    * Use `size_t' for index_type.  Add `front' and `back'.
    
    * Remove a batch of `static_cast'.
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Fix external memory for get column batches. (#4622)
    
    * Fix external memory for get column batches.
    
    This fixes two bugs:
    
    * Use PushCSC for get column batches.
    * Don't remove the created temporary directory before finishing test.
    
    * Check all pages.
    Fix crash with approx tree method on cpu (#4510)
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Add PushCSC for SparsePage. (#4193)
    
    * Add PushCSC for SparsePage.
    
    * Move Push* definitions into cc file.
    * Add std:: prefix to `size_t` make clang++ happy.
    * Address monitor count == 0.
    Loop over `thrust::reduce`. (#6229)
    
    * Check input chunk size of dqdm.
    * Add doc for current limitation.
    Support categorical data in ellpack. (#6140)
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Split up test helpers header. (#5455)
    Refactor tests with data generator. (#5439)
    Fix memory usage of device sketching (#5407)
    Partial rewrite EllpackPage (#5352)
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    Move ellpack page construction into DMatrix (#4833)
    Implement a DMatrix Proxy. (#5803)
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Remove column major specialization. (#5755)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Support dmatrix construction from cupy array (#5206)
    Implement cudf construction with adapters. (#5189)
    [BLOCKING] Handle empty rows in data iterators correctly (#5929)
    
    * [jvm-packages] Handle empty rows in data iterators correctly
    
    * Fix clang-tidy error
    
    * last empty row
    
    * Add comments [skip ci]
    
    Co-authored-by: Nan Zhu <nanzhu@uber.com>
    Fix slice and get info. (#5552)
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Prevent copying SimpleDMatrix. (#5453)
    
    * Set default dtor for SimpleDMatrix to initialize default copy ctor, which is
    deleted due to unique ptr.
    
    * Remove commented code.
    * Remove warning for calling host function (std::max).
    * Remove warning for initialization order.
    * Remove warning for unused variables.
    Refactor tests with data generator. (#5439)
    Add number of columns to native data iterator. (#5202)
    
    * Change native data iter into an adapter.
    Remove unnecessary DMatrix methods (#5324)
    Remove SimpleCSRSource (#5315)
    Cleanup warnings. (#5247)
    
    From clang-tidy-9 and gcc-7: Invalid case style, narrowing definition, wrong
    initialization order, unused variables.
    Implement slice via adapters (#5198)
    Use adapters for SparsePageDMatrix (#5092)
    Group builder modified for incremental building (#5098)
    External data adapters (#5044)
    
    * Use external data adapters as lightweight intermediate layer between external data and DMatrix
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way (#3783)
    
    * Fix #3708: Use dmlc::TemporaryDirectory to handle temporaries in cross-platform way
    
    Also install git inside NVIDIA GPU container
    
    * Update dmlc-core
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    [GPU-Plugin] Integration of a faster version of grow_gpu plugin into mainstream (#2360)
    
    * Integrating a faster version of grow_gpu plugin
    1. Removed the older files to reduce duplication
    2. Moved all of the grow_gpu files under 'exact' folder
    3. All of them are inside 'exact' namespace to avoid any conflicts
    4. Fixed a bug in benchmark.py while running only 'grow_gpu' plugin
    5. Added cub and googletest submodules to ease integration and unit-testing
    6. Updates to CMakeLists.txt to directly build cuda objects into libxgboost
    
    * Added support for building gpu plugins through make flow
    1. updated makefile and config.mk to add right targets
    2. added unit-tests for gpu exact plugin code
    
    * 1. Added support for building gpu plugin using 'make' flow as well
    2. Updated instructions for building and testing gpu plugin
    
    * Fix travis-ci errors for PR#2360
    1. lint errors on unit-tests
    2. removed googletest, instead depended upon dmlc-core provide gtest cache
    
    * Some more fixes to travis-ci lint failures PR#2360
    
    * Added Rory's copyrights to the files containing code from both.
    
    * updated copyright statement as per Rory's request
    
    * moved the static datasets into a script to generate them at runtime
    
    * 1. memory usage print when silent=0
    2. tests/ and test/ folder organization
    3. removal of the dependency of googletest for just building xgboost
    4. coding style updates for .cuh as well
    
    * Fixes for compilation warnings
    
    * add cuda object files as well when JVM_BINDINGS=ON
    tests/cpp: Add tests for SimpleDMatrix
    Catch all standard exceptions in C API. (#6220)
    
    
    * `std::bad_alloc` is not guaranteed to be caught.
    Optimize cpu sketch allreduce for sparse data. (#6009)
    
    * Bypass RABIT serialization reducer and use custom allgather based merging.
    Add XGBoosterGetNumFeature (#5856)
    
    - add GetNumFeature to Learner
    - add XGBoosterGetNumFeature to C API
    - update c-api-demo accordingly
    Move feature names and types of DMatrix from Python to C++. (#5858)
    
    
    * Add thread local return entry for DMatrix.
    * Save feature name and feature type in binary file.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor tests with data generator. (#5439)
    Pass shared pointer instead of raw pointer to Learner. (#5302)
    
    Extracted from https://github.com/dmlc/xgboost/pull/5220 .
    Config for linear updaters. (#5222)
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Model IO in JSON. (#5110)
    Use configure_file() to configure version only (#4974)
    
    * Avoid writing build_config.h
    
    * Remove build_config.h all together.
    
    * Lint.
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Allow import via python datatable. (#3272)
    
    * Allow import via python datatable.
    
    * Write unit tests
    
    * Refactor dt API functions
    
    * Refactor python code
    
    * Lint fixes
    
    * Address review comments
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation (#2530)
    
    * Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation from numpy arrays for python interface.
    Add categorical data support to GPU predictor. (#6165)
    Fix prediction heuristic (#5955)
    
    
    * Relax check for prediction.
    * Relax test in spark test.
    * Add tests in C++.
    Accept iterator in device dmatrix. (#5783)
    
    
    * Remove Device DMatrix.
    Implement iterative DMatrix. (#5837)
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Refactor tests with data generator. (#5439)
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Add categorical data support to GPU predictor. (#6165)
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
    GPUTreeShap (#6038)
    Fix prediction heuristic (#5955)
    
    
    * Relax check for prediction.
    * Relax test in spark test.
    * Add tests in C++.
    Fix mingw build with R. (#5918)
    Implement iterative DMatrix. (#5837)
    Remove column major specialization. (#5755)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Refactor tests with data generator. (#5439)
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    further cleanup of single process multi-GPU code (#4810)
    
    
    * use subspan in gpu predictor instead of copying
    * Revise `HostDeviceVector`
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    [BREAKING] prevent multi-gpu usage (#4749)
    
    * prevent multi-gpu usage
    
    * fix distributed test
    
    * combine gpu predictor tests
    
    * set upper bound on n_gpus
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix external memory for get column batches. (#4622)
    
    * Fix external memory for get column batches.
    
    This fixes two bugs:
    
    * Use PushCSC for get column batches.
    * Don't remove the created temporary directory before finishing test.
    
    * Check all pages.
    fix gpu predictor when dmatrix is mismatched with model (#4613)
    Set the appropriate device before freeing device memory... (#4566)
    
    * - set the appropriate device before freeing device memory...
       - pr #4532 added a global memory tracker/logger to keep track of number of (de)allocations
         and peak memory usage on a per device basis.
       - this pr adds the appropriate check to make sure that the (de)allocation counts and memory usages
         makes sense for the device. since verbosity is typically increased on debug/non-retail builds.
    * - pre-create cub allocators and reuse them
       - create them once and not resize them dynamically. we need to ensure that these allocators
         are created and destroyed exactly once so that the appropriate device id's are set
    Offload some configurations into GBM. (#4553)
    
    
    This is part 1 of refactoring configuration.
    
    * Move tree heuristic configurations.
    * Split up declarations and definitions for GBTree.
    * Implement UseGPU in gbm.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    mgpu predictor using explicit offsets (#4438)
    
    * mgpu prediction using explicit sharding
    Initial support for external memory in gpu_predictor (#4284)
    refactor tests to get rid of duplication (#4358)
    
    * refactor tests to get rid of duplication
    
    * address review comments
    Fix gpu_hist apply_split test. (#4158)
    Combine TreeModel and RegTree (#3995)
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Fix #3342 and h2oai/h2o4gpu#625: Save predictor parameters in model file (#3856)
    
    * Fix #3342 and h2oai/h2o4gpu#625: Save predictor parameters in model file
    
    This allows pickled models to retain predictor attributes, such as
    'predictor' (whether to use CPU or GPU) and 'n_gpu' (number of GPUs
    to use). Related: h2oai/h2o4gpu#625
    
    Closes #3342.
    
    TODO. Write a test.
    
    * Fix lint
    
    * Do not load GPU predictor into CPU-only XGBoost
    
    * Add a test for pickling GPU predictors
    
    * Make sample data big enough to pass multi GPU test
    
    * Update test_gpu_predictor.cu
    Multi-GPU support in GPUPredictor. (#3738)
    
    * Multi-GPU support in GPUPredictor.
    
    - GPUPredictor is multi-GPU
    - removed DeviceMatrix, as it has been made obsolete by using HostDeviceVector in DMatrix
    
    * Replaced pointers with spans in GPUPredictor.
    
    * Added a multi-GPU predictor test.
    
    * Fix multi-gpu test.
    
    * Fix n_rows < n_gpus.
    
    * Reinitialize shards when GPUSet is changed.
    * Tests range of data.
    
    * Remove commented code.
    
    * Remove commented code.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Fix Google test warnings and error (#2957)
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    Integer gradient summation for GPU histogram algorithm. (#2681)
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    Add categorical data support to GPU predictor. (#6165)
    Fix prediction heuristic (#5955)
    
    
    * Relax check for prediction.
    * Relax test in spark test.
    * Add tests in C++.
    Implement iterative DMatrix. (#5837)
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Refactor tests with data generator. (#5439)
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
    Fix prediction heuristic (#5955)
    
    
    * Relax check for prediction.
    * Relax test in spark test.
    * Add tests in C++.
    Remove column major specialization. (#5755)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Refactor tests with data generator. (#5439)
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Fix external memory for get column batches. (#4622)
    
    * Fix external memory for get column batches.
    
    This fixes two bugs:
    
    * Use PushCSC for get column batches.
    * Don't remove the created temporary directory before finishing test.
    
    * Check all pages.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Initial support for external memory in gpu_predictor (#4284)
    refactor tests to get rid of duplication (#4358)
    
    * refactor tests to get rid of duplication
    
    * address review comments
    add a test for cpu predictor using external memory (#4308)
    
    * add a test for cpu predictor using external memory
    
    * allow different page size for testing
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Combine TreeModel and RegTree (#3995)
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    Serialise booster after training to reset state (#5484)
    
    * Serialise booster after training to reset state
    
    * Prevent process_type being set on load
    
    * Check for correct updater sequence
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Refactor tests with data generator. (#5439)
    Check whether current updater can modify a tree. (#5406)
    
    * Check whether current updater can modify a tree.
    
    * Fix tree model JSON IO for pruned trees.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Pass shared pointer instead of raw pointer to Learner. (#5302)
    
    Extracted from https://github.com/dmlc/xgboost/pull/5220 .
    Config for linear updaters. (#5222)
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Small refinements for JSON model. (#5112)
    
    * Naming consistency.
    
    * Remove duplicated test.
    Model IO in JSON. (#5110)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Fix calling GPU predictor (#4836)
    
    * Fix calling GPU predictor
    Restrict access to `cfg_` in gbm. (#4801)
    
    * Restrict access to `cfg_` in gbm.
    
    * Verify having correct updaters.
    
    * Remove `grow_global_histmaker`
    
    This updater is the same as `grow_histmaker`.  The former is not in our
    document so we just remove it.
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    Remove gpu_exact tree method (#4742)
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Offload some configurations into GBM. (#4553)
    
    
    This is part 1 of refactoring configuration.
    
    * Move tree heuristic configurations.
    * Split up declarations and definitions for GBTree.
    * Implement UseGPU in gbm.
    Config for linear updaters. (#5222)
    Small refinements for JSON model. (#5112)
    
    * Naming consistency.
    
    * Remove duplicated test.
    Model IO in JSON. (#5110)
    Refactor rabit tests (#6096)
    
    
    * Merge rabit tests into XGBoost.
    * Run them On CI.
    * Simplification for CMake scripts.
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Refactor rabit tests (#6096)
    
    
    * Merge rabit tests into XGBoost.
    * Run them On CI.
    * Simplification for CMake scripts.
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Refactor rabit tests (#6096)
    
    
    * Merge rabit tests into XGBoost.
    * Run them On CI.
    * Simplification for CMake scripts.
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Refactor rabit tests (#6096)
    
    
    * Merge rabit tests into XGBoost.
    * Run them On CI.
    * Simplification for CMake scripts.
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Refactor rabit tests (#6096)
    
    
    * Merge rabit tests into XGBoost.
    * Run them On CI.
    * Simplification for CMake scripts.
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Enable building rabit on Windows (#6105)
    Update dmlc-core submodule (#3221)
    
    * Update dmlc-core submodule
    
    * Fix dense_parser to work with the latest dmlc-core
    
    * Specify location of Google Test
    
    * Add more source files in dmlc-minimum to get latest dmlc-core working
    
    * Update dmlc-core submodule
    [R] enable R compile
    
    [R] Enable R build for windows and linux
    Unify evaluation functions. (#6037)
    Feature weights (#5962)
    Unify CPU hist sketching (#5880)
    Remove skmaker. (#5971)
    GPU implementation of AFT survival objective and metric (#5714)
    
    * Add interval accuracy
    
    * De-virtualize AFT functions
    
    * Lint
    
    * Refactor AFT metric using GPU-CPU reducer
    
    * Fix R build
    
    * Fix build on Windows
    
    * Fix copyright header
    
    * Clang-tidy
    
    * Fix crashing demo
    
    * Fix typos in comment; explain GPU ID
    
    * Remove unnecessary #include
    
    * Add C++ test for interval accuracy
    
    * Fix a bug in accuracy metric: use log pred
    
    * Refactor AFT objective using GPU-CPU Transform
    
    * Lint
    
    * Fix lint
    
    * Use Ninja to speed up build
    
    * Use time, not /usr/bin/time
    
    * Add cpu_build worker class, with concurrency = 1
    
    * Use concurrency = 1 only for CUDA build
    
    * concurrency = 1 for clang-tidy
    
    * Address reviewer's feedback
    
    * Update link to AFT paper
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Remove SimpleCSRSource (#5315)
    Model IO in JSON. (#5110)
    Fix parsing empty vector in parameter. (#5087)
    Use DART tree weights when computing SHAPs (#5050)
    
    This PR fixes tree weights in dart being ignored when computing contributions.
    
    * Fix ellpack page source link.
    * Add tree weights to compute contribution.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Move ellpack page construction into DMatrix (#4833)
    monitor for distributed envorinment. (#4829)
    
    * Collect statistics from other ranks in monitor.
    
    * Workaround old GCC bug.
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    A simple Json implementation for future use. (#4708)
    
    * A simple Json implementation for future use.
    Refactor fast-hist, add tests for some updaters. (#3836)
    
    Add unittest for prune.
    
    Add unittest for refresh.
    
    Refactor fast_hist.
    
    * Remove fast_hist_param.
    * Rename to quantile_hist.
    
    Add unittests for QuantileHist.
    
    * Refactor QuantileHist into .h and .cc file.
    * Remove sync.h.
    * Remove MGPU_mock test.
    
    Rename fast hist method to quantile hist.
    Implementation of hinge loss for binary classification (#3477)
    Refactor of FastHistMaker to allow for custom regularisation methods (#3335)
    
    * Refactor to allow for custom regularisation methods
    
    * Implement compositional SplitEvaluator framework
    
    * Fixed segfault when no monotone_constraints are supplied.
    
    * Change pid to parentID
    
    * test_monotone_constraints.py now passes
    
    * Refactor ColMaker and DistColMaker to use SplitEvaluator
    
    * Performance optimisation when no monotone_constraints specified
    
    * Fix linter messages
    
    * Fix a few more linter errors
    
    * Update the amalgamation
    
    * Add bounds check
    
    * Add check for leaf node
    
    * Fix linter error in param.h
    
    * Fix clang-tidy errors on CI
    
    * Fix incorrect function name
    
    * Fix clang-tidy error in updater_fast_hist.cc
    
    * Enable SSE2 for Win32 R MinGW
    
    Addresses https://github.com/dmlc/xgboost/pull/3335#issuecomment-400535752
    
    * Add contributor
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    [WIP] Extract prediction into separate interface (#2531)
    
    * [WIP] Extract prediction into separate interface
    
    * Add copyright, fix linter errors
    
    * Add predictor to amalgamation
    
    * Fix documentation
    
    * Move prediction cache into predictor, add GBTreeModel
    
    * Updated predictor doc comments
    Histogram Optimized Tree Grower (#1940)
    
    * Support histogram-based algorithm + multiple tree growing strategy
    
    * Add a brand new updater to support histogram-based algorithm, which buckets
      continuous features into discrete bins to speed up training. To use it, set
      `tree_method = fast_hist` to configuration.
    * Support multiple tree growing strategies. For now, two policies are supported:
      * `grow_policy=depthwise` (default):  favor splitting at nodes closest to the
        root, i.e. grow depth-wise.
      * `grow_policy=lossguide`: favor splitting at nodes with highest loss change
    * Improve single-threaded performance
      * Unroll critical loops
      * Introduce specialized code for dense data (i.e. no missing values)
    * Additional training parameters: `max_leaves`, `max_bin`, `grow_policy`, `verbose`
    
    * Adding a small test for hist method
    
    * Fix memory error in row_set.h
    
    When std::vector is resized, a reference to one of its element may become
    stale. Any such reference must be updated as well.
    
    * Resolve cross-platform compilation issues
    
    * Versions of g++ older than 4.8 lacks support for a few C++11 features, e.g.
      alignas(*) and new initializer syntax. To support g++ 4.6, use pre-C++11
      initializer and remove alignas(*).
    * Versions of MSVC older than 2015 does not support alignas(*). To support
      MSVC 2012, remove alignas(*).
    * For g++ 4.8 and newer, alignas(*) is enabled for performance benefits.
    * Some old compilers (MSVC 2012, g++ 4.6) do not support template aliases
      (which uses `using` to declate type aliases). So always use `typedef`.
    
    * Fix a host of CI issues
    
    * Remove dependency for libz on osx
    * Fix heading for hist_util
    * Fix minor style issues
    * Add missing #include
    * Remove extraneous logging
    
    * Enable tree_method=hist in R
    
    * Renaming HistMaker to GHistBuilder to avoid confusion
    
    * Fix R integration
    
    * Respond to style comments
    
    * Consistent tie-breaking for priority queue using timestamps
    
    * Last-minute style fixes
    
    * Fix issuecomment-271977647
    
    The way we quantize data is broken. The agaricus data consists of all
    categorical values. When NAs are converted into 0's,
    `HistCutMatrix::Init` assign both 0's and 1's to the same single bin.
    
    Why? gmat only the smallest value (0) and an upper bound (2), which is twice
    the maximum value (1). Add the maximum value itself to gmat to fix the issue.
    
    * Fix issuecomment-272266358
    
    * Remove padding from cut values for the continuous case
    * For categorical/ordinal values, use midpoints as bin boundaries to be safe
    
    * Fix CI issue -- do not use xrange(*)
    
    * Fix corner case in quantile sketch
    
    Signed-off-by: Philip Cho <chohyu01@cs.washington.edu>
    
    * Adding a test for an edge case in quantile sketcher
    
    max_bin=2 used to cause an exception.
    
    * Fix fast_hist test
    
    The test used to require a strictly increasing Test AUC for all examples.
    One of them exhibits a small blip in Test AUC before achieving a Test AUC
    of 1. (See bottom.)
    
    Solution: do not require monotonic increase for this particular example.
    
    [0] train-auc:0.99989 test-auc:0.999497
    [1] train-auc:1 test-auc:0.999749
    [2] train-auc:1 test-auc:0.999749
    [3] train-auc:1 test-auc:0.999749
    [4] train-auc:1 test-auc:0.999749
    [5] train-auc:1 test-auc:0.999497
    [6] train-auc:1 test-auc:1
    [7] train-auc:1 test-auc:1
    [8] train-auc:1 test-auc:1
    [9] train-auc:1 test-auc:1
    [R-package] GPL2 dependency reduction and some fixes (#1401)
    
    * [R] do not remove zero coefficients from gblinear dump
    
    * [R] switch from stringr to stringi
    
    * fix #1399
    
    * [R] separate ggplot backend, add base r graphics, cleanup, more plots, tests
    
    * add missing include in amalgamation - fixes building R package in linux
    
    * add forgotten file
    
    * [R] fix DESCRIPTION
    
    * [R] fix travis check issue and some cleanup
    [DATA] Isolate the format of page file
    [R] enable R compile
    
    [R] Enable R build for windows and linux
    [DATA] Make it fully compatible with rank
    [LIBXGBOOST] pass demo running.
    apply  google-java-style indentation and impose import orders....
    add style check for java and scala code
    [jvm-packages] Fix up build for xgboost4j-gpu, xgboost4j-spark-gpu (#6216)
    
    * [CI] Clean up build for JVM packages
    
    * Use correct path for saving native lib
    
    * Fix groupId of maven-surefire-plugin
    
    * Fix stashing of xgboost4j_jar_gpu
    
    * [CI] Don't run xgboost4j-tester with GPU, since it doesn't use gpu_hist
    [jvm-packages]add xgboost4j-gpu/xgboost4j-spark-gpu module to facilitate release (#6136)
    
    * add xgboost4j-gpu/xgboost4j-spark-gpu module to facilitate release
    
    * Update pom.xml
    [CI] Improve JVM test in GitHub Actions (#5930)
    
    * [CI] Improve JVM test in GitHub Actions
    
    * Use env var for Wagon options [skip ci]
    
    * Move the retry flag to pom.xml [skip ci]
    
    * Export env var RABIT_MOCK to run Spark tests [skip ci]
    
    * Correct location of env var
    
    * Re-try up to 5 times [skip ci]
    
    * Don't run distributed training test on Windows
    
    * Fix typo
    
    * Update main.yml
    Bump version to 1.3.0 snapshot in master (#6052)
    Add CMake flag to log C API invocations, to aid debugging (#5925)
    
    * Add CMake flag to log C API invocations, to aid debugging
    
    * Remove unnecessary parentheses
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] update spark dependency to 3.0.0 (#5836)
    Bump com.esotericsoftware to 4.0.2 (#5690)
    
    Co-authored-by: Antti Saukko <antti.saukko@verizonmedia.com>
    Bump version to 1.2.0 snapshot in master (#5733)
    [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    upgrade version num (#4670)
    
    * upgrade version num
    
    * missign changes
    
    * fix version script
    
    * change versions
    
    * rm files
    
    * Update CMakeLists.txt
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [jvm-packages] Fix maven warnings (#4664)
    
    * exec plugin was missing a version
    * reportPlugins has been deprecated:
      see https://maven.apache.org/plugins/maven-site-plugin/maven-3.html#Classic_configuration_Maven_2__3
    [jvm-packages] updated kryo dependency to 2.22 (#4575)
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    [BLOCKING][CI] Upgrade to Spark 2.4.3 (#4414)
    
    * [CI] Upgrade to Spark 2.4.2
    
    * Pass Spark version to build script
    
    * Allow multiple --build-arg in ci_build.sh
    
    * Fix syntax
    
    * Fix container name
    
    * Update pom.xml
    
    * Fix container name
    
    * Update Jenkinsfile
    
    * Update pom.xml
    
    * Update Dockerfile.jvm_cross
    [jvm-packages] support spark 2.4 and compatibility test with previous xgboost version (#4377)
    
    * bump spark version
    
    * keep float.nan
    
    * handle brokenly changed name/value
    
    * add test
    
    * add model files
    
    * add model files
    
    * update doc
    [jvm-packages] logging version number (#4271)
    
    * print version number
    
    * add property file
    [jvm-packages] bump version for master (#4209)
    
    * update version
    
    * bump version
    [jvm-packages] upgrade spark version (#4170)
    [jvm-packages] fix the scalability issue of prediction (#4033)
    [jvm-packages] Fix #3898: use correct group ID for maven-site-plugin (#3937)
    [jvm-packages] update version to 0.82-SNAPSHOT (#3920)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update version
    
    * 0.82
    Release 0.81 version (#3864)
    
    * Release 0.81 version
    
    * Update NEWS.md
    [jvm-packages] Fix JVM doc build (#3853)
    
    To get around of the bug https://issues.apache.org/jira/browse/SUREFIRE-1588,
    set useSystemClassLoader=false.
    [jvm-packages] Upgrade Scala to 2.11.12 to address CVE-2017-15288 (#3816)
    
    A privilege escalation vulnerability (CVE-2017-15288) has been
    identified in the Scala compilation daemon. See
    https://nvd.nist.gov/vuln/detail/CVE-2017-15288
    
    Fix: Upgrade Scala to 2.11.12.
    [jvm-packages] bump spark version (#3709)
    [jvm-packages] bump flink version number (#3686)
    
    * bump flink version number
    
    * bump flink version number
    
    * add missing hadoop dependency
    [jvm-packages] add the missing scm urls (#3589)
    
    for some reason this part was missing in master branch????
    Update JVM packages version to 0.81-SNAPSHOT (#3584)
    Release version 0.80 (#3541)
    
    * Up versions
    
    * Write release note for 0.80
    [jvm-packages] Maven central release  stuffs (#3401)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * maven central release
    Update 0.8 version num (#3358)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update 0.80
    [jvm-packages] scripts to cross-build and deploy artifacts to github (#3276)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * cross building files
    
    * update
    
    * build with docker
    
    * remove
    
    * temp
    
    * update build script
    
    * update pom
    
    * update
    
    * update version
    
    * upload build
    
    * fix path
    
    * update README.md
    
    * fix compiler version to 4.8.5
    [jvm-packages] change version of jvm to keep consistent with other pkgs  (#3253)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * change version of jvm to keep consistent with other pkgs
    [jvm-packages] upgrade spark version to 2.3 (#3254)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update default spark version to 2.3
    [jvm-packages] Declared Spark as provided in the POM (#3093)
    
    * [jvm-packages] Explicitly declared Spark dependencies as provided
    
    * Removed noop spark-2.x profile
    [jvm-packages] add dev script to update version and update versions (#2998)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add dev script to update version and update versions
    Add Coverage Report for Java and Python (#2667)
    
    * Add coverage report for java
    
    * Add coverage report for python
    
    * Increase memory for JVM unit tests
    
    * Increase memory for JVM unit tests
    Removed 'flink.suffix' and added 'flink.version' (#2277)
    
    The former was just Scala binary tag, and the latter was hardcoded in
    the 'xgboost4j-flink' POM.
    [jvm-packages] Scala/Java interface for Fast Histogram Algorithm (#1966)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * first commit in scala binding for fast histo
    
    * java test
    
    * add missed scala tests
    
    * spark training
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * local change
    
    * first commit in scala binding for fast histo
    
    * local change
    
    * fix df frame test
    [jvm-packages] Bump spark to 2.1 (#2046)
    [jvm-packages] try setting default profile (#1891)
    
    * try setting default profile
    
    * add spark pipeline persistence
    
    * access spark session
    
    * copy paste sparks default parameter reader
    
    * remove unnecessary parameters, only change xml
    
    * remove unnecessary changes 2
    [jvm-packages] xgboost4j: publishing sources along with bins (#1797)
    
    * xgboost4j: publishing sources along with bins
    
    * description about building maven artifacts
    
    * publishing scala source to local m2 as well
    stylistic fix (#1789)
    
    * stylistic fix
    
    * try multiple repos
    
    * fix
    
    * fix
    [jvm-packages] Parameter tuning tool for XGBoost (#1664)
    [jvm-packages] add apache maven repo url and bump up default spark version to 2.0.1 (#1650)
    
    * add apache maven repo url and bump up default spark version to 2.0.1
    [jvm-packages] Integration with Spark Dataframe/Dataset (#1559)
    
    * bump up to scala 2.11
    
    * framework of data frame integration
    
    * test consistency between RDD and DataFrame
    
    * order preservation
    
    * test order preservation
    
    * example code and fix makefile
    
    * improve type checking
    
    * improve APIs
    
    * user docs
    
    * work around travis CI's limitation on log length
    
    * adjust test structure
    
    * integrate with Spark -1 .x
    
    * spark 2.x integration
    
    * remove spark 1.x implementation but provide instructions on how to downgrade
    [jvm-packages] bump to next version (#1535)
    
    * bump to next version
    
    * fix
    
    * fix
    specify spark version (#1224)
    support kryo serialization
    jvm doc index
    [DOC-JVM] Refactor JVM docs
    adjust the return values of RabitTracker.waitFor(), remove typesafe.Config
    distributed in RDD
    [JVM] Add LabeledPoint read support
    
    fix
    Add Labeled Point, minor fix build
    try to get more memory from Travis
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    sketch of xgboost-spark
    
    chooseBestBooster shall be in Boosters
    
    remove tracker.py
    
    rename XGBoost
    
    remove cross-validation
    add maven-assembly plugins
    add test cases for Scala API
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    Add option to enable all compiler warnings in GCC/Clang (#5897)
    
    * Add option to enable all compiler warnings in GCC/Clang
    
    * Fix -Wall for CUDA sources
    
    * Make -Wall private req for xgboost-r
    [CI] Simplify CMake build with modern CMake techniques (#5871)
    
    * [CI] Simplify CMake build
    
    * Make sure that plugins can be built
    
    * [CI] Install lz4 on Mac
    C++14 for xgboost (#5664)
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Fix prediction heuristic (#5955)
    
    
    * Relax check for prediction.
    * Relax test in spark test.
    * Add tests in C++.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    [jvm-packages] Fix up build for xgboost4j-gpu, xgboost4j-spark-gpu (#6216)
    
    * [CI] Clean up build for JVM packages
    
    * Use correct path for saving native lib
    
    * Fix groupId of maven-surefire-plugin
    
    * Fix stashing of xgboost4j_jar_gpu
    
    * [CI] Don't run xgboost4j-tester with GPU, since it doesn't use gpu_hist
    Add CMake flag to log C API invocations, to aid debugging (#5925)
    
    * Add CMake flag to log C API invocations, to aid debugging
    
    * Remove unnecessary parentheses
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix Windows 2016 build. (#5902)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Add BigDenseMatrix (#4383)
    
    * Add BigDenseMatrix
    
    * ability to create DMatrix with bigger than Integer.MAX_VALUE size arrays
    * uses sun.misc.Unsafe
    
    * make DMatrix test work from a jar as well
    use all cores to build on linux (#4304)
    Correct JVM CMake GPU flag. (#4071)
    Cmake improvements (#2487)
    
    * Cmake improvements
    * Add google test to cmake
    [jvm-packages] Another pack of build/CI improvements (#2422)
    
    * [jvm-packages] Fixed compilation on Windows
    
    * [jvm-packages] Build the JNI bindings on Appveyor
    
    * [jvm-packages] Build & test on OS X
    
    * [jvm-packages] Re-applied the CMake build changes reverted by #2395
    
    * Fixed Appveyor JVM build
    
    * Muted Maven on Travis
    
    * Don't link with libawt
    
    * "linux2"->"linux"
    
    Python2.x and 3.X use slightly different values for ``sys.platform``.
    [jvm-packages] Replaced create_jni.{bat,sh} with a Python version (#2383)
    
    * [jvm-packages] Replaced create_jni.{bat,sh} with a Python version
    
    This allows to have a single script for all platforms.
    
    * [jvm-packages] Added all configuration options to create_jni.py
    [jvm-packages] [doc] Update install doc for JVM packages (#6051)
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [jvm-packages] update local dev build process (#4640)
    [jvm-packages] a better explanation about the inconsistent issue (#3524)
    [jvm-packages] Maven central release  stuffs (#3401)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * maven central release
    [jvm-packages] Added latest version number example (#3374)
    
    * Added latest version number example
    
    * Added latest version number example
    [jvm-packages] scripts to cross-build and deploy artifacts to github (#3276)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * cross building files
    
    * update
    
    * build with docker
    
    * remove
    
    * temp
    
    * update build script
    
    * update pom
    
    * update
    
    * update version
    
    * upload build
    
    * fix path
    
    * update README.md
    
    * fix compiler version to 4.8.5
    [jvm-packages] add back libsvm notes (#3232)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add back libsvm notes
    remove stale examples (#2816)
    [jvm-packages] Integration with Spark Dataframe/Dataset (#1559)
    
    * bump up to scala 2.11
    
    * framework of data frame integration
    
    * test consistency between RDD and DataFrame
    
    * order preservation
    
    * test order preservation
    
    * example code and fix makefile
    
    * improve type checking
    
    * improve APIs
    
    * user docs
    
    * work around travis CI's limitation on log length
    
    * adjust test structure
    
    * integrate with Spark -1 .x
    
    * spark 2.x integration
    
    * remove spark 1.x implementation but provide instructions on how to downgrade
    Typos in README (#1326)
    
    * Inconsistency in libsvm formats
    
    * note on libsvm formats
    
    * typos in README
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md
    Inconsistency in libsvm formats (#1325)
    
    * Inconsistency in libsvm formats
    
    * note on libsvm formats
    [jvm-packages] xgboost4j-spark external memory (#1219)
    
    * implement external memory support for XGBoost4J
    
    * remove extra space
    
    * enable external memory for prediction
    
    * update doc
    [FLINK] remove nWorker from API
    support kryo serialization
    force the user to set number of workers
    update README for jvm-packages
    [Spark] Refactor train, predict, add save
    Update README.md
    Update README.md
    Add doc badge
    [DOC-JVM] Refactor JVM docs
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    add osx build instruction
    make some fix
    add java wrapper
    [jvm-packages] Fixed checkstyle excludes on Windows (#2370)
    
    XGBoostJNI.java was not excluded on Windows, probably because the path
    specified in 'checkstyle-suppressions.xml' used UNIX file separators.
    merge
    [refactor] move java package to namespace java
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    sketch of xgboost-spark
    
    chooseBestBooster shall be in Boosters
    
    remove tracker.py
    
    rename XGBoost
    
    remove cross-validation
    add test cases for Scala API
    add style check for java and scala code
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Bump com.esotericsoftware to 4.0.2 (#5690)
    
    Co-authored-by: Antti Saukko <antti.saukko@verizonmedia.com>
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Bump junit from 4.11 to 4.13.1 in /jvm-packages/xgboost4j (#6230)
    
    Bumps [junit](https://github.com/junit-team/junit4) from 4.11 to 4.13.1.
    - [Release notes](https://github.com/junit-team/junit4/releases)
    - [Changelog](https://github.com/junit-team/junit4/blob/main/doc/ReleaseNotes4.11.md)
    - [Commits](https://github.com/junit-team/junit4/compare/r4.11...r4.13.1)
    
    Signed-off-by: dependabot[bot] <support@github.com>
    
    Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
    [jvm-packages] Fix up build for xgboost4j-gpu, xgboost4j-spark-gpu (#6216)
    
    * [CI] Clean up build for JVM packages
    
    * Use correct path for saving native lib
    
    * Fix groupId of maven-surefire-plugin
    
    * Fix stashing of xgboost4j_jar_gpu
    
    * [CI] Don't run xgboost4j-tester with GPU, since it doesn't use gpu_hist
    Bump version to 1.3.0 snapshot in master (#6052)
    Add CMake flag to log C API invocations, to aid debugging (#5925)
    
    * Add CMake flag to log C API invocations, to aid debugging
    
    * Remove unnecessary parentheses
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Bump version to 1.2.0 snapshot in master (#5733)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    upgrade version num (#4670)
    
    * upgrade version num
    
    * missign changes
    
    * fix version script
    
    * change versions
    
    * rm files
    
    * Update CMakeLists.txt
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [jvm-packages] Fix maven warnings (#4664)
    
    * exec plugin was missing a version
    * reportPlugins has been deprecated:
      see https://maven.apache.org/plugins/maven-site-plugin/maven-3.html#Classic_configuration_Maven_2__3
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    do not filter shared library files (#4303)
    [jvm-packages] bump version for master (#4209)
    
    * update version
    
    * bump version
    [jvm-packages] update version to 0.82-SNAPSHOT (#3920)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update version
    
    * 0.82
    Release 0.81 version (#3864)
    
    * Release 0.81 version
    
    * Update NEWS.md
    Update JVM packages version to 0.81-SNAPSHOT (#3584)
    Release version 0.80 (#3541)
    
    * Up versions
    
    * Write release note for 0.80
    Update 0.8 version num (#3358)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update 0.80
    [jvm-packages] scripts to cross-build and deploy artifacts to github (#3276)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * cross building files
    
    * update
    
    * build with docker
    
    * remove
    
    * temp
    
    * update build script
    
    * update pom
    
    * update
    
    * update version
    
    * upload build
    
    * fix path
    
    * update README.md
    
    * fix compiler version to 4.8.5
    [jvm-packages] change version of jvm to keep consistent with other pkgs  (#3253)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * change version of jvm to keep consistent with other pkgs
    [jvm-packages] fix the pattern in dev script and version mismatch (#3009)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix the pattern in dev script and version mismatch
    [jvm-packages] add dev script to update version and update versions (#2998)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add dev script to update version and update versions
    [jvm-packages] Replaced create_jni.{bat,sh} with a Python version (#2383)
    
    * [jvm-packages] Replaced create_jni.{bat,sh} with a Python version
    
    This allows to have a single script for all platforms.
    
    * [jvm-packages] Added all configuration options to create_jni.py
    Minor improvements to xgboost/jvm-packages build  (#2356)
    
    * Specified 'exec-maven-plugin' version
    
    * Changed 'create_jni.sh' to fail on error
    
    and also report each of the executed commands, which makes it easier
    to debug.
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    [jvm-packages] XGBoost4j Windows fixes (#1639)
    
    * Changes for Mingw64 compilation to ensure long is a consistent size.
    
    Mainly impacts the Java API which would not compile, but there may be
    silent errors on Windows with large datasets before this patch (as long
    is 32-bits when compiled with mingw64 even in 64-bit mode).
    
    * Adding ifdefs to ensure it still compiles on MacOS
    
    * Makefile and create_jni.bat changes for Windows.
    
    * Switching XGDMatrixCreateFromCSREx JNI call to use size_t cast
    
    * Fixing lint error, adding profile switching to jvm-packages build to make create-jni.bat get called, adding myself to Contributors.Md
    [jvm-packages] bump to next version (#1535)
    
    * bump to next version
    
    * fix
    
    * fix
    run native lib building command from maven
    [DOC-JVM] Refactor JVM docs
    distributed in RDD
    try to get more memory from Travis
    sketch of xgboost-spark
    
    chooseBestBooster shall be in Boosters
    
    remove tracker.py
    
    rename XGBoost
    
    remove cross-validation
    add test cases for Scala API
    rename files/packages
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    add java wrapper
    Update LICENSE
    Squashed 'subtree/rabit/' changes from 1bb8fe9..4db0a62
    
    4db0a62 bugfix of lazy prepare
    87017bd license
    dc703e1 license
    c171440 change license to bsd
    7db2070 Update README.md
    581fe06 add mocktest
    d2f252f ok
    4a5b9e5 add all
    12ee049 init version of lbfgs
    37a2837 complete lbfgs solver
    6ade7cb complete lbfgs
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: 4db0a62a068894a55f70bad5e80c33d4434fc834
    license
    license
    change license to bsd
    Squashed 'subtree/rabit/' content from commit c7282ac
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: c7282acb2a92e1d6a32614889ff466fec58937f3
    check in license
    fix numpy convert
    chg license, README
    update license
    update license
    Initial commit
    [jvm-packages] Add getNumFeature method (#6075)
    
    * Add getNumFeature to the Java API
    * Add getNumFeature to the Scala API
    * Add unit tests for getNumFeature
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] Add BigDenseMatrix (#4383)
    
    * Add BigDenseMatrix
    
    * ability to create DMatrix with bigger than Integer.MAX_VALUE size arrays
    * uses sun.misc.Unsafe
    
    * make DMatrix test work from a jar as well
    Deprecate set group (#4864)
    
    
    
    * Convert jvm package and R package.
    
    * Restore for compatibility.
    [jvm-packages] Add methods operating attributes of booster in jvm package, which follow API design in python package. (#4336)
    [jvm-packages] support specified feature names when getModelDump and getFeatureScore (#3733)
    
    * [jvm-packages] support specified feature names for jvm when get ModelDump and get FeatureScore (#3725)
    
    * typo and style fix
    [jvm-packages] Expose json formatted booster dumps (#2233) (#2234)
    
    * Change Booster dump from XGBoosterDumpModel to XGBoosterDumpModelEx
    
    Allows exposing multiple formatting options of model dumping.
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    [jvm-packages] Robust dmatrix creation (#1613)
    
    * add back train method but mark as deprecated
    
    * robust matrix creation in jvm
    [refactor] move java package to namespace java
    [JVM] Add Iterator loading API
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    [JVM-PKG] Update JNI to include rabit codes
    rename files/packages
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    [jvm-packages] Add getNumFeature method (#6075)
    
    * Add getNumFeature to the Java API
    * Add getNumFeature to the Scala API
    * Add unit tests for getNumFeature
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [JVM Packages] Catch dmlc error by ref. (#5678)
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Add number of columns to native data iterator. (#5202)
    
    * Change native data iter into an adapter.
    Add JVM_CHECK_CALL. (#5199)
    
    * Added a check call macro in jvm package, prevents executing other functions
    from jvm when error occurred in XGBoost. For example, when prediction fails jvm
    should not try to allocate memory based on the output prediction size.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    [jvm-packages] Add BigDenseMatrix (#4383)
    
    * Add BigDenseMatrix
    
    * ability to create DMatrix with bigger than Integer.MAX_VALUE size arrays
    * uses sun.misc.Unsafe
    
    * make DMatrix test work from a jar as well
    Deprecate set group (#4864)
    
    
    
    * Convert jvm package and R package.
    
    * Restore for compatibility.
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    Add support for cross-validation using query ID (#4474)
    
    * adding support for matrix slicing with query ID for cross-validation
    
    * hail mary test of unrar installation for windows tests
    
    * trying to modify tests to run in Github CI
    
    * Remove dependency on wget and unrar
    
    * Save error log from R test
    
    * Relax assertion in test_training
    
    * Use int instead of bool in C function interface
    
    * Revise R interface
    
    * Add XGDMatrixSliceDMatrixEx and keep old XGDMatrixSliceDMatrix for API compatibility
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    [jvm-packages] Add methods operating attributes of booster in jvm package, which follow API design in python package. (#4336)
    [jvm-packages] support specified feature names when getModelDump and getFeatureScore (#3733)
    
    * [jvm-packages] support specified feature names for jvm when get ModelDump and get FeatureScore (#3725)
    
    * typo and style fix
    Convert handle == nullptr from SegFault to user-friendly error. (#3021)
    
    * Convert SegFault to user-friendly error.
    
    * Apply the change to DMatrix API as well
    [jvm-packages] Exposed train-time evaluation metrics (#2836)
    
    * [jvm-packages] Exposed train-time evaluation metrics
    
    They are accessible via 'XGBoostModel.summary'. The summary is not
    serialized with the model and is only available after the training.
    
    * Addressed review comments
    
    * Extracted model-related tests into 'XGBoostModelSuite'
    
    * Added tests for copying the 'XGBoostModel'
    
    * [jvm-packages] Fixed a subtle bug in train/test split
    
    Iterator.partition (naturally) assumes that the predicate is deterministic
    but this is not the case for
    
        r.nextDouble() <= trainTestRatio
    
    therefore sometimes the DMatrix(...) call got a NoSuchElementException
    and crashed the JVM due to lack of exception handling in
    XGBoost4jCallbackDataIterNext.
    
    * Make sure train/test objectives are different
    [jvm-packages] Another pack of build/CI improvements (#2422)
    
    * [jvm-packages] Fixed compilation on Windows
    
    * [jvm-packages] Build the JNI bindings on Appveyor
    
    * [jvm-packages] Build & test on OS X
    
    * [jvm-packages] Re-applied the CMake build changes reverted by #2395
    
    * Fixed Appveyor JVM build
    
    * Muted Maven on Travis
    
    * Don't link with libawt
    
    * "linux2"->"linux"
    
    Python2.x and 3.X use slightly different values for ``sys.platform``.
    [GPU-Plugin] Multi-GPU for grow_gpu_hist histogram method using NVIDIA NCCL. (#2395)
    [jvm-packages] Minor improvements to the CMake build (#2379)
    
    * [jvm-packages] Fixed JNI_OnLoad overload
    
    It does not compile on Windows without proper export flags.
    
    * [jvm-packages] Use JNI types directly where appropriate
    
    * Removed lib hack from CMake build
    
    Prior to this commit the CMake build use hardcoded lib prefix for
    libxgboost and libxgboost4j. Unfortunatelly this did not play well with
    Windows, which does not use the lib- prefix.
    libxgboost4j is now part of the CMake build (#2373)
    
    * [jvm-packages] Added libxgboost4j to CMake build
    
    * [jvm-packages] Wired CMake build into create_jni.sh
    
    * User newer CMake version on Travis
    
    * Lowered CMake version constraints
    
    * Fixed various quirks in the new CMake build
    [jvm-packages] Expose json formatted booster dumps (#2233) (#2234)
    
    * Change Booster dump from XGBoosterDumpModel to XGBoosterDumpModelEx
    
    Allows exposing multiple formatting options of model dumping.
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    [jvm-packages] XGBoost4j Windows fixes (#1639)
    
    * Changes for Mingw64 compilation to ensure long is a consistent size.
    
    Mainly impacts the Java API which would not compile, but there may be
    silent errors on Windows with large datasets before this patch (as long
    is 32-bits when compiled with mingw64 even in 64-bit mode).
    
    * Adding ifdefs to ensure it still compiles on MacOS
    
    * Makefile and create_jni.bat changes for Windows.
    
    * Switching XGDMatrixCreateFromCSREx JNI call to use size_t cast
    
    * Fixing lint error, adding profile switching to jvm-packages build to make create-jni.bat get called, adding myself to Contributors.Md
    [jvm-packages] Robust dmatrix creation (#1613)
    
    * add back train method but mark as deprecated
    
    * robust matrix creation in jvm
    [refactor] move java package to namespace java
    [JVM] Add Iterator loading API
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    [JVM-PKG] Update JNI to include rabit codes
    rename files/packages
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    modify java wrapper settings for new refactor
    modify jni code
    small change for jni wrapper
    refactor jni code and rename libxgboostjavawrapper.so to libxgboost4j.so
    update java wrapper for new fault handle API
    add java wrapper
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Robust dmatrix creation (#1613)
    
    * add back train method but mark as deprecated
    
    * robust matrix creation in jvm
    [jvm-packages] create dmatrix with specified missing value (#1272)
    
    * create dmatrix with specified missing value
    
    * update dmlc-core
    
    * support for predict method in spark package
    
    repartitioning
    
    work around
    
    * add more elements to work around training set empty partition issue
    [refactor] move java package to namespace java
    [refactor] move java package to namespace java
    add test cases for Scala API
    [jvm-packages] Add getNumFeature method (#6075)
    
    * Add getNumFeature to the Java API
    * Add getNumFeature to the Scala API
    * Add unit tests for getNumFeature
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Allow supression of Rabit output in Booster::train in xgboost4j (#4262)
    
    * Make train in xgboost4j respect print params
    
    Previously no setting in params argument of Booster::train would prevent
    the Rabit.trackerPrint call. This can fill up a lot of screen space in
    the case that many folds are being trained.
    * Setting "silent" in this map to "true", "True", a non-zero integer, or
      a string that can be parsed to such an int will prevent printing.
    * Setting "verbose_eval" to "False" or "false" will prevent printing.
    * Setting "verbose_eval" to an int (or a String parseable to an int) n
      will result in printing every n steps, or no printing is n is zero.
    
    This is to match the python behaviour described here:
    https://www.kaggle.com/c/rossmann-store-sales/discussion/17499
    
    * Fixed 'slient' typo in xgboost4j test
    
    * private access on two methods
    Separate Depthwidth and Lossguide growing policy in fast histogram (#4102)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * more changes
    
    * temp
    
    * update
    
    * udpate rabit
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * broadcast subsampled feature correctly
    
    * init col
    
    * temp
    
    * col sampling
    
    * fix histmastrix init
    
    * fix col sampling
    
    * remove cout
    
    * fix out of bound access
    
    * fix core dump
    
    remove core dump file
    
    * disbale test temporarily
    
    * update
    
    * add fid
    
    * print perf data
    
    * update
    
    * revert some changes
    
    * temp
    
    * temp
    
    * pass all tests
    
    * bring back some tests
    
    * recover some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * recover column init part
    
    * more recovery
    
    * fix core dumps
    
    * code clean
    
    * revert some changes
    
    * fix test compilation issue
    
    * fix lint issue
    
    * resolve compilation issue
    
    * fix issues of lint caused by rebase
    
    * fix stylistic changes and change variable names
    
    * use regtree internal function
    
    * modularize depth width
    
    * address the comments
    
    * fix failed tests
    
    * wrap perf timers with class
    
    * fix lint
    
    * fix num_leaves count
    
    * fix indention
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * merge
    
    * fix compilation
    [jvm-packages] Prevent dispose being called on unfinalized JBooster (#3005)
    
    * [jvm-packages] Prevent dispose being called twice when finalize
    
    * Convert SIGSEGV to XGBoostError
    
    * Avoid creating a new SBooster with the same JBooster
    
    * Address CR Comments
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    [jvm-packages] Scala/Java interface for Fast Histogram Algorithm (#1966)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * first commit in scala binding for fast histo
    
    * java test
    
    * add missed scala tests
    
    * spark training
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * local change
    
    * first commit in scala binding for fast histo
    
    * local change
    
    * fix df frame test
    adjust the API signature as well as the docs
    revise current API
    [refactor] move java package to namespace java
    sketch of xgboost-spark
    
    chooseBestBooster shall be in Boosters
    
    remove tracker.py
    
    rename XGBoost
    
    remove cross-validation
    add test cases for Scala API
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    Fixed Exception handling for fragmented Rabit 'print' tracker command. Fixed unit test. (#2081)
    [jvm-packages] Fixed java.nio.BufferUnderFlow issue in Scala Rabit tracker. (#1993)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    
    * Fixed BufferUnderFlow bug in decoding tracker 'print' command.
    
    * Merge conflicts resolution.
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    [jvm-packages] Add getNumFeature method (#6075)
    
    * Add getNumFeature to the Java API
    * Add getNumFeature to the Scala API
    * Add unit tests for getNumFeature
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Set correct file permission. (#4964)
    [jvm-packages] jvm test should clean up after themselfs (#4706)
    [jvm-packages] Add methods operating attributes of booster in jvm package, which follow API design in python package. (#4336)
    [jvm-packages] Fix early stop with xgboost4j-spark (#4176)
    
    * Fix early stop with xgboost4j-spark
    
    * Update XGBoost.java
    
    * Update XGBoost.java
    
    * Update XGBoost.java
    
    To use -Float.MAX_VALUE as the lower bound, in case there is positive metric.
    
    * Only update best score if the current score is better (no update when equal)
    
    * Update xgboost-spark tutorial to fix early stopping docs.
    Separate Depthwidth and Lossguide growing policy in fast histogram (#4102)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * more changes
    
    * temp
    
    * update
    
    * udpate rabit
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * broadcast subsampled feature correctly
    
    * init col
    
    * temp
    
    * col sampling
    
    * fix histmastrix init
    
    * fix col sampling
    
    * remove cout
    
    * fix out of bound access
    
    * fix core dump
    
    remove core dump file
    
    * disbale test temporarily
    
    * update
    
    * add fid
    
    * print perf data
    
    * update
    
    * revert some changes
    
    * temp
    
    * temp
    
    * pass all tests
    
    * bring back some tests
    
    * recover some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * recover column init part
    
    * more recovery
    
    * fix core dumps
    
    * code clean
    
    * revert some changes
    
    * fix test compilation issue
    
    * fix lint issue
    
    * resolve compilation issue
    
    * fix issues of lint caused by rebase
    
    * fix stylistic changes and change variable names
    
    * use regtree internal function
    
    * modularize depth width
    
    * address the comments
    
    * fix failed tests
    
    * wrap perf timers with class
    
    * fix lint
    
    * fix num_leaves count
    
    * fix indention
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * merge
    
    * fix compilation
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    [jvm-packages] Updates to Java Booster to support other feature importance measures (#3801)
    
    * Updates to Booster to support other feature importances
    
    * Add returns for Java methods
    
    * Pass Scala style checks
    
    * Pass Java style checks
    
    * Fix indents
    
    * Use class instead of enum
    
    * Return map string double
    
    * A no longer broken build, thanks to mvn package local build
    
    * Add a unit test to increase code coverage back
    
    * Address code review on main code
    
    * Add more unit tests for different feature importance scores
    
    * Address more CR
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages]Fix early stopping condition (#3928)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update version
    
    * 0.82
    
    * fix early stopping condition
    
    * remove unused
    
    * update comments
    
    * udpate comments
    
    * update test
    [Blocking][jvm-packages] fix the early stopping feature (#3808)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * temp
    
    * add method for classifier and regressor
    
    * update tutorial
    
    * address the comments
    
    * update
    [jvm-packages] support specified feature names when getModelDump and getFeatureScore (#3733)
    
    * [jvm-packages] support specified feature names for jvm when get ModelDump and get FeatureScore (#3725)
    
    * typo and style fix
    [jvm-packages] Saving models into a tmp folder every a few rounds (#2964)
    
    * [jvm-packages] Train Booster from an existing model
    
    * Align Scala API with Java API
    
    * Existing model should not load rabit checkpoint
    
    * Address minor comments
    
    * Implement saving temporary boosters and loading previous booster
    
    * Add more unit tests for loadPrevBooster
    
    * Add params to XGBoostEstimator
    
    * (1) Move repartition out of the temp model saving loop (2) Address CR comments
    
    * Catch a corner case of training next model with fewer rounds
    
    * Address comments
    
    * Refactor newly added methods into TmpBoosterManager
    
    * Add two files which is missing in previous commit
    
    * Rename TmpBooster to checkpoint
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    Fix documentation for a misspelled parameter (#2569)
    [jvm-packages] Expose json formatted booster dumps (#2233) (#2234)
    
    * Change Booster dump from XGBoosterDumpModel to XGBoosterDumpModelEx
    
    Allows exposing multiple formatting options of model dumping.
    [jvm-packages] Scala/Java interface for Fast Histogram Algorithm (#1966)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * first commit in scala binding for fast histo
    
    * java test
    
    * add missed scala tests
    
    * spark training
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * local change
    
    * first commit in scala binding for fast histo
    
    * local change
    
    * fix df frame test
    adjust the API signature as well as the docs
    revise current API
    [JVM] Refactor, add filesys API
    [refactor] move java package to namespace java
    sketch of xgboost-spark
    
    chooseBestBooster shall be in Boosters
    
    remove tracker.py
    
    rename XGBoost
    
    remove cross-validation
    apply  google-java-style indentation and impose import orders....
    add test cases for Scala API
    rename files/packages
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    [jvm-packages] Add BigDenseMatrix (#4383)
    
    * Add BigDenseMatrix
    
    * ability to create DMatrix with bigger than Integer.MAX_VALUE size arrays
    * uses sun.misc.Unsafe
    
    * make DMatrix test work from a jar as well
    Add public group getter for java and scala (#4838)
    
    * Add public group getter for java and scala
    
    * Remove unnecessary param from javadoc
    
    * Fix typo
    
    * Fix another typo
    
    * Add semicolon
    
    * Fix javadoc return statement
    
    * Fix missing return statement
    
    * Add a unit test
    [jvm-packages] Added baseMargin to ml.dmlc.xgboost4j.LabeledPoint (#2532)
    
    * Converted ml.dmlc.xgboost4j.LabeledPoint to Scala
    
    This allows to easily integrate LabeledPoint with Spark DataFrame APIs,
    which support encoding/decoding case classes out of the box. Alternative
    solution would be to keep LabeledPoint in Java and make it a Bean by
    generating boilerplate getters/setters. I have decided against that, even
    thought the conversion in this PR implies a public API change.
    
    I also had to remove the factory methods fromSparseVector and
    fromDenseVector because a) they would need to be duplicated to support
    overloaded calls with extra data (e.g. weight); and b) Scala would expose
    them via mangled $.MODULE$ which looks ugly in Java.
    
    Additionally, this commit makes it possible to switch to LabeledPoint in
    all public APIs and effectively to pass initial margin/group as part of
    the point. This seems to be the only reliable way of implementing distributed
    learning with these data. Note that group size format used by single-node
    XGBoost is not compatible with that scenario, since the partition split
    could divide a group into two chunks.
    
    * Switched to ml.dmlc.xgboost4j.LabeledPoint in RDD-based public APIs
    
    Note that DataFrame-based and Flink APIs are not affected by this change.
    
    * Removed baseMargin argument in favour of the LabeledPoint field
    
    * Do a single pass over the partition in buildDistributedBoosters
    
    Note that there is no formal guarantee that
    
        val repartitioned = rdd.repartition(42)
        repartitioned.zipPartitions(repartitioned.map(_ + 1)) { it1, it2, => ... }
    
    would do a single shuffle, but in practice it seems to be always the case.
    
    * Exposed baseMargin in DataFrame-based API
    
    * Addressed review comments
    
    * Pass baseMargin to XGBoost.trainWithDataFrame via params
    
    * Reverted MLLabeledPoint in Spark APIs
    
    As discussed, baseMargin would only be supported for DataFrame-based APIs.
    
    * Cleaned up baseMargin tests
    
    - Removed RDD-based test, since the option is no longer exposed via
      public APIs
    - Changed DataFrame-based one to check that adding a margin actually
      affects the prediction
    
    * Pleased Scalastyle
    
    * Addressed more review comments
    
    * Pleased scalastyle again
    
    * Fixed XGBoost.fromBaseMarginsToArray
    
    which always returned an array of NaNs even if base margin was not
    specified. Surprisingly this only failed a few tests.
    [jvm-packages] Robust dmatrix creation (#1613)
    
    * add back train method but mark as deprecated
    
    * robust matrix creation in jvm
    [jvm-packages] create dmatrix with specified missing value (#1272)
    
    * create dmatrix with specified missing value
    
    * update dmlc-core
    
    * support for predict method in spark package
    
    repartitioning
    
    work around
    
    * add more elements to work around training set empty partition issue
    [JVM] Add LabeledPoint read support
    
    fix
    [refactor] move java package to namespace java
    [JVM] Add Iterator loading API
    apply  google-java-style indentation and impose import orders....
    add test cases for Scala API
    rename files/packages
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    [jvm-packages] logging version number (#4271)
    
    * print version number
    
    * add property file
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Add some documentation to xgboost4j-spark plus minor style edits (#2823)
    
    * add scala docs to several methods
    
    * indentation
    
    * license formatting
    
    * clarify distributed boosters
    
    * address some review comments
    
    * reduce doc lengths
    
    * change method name, clarify  doc
    
    * reset make config
    
    * delete most comments
    
    * more review feedback
    Returning back LabeledPoint into public, in referece to the discussion in : https://github.com/dmlc/xgboost/pull/2532#discussion_r137172759 (#2677)
    [jvm-packages] Added baseMargin to ml.dmlc.xgboost4j.LabeledPoint (#2532)
    
    * Converted ml.dmlc.xgboost4j.LabeledPoint to Scala
    
    This allows to easily integrate LabeledPoint with Spark DataFrame APIs,
    which support encoding/decoding case classes out of the box. Alternative
    solution would be to keep LabeledPoint in Java and make it a Bean by
    generating boilerplate getters/setters. I have decided against that, even
    thought the conversion in this PR implies a public API change.
    
    I also had to remove the factory methods fromSparseVector and
    fromDenseVector because a) they would need to be duplicated to support
    overloaded calls with extra data (e.g. weight); and b) Scala would expose
    them via mangled $.MODULE$ which looks ugly in Java.
    
    Additionally, this commit makes it possible to switch to LabeledPoint in
    all public APIs and effectively to pass initial margin/group as part of
    the point. This seems to be the only reliable way of implementing distributed
    learning with these data. Note that group size format used by single-node
    XGBoost is not compatible with that scenario, since the partition split
    could divide a group into two chunks.
    
    * Switched to ml.dmlc.xgboost4j.LabeledPoint in RDD-based public APIs
    
    Note that DataFrame-based and Flink APIs are not affected by this change.
    
    * Removed baseMargin argument in favour of the LabeledPoint field
    
    * Do a single pass over the partition in buildDistributedBoosters
    
    Note that there is no formal guarantee that
    
        val repartitioned = rdd.repartition(42)
        repartitioned.zipPartitions(repartitioned.map(_ + 1)) { it1, it2, => ... }
    
    would do a single shuffle, but in practice it seems to be always the case.
    
    * Exposed baseMargin in DataFrame-based API
    
    * Addressed review comments
    
    * Pass baseMargin to XGBoost.trainWithDataFrame via params
    
    * Reverted MLLabeledPoint in Spark APIs
    
    As discussed, baseMargin would only be supported for DataFrame-based APIs.
    
    * Cleaned up baseMargin tests
    
    - Removed RDD-based test, since the option is no longer exposed via
      public APIs
    - Changed DataFrame-based one to check that adding a margin actually
      affects the prediction
    
    * Pleased Scalastyle
    
    * Addressed more review comments
    
    * Pleased scalastyle again
    
    * Fixed XGBoost.fromBaseMarginsToArray
    
    which always returned an array of NaNs even if base margin was not
    specified. Surprisingly this only failed a few tests.
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] fix return type of setEvalSets (#4105)
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages] Spark pipeline persistence (#1906)
    
    [jvm-packages] Spark pipeline persistence
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    add java wrapper
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    Add public group getter for java and scala (#4838)
    
    * Add public group getter for java and scala
    
    * Remove unnecessary param from javadoc
    
    * Fix typo
    
    * Fix another typo
    
    * Add semicolon
    
    * Fix javadoc return statement
    
    * Fix missing return statement
    
    * Add a unit test
    [jvm-packages] Robust dmatrix creation (#1613)
    
    * add back train method but mark as deprecated
    
    * robust matrix creation in jvm
    [jvm-packages] create dmatrix with specified missing value (#1272)
    
    * create dmatrix with specified missing value
    
    * update dmlc-core
    
    * support for predict method in spark package
    
    repartitioning
    
    work around
    
    * add more elements to work around training set empty partition issue
    fix examples
    [JVM] Add LabeledPoint read support
    
    fix
    [refactor] move java package to namespace java
    [refactor] move java package to namespace java
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    add test cases for Scala API
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] separate classification and regression model and integrate with ML package (#1608)
    [refactor] move java package to namespace java
    add test cases for Scala API
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    Fix warnings when generating javadoc
    add java wrapper
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Prevent dispose being called on unfinalized JBooster (#3005)
    
    * [jvm-packages] Prevent dispose being called twice when finalize
    
    * Convert SIGSEGV to XGBoostError
    
    * Avoid creating a new SBooster with the same JBooster
    
    * Address CR Comments
    [jvm-packages] Saving models into a tmp folder every a few rounds (#2964)
    
    * [jvm-packages] Train Booster from an existing model
    
    * Align Scala API with Java API
    
    * Existing model should not load rabit checkpoint
    
    * Address minor comments
    
    * Implement saving temporary boosters and loading previous booster
    
    * Add more unit tests for loadPrevBooster
    
    * Add params to XGBoostEstimator
    
    * (1) Move repartition out of the temp model saving loop (2) Address CR comments
    
    * Catch a corner case of training next model with fewer rounds
    
    * Address comments
    
    * Refactor newly added methods into TmpBoosterManager
    
    * Add two files which is missing in previous commit
    
    * Rename TmpBooster to checkpoint
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    [jvm-packages] Scala/Java interface for Fast Histogram Algorithm (#1966)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * first commit in scala binding for fast histo
    
    * java test
    
    * add missed scala tests
    
    * spark training
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * local change
    
    * first commit in scala binding for fast histo
    
    * local change
    
    * fix df frame test
    [jvm-packages] Parameter tuning tool for XGBoost (#1664)
    adjust the API signature as well as the docs
    [DOC-JVM] Refactor JVM docs
    example of DistTrainWithSpark and trigger job with foreachPartition
    [JVM] Refactor, add filesys API
    [refactor] move java package to namespace java
    [refactor] move java package to namespace java
    sketch of xgboost-spark
    
    chooseBestBooster shall be in Boosters
    
    remove tracker.py
    
    rename XGBoost
    
    remove cross-validation
    apply  google-java-style indentation and impose import orders....
    add default values for Scala API
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] fix comments in objectiveTrait (#4174)
    [refactor] move java package to namespace java
    [refactor] move java package to namespace java
    add test cases for Scala API
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    add java wrapper
    [jvm-packages] Add getNumFeature method (#6075)
    
    * Add getNumFeature to the Java API
    * Add getNumFeature to the Scala API
    * Add unit tests for getNumFeature
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Add methods operating attributes of booster in jvm package, which follow API design in python package. (#4336)
    [jvm-packages] Updates to Java Booster to support other feature importance measures (#3801)
    
    * Updates to Booster to support other feature importances
    
    * Add returns for Java methods
    
    * Pass Scala style checks
    
    * Pass Java style checks
    
    * Fix indents
    
    * Use class instead of enum
    
    * Return map string double
    
    * A no longer broken build, thanks to mvn package local build
    
    * Add a unit test to increase code coverage back
    
    * Address code review on main code
    
    * Add more unit tests for different feature importance scores
    
    * Address more CR
    [jvm-packages] support specified feature names when getModelDump and getFeatureScore (#3733)
    
    * [jvm-packages] support specified feature names for jvm when get ModelDump and get FeatureScore (#3725)
    
    * typo and style fix
    [jvm-packages] enable predictLeaf/predictContrib/treeLimit in 0.8  (#3532)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * partial finish
    
    * no test
    
    * add test cases
    
    * add test cases
    
    * address comments
    
    * add test for regressor
    
    * fix typo
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] Add back the overriden finalize() method for SBooster (#3011)
    
    * Convert SIGSEGV to XGBoostError
    
    * Address CR Comments
    
    * Address CR Comments
    [jvm-packages] Prevent dispose being called on unfinalized JBooster (#3005)
    
    * [jvm-packages] Prevent dispose being called twice when finalize
    
    * Convert SIGSEGV to XGBoostError
    
    * Avoid creating a new SBooster with the same JBooster
    
    * Address CR Comments
    [jvm-packages] Saving models into a tmp folder every a few rounds (#2964)
    
    * [jvm-packages] Train Booster from an existing model
    
    * Align Scala API with Java API
    
    * Existing model should not load rabit checkpoint
    
    * Address minor comments
    
    * Implement saving temporary boosters and loading previous booster
    
    * Add more unit tests for loadPrevBooster
    
    * Add params to XGBoostEstimator
    
    * (1) Move repartition out of the temp model saving loop (2) Address CR comments
    
    * Catch a corner case of training next model with fewer rounds
    
    * Address comments
    
    * Refactor newly added methods into TmpBoosterManager
    
    * Add two files which is missing in previous commit
    
    * Rename TmpBooster to checkpoint
    [jvm-packages] Expose prediction feature contribution on the Java side  (#2441)
    
    * Exposed prediction feature contribution on the Java side
    
    * was not supplying the newly added argument
    
    * Exposed from Scala-side as well
    
    * formatting (keep declaration in one line unless exceeding 100 chars)
    [jvm-packages] Expose json dumps to scala (#2247)
    
    * Add parameter passthru of format on Booster.getModelDump
    support kryo serialization
    revise current API
    example of DistTrainWithSpark and trigger job with foreachPartition
    [JVM] Refactor, add filesys API
    [refactor] move java package to namespace java
    [JVM] Make JVM Serializable
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [jvm-packages] RabitTracker for Scala: allow specifying host ip from the xgboost-tracker.properties file (#3833)
    [jvm-packages] Deterministically XGBoost training on exception (#2405)
    
    Previously the code relied on the tracker process being terminated
    by the OS, which was not the case on Windows.
    
    Closes #2394
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    Fixed Exception handling for fragmented Rabit 'print' tracker command. Fixed unit test. (#2081)
    [jvm-packages] Fixed java.nio.BufferUnderFlow issue in Scala Rabit tracker. (#1993)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    
    * Fixed BufferUnderFlow bug in decoding tracker 'print' command.
    
    * Merge conflicts resolution.
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] remove shutdown of handler shutdown (#4224)
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    make IEvaluation serializable (#1487)
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    Fix warnings when generating javadoc
    add java wrapper
    [jvm-packages] Add BigDenseMatrix (#4383)
    
    * Add BigDenseMatrix
    
    * ability to create DMatrix with bigger than Integer.MAX_VALUE size arrays
    * uses sun.misc.Unsafe
    
    * make DMatrix test work from a jar as well
    Deprecate set group (#4864)
    
    
    
    * Convert jvm package and R package.
    
    * Restore for compatibility.
    Add public group getter for java and scala (#4838)
    
    * Add public group getter for java and scala
    
    * Remove unnecessary param from javadoc
    
    * Fix typo
    
    * Fix another typo
    
    * Add semicolon
    
    * Fix javadoc return statement
    
    * Fix missing return statement
    
    * Add a unit test
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [jvm-packages] Exposed baseMargin (#2450)
    
    * Disabled excessive Spark logging in tests
    
    * Fixed a singature of XGBoostModel.predict
    
    Prior to this commit XGBoostModel.predict produced an RDD with
    an array of predictions for each partition, effectively changing
    the shape wrt the input RDD. A more natural contract for prediction
    API is that given an RDD it returns a new RDD with the same number
    of elements. This allows the users to easily match inputs with
    predictions.
    
    This commit removes one layer of nesting in XGBoostModel.predict output.
    Even though the change is clearly non-backward compatible, I still
    think it is well justified.
    
    * Removed boxing in XGBoost.fromDenseToSparseLabeledPoints
    
    * Inlined XGBoost.repartitionData
    
    An if is more explicit than an opaque method name.
    
    * Moved XGBoost.convertBoosterToXGBoostModel to XGBoostModel
    
    * Check the input dimension in DMatrix.setBaseMargin
    
    Prior to this commit providing an array of incorrect dimensions would
    have resulted in memory corruption. Maybe backport this to C++?
    
    * Reduced nesting in XGBoost.buildDistributedBoosters
    
    * Ensured consistent naming of the params map
    
    * Cleaned up DataBatch to make it easier to comprehend
    
    * Made scalastyle happy
    
    * Added baseMargin to XGBoost.train and trainWithRDD
    
    * Deprecated XGBoost.train
    
    It is ambiguous and work only for RDDs.
    
    * Addressed review comments
    
    * Revert "Fixed a singature of XGBoostModel.predict"
    
    This reverts commit 06bd5dcae7780265dd57e93ed7d4135f4e78f9b4.
    
    * Addressed more review comments
    
    * Fixed NullPointerException in buildDistributedBoosters
    [jvm-packages] JNI Cosmetics (#2448)
    
    * [jvm-packages] Ensure the native library is loaded once
    
    Previously any class using XGBoostJNI queried NativeLibLoader to make
    sure the native library is loaded. This commit moves the initXGBoost
    call to XGBoostJNI, effectively delegating the initialization to the class
    loader.
    
    Note also, that now XGBoostJNI would NOT suppress an IOException if it
    occured in initXGBoost.
    
    * [jvm-packages] Fused JNIErrorHandle with XGBoostJNI
    
    There was no reason for having a separate class.
    [jvm-packages] Robust dmatrix creation (#1613)
    
    * add back train method but mark as deprecated
    
    * robust matrix creation in jvm
    [jvm-packages] create dmatrix with specified missing value (#1272)
    
    * create dmatrix with specified missing value
    
    * update dmlc-core
    
    * support for predict method in spark package
    
    repartitioning
    
    work around
    
    * add more elements to work around training set empty partition issue
    [JVM] Refactor, add filesys API
    [JVM] Add LabeledPoint read support
    
    fix
    [refactor] move java package to namespace java
    [refactor] move java package to namespace java
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    apply  google-java-style indentation and impose import orders....
    [JVM-PKG] Update JNI to include rabit codes
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    [jvm-packages] JNI Cosmetics (#2448)
    
    * [jvm-packages] Ensure the native library is loaded once
    
    Previously any class using XGBoostJNI queried NativeLibLoader to make
    sure the native library is loaded. This commit moves the initXGBoost
    call to XGBoostJNI, effectively delegating the initialization to the class
    loader.
    
    Note also, that now XGBoostJNI would NOT suppress an IOException if it
    occured in initXGBoost.
    
    * [jvm-packages] Fused JNIErrorHandle with XGBoostJNI
    
    There was no reason for having a separate class.
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    fix the merge
    example of DistTrainWithSpark and trigger job with foreachPartition
    [JVM] Refactor, add filesys API
    [refactor] move java package to namespace java
    [refactor] move java package to namespace java
    apply  google-java-style indentation and impose import orders....
    [JVM-PKG] Update JNI to include rabit codes
    [jvm-packages] Deterministically XGBoost training on exception (#2405)
    
    Previously the code relied on the tracker process being terminated
    by the OS, which was not the case on Windows.
    
    Closes #2394
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    [jvm-packages] Allow supression of Rabit output in Booster::train in xgboost4j (#4262)
    
    * Make train in xgboost4j respect print params
    
    Previously no setting in params argument of Booster::train would prevent
    the Rabit.trackerPrint call. This can fill up a lot of screen space in
    the case that many folds are being trained.
    * Setting "silent" in this map to "true", "True", a non-zero integer, or
      a string that can be parsed to such an int will prevent printing.
    * Setting "verbose_eval" to "False" or "false" will prevent printing.
    * Setting "verbose_eval" to an int (or a String parseable to an int) n
      will result in printing every n steps, or no printing is n is zero.
    
    This is to match the python behaviour described here:
    https://www.kaggle.com/c/rossmann-store-sales/discussion/17499
    
    * Fixed 'slient' typo in xgboost4j test
    
    * private access on two methods
    [jvm-packages] Fix early stop with xgboost4j-spark (#4176)
    
    * Fix early stop with xgboost4j-spark
    
    * Update XGBoost.java
    
    * Update XGBoost.java
    
    * Update XGBoost.java
    
    To use -Float.MAX_VALUE as the lower bound, in case there is positive metric.
    
    * Only update best score if the current score is better (no update when equal)
    
    * Update xgboost-spark tutorial to fix early stopping docs.
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages]Fix early stopping condition (#3928)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update version
    
    * 0.82
    
    * fix early stopping condition
    
    * remove unused
    
    * update comments
    
    * udpate comments
    
    * update test
    [Blocking][jvm-packages] fix the early stopping feature (#3808)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * temp
    
    * add method for classifier and regressor
    
    * update tutorial
    
    * address the comments
    
    * update
    [jvm-packages] Saving models into a tmp folder every a few rounds (#2964)
    
    * [jvm-packages] Train Booster from an existing model
    
    * Align Scala API with Java API
    
    * Existing model should not load rabit checkpoint
    
    * Address minor comments
    
    * Implement saving temporary boosters and loading previous booster
    
    * Add more unit tests for loadPrevBooster
    
    * Add params to XGBoostEstimator
    
    * (1) Move repartition out of the temp model saving loop (2) Address CR comments
    
    * Catch a corner case of training next model with fewer rounds
    
    * Address comments
    
    * Refactor newly added methods into TmpBoosterManager
    
    * Add two files which is missing in previous commit
    
    * Rename TmpBooster to checkpoint
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    [jvm-packages] Scala/Java interface for Fast Histogram Algorithm (#1966)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * first commit in scala binding for fast histo
    
    * java test
    
    * add missed scala tests
    
    * spark training
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * local change
    
    * first commit in scala binding for fast histo
    
    * local change
    
    * fix df frame test
    adjust the API signature as well as the docs
    example of DistTrainWithSpark and trigger job with foreachPartition
    [JVM] Refactor, add filesys API
    [refactor] move java package to namespace java
    sketch of xgboost-spark
    
    chooseBestBooster shall be in Boosters
    
    remove tracker.py
    
    rename XGBoost
    
    remove cross-validation
    apply  google-java-style indentation and impose import orders....
    [JVM-PKG] add distributed test simple case
    [JVM-PKG] Update JNI to include rabit codes
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    Parameterize host-ip to pass to tracker.py (#2831)
    [jvm-packages] Add getNumFeature method (#6075)
    
    * Add getNumFeature to the Java API
    * Add getNumFeature to the Scala API
    * Add unit tests for getNumFeature
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] XGBoost Spark should deal with NaN when parsing evaluation output (#5546)
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    [jvm-packages]fix silly bug in feature scoring (#4604)
    [jvm-packages] Add methods operating attributes of booster in jvm package, which follow API design in python package. (#4336)
    [jvm-packages] Updates to Java Booster to support other feature importance measures (#3801)
    
    * Updates to Booster to support other feature importances
    
    * Add returns for Java methods
    
    * Pass Scala style checks
    
    * Pass Java style checks
    
    * Fix indents
    
    * Use class instead of enum
    
    * Return map string double
    
    * A no longer broken build, thanks to mvn package local build
    
    * Add a unit test to increase code coverage back
    
    * Address code review on main code
    
    * Add more unit tests for different feature importance scores
    
    * Address more CR
    [jvm-packages] support specified feature names when getModelDump and getFeatureScore (#3733)
    
    * [jvm-packages] support specified feature names for jvm when get ModelDump and get FeatureScore (#3725)
    
    * typo and style fix
    Eliminate use of System.out + proper error logging (#3572)
    [jvm-packages] Saving models into a tmp folder every a few rounds (#2964)
    
    * [jvm-packages] Train Booster from an existing model
    
    * Align Scala API with Java API
    
    * Existing model should not load rabit checkpoint
    
    * Address minor comments
    
    * Implement saving temporary boosters and loading previous booster
    
    * Add more unit tests for loadPrevBooster
    
    * Add params to XGBoostEstimator
    
    * (1) Move repartition out of the temp model saving loop (2) Address CR comments
    
    * Catch a corner case of training next model with fewer rounds
    
    * Address comments
    
    * Refactor newly added methods into TmpBoosterManager
    
    * Add two files which is missing in previous commit
    
    * Rename TmpBooster to checkpoint
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    [jvm-packages] Expose prediction feature contribution on the Java side  (#2441)
    
    * Exposed prediction feature contribution on the Java side
    
    * was not supplying the newly added argument
    
    * Exposed from Scala-side as well
    
    * formatting (keep declaration in one line unless exceeding 100 chars)
    [jvm-packages] JNI Cosmetics (#2448)
    
    * [jvm-packages] Ensure the native library is loaded once
    
    Previously any class using XGBoostJNI queried NativeLibLoader to make
    sure the native library is loaded. This commit moves the initXGBoost
    call to XGBoostJNI, effectively delegating the initialization to the class
    loader.
    
    Note also, that now XGBoostJNI would NOT suppress an IOException if it
    occured in initXGBoost.
    
    * [jvm-packages] Fused JNIErrorHandle with XGBoostJNI
    
    There was no reason for having a separate class.
    [jvm-packages] Expose json formatted booster dumps (#2233) (#2234)
    
    * Change Booster dump from XGBoosterDumpModel to XGBoosterDumpModelEx
    
    Allows exposing multiple formatting options of model dumping.
    [jvm-packages] Scala/Java interface for Fast Histogram Algorithm (#1966)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * first commit in scala binding for fast histo
    
    * java test
    
    * add missed scala tests
    
    * spark training
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * local change
    
    * first commit in scala binding for fast histo
    
    * local change
    
    * fix df frame test
    support kryo serialization
    fix the merge
    example of DistTrainWithSpark and trigger job with foreachPartition
    [JVM] Refactor, add filesys API
    [refactor] move java package to namespace java
    [JVM] Make JVM Serializable
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    add java wrapper
    Parameterize host-ip to pass to tracker.py (#2831)
    [jvm-packages] Deterministically XGBoost training on exception (#2405)
    
    Previously the code relied on the tracker process being terminated
    by the OS, which was not the case on Windows.
    
    Closes #2394
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    revise current API
    log tracker exit value  in logger
    
    capture InterruptedException
    adjust the return values of RabitTracker.waitFor(), remove typesafe.Config
    [refactor] move java package to namespace java
    [Flink] Check
    change initTracker() to static
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    add java wrapper
    [jvm-packages] Fixing the NativeLibLoader on Java 9+ (#4351)
    
    The old NativeLibLoader had a short-circuit load path which modified
    java.library.path and attempted to load the xgboost library from outside
    the jar first, falling back to loading the library from inside the jar.
    This path is a no-op every time when using XGBoost outside of it's
    source tree. Additionally it triggers an illegal reflective access
    warning in the module system in 9, 10, and 11.
    
    On Java 12 the ClassLoader fields are not accessible via reflection
    (separately from the illegal reflective acces warning), and so it fails
    in a way that isn't caught by the code which falls back to loading the
    library from inside the jar.
    
    This commit removes that code path and always loads the xgboost library
    from inside the jar file as it's a valid technique across multiple JVM
    implementations and works with all versions of Java.
    [jvm-packages] JNI Cosmetics (#2448)
    
    * [jvm-packages] Ensure the native library is loaded once
    
    Previously any class using XGBoostJNI queried NativeLibLoader to make
    sure the native library is loaded. This commit moves the initXGBoost
    call to XGBoostJNI, effectively delegating the initialization to the class
    loader.
    
    Note also, that now XGBoostJNI would NOT suppress an IOException if it
    occured in initXGBoost.
    
    * [jvm-packages] Fused JNIErrorHandle with XGBoostJNI
    
    There was no reason for having a separate class.
    libxgboost4j is now part of the CMake build (#2373)
    
    * [jvm-packages] Added libxgboost4j to CMake build
    
    * [jvm-packages] Wired CMake build into create_jni.sh
    
    * User newer CMake version on Travis
    
    * Lowered CMake version constraints
    
    * Fixed various quirks in the new CMake build
    [JVM] Refactor, add filesys API
    [refactor] move java package to namespace java
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    [BREAKING][jvm-packages] fix the non-zero missing value handling (#4349)
    
    * fix the nan and non-zero missing value handling
    
    * fix nan handling part
    
    * add missing value
    
    * Update MissingValueHandlingSuite.scala
    
    * Update MissingValueHandlingSuite.scala
    
    * stylistic fix
    Support instance weights for xgboost4j-spark (#2642)
    
    * Support instance weights for xgboost4j-spark
    
    * Use 0.001 instead of 0 for weights
    
    * Address CR comments
    [jvm-packages] Added baseMargin to ml.dmlc.xgboost4j.LabeledPoint (#2532)
    
    * Converted ml.dmlc.xgboost4j.LabeledPoint to Scala
    
    This allows to easily integrate LabeledPoint with Spark DataFrame APIs,
    which support encoding/decoding case classes out of the box. Alternative
    solution would be to keep LabeledPoint in Java and make it a Bean by
    generating boilerplate getters/setters. I have decided against that, even
    thought the conversion in this PR implies a public API change.
    
    I also had to remove the factory methods fromSparseVector and
    fromDenseVector because a) they would need to be duplicated to support
    overloaded calls with extra data (e.g. weight); and b) Scala would expose
    them via mangled $.MODULE$ which looks ugly in Java.
    
    Additionally, this commit makes it possible to switch to LabeledPoint in
    all public APIs and effectively to pass initial margin/group as part of
    the point. This seems to be the only reliable way of implementing distributed
    learning with these data. Note that group size format used by single-node
    XGBoost is not compatible with that scenario, since the partition split
    could divide a group into two chunks.
    
    * Switched to ml.dmlc.xgboost4j.LabeledPoint in RDD-based public APIs
    
    Note that DataFrame-based and Flink APIs are not affected by this change.
    
    * Removed baseMargin argument in favour of the LabeledPoint field
    
    * Do a single pass over the partition in buildDistributedBoosters
    
    Note that there is no formal guarantee that
    
        val repartitioned = rdd.repartition(42)
        repartitioned.zipPartitions(repartitioned.map(_ + 1)) { it1, it2, => ... }
    
    would do a single shuffle, but in practice it seems to be always the case.
    
    * Exposed baseMargin in DataFrame-based API
    
    * Addressed review comments
    
    * Pass baseMargin to XGBoost.trainWithDataFrame via params
    
    * Reverted MLLabeledPoint in Spark APIs
    
    As discussed, baseMargin would only be supported for DataFrame-based APIs.
    
    * Cleaned up baseMargin tests
    
    - Removed RDD-based test, since the option is no longer exposed via
      public APIs
    - Changed DataFrame-based one to check that adding a margin actually
      affects the prediction
    
    * Pleased Scalastyle
    
    * Addressed more review comments
    
    * Pleased scalastyle again
    
    * Fixed XGBoost.fromBaseMarginsToArray
    
    which always returned an array of NaNs even if base margin was not
    specified. Surprisingly this only failed a few tests.
    [jvm-packages] Exposed baseMargin (#2450)
    
    * Disabled excessive Spark logging in tests
    
    * Fixed a singature of XGBoostModel.predict
    
    Prior to this commit XGBoostModel.predict produced an RDD with
    an array of predictions for each partition, effectively changing
    the shape wrt the input RDD. A more natural contract for prediction
    API is that given an RDD it returns a new RDD with the same number
    of elements. This allows the users to easily match inputs with
    predictions.
    
    This commit removes one layer of nesting in XGBoostModel.predict output.
    Even though the change is clearly non-backward compatible, I still
    think it is well justified.
    
    * Removed boxing in XGBoost.fromDenseToSparseLabeledPoints
    
    * Inlined XGBoost.repartitionData
    
    An if is more explicit than an opaque method name.
    
    * Moved XGBoost.convertBoosterToXGBoostModel to XGBoostModel
    
    * Check the input dimension in DMatrix.setBaseMargin
    
    Prior to this commit providing an array of incorrect dimensions would
    have resulted in memory corruption. Maybe backport this to C++?
    
    * Reduced nesting in XGBoost.buildDistributedBoosters
    
    * Ensured consistent naming of the params map
    
    * Cleaned up DataBatch to make it easier to comprehend
    
    * Made scalastyle happy
    
    * Added baseMargin to XGBoost.train and trainWithRDD
    
    * Deprecated XGBoost.train
    
    It is ambiguous and work only for RDDs.
    
    * Addressed review comments
    
    * Revert "Fixed a singature of XGBoostModel.predict"
    
    This reverts commit 06bd5dcae7780265dd57e93ed7d4135f4e78f9b4.
    
    * Addressed more review comments
    
    * Fixed NullPointerException in buildDistributedBoosters
    fix the merge
    example of DistTrainWithSpark and trigger job with foreachPartition
    [JVM] Add LabeledPoint read support
    
    fix
    [refactor] move java package to namespace java
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    [refactor] move java package to namespace java
    sketch of xgboost-spark
    
    chooseBestBooster shall be in Boosters
    
    remove tracker.py
    
    rename XGBoost
    
    remove cross-validation
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    add java wrapper
    [jvm-packages] Add getNumFeature method (#6075)
    
    * Add getNumFeature to the Java API
    * Add getNumFeature to the Scala API
    * Add unit tests for getNumFeature
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] Add BigDenseMatrix (#4383)
    
    * Add BigDenseMatrix
    
    * ability to create DMatrix with bigger than Integer.MAX_VALUE size arrays
    * uses sun.misc.Unsafe
    
    * make DMatrix test work from a jar as well
    Deprecate set group (#4864)
    
    
    
    * Convert jvm package and R package.
    
    * Restore for compatibility.
    [jvm-packages] Add methods operating attributes of booster in jvm package, which follow API design in python package. (#4336)
    [jvm-packages] support specified feature names when getModelDump and getFeatureScore (#3733)
    
    * [jvm-packages] support specified feature names for jvm when get ModelDump and get FeatureScore (#3725)
    
    * typo and style fix
    [jvm-packages] JNI Cosmetics (#2448)
    
    * [jvm-packages] Ensure the native library is loaded once
    
    Previously any class using XGBoostJNI queried NativeLibLoader to make
    sure the native library is loaded. This commit moves the initXGBoost
    call to XGBoostJNI, effectively delegating the initialization to the class
    loader.
    
    Note also, that now XGBoostJNI would NOT suppress an IOException if it
    occured in initXGBoost.
    
    * [jvm-packages] Fused JNIErrorHandle with XGBoostJNI
    
    There was no reason for having a separate class.
    [jvm-packages] Expose json formatted booster dumps (#2233) (#2234)
    
    * Change Booster dump from XGBoosterDumpModel to XGBoosterDumpModelEx
    
    Allows exposing multiple formatting options of model dumping.
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    [jvm-packages] Robust dmatrix creation (#1613)
    
    * add back train method but mark as deprecated
    
    * robust matrix creation in jvm
    merge
    [refactor] move java package to namespace java
    [refactor] move java package to namespace java
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    [JVM-PKG] Update JNI to include rabit codes
    rename files/packages
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    refactor jni code and rename libxgboostjavawrapper.so to libxgboost4j.so
    update java wrapper for new fault handle API
    add java wrapper
    [jvm-packages] Add BigDenseMatrix (#4383)
    
    * Add BigDenseMatrix
    
    * ability to create DMatrix with bigger than Integer.MAX_VALUE size arrays
    * uses sun.misc.Unsafe
    
    * make DMatrix test work from a jar as well
    [jvm-packages] Add BigDenseMatrix (#4383)
    
    * Add BigDenseMatrix
    
    * ability to create DMatrix with bigger than Integer.MAX_VALUE size arrays
    * uses sun.misc.Unsafe
    
    * make DMatrix test work from a jar as well
    Bump junit from 4.11 to 4.13.1 in /jvm-packages/xgboost4j-gpu (#6233)
    
    Bumps [junit](https://github.com/junit-team/junit4) from 4.11 to 4.13.1.
    - [Release notes](https://github.com/junit-team/junit4/releases)
    - [Changelog](https://github.com/junit-team/junit4/blob/main/doc/ReleaseNotes4.11.md)
    - [Commits](https://github.com/junit-team/junit4/compare/r4.11...r4.13.1)
    
    Signed-off-by: dependabot[bot] <support@github.com>
    
    Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
    [jvm-packages]add xgboost4j-gpu/xgboost4j-spark-gpu module to facilitate release (#6136)
    
    * add xgboost4j-gpu/xgboost4j-spark-gpu module to facilitate release
    
    * Update pom.xml
    Bump version to 1.3.0 snapshot in master (#6052)
    Bump version to 1.2.0 snapshot in master (#5733)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    upgrade version num (#4670)
    
    * upgrade version num
    
    * missign changes
    
    * fix version script
    
    * change versions
    
    * rm files
    
    * Update CMakeLists.txt
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    [jvm-packages] bump version for master (#4209)
    
    * update version
    
    * bump version
    [jvm-packages] update version to 0.82-SNAPSHOT (#3920)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update version
    
    * 0.82
    Release 0.81 version (#3864)
    
    * Release 0.81 version
    
    * Update NEWS.md
    Update JVM packages version to 0.81-SNAPSHOT (#3584)
    Release version 0.80 (#3541)
    
    * Up versions
    
    * Write release note for 0.80
    Update 0.8 version num (#3358)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update 0.80
    [jvm-packages] scripts to cross-build and deploy artifacts to github (#3276)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * cross building files
    
    * update
    
    * build with docker
    
    * remove
    
    * temp
    
    * update build script
    
    * update pom
    
    * update
    
    * update version
    
    * upload build
    
    * fix path
    
    * update README.md
    
    * fix compiler version to 4.8.5
    [jvm-packages] change version of jvm to keep consistent with other pkgs  (#3253)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * change version of jvm to keep consistent with other pkgs
    [jvm-packages] add dev script to update version and update versions (#2998)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add dev script to update version and update versions
    [jvm-packages] bump to next version (#1535)
    
    * bump to next version
    
    * fix
    
    * fix
    fix #1377 spark-mllib scope: default => provided (#1381)
    [DOC-JVM] Refactor JVM docs
    example of DistTrainWithSpark and trigger job with foreachPartition
    try to get more memory from Travis
    sketch of xgboost-spark
    
    chooseBestBooster shall be in Boosters
    
    remove tracker.py
    
    rename XGBoost
    
    remove cross-validation
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    [DOC-JVM] Refactor JVM docs
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    add java wrapper
    Update LICENSE
    Squashed 'subtree/rabit/' changes from 1bb8fe9..4db0a62
    
    4db0a62 bugfix of lazy prepare
    87017bd license
    dc703e1 license
    c171440 change license to bsd
    7db2070 Update README.md
    581fe06 add mocktest
    d2f252f ok
    4a5b9e5 add all
    12ee049 init version of lbfgs
    37a2837 complete lbfgs solver
    6ade7cb complete lbfgs
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: 4db0a62a068894a55f70bad5e80c33d4434fc834
    license
    license
    change license to bsd
    Squashed 'subtree/rabit/' content from commit c7282ac
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: c7282acb2a92e1d6a32614889ff466fec58937f3
    check in license
    fix numpy convert
    chg license, README
    update license
    update license
    Initial commit
    Update README.md (#3872)
    
    SparkWithDataFrame was not there anymore. So replace with SparkMLlibPipeline.scala
    Fix broken link for xgboost-spark example. (#3275)
    fix typo in README
    update README
    [DOC-JVM] Refactor JVM docs
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    [jvm-packages] change class to object in examples (#1703)
    
    * change class to object in examples
    
    * fix compilation error
    adjust the API signature as well as the docs
    add scala examples
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    [jvm-packages] change class to object in examples (#1703)
    
    * change class to object in examples
    
    * fix compilation error
    adjust the API signature as well as the docs
    add scala examples
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    [jvm-packages] change class to object in examples (#1703)
    
    * change class to object in examples
    
    * fix compilation error
    adjust the API signature as well as the docs
    add scala examples
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] change class to object in examples (#1703)
    
    * change class to object in examples
    
    * fix compilation error
    adjust the API signature as well as the docs
    add scala examples
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] change class to object in examples (#1703)
    
    * change class to object in examples
    
    * fix compilation error
    adjust the API signature as well as the docs
    add scala examples
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    [jvm-packages] change class to object in examples (#1703)
    
    * change class to object in examples
    
    * fix compilation error
    adjust the API signature as well as the docs
    add scala examples
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] change class to object in examples (#1703)
    
    * change class to object in examples
    
    * fix compilation error
    adjust the API signature as well as the docs
    add scala examples
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    [jvm-pacakges] the first parameter in getModelDump should be featuremap path not model path (#1788)
    
    * fix the model dump in xgboost4j example
    
    * Modify the dump model part of scala version
    
    * add the forgotten modelInfos
    [jvm-packages] change class to object in examples (#1703)
    
    * change class to object in examples
    
    * fix compilation error
    adjust the API signature as well as the docs
    add scala examples
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] fix errors in example (#3719)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * instrumentation
    
    * use log console
    
    * better measurement
    
    * fix erros in example
    
    * update histmaker
    [jvm-packages] Tutorial of XGBoost4J-Spark (#3534)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add new
    
    * update doc
    
    * finish Gang Scheduling
    
    * more
    
    * intro
    
    * Add sections: Prediction, Model persistence and ML pipeline.
    
    * Add XGBoost4j-Spark MLlib pipeline example
    
    * partial finished version
    
    * finish the doc
    
    * adjust code
    
    * fix the doc
    
    * use rst
    
    * Convert XGBoost4J-Spark tutorial to reST
    
    * Bring XGBoost4J up to date
    
    * add note about using hdfs
    
    * remove duplicate file
    
    * fix descriptions
    
    * update doc
    
    * Wrap HDFS/S3 export support as a note
    
    * update
    
    * wrap indexing_mode example in code block
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages] Tutorial of XGBoost4J-Spark (#3534)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add new
    
    * update doc
    
    * finish Gang Scheduling
    
    * more
    
    * intro
    
    * Add sections: Prediction, Model persistence and ML pipeline.
    
    * Add XGBoost4j-Spark MLlib pipeline example
    
    * partial finished version
    
    * finish the doc
    
    * adjust code
    
    * fix the doc
    
    * use rst
    
    * Convert XGBoost4J-Spark tutorial to reST
    
    * Bring XGBoost4J up to date
    
    * add note about using hdfs
    
    * remove duplicate file
    
    * fix descriptions
    
    * update doc
    
    * Wrap HDFS/S3 export support as a note
    
    * update
    
    * wrap indexing_mode example in code block
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [FLINK] remove nWorker from API
    force the user to set number of workers
    fix examples
    more updates for Flink
    
    more fix
    remove spark/flink examples
    [DOC-JVM] Refactor JVM docs
    [JVM] Refactor, add filesys API
    [FLINK] Make runnable flink
    Add Labeled Point, minor fix build
    [refactor] move java package to namespace java
    [refactor] move java package to namespace java
    [Flink] Check
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    add scala examples
    adjust the API signature as well as the docs
    [DOC-JVM] Refactor JVM docs
    [refactor] move java package to namespace java
    [refactor] move java package to namespace java
    apply  google-java-style indentation and impose import orders....
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    rm WatchList class, take Iterable<Entry<String, DMatrix>> as eval param, change Params to Iterable<Entry<String, Object>>
    make some fix
    add java wrapper
    [jvm-pacakges] the first parameter in getModelDump should be featuremap path not model path (#1788)
    
    * fix the model dump in xgboost4j example
    
    * Modify the dump model part of scala version
    
    * add the forgotten modelInfos
    adjust the API signature as well as the docs
    [DOC-JVM] Refactor JVM docs
    [JVM] Refactor, add filesys API
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [jvm-packages] xgboost4j-spark external memory (#1219)
    
    * implement external memory support for XGBoost4J
    
    * remove extra space
    
    * enable external memory for prediction
    
    * update doc
    adjust the API signature as well as the docs
    [DOC-JVM] Refactor JVM docs
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    rm WatchList class, take Iterable<Entry<String, DMatrix>> as eval param, change Params to Iterable<Entry<String, Object>>
    make some fix
    add java wrapper
    adjust the API signature as well as the docs
    [DOC-JVM] Refactor JVM docs
    [refactor] move java package to namespace java
    apply  google-java-style indentation and impose import orders....
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    rm WatchList class, take Iterable<Entry<String, DMatrix>> as eval param, change Params to Iterable<Entry<String, Object>>
    make some fix
    add java wrapper
    adjust the API signature as well as the docs
    [DOC-JVM] Refactor JVM docs
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    rm WatchList class, take Iterable<Entry<String, DMatrix>> as eval param, change Params to Iterable<Entry<String, Object>>
    make some fix
    add java wrapper
    adjust the API signature as well as the docs
    [DOC-JVM] Refactor JVM docs
    [refactor] move java package to namespace java
    sketch of xgboost-spark
    
    chooseBestBooster shall be in Boosters
    
    remove tracker.py
    
    rename XGBoost
    
    remove cross-validation
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    adjust the API signature as well as the docs
    [DOC-JVM] Refactor JVM docs
    [JVM] Refactor, add filesys API
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    rm WatchList class, take Iterable<Entry<String, DMatrix>> as eval param, change Params to Iterable<Entry<String, Object>>
    make some fix
    add java wrapper
    adjust the API signature as well as the docs
    [DOC-JVM] Refactor JVM docs
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    rm WatchList class, take Iterable<Entry<String, DMatrix>> as eval param, change Params to Iterable<Entry<String, Object>>
    make some fix
    add java wrapper
    [DOC-JVM] Refactor JVM docs
    [refactor] move java package to namespace java
    apply  google-java-style indentation and impose import orders....
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    add java wrapper
    [DOC-JVM] Refactor JVM docs
    [refactor] move java package to namespace java
    apply  google-java-style indentation and impose import orders....
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    update java wrapper for new fault handle API
    add java wrapper
    [jvm-packages]add xgboost4j-gpu/xgboost4j-spark-gpu module to facilitate release (#6136)
    
    * add xgboost4j-gpu/xgboost4j-spark-gpu module to facilitate release
    
    * Update pom.xml
    Bump version to 1.3.0 snapshot in master (#6052)
    Bump version to 1.2.0 snapshot in master (#5733)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    upgrade version num (#4670)
    
    * upgrade version num
    
    * missign changes
    
    * fix version script
    
    * change versions
    
    * rm files
    
    * Update CMakeLists.txt
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    [jvm-packages] bump version for master (#4209)
    
    * update version
    
    * bump version
    [jvm-packages] update version to 0.82-SNAPSHOT (#3920)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update version
    
    * 0.82
    Release 0.81 version (#3864)
    
    * Release 0.81 version
    
    * Update NEWS.md
    Update JVM packages version to 0.81-SNAPSHOT (#3584)
    Release version 0.80 (#3541)
    
    * Up versions
    
    * Write release note for 0.80
    Update 0.8 version num (#3358)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update 0.80
    [jvm-packages] scripts to cross-build and deploy artifacts to github (#3276)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * cross building files
    
    * update
    
    * build with docker
    
    * remove
    
    * temp
    
    * update build script
    
    * update pom
    
    * update
    
    * update version
    
    * upload build
    
    * fix path
    
    * update README.md
    
    * fix compiler version to 4.8.5
    [jvm-packages] change version of jvm to keep consistent with other pkgs  (#3253)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * change version of jvm to keep consistent with other pkgs
    [jvm-packages] Declared Spark as provided in the POM (#3093)
    
    * [jvm-packages] Explicitly declared Spark dependencies as provided
    
    * Removed noop spark-2.x profile
    [jvm-packages] fix the pattern in dev script and version mismatch (#3009)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix the pattern in dev script and version mismatch
    [jvm-packages] add dev script to update version and update versions (#2998)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add dev script to update version and update versions
    [jvm-packages] bump to next version (#1535)
    
    * bump to next version
    
    * fix
    
    * fix
    fix #1377 spark-mllib scope: default => provided (#1381)
    specify spark version (#1224)
    update spark version to 1.6.1
    [DOC-JVM] Refactor JVM docs
    Add Labeled Point, minor fix build
    try to get more memory from Travis
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    sketch of xgboost-spark
    
    chooseBestBooster shall be in Boosters
    
    remove tracker.py
    
    rename XGBoost
    
    remove cross-validation
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    [CI] Use devtoolset-6 because devtoolset-4 is EOL and no longer available (#5506)
    
    * Use devtoolset-6.
    
    * [CI] Use devtoolset-6 because devtoolset-4 is EOL and no longer available
    
    * CUDA 9.0 doesn't work with devtoolset-6; use devtoolset-4 for GPU build only
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [CI] Repair download URL for Maven 3.6.1 (#5139)
    [jvm-packages] update local dev build process (#4640)
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    [jvm-packages] update local dev build process (#4640)
    [jvm-packages] update local dev build process (#4640)
    [jvm-packages] update local dev build process (#4640)
    [jvm-packages] update local dev build process (#4640)
    [jvm-packages] update local dev build process (#4640)
    upgrade version num (#4670)
    
    * upgrade version num
    
    * missign changes
    
    * fix version script
    
    * change versions
    
    * rm files
    
    * Update CMakeLists.txt
    [jvm-packages] update local dev build process (#4640)
    [jvm-packages] fix the pattern in dev script and version mismatch (#3009)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix the pattern in dev script and version mismatch
    [jvm-packages] add dev script to update version and update versions (#2998)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add dev script to update version and update versions
    Bump version to 1.3.0 snapshot in master (#6052)
    Bump version to 1.2.0 snapshot in master (#5733)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    upgrade version num (#4670)
    
    * upgrade version num
    
    * missign changes
    
    * fix version script
    
    * change versions
    
    * rm files
    
    * Update CMakeLists.txt
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    [jvm-packages] bump version for master (#4209)
    
    * update version
    
    * bump version
    [jvm-packages] update version to 0.82-SNAPSHOT (#3920)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update version
    
    * 0.82
    Release 0.81 version (#3864)
    
    * Release 0.81 version
    
    * Update NEWS.md
    Update JVM packages version to 0.81-SNAPSHOT (#3584)
    Release version 0.80 (#3541)
    
    * Up versions
    
    * Write release note for 0.80
    Update 0.8 version num (#3358)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update 0.80
    [jvm-packages] scripts to cross-build and deploy artifacts to github (#3276)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * cross building files
    
    * update
    
    * build with docker
    
    * remove
    
    * temp
    
    * update build script
    
    * update pom
    
    * update
    
    * update version
    
    * upload build
    
    * fix path
    
    * update README.md
    
    * fix compiler version to 4.8.5
    [jvm-packages] change version of jvm to keep consistent with other pkgs  (#3253)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * change version of jvm to keep consistent with other pkgs
    [jvm-packages] Declared Spark as provided in the POM (#3093)
    
    * [jvm-packages] Explicitly declared Spark dependencies as provided
    
    * Removed noop spark-2.x profile
    [jvm-packages] fix the pattern in dev script and version mismatch (#3009)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix the pattern in dev script and version mismatch
    [jvm-packages] add dev script to update version and update versions (#2998)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add dev script to update version and update versions
    [jvm-packages] bump to next version (#1535)
    
    * bump to next version
    
    * fix
    
    * fix
    fix #1377 spark-mllib scope: default => provided (#1381)
    specify spark version (#1224)
    update spark version to 1.6.1
    [DOC-JVM] Refactor JVM docs
    Add Labeled Point, minor fix build
    try to get more memory from Travis
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    sketch of xgboost-spark
    
    chooseBestBooster shall be in Boosters
    
    remove tracker.py
    
    rename XGBoost
    
    remove cross-validation
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages][spark]Preserve num classes (#2068)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * change class to object in examples
    
    * fix compilation error
    
    * bump spark version to 2.1
    
    * preserve num_class issues
    
    * fix failed test cases
    
    * rivising
    
    * add multi class test
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] call setGroup for ranking task (#2066)
    
    * [jvm-packages] call setGroup for ranking task
    
    * passing groupData through xgBoostConfMap
    
    * fix original comment position
    
    * make groupData param
    
    * remove groupData variable, use xgBoostConfMap directly
    
    * set default groupData value
    
    * add use groupData tests
    
    * reduce rank-demo size
    
    * use TaskContext.getPartitionId() instead of mapPartitionsWithIndex
    
    * add DF use groupData test
    
    * remove unused varable
    [jvm-packages] Exposed baseMargin (#2450)
    
    * Disabled excessive Spark logging in tests
    
    * Fixed a singature of XGBoostModel.predict
    
    Prior to this commit XGBoostModel.predict produced an RDD with
    an array of predictions for each partition, effectively changing
    the shape wrt the input RDD. A more natural contract for prediction
    API is that given an RDD it returns a new RDD with the same number
    of elements. This allows the users to easily match inputs with
    predictions.
    
    This commit removes one layer of nesting in XGBoostModel.predict output.
    Even though the change is clearly non-backward compatible, I still
    think it is well justified.
    
    * Removed boxing in XGBoost.fromDenseToSparseLabeledPoints
    
    * Inlined XGBoost.repartitionData
    
    An if is more explicit than an opaque method name.
    
    * Moved XGBoost.convertBoosterToXGBoostModel to XGBoostModel
    
    * Check the input dimension in DMatrix.setBaseMargin
    
    Prior to this commit providing an array of incorrect dimensions would
    have resulted in memory corruption. Maybe backport this to C++?
    
    * Reduced nesting in XGBoost.buildDistributedBoosters
    
    * Ensured consistent naming of the params map
    
    * Cleaned up DataBatch to make it easier to comprehend
    
    * Made scalastyle happy
    
    * Added baseMargin to XGBoost.train and trainWithRDD
    
    * Deprecated XGBoost.train
    
    It is ambiguous and work only for RDDs.
    
    * Addressed review comments
    
    * Revert "Fixed a singature of XGBoostModel.predict"
    
    This reverts commit 06bd5dcae7780265dd57e93ed7d4135f4e78f9b4.
    
    * Addressed more review comments
    
    * Fixed NullPointerException in buildDistributedBoosters
    [jvm-packages] support spark 2.4 and compatibility test with previous xgboost version (#4377)
    
    * bump spark version
    
    * keep float.nan
    
    * handle brokenly changed name/value
    
    * add test
    
    * add model files
    
    * add model files
    
    * update doc
    [jvm-packages] support spark 2.4 and compatibility test with previous xgboost version (#4377)
    
    * bump spark version
    
    * keep float.nan
    
    * handle brokenly changed name/value
    
    * add test
    
    * add model files
    
    * add model files
    
    * update doc
    [core] fix slow predict-caching with many classes (#3109)
    
    * fix prediction caching inefficiency for multiclass
    
    * silence some warnings
    
    * redundant if
    
    * workaround for R v3.4.3 bug; fixes #3081
    [jvm-packages] support spark 2.4 and compatibility test with previous xgboost version (#4377)
    
    * bump spark version
    
    * keep float.nan
    
    * handle brokenly changed name/value
    
    * add test
    
    * add model files
    
    * add model files
    
    * update doc
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] separate classification and regression model and integrate with ML package (#1608)
    default eval func (#1574)
    add scala examples
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    [jvm-packages] More brooming in tests (#2517)
    
    * Deduplicated DataFrame creation in XGBoostDFSuite
    
    * Extracted dermatology.data into MultiClassification
    
    * Moved cache cleaning to SharedSparkContext
    
    Cache files are prefixed with appName therefore this seems to be just the
    place to delete them.
    
    * Removed redundant JMatrix calls in xgboost4j-spark
    
    * Slightly more readable buildDenseRDD in XGBoostGeneralSuite
    
    * Generalized train/test DataFrame construction in XGBoostDFSuite
    
    * Changed SharedSparkContext to setup a new context per-test
    
    Hence the new name: PerTestSparkSession :)
    
    * Fused Utils into PerTestSparkSession
    
    * Whitespace fix in XGBoostDFSuite
    
    * Ensure SparkSession is always eagerly created in PerTestSparkSession
    
    * Renamed PerTestSparkSession->PerTest
    
    because it was doing slightly more than creating/stopping the session.
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    [jvm-packages] cancel job instead of killing SparkContext (#6019)
    
    * cancel job instead of killing SparkContext
    
    This PR changes the default behavior that kills SparkContext. Instead, This PR
    cancels jobs when coming across task failed. That means the SparkContext is
    still alive even some exceptions happen.
    
    * add a parameter to control if killing SparkContext
    
    * cancel the jobs the failed task belongs to
    
    * remove the jobId from the map when one job failed.
    
    * resolve comments
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] xgboost-spark warning when Spark encryption is turned on (#3667)
    
    * added test, commented out right now
    
    * reinstated test
    
    * added fix for checking encryption settings
    
    * fix by using RDD conf
    
    * fix compilation
    
    * renamed conf
    
    * use SparkSession if available
    
    * fix message
    
    * nop
    
    * code review fixes
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] Added baseMargin to ml.dmlc.xgboost4j.LabeledPoint (#2532)
    
    * Converted ml.dmlc.xgboost4j.LabeledPoint to Scala
    
    This allows to easily integrate LabeledPoint with Spark DataFrame APIs,
    which support encoding/decoding case classes out of the box. Alternative
    solution would be to keep LabeledPoint in Java and make it a Bean by
    generating boilerplate getters/setters. I have decided against that, even
    thought the conversion in this PR implies a public API change.
    
    I also had to remove the factory methods fromSparseVector and
    fromDenseVector because a) they would need to be duplicated to support
    overloaded calls with extra data (e.g. weight); and b) Scala would expose
    them via mangled $.MODULE$ which looks ugly in Java.
    
    Additionally, this commit makes it possible to switch to LabeledPoint in
    all public APIs and effectively to pass initial margin/group as part of
    the point. This seems to be the only reliable way of implementing distributed
    learning with these data. Note that group size format used by single-node
    XGBoost is not compatible with that scenario, since the partition split
    could divide a group into two chunks.
    
    * Switched to ml.dmlc.xgboost4j.LabeledPoint in RDD-based public APIs
    
    Note that DataFrame-based and Flink APIs are not affected by this change.
    
    * Removed baseMargin argument in favour of the LabeledPoint field
    
    * Do a single pass over the partition in buildDistributedBoosters
    
    Note that there is no formal guarantee that
    
        val repartitioned = rdd.repartition(42)
        repartitioned.zipPartitions(repartitioned.map(_ + 1)) { it1, it2, => ... }
    
    would do a single shuffle, but in practice it seems to be always the case.
    
    * Exposed baseMargin in DataFrame-based API
    
    * Addressed review comments
    
    * Pass baseMargin to XGBoost.trainWithDataFrame via params
    
    * Reverted MLLabeledPoint in Spark APIs
    
    As discussed, baseMargin would only be supported for DataFrame-based APIs.
    
    * Cleaned up baseMargin tests
    
    - Removed RDD-based test, since the option is no longer exposed via
      public APIs
    - Changed DataFrame-based one to check that adding a margin actually
      affects the prediction
    
    * Pleased Scalastyle
    
    * Addressed more review comments
    
    * Pleased scalastyle again
    
    * Fixed XGBoost.fromBaseMarginsToArray
    
    which always returned an array of NaNs even if base margin was not
    specified. Surprisingly this only failed a few tests.
    [jvm-packages] More brooming in tests (#2517)
    
    * Deduplicated DataFrame creation in XGBoostDFSuite
    
    * Extracted dermatology.data into MultiClassification
    
    * Moved cache cleaning to SharedSparkContext
    
    Cache files are prefixed with appName therefore this seems to be just the
    place to delete them.
    
    * Removed redundant JMatrix calls in xgboost4j-spark
    
    * Slightly more readable buildDenseRDD in XGBoostGeneralSuite
    
    * Generalized train/test DataFrame construction in XGBoostDFSuite
    
    * Changed SharedSparkContext to setup a new context per-test
    
    Hence the new name: PerTestSparkSession :)
    
    * Fused Utils into PerTestSparkSession
    
    * Whitespace fix in XGBoostDFSuite
    
    * Ensure SparkSession is always eagerly created in PerTestSparkSession
    
    * Renamed PerTestSparkSession->PerTest
    
    because it was doing slightly more than creating/stopping the session.
    [jvm-packages] Deduplicated train/test data access in tests (#2507)
    
    * [jvm-packages] Deduplicated train/test data access in tests
    
    All datasets are now available via a unified API, e.g. Agaricus.test.
    The only exception is the dermatology data which requires parsing a
    CSV file.
    
    * Inlined Utils.buildTrainingRDD
    
    The default number of partitions for local mode is equal to the number
    of available CPUs.
    
    * Replaced dataset names with problem types
    [jvm-packages] separate classification and regression model and integrate with ML package (#1608)
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    fix spark tests on machines with many cores (#4634)
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [BLOCKING][jvm-packages] fix non-deterministic order within a partition  (in the case of an upstream shuffle) on prediction  (#4388)
    
    * [jvm-packages][hot-fix] fix column mismatch caused by zip actions at XGBooostModel.transformInternal
    
    * apply minibatch in prediction
    
    * an iterator-compatible minibatch prediction
    
    * regressor impl
    
    * continuous working on mini-batch prediction of xgboost4j-spark
    
    * Update Booster.java
    [jvm-packages]  consider spark.task.cpus when controlling parallelism (#3530)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * consider spark.task.cpus when controlling parallelism
    
    * fix bug
    
    * fix conf setup
    
    * calculate requestedCores within ParallelismController
    
    * enforce spark.task.cpus = 1
    
    * unify unit test case framework
    
    * enable spark ui
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] More brooming in tests (#2517)
    
    * Deduplicated DataFrame creation in XGBoostDFSuite
    
    * Extracted dermatology.data into MultiClassification
    
    * Moved cache cleaning to SharedSparkContext
    
    Cache files are prefixed with appName therefore this seems to be just the
    place to delete them.
    
    * Removed redundant JMatrix calls in xgboost4j-spark
    
    * Slightly more readable buildDenseRDD in XGBoostGeneralSuite
    
    * Generalized train/test DataFrame construction in XGBoostDFSuite
    
    * Changed SharedSparkContext to setup a new context per-test
    
    Hence the new name: PerTestSparkSession :)
    
    * Fused Utils into PerTestSparkSession
    
    * Whitespace fix in XGBoostDFSuite
    
    * Ensure SparkSession is always eagerly created in PerTestSparkSession
    
    * Renamed PerTestSparkSession->PerTest
    
    because it was doing slightly more than creating/stopping the session.
    Fix deterministic partitioning with dataset containing Double.NaN (#5996)
    
    The functions featureValueOfSparseVector or featureValueOfDenseVector could return a Float.NaN if the input vectore was containing any missing values. This would make fail the partition key computation and most of the vectors would end up in the same partition. We fix this by avoid returning a NaN and simply use the row HashCode in this case.
    We added a test to ensure that the repartition is indeed now uniform on input dataset containing values by checking that the partitions size variance is below a certain threshold.
    
    Signed-off-by: Anthony D'Amato <anthony.damato@hotmail.fr>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] enable deterministic repartitioning when checkpoint is enabled (#4807)
    
    * do reparititoning in DataUtil
    
    * keep previous behavior of partitioning without checkpoint
    
    * deterministic repartitioning
    
    * change
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] cleaning checkpoint file after a successful training (#4754)
    
    * cleaning checkpoint file after a successful file
    
    * address comments
    [jvm-packages] jvm test should clean up after themselfs (#4706)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] fix the split of input (#4417)
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    [BREAKING][jvm-packages] fix the non-zero missing value handling (#4349)
    
    * fix the nan and non-zero missing value handling
    
    * fix nan handling part
    
    * add missing value
    
    * Update MissingValueHandlingSuite.scala
    
    * Update MissingValueHandlingSuite.scala
    
    * stylistic fix
    [BLOCKING][jvm-packages] fix non-deterministic order within a partition  (in the case of an upstream shuffle) on prediction  (#4388)
    
    * [jvm-packages][hot-fix] fix column mismatch caused by zip actions at XGBooostModel.transformInternal
    
    * apply minibatch in prediction
    
    * an iterator-compatible minibatch prediction
    
    * regressor impl
    
    * continuous working on mini-batch prediction of xgboost4j-spark
    
    * Update Booster.java
    [jvm-packages] support spark 2.4 and compatibility test with previous xgboost version (#4377)
    
    * bump spark version
    
    * keep float.nan
    
    * handle brokenly changed name/value
    
    * add test
    
    * add model files
    
    * add model files
    
    * update doc
    [jvm-packages] handle NaN as missing value explicitly (#4309)
    
    * handle nan
    
    * handle nan explicitly
    
    * make code better and handle sparse vector in spark
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] add configuration flag to control whether to cache transformed training set (#4268)
    
    * control whether to cache data
    
    * uncache
    Separate Depthwidth and Lossguide growing policy in fast histogram (#4102)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * more changes
    
    * temp
    
    * update
    
    * udpate rabit
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * broadcast subsampled feature correctly
    
    * init col
    
    * temp
    
    * col sampling
    
    * fix histmastrix init
    
    * fix col sampling
    
    * remove cout
    
    * fix out of bound access
    
    * fix core dump
    
    remove core dump file
    
    * disbale test temporarily
    
    * update
    
    * add fid
    
    * print perf data
    
    * update
    
    * revert some changes
    
    * temp
    
    * temp
    
    * pass all tests
    
    * bring back some tests
    
    * recover some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * recover column init part
    
    * more recovery
    
    * fix core dumps
    
    * code clean
    
    * revert some changes
    
    * fix test compilation issue
    
    * fix lint issue
    
    * resolve compilation issue
    
    * fix issues of lint caused by rebase
    
    * fix stylistic changes and change variable names
    
    * use regtree internal function
    
    * modularize depth width
    
    * address the comments
    
    * fix failed tests
    
    * wrap perf timers with class
    
    * fix lint
    
    * fix num_leaves count
    
    * fix indention
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.cc
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * Update src/tree/updater_quantile_hist.h
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    
    * merge
    
    * fix compilation
    [jvm-packages] force use per-group weights in spark layer  (#4118)
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages][refactor] refactor XGBoost.scala (spark) (#3904)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * remove unused code
    
    * refactor
    
    * fix typo
    [jvm-packages] For training data with group, empty RDD partition threw exception (#3749) (#3750)
    [jvm-packages] Fix #3489: Spark repartitionForData can potentially shuffle all data and lose ordering required for ranking objectives (#3654)
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] Update docs and unify the terminology (#3024)
    
    * [jvm-packages] Move cache files to tmp dir and delete on exit
    
    * [jvm-packages] Update docs and unify terminology
    
    * Address CR Comments
    [jvm-packages] Saving models into a tmp folder every a few rounds (#2964)
    
    * [jvm-packages] Train Booster from an existing model
    
    * Align Scala API with Java API
    
    * Existing model should not load rabit checkpoint
    
    * Address minor comments
    
    * Implement saving temporary boosters and loading previous booster
    
    * Add more unit tests for loadPrevBooster
    
    * Add params to XGBoostEstimator
    
    * (1) Move repartition out of the temp model saving loop (2) Address CR comments
    
    * Catch a corner case of training next model with fewer rounds
    
    * Address comments
    
    * Refactor newly added methods into TmpBoosterManager
    
    * Add two files which is missing in previous commit
    
    * Rename TmpBooster to checkpoint
    [jvm-packages] Exposed train-time evaluation metrics (#2836)
    
    * [jvm-packages] Exposed train-time evaluation metrics
    
    They are accessible via 'XGBoostModel.summary'. The summary is not
    serialized with the model and is only available after the training.
    
    * Addressed review comments
    
    * Extracted model-related tests into 'XGBoostModelSuite'
    
    * Added tests for copying the 'XGBoostModel'
    
    * [jvm-packages] Fixed a subtle bug in train/test split
    
    Iterator.partition (naturally) assumes that the predicate is deterministic
    but this is not the case for
    
        r.nextDouble() <= trainTestRatio
    
    therefore sometimes the DMatrix(...) call got a NoSuchElementException
    and crashed the JVM due to lack of exception handling in
    XGBoost4jCallbackDataIterNext.
    
    * Make sure train/test objectives are different
    [jvm-packages] Repair spark model eval (#2841)
    
    In the refactor to add base margins, #2532, all of the labels were lost
    when creating the dmatrix. This became obvious as metrics like ndcg
    always returned 1.0 regardless of the results.
    
    Change-Id: I88be047e1c108afba4784bd3d892bfc9edeabe55
    [jvm-packages] Objectives starting with rank: are never classification (#2837)
    
    Training a model with the experimental rank:ndcg objective incorrectly
    returns a Classification model. Adjust the classification check to
    not recognize rank:* objectives as classification.
    
    While writing tests for isClassificationTask also turned up that
    obj_type -> regression was incorrectly identified as a classification
    task so the function was slightly adjusted to pass the new tests.
    [jvm-packages] Add SparkParallelismTracker to prevent job from hanging (#2697)
    
    * Add SparkParallelismTracker to prevent job from hanging
    
    * Code review comments
    
    * Code Review Comments
    
    * Fix unit tests
    
    * Changes and unit test to catch the corner case.
    
    * Update documentations
    
    * Small improvements
    
    * cancalAllJobs is problematic with scalatest. Remove it
    
    * Code Review Comments
    
    * Check number of executor cores beforehand, and throw exeception if any core is lost.
    
    * Address CR Comments
    
    * Add missing class
    
    * Fix flaky unit test
    
    * Address CR comments
    
    * Remove redundant param for TaskFailedListener
    [jvm-packages] (xgboost-spark) preserving num_class across save & load (#2742)
    
    * [bugfix] (xgboost-spark) preserving num_class across save & load
    
    * add testcase for save & load of multiclass model
    Fixed compilation on Scala 2.10 (#2629)
    [jvm-packages] Added baseMargin to ml.dmlc.xgboost4j.LabeledPoint (#2532)
    
    * Converted ml.dmlc.xgboost4j.LabeledPoint to Scala
    
    This allows to easily integrate LabeledPoint with Spark DataFrame APIs,
    which support encoding/decoding case classes out of the box. Alternative
    solution would be to keep LabeledPoint in Java and make it a Bean by
    generating boilerplate getters/setters. I have decided against that, even
    thought the conversion in this PR implies a public API change.
    
    I also had to remove the factory methods fromSparseVector and
    fromDenseVector because a) they would need to be duplicated to support
    overloaded calls with extra data (e.g. weight); and b) Scala would expose
    them via mangled $.MODULE$ which looks ugly in Java.
    
    Additionally, this commit makes it possible to switch to LabeledPoint in
    all public APIs and effectively to pass initial margin/group as part of
    the point. This seems to be the only reliable way of implementing distributed
    learning with these data. Note that group size format used by single-node
    XGBoost is not compatible with that scenario, since the partition split
    could divide a group into two chunks.
    
    * Switched to ml.dmlc.xgboost4j.LabeledPoint in RDD-based public APIs
    
    Note that DataFrame-based and Flink APIs are not affected by this change.
    
    * Removed baseMargin argument in favour of the LabeledPoint field
    
    * Do a single pass over the partition in buildDistributedBoosters
    
    Note that there is no formal guarantee that
    
        val repartitioned = rdd.repartition(42)
        repartitioned.zipPartitions(repartitioned.map(_ + 1)) { it1, it2, => ... }
    
    would do a single shuffle, but in practice it seems to be always the case.
    
    * Exposed baseMargin in DataFrame-based API
    
    * Addressed review comments
    
    * Pass baseMargin to XGBoost.trainWithDataFrame via params
    
    * Reverted MLLabeledPoint in Spark APIs
    
    As discussed, baseMargin would only be supported for DataFrame-based APIs.
    
    * Cleaned up baseMargin tests
    
    - Removed RDD-based test, since the option is no longer exposed via
      public APIs
    - Changed DataFrame-based one to check that adding a margin actually
      affects the prediction
    
    * Pleased Scalastyle
    
    * Addressed more review comments
    
    * Pleased scalastyle again
    
    * Fixed XGBoost.fromBaseMarginsToArray
    
    which always returned an array of NaNs even if base margin was not
    specified. Surprisingly this only failed a few tests.
    [jvm-packages] More brooming in tests (#2517)
    
    * Deduplicated DataFrame creation in XGBoostDFSuite
    
    * Extracted dermatology.data into MultiClassification
    
    * Moved cache cleaning to SharedSparkContext
    
    Cache files are prefixed with appName therefore this seems to be just the
    place to delete them.
    
    * Removed redundant JMatrix calls in xgboost4j-spark
    
    * Slightly more readable buildDenseRDD in XGBoostGeneralSuite
    
    * Generalized train/test DataFrame construction in XGBoostDFSuite
    
    * Changed SharedSparkContext to setup a new context per-test
    
    Hence the new name: PerTestSparkSession :)
    
    * Fused Utils into PerTestSparkSession
    
    * Whitespace fix in XGBoostDFSuite
    
    * Ensure SparkSession is always eagerly created in PerTestSparkSession
    
    * Renamed PerTestSparkSession->PerTest
    
    because it was doing slightly more than creating/stopping the session.
    [jvm-packages] Deduplicated train/test data access in tests (#2507)
    
    * [jvm-packages] Deduplicated train/test data access in tests
    
    All datasets are now available via a unified API, e.g. Agaricus.test.
    The only exception is the dermatology data which requires parsing a
    CSV file.
    
    * Inlined Utils.buildTrainingRDD
    
    The default number of partitions for local mode is equal to the number
    of available CPUs.
    
    * Replaced dataset names with problem types
    Fixed a signature of XGBoostModel.predict (#2476)
    
    Prior to this commit XGBoostModel.predict produced an RDD with
    an array of predictions for each partition, effectively changing
    the shape wrt the input RDD. A more natural contract for prediction
    API is that given an RDD it returns a new RDD with the same number
    of elements. This allows the users to easily match inputs with
    predictions.
    
    This commit removes one layer of nesting in XGBoostModel.predict output.
    Even though the change is clearly non-backward compatible, I still
    think it is well justified. See discussion in 06bd5dca for motivation.
    [jvm-packages] Exposed baseMargin (#2450)
    
    * Disabled excessive Spark logging in tests
    
    * Fixed a singature of XGBoostModel.predict
    
    Prior to this commit XGBoostModel.predict produced an RDD with
    an array of predictions for each partition, effectively changing
    the shape wrt the input RDD. A more natural contract for prediction
    API is that given an RDD it returns a new RDD with the same number
    of elements. This allows the users to easily match inputs with
    predictions.
    
    This commit removes one layer of nesting in XGBoostModel.predict output.
    Even though the change is clearly non-backward compatible, I still
    think it is well justified.
    
    * Removed boxing in XGBoost.fromDenseToSparseLabeledPoints
    
    * Inlined XGBoost.repartitionData
    
    An if is more explicit than an opaque method name.
    
    * Moved XGBoost.convertBoosterToXGBoostModel to XGBoostModel
    
    * Check the input dimension in DMatrix.setBaseMargin
    
    Prior to this commit providing an array of incorrect dimensions would
    have resulted in memory corruption. Maybe backport this to C++?
    
    * Reduced nesting in XGBoost.buildDistributedBoosters
    
    * Ensured consistent naming of the params map
    
    * Cleaned up DataBatch to make it easier to comprehend
    
    * Made scalastyle happy
    
    * Added baseMargin to XGBoost.train and trainWithRDD
    
    * Deprecated XGBoost.train
    
    It is ambiguous and work only for RDDs.
    
    * Addressed review comments
    
    * Revert "Fixed a singature of XGBoostModel.predict"
    
    This reverts commit 06bd5dcae7780265dd57e93ed7d4135f4e78f9b4.
    
    * Addressed more review comments
    
    * Fixed NullPointerException in buildDistributedBoosters
    [jvm-packages] Disable fast histo for spark (#2296)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * disable fast histogram in xgboost4j-spark temporarily
    [jvm-packages] fix the persistence of XGBoostEstimator (#2265)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix the persistence of XGBoostEstimator
    
    * test persistence of a complete pipeline
    
    * fix compilation issue
    
    * do not allow persist custom_eval and custom_obj
    
    * fix the failed tesl
    [jvm-packages] Accept groupData in spark model eval (#2244)
    
    * Support model evaluation for ranking tasks by accepting
     groupData in XGBoostModel.eval
    [jvm-packages] call setGroup for ranking task (#2066)
    
    * [jvm-packages] call setGroup for ranking task
    
    * passing groupData through xgBoostConfMap
    
    * fix original comment position
    
    * make groupData param
    
    * remove groupData variable, use xgBoostConfMap directly
    
    * set default groupData value
    
    * add use groupData tests
    
    * reduce rank-demo size
    
    * use TaskContext.getPartitionId() instead of mapPartitionsWithIndex
    
    * add DF use groupData test
    
    * remove unused varable
    [jvm-packages] Scala/Java interface for Fast Histogram Algorithm (#1966)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * first commit in scala binding for fast histo
    
    * java test
    
    * add missed scala tests
    
    * spark training
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * local change
    
    * first commit in scala binding for fast histo
    
    * local change
    
    * fix df frame test
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    [jvm-packages] deprecate Flaky test (#1662)
    
    
    
    * deprecate flaky test
    [jvm-packages] separate classification and regression model and integrate with ML package (#1608)
    [jvm-packages] predictLeaf with Dataframe (#1576)
    
    * add back train method but mark as deprecated
    
    * predictLeaf with Dataset
    
    * fix
    
    * fix
    default eval func (#1574)
    [jvm-packages] Integration with Spark Dataframe/Dataset (#1559)
    
    * bump up to scala 2.11
    
    * framework of data frame integration
    
    * test consistency between RDD and DataFrame
    
    * order preservation
    
    * test order preservation
    
    * example code and fix makefile
    
    * improve type checking
    
    * improve APIs
    
    * user docs
    
    * work around travis CI's limitation on log length
    
    * adjust test structure
    
    * integrate with Spark -1 .x
    
    * spark 2.x integration
    
    * remove spark 1.x implementation but provide instructions on how to downgrade
    [jvm-packages] allow training with missing values in xgboost-spark (#1525)
    
    * allow training with missing values in xgboost-spark
    
    * fix compilation error
    
    * fix bug
    [jvm-packages] remove APIs with DMatrix from xgboost-spark  (#1519)
    
    * test consistency of prediction functions between DMatrix and RDD
    
    * remove APIs with DMatrix from xgboost-spark
    
    * fix compilation error in xgboost4j-example
    
    * fix test cases
    [jvm-packages] test consistency of prediction functions with DMatrix and RDD (#1518)
    
    * test consistency of prediction functions between DMatrix and RDD
    
    * fix the failed test cases
    improve test of save/load model (#1515)
    [jvm-packages] create dmatrix with specified missing value (#1272)
    
    * create dmatrix with specified missing value
    
    * update dmlc-core
    
    * support for predict method in spark package
    
    repartitioning
    
    work around
    
    * add more elements to work around training set empty partition issue
    [jvm-packages] xgboost4j-spark external memory (#1219)
    
    * implement external memory support for XGBoost4J
    
    * remove extra space
    
    * enable external memory for prediction
    
    * update doc
    allow empty partitions
    adjust numWorkers for test
    support kryo serialization
    force the user to set number of workers
    change the API name since we support not only HDFS and local file system
    allow the user to specify the worker number and avoid unnecessary shuffle
    nthread no larger than spark.task.cpus
    revise current API
    fix the merge
    test case for XGBoostSpark
    distributed in RDD
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] update spark dependency to 3.0.0 (#5836)
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [BLOCKING][jvm-packages] fix non-deterministic order within a partition  (in the case of an upstream shuffle) on prediction  (#4388)
    
    * [jvm-packages][hot-fix] fix column mismatch caused by zip actions at XGBooostModel.transformInternal
    
    * apply minibatch in prediction
    
    * an iterator-compatible minibatch prediction
    
    * regressor impl
    
    * continuous working on mini-batch prediction of xgboost4j-spark
    
    * Update Booster.java
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    [jvm-packages] enable predictLeaf/predictContrib/treeLimit in 0.8  (#3532)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * partial finish
    
    * no test
    
    * add test cases
    
    * add test cases
    
    * address comments
    
    * add test for regressor
    
    * fix typo
    [JVM-packages] Support single instance prediction. (#3464)
    
    * Support single instance prediction.
    
    * Address comments.
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] Fix wrong method name `setAllowZeroForMissingValue`. (#5740)
    
    * Allow non-zero for missing value when training.
    
    * Fix wrong method names.
    
    * Add a unit test
    
    * Move the getter/setter unit test to MissingValueHandlingSuite
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [BLOCKING] Handle empty rows in data iterators correctly (#5929)
    
    * [jvm-packages] Handle empty rows in data iterators correctly
    
    * Fix clang-tidy error
    
    * last empty row
    
    * Add comments [skip ci]
    
    Co-authored-by: Nan Zhu <nanzhu@uber.com>
    [jvm-packages] update spark dependency to 3.0.0 (#5836)
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    [jvm-packages] Comply with scala style convention + fix broken unit test (#5134)
    
    * Fix scala style check
    
    * fix messed unit test
    [jvm-packages] Allow for bypassing spark missing value check (#4805)
    
    * Allow for bypassing spark missing value check
    
    * Update documentation for dealing with missing values in spark xgboost
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] fix compatibility problem of spark version (#4411)
    
    * fix compatibility problem of spark version on MissingValueHandlingSuite.scala
    
    * call setHandleInvalid by runtime reflection
    [BREAKING][jvm-packages] fix the non-zero missing value handling (#4349)
    
    * fix the nan and non-zero missing value handling
    
    * fix nan handling part
    
    * add missing value
    
    * Update MissingValueHandlingSuite.scala
    
    * Update MissingValueHandlingSuite.scala
    
    * stylistic fix
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [BLOCKING] fix the issue with infrequent feature (#4045)
    
    * fix the issue with infrequent feature
    
    * handle exception
    
    * use only 2 workers
    
    * address the comments
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] Added baseMargin to ml.dmlc.xgboost4j.LabeledPoint (#2532)
    
    * Converted ml.dmlc.xgboost4j.LabeledPoint to Scala
    
    This allows to easily integrate LabeledPoint with Spark DataFrame APIs,
    which support encoding/decoding case classes out of the box. Alternative
    solution would be to keep LabeledPoint in Java and make it a Bean by
    generating boilerplate getters/setters. I have decided against that, even
    thought the conversion in this PR implies a public API change.
    
    I also had to remove the factory methods fromSparseVector and
    fromDenseVector because a) they would need to be duplicated to support
    overloaded calls with extra data (e.g. weight); and b) Scala would expose
    them via mangled $.MODULE$ which looks ugly in Java.
    
    Additionally, this commit makes it possible to switch to LabeledPoint in
    all public APIs and effectively to pass initial margin/group as part of
    the point. This seems to be the only reliable way of implementing distributed
    learning with these data. Note that group size format used by single-node
    XGBoost is not compatible with that scenario, since the partition split
    could divide a group into two chunks.
    
    * Switched to ml.dmlc.xgboost4j.LabeledPoint in RDD-based public APIs
    
    Note that DataFrame-based and Flink APIs are not affected by this change.
    
    * Removed baseMargin argument in favour of the LabeledPoint field
    
    * Do a single pass over the partition in buildDistributedBoosters
    
    Note that there is no formal guarantee that
    
        val repartitioned = rdd.repartition(42)
        repartitioned.zipPartitions(repartitioned.map(_ + 1)) { it1, it2, => ... }
    
    would do a single shuffle, but in practice it seems to be always the case.
    
    * Exposed baseMargin in DataFrame-based API
    
    * Addressed review comments
    
    * Pass baseMargin to XGBoost.trainWithDataFrame via params
    
    * Reverted MLLabeledPoint in Spark APIs
    
    As discussed, baseMargin would only be supported for DataFrame-based APIs.
    
    * Cleaned up baseMargin tests
    
    - Removed RDD-based test, since the option is no longer exposed via
      public APIs
    - Changed DataFrame-based one to check that adding a margin actually
      affects the prediction
    
    * Pleased Scalastyle
    
    * Addressed more review comments
    
    * Pleased scalastyle again
    
    * Fixed XGBoost.fromBaseMarginsToArray
    
    which always returned an array of NaNs even if base margin was not
    specified. Surprisingly this only failed a few tests.
    [jvm-packages] More brooming in tests (#2517)
    
    * Deduplicated DataFrame creation in XGBoostDFSuite
    
    * Extracted dermatology.data into MultiClassification
    
    * Moved cache cleaning to SharedSparkContext
    
    Cache files are prefixed with appName therefore this seems to be just the
    place to delete them.
    
    * Removed redundant JMatrix calls in xgboost4j-spark
    
    * Slightly more readable buildDenseRDD in XGBoostGeneralSuite
    
    * Generalized train/test DataFrame construction in XGBoostDFSuite
    
    * Changed SharedSparkContext to setup a new context per-test
    
    Hence the new name: PerTestSparkSession :)
    
    * Fused Utils into PerTestSparkSession
    
    * Whitespace fix in XGBoostDFSuite
    
    * Ensure SparkSession is always eagerly created in PerTestSparkSession
    
    * Renamed PerTestSparkSession->PerTest
    
    because it was doing slightly more than creating/stopping the session.
    [jvm-packages] Deduplicated train/test data access in tests (#2507)
    
    * [jvm-packages] Deduplicated train/test data access in tests
    
    All datasets are now available via a unified API, e.g. Agaricus.test.
    The only exception is the dermatology data which requires parsing a
    CSV file.
    
    * Inlined Utils.buildTrainingRDD
    
    The default number of partitions for local mode is equal to the number
    of available CPUs.
    
    * Replaced dataset names with problem types
    Fix prediction heuristic (#5955)
    
    
    * Relax check for prediction.
    * Relax test in spark test.
    * Add tests in C++.
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [BREAKING][jvm-packages] fix the non-zero missing value handling (#4349)
    
    * fix the nan and non-zero missing value handling
    
    * fix nan handling part
    
    * add missing value
    
    * Update MissingValueHandlingSuite.scala
    
    * Update MissingValueHandlingSuite.scala
    
    * stylistic fix
    [BLOCKING][jvm-packages] fix non-deterministic order within a partition  (in the case of an upstream shuffle) on prediction  (#4388)
    
    * [jvm-packages][hot-fix] fix column mismatch caused by zip actions at XGBooostModel.transformInternal
    
    * apply minibatch in prediction
    
    * an iterator-compatible minibatch prediction
    
    * regressor impl
    
    * continuous working on mini-batch prediction of xgboost4j-spark
    
    * Update Booster.java
    [BLOCKING] fix the issue with infrequent feature (#4045)
    
    * fix the issue with infrequent feature
    
    * handle exception
    
    * use only 2 workers
    
    * address the comments
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages] Fix vector size of 'rawPredictionCol' in XGBoostClassificationModel (#3932)
    
    * Fix vector size of 'rawPredictionCol' in XGBoostClassificationModel
    
    * Fix UT
    [jvm-packages] Fix #3489: Spark repartitionForData can potentially shuffle all data and lose ordering required for ranking objectives (#3654)
    [jvm-packages] Fix "obj_type" error to enable custom objectives and evaluations (#3646)
    
    credits to @mmui
    [jvm-packages] enable predictLeaf/predictContrib/treeLimit in 0.8  (#3532)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * partial finish
    
    * no test
    
    * add test cases
    
    * add test cases
    
    * address comments
    
    * add test for regressor
    
    * fix typo
    [JVM-packages] Support single instance prediction. (#3464)
    
    * Support single instance prediction.
    
    * Address comments.
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] jvm test should clean up after themselfs (#4706)
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] cleaning checkpoint file after a successful training (#4754)
    
    * cleaning checkpoint file after a successful file
    
    * address comments
    [jvm-packages] jvm test should clean up after themselfs (#4706)
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] Fixed CheckpointManagerSuite for Scala 2.10 (#3332)
    
    As before, the compilation error is caused by mixing positional and
    labelled arguments.
    [jvm-packages] Update docs and unify the terminology (#3024)
    
    * [jvm-packages] Move cache files to tmp dir and delete on exit
    
    * [jvm-packages] Update docs and unify terminology
    
    * Address CR Comments
    [jvm-packages] Saving models into a tmp folder every a few rounds (#2964)
    
    * [jvm-packages] Train Booster from an existing model
    
    * Align Scala API with Java API
    
    * Existing model should not load rabit checkpoint
    
    * Address minor comments
    
    * Implement saving temporary boosters and loading previous booster
    
    * Add more unit tests for loadPrevBooster
    
    * Add params to XGBoostEstimator
    
    * (1) Move repartition out of the temp model saving loop (2) Address CR comments
    
    * Catch a corner case of training next model with fewer rounds
    
    * Address comments
    
    * Refactor newly added methods into TmpBoosterManager
    
    * Add two files which is missing in previous commit
    
    * Rename TmpBooster to checkpoint
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] jvm test should clean up after themselfs (#4706)
    [jvm-packages] support spark 2.4 and compatibility test with previous xgboost version (#4377)
    
    * bump spark version
    
    * keep float.nan
    
    * handle brokenly changed name/value
    
    * add test
    
    * add model files
    
    * add model files
    
    * update doc
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    [jvm-packages] Remove copy paste error in test suite (#3692)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * remove copy paste error
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] cancel job instead of killing SparkContext (#6019)
    
    * cancel job instead of killing SparkContext
    
    This PR changes the default behavior that kills SparkContext. Instead, This PR
    cancels jobs when coming across task failed. That means the SparkContext is
    still alive even some exceptions happen.
    
    * add a parameter to control if killing SparkContext
    
    * cancel the jobs the failed task belongs to
    
    * remove the jobId from the map when one job failed.
    
    * resolve comments
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    fix spark tests on machines with many cores (#4634)
    [jvm-packages]  consider spark.task.cpus when controlling parallelism (#3530)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * consider spark.task.cpus when controlling parallelism
    
    * fix bug
    
    * fix conf setup
    
    * calculate requestedCores within ParallelismController
    
    * enforce spark.task.cpus = 1
    
    * unify unit test case framework
    
    * enable spark ui
    [jvm-packages] Add SparkParallelismTracker to prevent job from hanging (#2697)
    
    * Add SparkParallelismTracker to prevent job from hanging
    
    * Code review comments
    
    * Code Review Comments
    
    * Fix unit tests
    
    * Changes and unit test to catch the corner case.
    
    * Update documentations
    
    * Small improvements
    
    * cancalAllJobs is problematic with scalatest. Remove it
    
    * Code Review Comments
    
    * Check number of executor cores beforehand, and throw exeception if any core is lost.
    
    * Address CR Comments
    
    * Add missing class
    
    * Fix flaky unit test
    
    * Address CR comments
    
    * Remove redundant param for TaskFailedListener
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] logging version number (#4271)
    
    * print version number
    
    * add property file
    [jvm-packages] cancel job instead of killing SparkContext (#6019)
    
    * cancel job instead of killing SparkContext
    
    This PR changes the default behavior that kills SparkContext. Instead, This PR
    cancels jobs when coming across task failed. That means the SparkContext is
    still alive even some exceptions happen.
    
    * add a parameter to control if killing SparkContext
    
    * cancel the jobs the failed task belongs to
    
    * remove the jobId from the map when one job failed.
    
    * resolve comments
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    [jvm-packages] Comply with scala style convention + fix broken unit test (#5134)
    
    * Fix scala style check
    
    * fix messed unit test
    [jvm-packages] Allow for bypassing spark missing value check (#4805)
    
    * Allow for bypassing spark missing value check
    
    * Update documentation for dealing with missing values in spark xgboost
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] enable deterministic repartitioning when checkpoint is enabled (#4807)
    
    * do reparititoning in DataUtil
    
    * keep previous behavior of partitioning without checkpoint
    
    * deterministic repartitioning
    
    * change
    [jvm-packages] Refactor XGBoost.scala to put all params processing in one place (#4815)
    
    * cleaning checkpoint file after a successful file
    
    * address comments
    
    * refactor xgboost.scala to avoid multiple changes when adding params
    
    * consolidate params
    
    * fix compilation issue
    
    * fix failed test
    
    * fix wrong name
    
    * tyep conversion
    [jvm-packages] cleaning checkpoint file after a successful training (#4754)
    
    * cleaning checkpoint file after a successful file
    
    * address comments
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] Automatically set maximize_evaluation_metrics if not explicitly given in XGBoost4J-Spark (#4446)
    
    * Automatically set maximize_evaluation_metrics if not explicitly given.
    
    * When custom_eval is set, require maximize_evaluation_metrics.
    
    * Update documents on early stop in XGBoost4J-Spark.
    
    * Fix code error.
    [BREAKING][jvm-packages] fix the non-zero missing value handling (#4349)
    
    * fix the nan and non-zero missing value handling
    
    * fix nan handling part
    
    * add missing value
    
    * Update MissingValueHandlingSuite.scala
    
    * Update MissingValueHandlingSuite.scala
    
    * stylistic fix
    [jvm-packages] handle NaN as missing value explicitly (#4309)
    
    * handle nan
    
    * handle nan explicitly
    
    * make code better and handle sparse vector in spark
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] logging version number (#4271)
    
    * print version number
    
    * add property file
    [jvm-packages] add configuration flag to control whether to cache transformed training set (#4268)
    
    * control whether to cache data
    
    * uncache
    [jvm-packages] force use per-group weights in spark layer  (#4118)
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    [jvm-packages] adding logs for parameters (#4091)
    [jvm-packages] Performance consideration and Alignment input parameter of repartition function (#4049)
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages][refactor] refactor XGBoost.scala (spark) (#3904)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * remove unused code
    
    * refactor
    
    * fix typo
    [Blocking][jvm-packages] fix the early stopping feature (#3808)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * temp
    
    * add method for classifier and regressor
    
    * update tutorial
    
    * address the comments
    
    * update
    [jvm-packages] For training data with group, empty RDD partition threw exception (#3749) (#3750)
    [jvm-packages]  fix issue when spark job execution thread cannot return before we execute first() (#3758)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * sparjJobThread
    
    * update
    
    * fix issue when spark job execution thread cannot return before we execute first()
    [jvm-packages] Fix #3489: Spark repartitionForData can potentially shuffle all data and lose ordering required for ranking objectives (#3654)
    [jvm-packages] Fix "obj_type" error to enable custom objectives and evaluations (#3646)
    
    credits to @mmui
    [jvm-packages] xgboost-spark warning when Spark encryption is turned on (#3667)
    
    * added test, commented out right now
    
    * reinstated test
    
    * added fix for checking encryption settings
    
    * fix by using RDD conf
    
    * fix compilation
    
    * renamed conf
    
    * use SparkSession if available
    
    * fix message
    
    * nop
    
    * code review fixes
    [jvm-packages]  consider spark.task.cpus when controlling parallelism (#3530)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * consider spark.task.cpus when controlling parallelism
    
    * fix bug
    
    * fix conf setup
    
    * calculate requestedCores within ParallelismController
    
    * enforce spark.task.cpus = 1
    
    * unify unit test case framework
    
    * enable spark ui
    [jvm-packages] the current version of xgboost does not consider missing value in prediction (#3529)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * consider missing value in prediction
    
    * handle single prediction instance
    
    * fix type conversion
    [jvm-packages] avoid use of Seq.apply in buildGroups (#3413)
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] Move cache files to TempDirectory and delete this directory after XGBoost job finishes (#3022)
    
    * [jvm-packages] Move cache files to tmp dir and delete on exit
    
    * Delete the cache dir when watches are deleted
    [jvm-packages] Update docs and unify the terminology (#3024)
    
    * [jvm-packages] Move cache files to tmp dir and delete on exit
    
    * [jvm-packages] Update docs and unify terminology
    
    * Address CR Comments
    [jvm-packages] Saving models into a tmp folder every a few rounds (#2964)
    
    * [jvm-packages] Train Booster from an existing model
    
    * Align Scala API with Java API
    
    * Existing model should not load rabit checkpoint
    
    * Address minor comments
    
    * Implement saving temporary boosters and loading previous booster
    
    * Add more unit tests for loadPrevBooster
    
    * Add params to XGBoostEstimator
    
    * (1) Move repartition out of the temp model saving loop (2) Address CR comments
    
    * Catch a corner case of training next model with fewer rounds
    
    * Address comments
    
    * Refactor newly added methods into TmpBoosterManager
    
    * Add two files which is missing in previous commit
    
    * Rename TmpBooster to checkpoint
    [jvm-packages] Fixed test/train persistence (#2949)
    
    * [jvm-packages] Fixed test/train persistence
    
    Prior to this patch both data sets were persisted in the same directory,
    i.e. the test data replaced the training one which led to
    
    * training on less data (since usually test < train) and
    * test loss being exactly equal to the training loss.
    
    Closes #2945.
    
    * Cleanup file cache after the training
    
    * Addressed review comments
    [jvm-packages] Exposed train-time evaluation metrics (#2836)
    
    * [jvm-packages] Exposed train-time evaluation metrics
    
    They are accessible via 'XGBoostModel.summary'. The summary is not
    serialized with the model and is only available after the training.
    
    * Addressed review comments
    
    * Extracted model-related tests into 'XGBoostModelSuite'
    
    * Added tests for copying the 'XGBoostModel'
    
    * [jvm-packages] Fixed a subtle bug in train/test split
    
    Iterator.partition (naturally) assumes that the predicate is deterministic
    but this is not the case for
    
        r.nextDouble() <= trainTestRatio
    
    therefore sometimes the DMatrix(...) call got a NoSuchElementException
    and crashed the JVM due to lack of exception handling in
    XGBoost4jCallbackDataIterNext.
    
    * Make sure train/test objectives are different
    [jvm-packages] Add some documentation to xgboost4j-spark plus minor style edits (#2823)
    
    * add scala docs to several methods
    
    * indentation
    
    * license formatting
    
    * clarify distributed boosters
    
    * address some review comments
    
    * reduce doc lengths
    
    * change method name, clarify  doc
    
    * reset make config
    
    * delete most comments
    
    * more review feedback
    [jvm-packages] Objectives starting with rank: are never classification (#2837)
    
    Training a model with the experimental rank:ndcg objective incorrectly
    returns a Classification model. Adjust the classification check to
    not recognize rank:* objectives as classification.
    
    While writing tests for isClassificationTask also turned up that
    obj_type -> regression was incorrectly identified as a classification
    task so the function was slightly adjusted to pass the new tests.
    [jvm-packages] Add SparkParallelismTracker to prevent job from hanging (#2697)
    
    * Add SparkParallelismTracker to prevent job from hanging
    
    * Code review comments
    
    * Code Review Comments
    
    * Fix unit tests
    
    * Changes and unit test to catch the corner case.
    
    * Update documentations
    
    * Small improvements
    
    * cancalAllJobs is problematic with scalatest. Remove it
    
    * Code Review Comments
    
    * Check number of executor cores beforehand, and throw exeception if any core is lost.
    
    * Address CR Comments
    
    * Add missing class
    
    * Fix flaky unit test
    
    * Address CR comments
    
    * Remove redundant param for TaskFailedListener
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    [jvm-packages] (xgboost-spark) preserving num_class across save & load (#2742)
    
    * [bugfix] (xgboost-spark) preserving num_class across save & load
    
    * add testcase for save & load of multiclass model
    [jvm-packages] Added baseMargin to ml.dmlc.xgboost4j.LabeledPoint (#2532)
    
    * Converted ml.dmlc.xgboost4j.LabeledPoint to Scala
    
    This allows to easily integrate LabeledPoint with Spark DataFrame APIs,
    which support encoding/decoding case classes out of the box. Alternative
    solution would be to keep LabeledPoint in Java and make it a Bean by
    generating boilerplate getters/setters. I have decided against that, even
    thought the conversion in this PR implies a public API change.
    
    I also had to remove the factory methods fromSparseVector and
    fromDenseVector because a) they would need to be duplicated to support
    overloaded calls with extra data (e.g. weight); and b) Scala would expose
    them via mangled $.MODULE$ which looks ugly in Java.
    
    Additionally, this commit makes it possible to switch to LabeledPoint in
    all public APIs and effectively to pass initial margin/group as part of
    the point. This seems to be the only reliable way of implementing distributed
    learning with these data. Note that group size format used by single-node
    XGBoost is not compatible with that scenario, since the partition split
    could divide a group into two chunks.
    
    * Switched to ml.dmlc.xgboost4j.LabeledPoint in RDD-based public APIs
    
    Note that DataFrame-based and Flink APIs are not affected by this change.
    
    * Removed baseMargin argument in favour of the LabeledPoint field
    
    * Do a single pass over the partition in buildDistributedBoosters
    
    Note that there is no formal guarantee that
    
        val repartitioned = rdd.repartition(42)
        repartitioned.zipPartitions(repartitioned.map(_ + 1)) { it1, it2, => ... }
    
    would do a single shuffle, but in practice it seems to be always the case.
    
    * Exposed baseMargin in DataFrame-based API
    
    * Addressed review comments
    
    * Pass baseMargin to XGBoost.trainWithDataFrame via params
    
    * Reverted MLLabeledPoint in Spark APIs
    
    As discussed, baseMargin would only be supported for DataFrame-based APIs.
    
    * Cleaned up baseMargin tests
    
    - Removed RDD-based test, since the option is no longer exposed via
      public APIs
    - Changed DataFrame-based one to check that adding a margin actually
      affects the prediction
    
    * Pleased Scalastyle
    
    * Addressed more review comments
    
    * Pleased scalastyle again
    
    * Fixed XGBoost.fromBaseMarginsToArray
    
    which always returned an array of NaNs even if base margin was not
    specified. Surprisingly this only failed a few tests.
    [jvm-packages] More brooming in tests (#2517)
    
    * Deduplicated DataFrame creation in XGBoostDFSuite
    
    * Extracted dermatology.data into MultiClassification
    
    * Moved cache cleaning to SharedSparkContext
    
    Cache files are prefixed with appName therefore this seems to be just the
    place to delete them.
    
    * Removed redundant JMatrix calls in xgboost4j-spark
    
    * Slightly more readable buildDenseRDD in XGBoostGeneralSuite
    
    * Generalized train/test DataFrame construction in XGBoostDFSuite
    
    * Changed SharedSparkContext to setup a new context per-test
    
    Hence the new name: PerTestSparkSession :)
    
    * Fused Utils into PerTestSparkSession
    
    * Whitespace fix in XGBoostDFSuite
    
    * Ensure SparkSession is always eagerly created in PerTestSparkSession
    
    * Renamed PerTestSparkSession->PerTest
    
    because it was doing slightly more than creating/stopping the session.
    [jvm-packages] Exposed baseMargin (#2450)
    
    * Disabled excessive Spark logging in tests
    
    * Fixed a singature of XGBoostModel.predict
    
    Prior to this commit XGBoostModel.predict produced an RDD with
    an array of predictions for each partition, effectively changing
    the shape wrt the input RDD. A more natural contract for prediction
    API is that given an RDD it returns a new RDD with the same number
    of elements. This allows the users to easily match inputs with
    predictions.
    
    This commit removes one layer of nesting in XGBoostModel.predict output.
    Even though the change is clearly non-backward compatible, I still
    think it is well justified.
    
    * Removed boxing in XGBoost.fromDenseToSparseLabeledPoints
    
    * Inlined XGBoost.repartitionData
    
    An if is more explicit than an opaque method name.
    
    * Moved XGBoost.convertBoosterToXGBoostModel to XGBoostModel
    
    * Check the input dimension in DMatrix.setBaseMargin
    
    Prior to this commit providing an array of incorrect dimensions would
    have resulted in memory corruption. Maybe backport this to C++?
    
    * Reduced nesting in XGBoost.buildDistributedBoosters
    
    * Ensured consistent naming of the params map
    
    * Cleaned up DataBatch to make it easier to comprehend
    
    * Made scalastyle happy
    
    * Added baseMargin to XGBoost.train and trainWithRDD
    
    * Deprecated XGBoost.train
    
    It is ambiguous and work only for RDDs.
    
    * Addressed review comments
    
    * Revert "Fixed a singature of XGBoostModel.predict"
    
    This reverts commit 06bd5dcae7780265dd57e93ed7d4135f4e78f9b4.
    
    * Addressed more review comments
    
    * Fixed NullPointerException in buildDistributedBoosters
    [jvm-packages] Release dmatrix when no longer needed (#2436)
    
    When using xgboost4j-spark I had executors getting killed much more
    often than i would expect by yarn for overrunning their memory limits,
    based on the memoryOverhead provided. It looks like a significant
    amount of this is because dmatrix's were being created but not released,
    because they were only released when the GC decided it was time to
    cleanup the references.
    
    Rather than waiting for the GC, relesae the DMatrix's when we know
    they are no longer necessary.
    [jvm-packages] Deterministically XGBoost training on exception (#2405)
    
    Previously the code relied on the tracker process being terminated
    by the OS, which was not the case on Windows.
    
    Closes #2394
    [jvm-packages] Disable fast histo for spark (#2296)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * disable fast histogram in xgboost4j-spark temporarily
    [jvm-packages] fix the persistence of XGBoostEstimator (#2265)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix the persistence of XGBoostEstimator
    
    * test persistence of a complete pipeline
    
    * fix compilation issue
    
    * do not allow persist custom_eval and custom_obj
    
    * fix the failed tesl
    [jvm-packages] rdds containing boosters should be cleaned once we got boosters to driver (#2183)
    [jvm-packages] call setGroup for ranking task (#2066)
    
    * [jvm-packages] call setGroup for ranking task
    
    * passing groupData through xgBoostConfMap
    
    * fix original comment position
    
    * make groupData param
    
    * remove groupData variable, use xgBoostConfMap directly
    
    * set default groupData value
    
    * add use groupData tests
    
    * reduce rank-demo size
    
    * use TaskContext.getPartitionId() instead of mapPartitionsWithIndex
    
    * add DF use groupData test
    
    * remove unused varable
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    [jvm-packages] update API docs (#1713)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * update java doc
    
    * update
    [jvm-packages] Parameter tuning tool for XGBoost (#1664)
    [jvm-packages] separate classification and regression model and integrate with ML package (#1608)
    [jvm-packages] IndexOutOfBoundsException (#1589)
    
    ml.dmlc.xgboost4j.scala.spark.XGBoost.scala:51
    
    values is empty when we meet it at first time, so values(0) throw an IndexOutOfBoundsException.
    It should be  dVector.values(i) instead of values(i).
    [jvm-packages] Fixed the sanity check for parameter 'nthread' against 'spark.task.cpus'. (#1582)
    [jvm-packages] predictLeaf with Dataframe (#1576)
    
    * add back train method but mark as deprecated
    
    * predictLeaf with Dataset
    
    * fix
    
    * fix
    [jvm-packages] Integration with Spark Dataframe/Dataset (#1559)
    
    * bump up to scala 2.11
    
    * framework of data frame integration
    
    * test consistency between RDD and DataFrame
    
    * order preservation
    
    * test order preservation
    
    * example code and fix makefile
    
    * improve type checking
    
    * improve APIs
    
    * user docs
    
    * work around travis CI's limitation on log length
    
    * adjust test structure
    
    * integrate with Spark -1 .x
    
    * spark 2.x integration
    
    * remove spark 1.x implementation but provide instructions on how to downgrade
    impose shuffle when creating training RDD (#1531)
    [jvm-packages] allow training with missing values in xgboost-spark (#1525)
    
    * allow training with missing values in xgboost-spark
    
    * fix compilation error
    
    * fix bug
    enable train multiple models by distinguishing stage IDs (#1493)
    explicitly throw exception when detecting empty partition in training dataset (#1281)
    [jvm-packages] xgboost4j-spark external memory (#1219)
    
    * implement external memory support for XGBoost4J
    
    * remove extra space
    
    * enable external memory for prediction
    
    * update doc
    force the user to set number of workers
    set nthread to spark.task.cpus by default
    adjust the API signature as well as the docs
    change the API name since we support not only HDFS and local file system
    support different types of filesystems
    allow the user to specify the worker number and avoid unnecessary shuffle
    nthread no larger than spark.task.cpus
    remove duplicate in stream close
    revise current API
    [Spark] Refactor train, predict, add save
    use another thread to control spark job
    fix the merge
    fix rabitEnv
    example of DistTrainWithSpark and trigger job with foreachPartition
    adjust the return values of RabitTracker.waitFor(), remove typesafe.Config
    test case for XGBoostSpark
    distributed in RDD
    spark with new labeledpoint
    
    fix import order
    merge
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages] Exposed train-time evaluation metrics (#2836)
    
    * [jvm-packages] Exposed train-time evaluation metrics
    
    They are accessible via 'XGBoostModel.summary'. The summary is not
    serialized with the model and is only available after the training.
    
    * Addressed review comments
    
    * Extracted model-related tests into 'XGBoostModelSuite'
    
    * Added tests for copying the 'XGBoostModel'
    
    * [jvm-packages] Fixed a subtle bug in train/test split
    
    Iterator.partition (naturally) assumes that the predicate is deterministic
    but this is not the case for
    
        r.nextDouble() <= trainTestRatio
    
    therefore sometimes the DMatrix(...) call got a NoSuchElementException
    and crashed the JVM due to lack of exception handling in
    XGBoost4jCallbackDataIterNext.
    
    * Make sure train/test objectives are different
    Clean the way deterministic paritioning is computed (#6033)
    
    We propose to only use the rowHashCode to compute the partitionKey, adding the FeatureValue hashCode does not bring more value and would make the computation slower. Even though a collision would appear at 0.2% with MurmurHash3 this is bearable for partitioning, this won't have any impact on the data balancing.
    Fix deterministic partitioning with dataset containing Double.NaN (#5996)
    
    The functions featureValueOfSparseVector or featureValueOfDenseVector could return a Float.NaN if the input vectore was containing any missing values. This would make fail the partition key computation and most of the vectors would end up in the same partition. We fix this by avoid returning a NaN and simply use the row HashCode in this case.
    We added a test to ensure that the repartition is indeed now uniform on input dataset containing values by checking that the partitions size variance is below a certain threshold.
    
    Signed-off-by: Anthony D'Amato <anthony.damato@hotmail.fr>
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] enable deterministic repartitioning when checkpoint is enabled (#4807)
    
    * do reparititoning in DataUtil
    
    * keep previous behavior of partitioning without checkpoint
    
    * deterministic repartitioning
    
    * change
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages] Added baseMargin to ml.dmlc.xgboost4j.LabeledPoint (#2532)
    
    * Converted ml.dmlc.xgboost4j.LabeledPoint to Scala
    
    This allows to easily integrate LabeledPoint with Spark DataFrame APIs,
    which support encoding/decoding case classes out of the box. Alternative
    solution would be to keep LabeledPoint in Java and make it a Bean by
    generating boilerplate getters/setters. I have decided against that, even
    thought the conversion in this PR implies a public API change.
    
    I also had to remove the factory methods fromSparseVector and
    fromDenseVector because a) they would need to be duplicated to support
    overloaded calls with extra data (e.g. weight); and b) Scala would expose
    them via mangled $.MODULE$ which looks ugly in Java.
    
    Additionally, this commit makes it possible to switch to LabeledPoint in
    all public APIs and effectively to pass initial margin/group as part of
    the point. This seems to be the only reliable way of implementing distributed
    learning with these data. Note that group size format used by single-node
    XGBoost is not compatible with that scenario, since the partition split
    could divide a group into two chunks.
    
    * Switched to ml.dmlc.xgboost4j.LabeledPoint in RDD-based public APIs
    
    Note that DataFrame-based and Flink APIs are not affected by this change.
    
    * Removed baseMargin argument in favour of the LabeledPoint field
    
    * Do a single pass over the partition in buildDistributedBoosters
    
    Note that there is no formal guarantee that
    
        val repartitioned = rdd.repartition(42)
        repartitioned.zipPartitions(repartitioned.map(_ + 1)) { it1, it2, => ... }
    
    would do a single shuffle, but in practice it seems to be always the case.
    
    * Exposed baseMargin in DataFrame-based API
    
    * Addressed review comments
    
    * Pass baseMargin to XGBoost.trainWithDataFrame via params
    
    * Reverted MLLabeledPoint in Spark APIs
    
    As discussed, baseMargin would only be supported for DataFrame-based APIs.
    
    * Cleaned up baseMargin tests
    
    - Removed RDD-based test, since the option is no longer exposed via
      public APIs
    - Changed DataFrame-based one to check that adding a margin actually
      affects the prediction
    
    * Pleased Scalastyle
    
    * Addressed more review comments
    
    * Pleased scalastyle again
    
    * Fixed XGBoost.fromBaseMarginsToArray
    
    which always returned an array of NaNs even if base margin was not
    specified. Surprisingly this only failed a few tests.
    [jvm-packages] separate classification and regression model and integrate with ML package (#1608)
    [jvm-packages] Integration with Spark Dataframe/Dataset (#1559)
    
    * bump up to scala 2.11
    
    * framework of data frame integration
    
    * test consistency between RDD and DataFrame
    
    * order preservation
    
    * test order preservation
    
    * example code and fix makefile
    
    * improve type checking
    
    * improve APIs
    
    * user docs
    
    * work around travis CI's limitation on log length
    
    * adjust test structure
    
    * integrate with Spark -1 .x
    
    * spark 2.x integration
    
    * remove spark 1.x implementation but provide instructions on how to downgrade
    [Spark] Refactor train, predict, add save
    example of DistTrainWithSpark and trigger job with foreachPartition
    spark with new labeledpoint
    
    fix import order
    merge
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    [jvm-packages] Fix wrong method name `setAllowZeroForMissingValue`. (#5740)
    
    * Allow non-zero for missing value when training.
    
    * Fix wrong method names.
    
    * Add a unit test
    
    * Move the getter/setter unit test to MissingValueHandlingSuite
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] update spark dependency to 3.0.0 (#5836)
    Add new parameter singlePrecisionHistogram to xgboost4j-spark (#5811)
    
    Expose the existing 'singlePrecisionHistogram' param to the Spark layer.
    [jvm-packages] Comply with scala style convention + fix broken unit test (#5134)
    
    * Fix scala style check
    
    * fix messed unit test
    [jvm-packages] Allow for bypassing spark missing value check (#4805)
    
    * Allow for bypassing spark missing value check
    
    * Update documentation for dealing with missing values in spark xgboost
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] enable deterministic repartitioning when checkpoint is enabled (#4807)
    
    * do reparititoning in DataUtil
    
    * keep previous behavior of partitioning without checkpoint
    
    * deterministic repartitioning
    
    * change
    [jvm-packages] Expose setMissing method in XGBoostClassificationModel / XGBoostRegressionModel (#4643)
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [BLOCKING][jvm-packages] fix non-deterministic order within a partition  (in the case of an upstream shuffle) on prediction  (#4388)
    
    * [jvm-packages][hot-fix] fix column mismatch caused by zip actions at XGBooostModel.transformInternal
    
    * apply minibatch in prediction
    
    * an iterator-compatible minibatch prediction
    
    * regressor impl
    
    * continuous working on mini-batch prediction of xgboost4j-spark
    
    * Update Booster.java
    [jvm-packages] handle NaN as missing value explicitly (#4309)
    
    * handle nan
    
    * handle nan explicitly
    
    * make code better and handle sparse vector in spark
    
    * Update XGBoostGeneralSuite.scala
    [xgboost4j-spark] Allow set the parameter "maxLeaves". (#4226)
    
    * Allow set the parameter "maxLeaves".
    
    * Add "setMaxLeaves" to XGBoostRegressor.
    [jvm-packages] fix the scalability issue of prediction (#4033)
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages] Fix vector size of 'rawPredictionCol' in XGBoostClassificationModel (#3932)
    
    * Fix vector size of 'rawPredictionCol' in XGBoostClassificationModel
    
    * Fix UT
    [jvm-packages][refactor] refactor XGBoost.scala (spark) (#3904)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * remove unused code
    
    * refactor
    
    * fix typo
    [Blocking][jvm-packages] fix the early stopping feature (#3808)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * temp
    
    * add method for classifier and regressor
    
    * update tutorial
    
    * address the comments
    
    * update
    [jvm-packages] Fix #3489: Spark repartitionForData can potentially shuffle all data and lose ordering required for ranking objectives (#3654)
    [jvm-packages] Fix "obj_type" error to enable custom objectives and evaluations (#3646)
    
    credits to @mmui
    [jvm-packages] Avoid loosing precision when computing probabilities by converting to Double early (#3576)
    Make sure 'thresholds' are considered when executing predict method (#3577)
    [jvm-packages] enable predictLeaf/predictContrib/treeLimit in 0.8  (#3532)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * partial finish
    
    * no test
    
    * add test cases
    
    * add test cases
    
    * address comments
    
    * add test for regressor
    
    * fix typo
    [jvm-packages] the current version of xgboost does not consider missing value in prediction (#3529)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * consider missing value in prediction
    
    * handle single prediction instance
    
    * fix type conversion
    Expose setCustomObj & setCustomEval for XGBoostClassifier & XGBoostRegressor. (#3486)
    [JVM-packages] Support single instance prediction. (#3464)
    
    * Support single instance prediction.
    
    * Address comments.
    [jvm-packages] disable booster setup for xgboost4j-spark (#3456)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * disable booster setup in spark
    
    * check in parameter conversion
    
    * fix compilation issue
    
    * update exception type
    [jvm-packages] Expose nativeBooster for XGBoostClassificationModel and XGBoostRegressionModel. (#3428)
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    [jvm-packages] Fix wrong method name `setAllowZeroForMissingValue`. (#5740)
    
    * Allow non-zero for missing value when training.
    
    * Fix wrong method names.
    
    * Add a unit test
    
    * Move the getter/setter unit test to MissingValueHandlingSuite
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add new parameter singlePrecisionHistogram to xgboost4j-spark (#5811)
    
    Expose the existing 'singlePrecisionHistogram' param to the Spark layer.
    [jvm-packages] Comply with scala style convention + fix broken unit test (#5134)
    
    * Fix scala style check
    
    * fix messed unit test
    [jvm-packages] Allow for bypassing spark missing value check (#4805)
    
    * Allow for bypassing spark missing value check
    
    * Update documentation for dealing with missing values in spark xgboost
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] enable deterministic repartitioning when checkpoint is enabled (#4807)
    
    * do reparititoning in DataUtil
    
    * keep previous behavior of partitioning without checkpoint
    
    * deterministic repartitioning
    
    * change
    [jvm-packages] Expose setMissing method in XGBoostClassificationModel / XGBoostRegressionModel (#4643)
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [BLOCKING][jvm-packages] fix non-deterministic order within a partition  (in the case of an upstream shuffle) on prediction  (#4388)
    
    * [jvm-packages][hot-fix] fix column mismatch caused by zip actions at XGBooostModel.transformInternal
    
    * apply minibatch in prediction
    
    * an iterator-compatible minibatch prediction
    
    * regressor impl
    
    * continuous working on mini-batch prediction of xgboost4j-spark
    
    * Update Booster.java
    [jvm-packages] handle NaN as missing value explicitly (#4309)
    
    * handle nan
    
    * handle nan explicitly
    
    * make code better and handle sparse vector in spark
    
    * Update XGBoostGeneralSuite.scala
    [xgboost4j-spark] Allow set the parameter "maxLeaves". (#4226)
    
    * Allow set the parameter "maxLeaves".
    
    * Add "setMaxLeaves" to XGBoostRegressor.
    [jvm-packages] fix the scalability issue of prediction (#4033)
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages][refactor] refactor XGBoost.scala (spark) (#3904)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * remove unused code
    
    * refactor
    
    * fix typo
    [Blocking][jvm-packages] fix the early stopping feature (#3808)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * temp
    
    * add method for classifier and regressor
    
    * update tutorial
    
    * address the comments
    
    * update
    [jvm-packages] Fix #3489: Spark repartitionForData can potentially shuffle all data and lose ordering required for ranking objectives (#3654)
    [jvm-packages] Fix "obj_type" error to enable custom objectives and evaluations (#3646)
    
    credits to @mmui
    [jvm-packages] enable predictLeaf/predictContrib/treeLimit in 0.8  (#3532)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * partial finish
    
    * no test
    
    * add test cases
    
    * add test cases
    
    * address comments
    
    * add test for regressor
    
    * fix typo
    [jvm-packages] the current version of xgboost does not consider missing value in prediction (#3529)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * consider missing value in prediction
    
    * handle single prediction instance
    
    * fix type conversion
    Expose setCustomObj & setCustomEval for XGBoostClassifier & XGBoostRegressor. (#3486)
    [JVM-packages] Support single instance prediction. (#3464)
    
    * Support single instance prediction.
    
    * Address comments.
    [jvm-packages] disable booster setup for xgboost4j-spark (#3456)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * disable booster setup in spark
    
    * check in parameter conversion
    
    * fix compilation issue
    
    * update exception type
    [jvm-packages] Expose nativeBooster for XGBoostClassificationModel and XGBoostRegressionModel. (#3428)
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] do not use multiple jobs to make checkpoints (#5082)
    
    * temp
    
    * temp
    
    * tep
    
    * address the comments
    
    * fix stylistic issues
    
    * fix
    
    * external checkpoint
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] enable deterministic repartitioning when checkpoint is enabled (#4807)
    
    * do reparititoning in DataUtil
    
    * keep previous behavior of partitioning without checkpoint
    
    * deterministic repartitioning
    
    * change
    set maxBins to 256. Align with c code in src/tree/param.h (#6066)
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add new parameter singlePrecisionHistogram to xgboost4j-spark (#5811)
    
    Expose the existing 'singlePrecisionHistogram' param to the Spark layer.
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] minor fix of params (#4114)
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    [jvm-packages] getTreeLimit return type should be Int
    [jvm-packages] Use treeLimit param in getTreeLimit (#3575)
    [jvm-packages] enable predictLeaf/predictContrib/treeLimit in 0.8  (#3532)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * partial finish
    
    * no test
    
    * add test cases
    
    * add test cases
    
    * address comments
    
    * add test for regressor
    
    * fix typo
    [jvm-packages] disable booster setup for xgboost4j-spark (#3456)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * disable booster setup in spark
    
    * check in parameter conversion
    
    * fix compilation issue
    
    * update exception type
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    params confusion fixed (#3386)
    [jvm-packages] make XGBoostModel hold BoosterParams as well (#2214)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * make XGBoostModel hold BoosterParams as well
    [jvm-packages] Scala/Java interface for Fast Histogram Algorithm (#1966)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * first commit in scala binding for fast histo
    
    * java test
    
    * add missed scala tests
    
    * spark training
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * local change
    
    * first commit in scala binding for fast histo
    
    * local change
    
    * fix df frame test
    [jvm-packages] use ML's para system to build the passed-in params to XGBoost (#2043)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * use ML's para system to build the passed-in params to XGBoost
    
    * clean
    fix MLlib CrossValidator issues (wrong default value configuration)  #1941 (#2042)
    [jvm-packages] update API docs (#1713)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * update java doc
    
    * update
    [jvm-packages] Parameter tuning tool for XGBoost (#1664)
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] Comply with scala style convention + fix broken unit test (#5134)
    
    * Fix scala style check
    
    * fix messed unit test
    [jvm-packages] Allow for bypassing spark missing value check (#4805)
    
    * Allow for bypassing spark missing value check
    
    * Update documentation for dealing with missing values in spark xgboost
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    Add case for LongParam. (#4885)
    
    To support specifying long parameter as String, the same as other basic
    type, such as Int, Double ...
    [jvm-packages] refine numAliveCores method of SparkParallelismTracker  (#4858)
    
    * refine numAliveCores
    
    * refine XGBoostToMLlibParams
    
    * fix waitForCondition
    
    * resolve conflicts
    
    * Update SparkParallelismTracker.scala
    [jvm-packages] add `verbosity` param (#4138)
    [jvm-packages] fix simple logic error :) (#4128)
    
    @CodingCat
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages] Fixed the distributed updater check (#3739)
    
    The updater used in distributed training is grow_histmaker and not
    grow_colmaker as the error message stated prior to this commit.
    [jvm-packages] enable predictLeaf/predictContrib/treeLimit in 0.8  (#3532)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * partial finish
    
    * no test
    
    * add test cases
    
    * add test cases
    
    * address comments
    
    * add test for regressor
    
    * fix typo
    [jvm-packages] disable booster setup for xgboost4j-spark (#3456)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * disable booster setup in spark
    
    * check in parameter conversion
    
    * fix compilation issue
    
    * update exception type
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    [jvm-packages] Update docs and unify the terminology (#3024)
    
    * [jvm-packages] Move cache files to tmp dir and delete on exit
    
    * [jvm-packages] Update docs and unify terminology
    
    * Address CR Comments
    [jvm-packages] Saving models into a tmp folder every a few rounds (#2964)
    
    * [jvm-packages] Train Booster from an existing model
    
    * Align Scala API with Java API
    
    * Existing model should not load rabit checkpoint
    
    * Address minor comments
    
    * Implement saving temporary boosters and loading previous booster
    
    * Add more unit tests for loadPrevBooster
    
    * Add params to XGBoostEstimator
    
    * (1) Move repartition out of the temp model saving loop (2) Address CR comments
    
    * Catch a corner case of training next model with fewer rounds
    
    * Address comments
    
    * Refactor newly added methods into TmpBoosterManager
    
    * Add two files which is missing in previous commit
    
    * Rename TmpBooster to checkpoint
    [jvm-packages] Add SparkParallelismTracker to prevent job from hanging (#2697)
    
    * Add SparkParallelismTracker to prevent job from hanging
    
    * Code review comments
    
    * Code Review Comments
    
    * Fix unit tests
    
    * Changes and unit test to catch the corner case.
    
    * Update documentations
    
    * Small improvements
    
    * cancalAllJobs is problematic with scalatest. Remove it
    
    * Code Review Comments
    
    * Check number of executor cores beforehand, and throw exeception if any core is lost.
    
    * Address CR Comments
    
    * Add missing class
    
    * Fix flaky unit test
    
    * Address CR comments
    
    * Remove redundant param for TaskFailedListener
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    [jvm-packages] fix the persistence of XGBoostEstimator (#2265)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix the persistence of XGBoostEstimator
    
    * test persistence of a complete pipeline
    
    * fix compilation issue
    
    * do not allow persist custom_eval and custom_obj
    
    * fix the failed tesl
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    Fix a small typo in GeneralParams class. Change customEval parameter name from "custom_obj" to "custom_eval". (#1741)
    [jvm-packages] update API docs (#1713)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * update java doc
    
    * update
    [jvm-packages] Fix mis configure of nthread (#1709)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * change class to object in examples
    
    * fix compilation error
    
    * fix mis configuration
    [jvm-packages] Parameter tuning tool for XGBoost (#1664)
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] fix return type of setEvalSets (#4105)
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages] Spark pipeline persistence (#1906)
    
    [jvm-packages] Spark pipeline persistence
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    add java wrapper
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] fix the persistence of XGBoostEstimator (#2265)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix the persistence of XGBoostEstimator
    
    * test persistence of a complete pipeline
    
    * fix compilation issue
    
    * do not allow persist custom_eval and custom_obj
    
    * fix the failed tesl
    [jvm-packages] Spark pipeline persistence (#1906)
    
    [jvm-packages] Spark pipeline persistence
    Add MAPE metric (#6119)
    [jvm-packages] cancel job instead of killing SparkContext (#6019)
    
    * cancel job instead of killing SparkContext
    
    This PR changes the default behavior that kills SparkContext. Instead, This PR
    cancels jobs when coming across task failed. That means the SparkContext is
    still alive even some exceptions happen.
    
    * add a parameter to control if killing SparkContext
    
    * cancel the jobs the failed task belongs to
    
    * remove the jobId from the map when one job failed.
    
    * resolve comments
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] cleaning checkpoint file after a successful training (#4754)
    
    * cleaning checkpoint file after a successful file
    
    * address comments
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    Add `rmsle` metric and `reg:squaredlogerror` objective (#4541)
    [jvm-packages] Add back reg:linear for scala. (#4490)
    
    * Add back reg:linear for scala.
    
    * Fix linter.
    [jvm-packages] Automatically set maximize_evaluation_metrics if not explicitly given in XGBoost4J-Spark (#4446)
    
    * Automatically set maximize_evaluation_metrics if not explicitly given.
    
    * When custom_eval is set, require maximize_evaluation_metrics.
    
    * Update documents on early stop in XGBoost4J-Spark.
    
    * Fix code error.
    [jvm-packages] add configuration flag to control whether to cache transformed training set (#4268)
    
    * control whether to cache data
    
    * uncache
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    [Blocking][jvm-packages] fix the early stopping feature (#3808)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * temp
    
    * add method for classifier and regressor
    
    * update tutorial
    
    * address the comments
    
    * update
    [jvm-packages] Fix "obj_type" error to enable custom objectives and evaluations (#3646)
    
    credits to @mmui
    [jvm-packages] Add rank:ndcg and rank:map to Spark supported objectives (#3697)
    Add reg:tweedie to supported objectives in XGBoost4J-Spark (#3552)
    [jvm-packages] removed old group_data from spark api (#3451)
    [jvm-packages] XGBoost Spark integration refactor (#3387)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * [jvm-packages] XGBoost Spark integration refactor. (#3313)
    
    * XGBoost Spark integration refactor.
    
    * Make corresponding update for xgboost4j-example
    
    * Address comments.
    
    * [jvm-packages] Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib (#3326)
    
    * Refactor XGBoost-Spark params to make it compatible with both XGBoost and Spark MLLib
    
    * Fix extra space.
    
    * [jvm-packages] XGBoost Spark supports ranking with group data. (#3369)
    
    * XGBoost Spark supports ranking with group data.
    
    * Use Iterator.duplicate to prevent OOM.
    
    * Update CheckpointManagerSuite.scala
    
    * Resolve conflicts
    rank_metric: add AUC-PR (#3172)
    
    * rank_metric: add AUC-PR
    
    Implementation of the AUC-PR calculation for weighted data, proposed by Keilwagen, Grosse and Grau (https://doi.org/10.1371/journal.pone.0092209)
    
    * rank_metric: fix lint warnings
    
    * Implement tests for AUC-PR and fix implementation
    
    * add aucpr to documentation for other languages
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    Support instance weights for xgboost4j-spark (#2642)
    
    * Support instance weights for xgboost4j-spark
    
    * Use 0.001 instead of 0 for weights
    
    * Address CR comments
    [jvm-packages] Added baseMargin to ml.dmlc.xgboost4j.LabeledPoint (#2532)
    
    * Converted ml.dmlc.xgboost4j.LabeledPoint to Scala
    
    This allows to easily integrate LabeledPoint with Spark DataFrame APIs,
    which support encoding/decoding case classes out of the box. Alternative
    solution would be to keep LabeledPoint in Java and make it a Bean by
    generating boilerplate getters/setters. I have decided against that, even
    thought the conversion in this PR implies a public API change.
    
    I also had to remove the factory methods fromSparseVector and
    fromDenseVector because a) they would need to be duplicated to support
    overloaded calls with extra data (e.g. weight); and b) Scala would expose
    them via mangled $.MODULE$ which looks ugly in Java.
    
    Additionally, this commit makes it possible to switch to LabeledPoint in
    all public APIs and effectively to pass initial margin/group as part of
    the point. This seems to be the only reliable way of implementing distributed
    learning with these data. Note that group size format used by single-node
    XGBoost is not compatible with that scenario, since the partition split
    could divide a group into two chunks.
    
    * Switched to ml.dmlc.xgboost4j.LabeledPoint in RDD-based public APIs
    
    Note that DataFrame-based and Flink APIs are not affected by this change.
    
    * Removed baseMargin argument in favour of the LabeledPoint field
    
    * Do a single pass over the partition in buildDistributedBoosters
    
    Note that there is no formal guarantee that
    
        val repartitioned = rdd.repartition(42)
        repartitioned.zipPartitions(repartitioned.map(_ + 1)) { it1, it2, => ... }
    
    would do a single shuffle, but in practice it seems to be always the case.
    
    * Exposed baseMargin in DataFrame-based API
    
    * Addressed review comments
    
    * Pass baseMargin to XGBoost.trainWithDataFrame via params
    
    * Reverted MLLabeledPoint in Spark APIs
    
    As discussed, baseMargin would only be supported for DataFrame-based APIs.
    
    * Cleaned up baseMargin tests
    
    - Removed RDD-based test, since the option is no longer exposed via
      public APIs
    - Changed DataFrame-based one to check that adding a margin actually
      affects the prediction
    
    * Pleased Scalastyle
    
    * Addressed more review comments
    
    * Pleased scalastyle again
    
    * Fixed XGBoost.fromBaseMarginsToArray
    
    which always returned an array of NaNs even if base margin was not
    specified. Surprisingly this only failed a few tests.
    [jvm-packages] fix the persistence of XGBoostEstimator (#2265)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix the persistence of XGBoostEstimator
    
    * test persistence of a complete pipeline
    
    * fix compilation issue
    
    * do not allow persist custom_eval and custom_obj
    
    * fix the failed tesl
    [jvm-packages] call setGroup for ranking task (#2066)
    
    * [jvm-packages] call setGroup for ranking task
    
    * passing groupData through xgBoostConfMap
    
    * fix original comment position
    
    * make groupData param
    
    * remove groupData variable, use xgBoostConfMap directly
    
    * set default groupData value
    
    * add use groupData tests
    
    * reduce rank-demo size
    
    * use TaskContext.getPartitionId() instead of mapPartitionsWithIndex
    
    * add DF use groupData test
    
    * remove unused varable
    [jvm-packages][spark]Preserve num classes (#2068)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * change class to object in examples
    
    * fix compilation error
    
    * bump spark version to 2.1
    
    * preserve num_class issues
    
    * fix failed test cases
    
    * rivising
    
    * add multi class test
    [jvm-packages] update API docs (#1713)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * update java doc
    
    * update
    [jvm-packages] Parameter tuning tool for XGBoost (#1664)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] support spark 2.4 and compatibility test with previous xgboost version (#4377)
    
    * bump spark version
    
    * keep float.nan
    
    * handle brokenly changed name/value
    
    * add test
    
    * add model files
    
    * add model files
    
    * update doc
    [jvm-packages] fix the persistence of XGBoostEstimator (#2265)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix the persistence of XGBoostEstimator
    
    * test persistence of a complete pipeline
    
    * fix compilation issue
    
    * do not allow persist custom_eval and custom_obj
    
    * fix the failed tesl
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Spark pipeline persistence (#1906)
    
    [jvm-packages] Spark pipeline persistence
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    add java wrapper
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [BLOCKING][jvm-packages] fix non-deterministic order within a partition  (in the case of an upstream shuffle) on prediction  (#4388)
    
    * [jvm-packages][hot-fix] fix column mismatch caused by zip actions at XGBooostModel.transformInternal
    
    * apply minibatch in prediction
    
    * an iterator-compatible minibatch prediction
    
    * regressor impl
    
    * continuous working on mini-batch prediction of xgboost4j-spark
    
    * Update Booster.java
    [jvm-packages] fix return type of setEvalSets (#4105)
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages] Spark pipeline persistence (#1906)
    
    [jvm-packages] Spark pipeline persistence
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    add java wrapper
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    Remove println in jsonDecode (#3665)
    
    Following issue  #3578
    [jvm-packages] fix the persistence of XGBoostEstimator (#2265)
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix the persistence of XGBoostEstimator
    
    * test persistence of a complete pipeline
    
    * fix compilation issue
    
    * do not allow persist custom_eval and custom_obj
    
    * fix the failed tesl
    [jvm-packages] cancel job instead of killing SparkContext (#6019)
    
    * cancel job instead of killing SparkContext
    
    This PR changes the default behavior that kills SparkContext. Instead, This PR
    cancels jobs when coming across task failed. That means the SparkContext is
    still alive even some exceptions happen.
    
    * add a parameter to control if killing SparkContext
    
    * cancel the jobs the failed task belongs to
    
    * remove the jobId from the map when one job failed.
    
    * resolve comments
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] refine numAliveCores method of SparkParallelismTracker  (#4858)
    
    * refine numAliveCores
    
    * refine XGBoostToMLlibParams
    
    * fix waitForCondition
    
    * resolve conflicts
    
    * Update SparkParallelismTracker.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] better fix for shutdown applications  (#4108)
    
    * intentionally failed task
    
    * throw exception
    
    * more
    
    * stop sparkcontext directly
    
    * stop from another thread
    
    * new scope
    
    * use a new thread
    
    * daemon threads
    
    * don't join the killer thread
    
    * remove injected errors
    
    * add comments
    [jvm-packages] fix safe execution (#4046)
    [jvm-packages] throw ControlThrowable instead of InterruptedException (#3632)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * interrupted exception is not rethrown
    Fix Spark 2.2 Support (Amending #3062) (#3325)
    
    This pull request amends the broken #3062 allow Spark 2.2 to work.
    
    Please note this won't work in Spark <=2.1 as sc.removeSparkListener was implemented in Spark 2.2. (So perhaps a more general method is better, although that is what was attempted in #3062)
    
    This PR fixes: #3208, #3151 and the discussion in #1927.
    
    I do find it strange that #3062 dose not work in Spark 2.2, it's probably due to some sort of public/private issue in the org.apache.spark.scheduler.LiveListenerBus class inheritance (In Spark itself). The error is: `java.lang.NoSuchMethodError: org.apache.spark.scheduler.LiveListenerBus.removeListener(Ljava/lang/Object;)V`
    [jvm-packages]  consider spark.task.cpus when controlling parallelism (#3530)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * consider spark.task.cpus when controlling parallelism
    
    * fix bug
    
    * fix conf setup
    
    * calculate requestedCores within ParallelismController
    
    * enforce spark.task.cpus = 1
    
    * unify unit test case framework
    
    * enable spark ui
    Fixed SparkParallelTracker to work with Spark2.3 (#3062)
    [jvm-packages] fix numAliveCores in SparkParallelismTracker when WebUI is disabled (#2990)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update resource files
    
    * Update SparkParallelismTracker.scala
    
    * remove xgboost-tracker.properties
    [jvm-packages] Add SparkParallelismTracker to prevent job from hanging (#2697)
    
    * Add SparkParallelismTracker to prevent job from hanging
    
    * Code review comments
    
    * Code Review Comments
    
    * Fix unit tests
    
    * Changes and unit test to catch the corner case.
    
    * Update documentations
    
    * Small improvements
    
    * cancalAllJobs is problematic with scalatest. Remove it
    
    * Code Review Comments
    
    * Check number of executor cores beforehand, and throw exeception if any core is lost.
    
    * Address CR Comments
    
    * Add missing class
    
    * Fix flaky unit test
    
    * Address CR comments
    
    * Remove redundant param for TaskFailedListener
    Bump version to 1.3.0 snapshot in master (#6052)
    Bump version to 1.2.0 snapshot in master (#5733)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    upgrade version num (#4670)
    
    * upgrade version num
    
    * missign changes
    
    * fix version script
    
    * change versions
    
    * rm files
    
    * Update CMakeLists.txt
    [jvm-packages] upgrade to Scala 2.12 (#4574)
    
    * bump scala to 2.12 which requires java 8 and also newer flink and akka
    
    * put scala version in artifactId
    
    * fix appveyor
    
    * fix for scaladoc issue that looks like https://github.com/scala/bug/issues/10509
    
    * fix ci_build
    
    * update versions in generate_pom.py
    
    * fix generate_pom.py
    
    * apache does not have a download for spark 2.4.3 distro using scala 2.12 yet, so for now i use a tgz i put on s3
    
    * Upload spark-2.4.3-bin-scala2.12-hadoop2.7.tgz to our own S3
    
    * Update Dockerfile.jvm_cross
    
    * Update Dockerfile.jvm_cross
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    [jvm-packages] bump version for master (#4209)
    
    * update version
    
    * bump version
    [jvm-packages] update version to 0.82-SNAPSHOT (#3920)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update version
    
    * 0.82
    Release 0.81 version (#3864)
    
    * Release 0.81 version
    
    * Update NEWS.md
    [jvm-packages] bump flink version number (#3686)
    
    * bump flink version number
    
    * bump flink version number
    
    * add missing hadoop dependency
    Update JVM packages version to 0.81-SNAPSHOT (#3584)
    Release version 0.80 (#3541)
    
    * Up versions
    
    * Write release note for 0.80
    Update 0.8 version num (#3358)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * update 0.80
    [jvm-packages] scripts to cross-build and deploy artifacts to github (#3276)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * cross building files
    
    * update
    
    * build with docker
    
    * remove
    
    * temp
    
    * update build script
    
    * update pom
    
    * update
    
    * update version
    
    * upload build
    
    * fix path
    
    * update README.md
    
    * fix compiler version to 4.8.5
    [jvm-packages] change version of jvm to keep consistent with other pkgs  (#3253)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * change version of jvm to keep consistent with other pkgs
    [jvm-packages] fix the pattern in dev script and version mismatch (#3009)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix the pattern in dev script and version mismatch
    [jvm-packages] add dev script to update version and update versions (#2998)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add dev script to update version and update versions
    Removed 'flink.suffix' and added 'flink.version' (#2277)
    
    The former was just Scala binary tag, and the latter was hardcoded in
    the 'xgboost4j-flink' POM.
    [jvm-packages] Integration with Spark Dataframe/Dataset (#1559)
    
    * bump up to scala 2.11
    
    * framework of data frame integration
    
    * test consistency between RDD and DataFrame
    
    * order preservation
    
    * test order preservation
    
    * example code and fix makefile
    
    * improve type checking
    
    * improve APIs
    
    * user docs
    
    * work around travis CI's limitation on log length
    
    * adjust test structure
    
    * integrate with Spark -1 .x
    
    * spark 2.x integration
    
    * remove spark 1.x implementation but provide instructions on how to downgrade
    [jvm-packages] bump to next version (#1535)
    
    * bump to next version
    
    * fix
    
    * fix
    [DOC-JVM] Refactor JVM docs
    [FLINK] Make runnable flink
    Add Labeled Point, minor fix build
    [Flink] Check
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Added baseMargin to ml.dmlc.xgboost4j.LabeledPoint (#2532)
    
    * Converted ml.dmlc.xgboost4j.LabeledPoint to Scala
    
    This allows to easily integrate LabeledPoint with Spark DataFrame APIs,
    which support encoding/decoding case classes out of the box. Alternative
    solution would be to keep LabeledPoint in Java and make it a Bean by
    generating boilerplate getters/setters. I have decided against that, even
    thought the conversion in this PR implies a public API change.
    
    I also had to remove the factory methods fromSparseVector and
    fromDenseVector because a) they would need to be duplicated to support
    overloaded calls with extra data (e.g. weight); and b) Scala would expose
    them via mangled $.MODULE$ which looks ugly in Java.
    
    Additionally, this commit makes it possible to switch to LabeledPoint in
    all public APIs and effectively to pass initial margin/group as part of
    the point. This seems to be the only reliable way of implementing distributed
    learning with these data. Note that group size format used by single-node
    XGBoost is not compatible with that scenario, since the partition split
    could divide a group into two chunks.
    
    * Switched to ml.dmlc.xgboost4j.LabeledPoint in RDD-based public APIs
    
    Note that DataFrame-based and Flink APIs are not affected by this change.
    
    * Removed baseMargin argument in favour of the LabeledPoint field
    
    * Do a single pass over the partition in buildDistributedBoosters
    
    Note that there is no formal guarantee that
    
        val repartitioned = rdd.repartition(42)
        repartitioned.zipPartitions(repartitioned.map(_ + 1)) { it1, it2, => ... }
    
    would do a single shuffle, but in practice it seems to be always the case.
    
    * Exposed baseMargin in DataFrame-based API
    
    * Addressed review comments
    
    * Pass baseMargin to XGBoost.trainWithDataFrame via params
    
    * Reverted MLLabeledPoint in Spark APIs
    
    As discussed, baseMargin would only be supported for DataFrame-based APIs.
    
    * Cleaned up baseMargin tests
    
    - Removed RDD-based test, since the option is no longer exposed via
      public APIs
    - Changed DataFrame-based one to check that adding a margin actually
      affects the prediction
    
    * Pleased Scalastyle
    
    * Addressed more review comments
    
    * Pleased scalastyle again
    
    * Fixed XGBoost.fromBaseMarginsToArray
    
    which always returned an array of NaNs even if base margin was not
    specified. Surprisingly this only failed a few tests.
    fix examples
    more updates for Flink
    
    more fix
    [DOC-JVM] Refactor JVM docs
    [JVM] Refactor, add filesys API
    [FLINK] Make runnable flink
    [refactor] move java package to namespace java
    rename files/packages
    add style check for java and scala code
    re-structure Java API, add Scala API and consolidate the names of Java/Scala API
    rename xgboosterror
    update java wrapper for new fault handle API
    add java wrapper
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [jvm-packages] Implemented early stopping (#2710)
    
    * Allowed subsampling test from the training data frame/RDD
    
    The implementation requires storing 1 - trainTestRatio points in memory
    to make the sampling work.
    
    An alternative approach would be to construct the full DMatrix and then
    slice it deterministically into train/test. The peak memory consumption
    of such scenario, however, is twice the dataset size.
    
    * Removed duplication from 'XGBoost.train'
    
    Scala callers can (and should) use names to supply a subset of
    parameters. Method overloading is not required.
    
    * Reuse XGBoost seed parameter to stabilize train/test splitting
    
    * Added early stopping support to non-distributed XGBoost
    
    Closes #1544
    
    * Added early-stopping to distributed XGBoost
    
    * Moved construction of 'watches' into a separate method
    
    This commit also fixes the handling of 'baseMargin' which previously
    was not added to the validation matrix.
    
    * Addressed review comments
    [jvm-packages] Added baseMargin to ml.dmlc.xgboost4j.LabeledPoint (#2532)
    
    * Converted ml.dmlc.xgboost4j.LabeledPoint to Scala
    
    This allows to easily integrate LabeledPoint with Spark DataFrame APIs,
    which support encoding/decoding case classes out of the box. Alternative
    solution would be to keep LabeledPoint in Java and make it a Bean by
    generating boilerplate getters/setters. I have decided against that, even
    thought the conversion in this PR implies a public API change.
    
    I also had to remove the factory methods fromSparseVector and
    fromDenseVector because a) they would need to be duplicated to support
    overloaded calls with extra data (e.g. weight); and b) Scala would expose
    them via mangled $.MODULE$ which looks ugly in Java.
    
    Additionally, this commit makes it possible to switch to LabeledPoint in
    all public APIs and effectively to pass initial margin/group as part of
    the point. This seems to be the only reliable way of implementing distributed
    learning with these data. Note that group size format used by single-node
    XGBoost is not compatible with that scenario, since the partition split
    could divide a group into two chunks.
    
    * Switched to ml.dmlc.xgboost4j.LabeledPoint in RDD-based public APIs
    
    Note that DataFrame-based and Flink APIs are not affected by this change.
    
    * Removed baseMargin argument in favour of the LabeledPoint field
    
    * Do a single pass over the partition in buildDistributedBoosters
    
    Note that there is no formal guarantee that
    
        val repartitioned = rdd.repartition(42)
        repartitioned.zipPartitions(repartitioned.map(_ + 1)) { it1, it2, => ... }
    
    would do a single shuffle, but in practice it seems to be always the case.
    
    * Exposed baseMargin in DataFrame-based API
    
    * Addressed review comments
    
    * Pass baseMargin to XGBoost.trainWithDataFrame via params
    
    * Reverted MLLabeledPoint in Spark APIs
    
    As discussed, baseMargin would only be supported for DataFrame-based APIs.
    
    * Cleaned up baseMargin tests
    
    - Removed RDD-based test, since the option is no longer exposed via
      public APIs
    - Changed DataFrame-based one to check that adding a margin actually
      affects the prediction
    
    * Pleased Scalastyle
    
    * Addressed more review comments
    
    * Pleased scalastyle again
    
    * Fixed XGBoost.fromBaseMarginsToArray
    
    which always returned an array of NaNs even if base margin was not
    specified. Surprisingly this only failed a few tests.
    [jvm-packages] Scala implementation of the Rabit tracker. (#1612)
    
    * [jvm-packages] Scala implementation of the Rabit tracker.
    
    A Scala implementation of RabitTracker that is interface-interchangable with the
    Java implementation, ported from `tracker.py` in the
    [dmlc-core project](https://github.com/dmlc/dmlc-core).
    
    * [jvm-packages] Updated Akka dependency in pom.xml.
    
    * Refactored the RabitTracker directory structure.
    
    * Fixed premature stopping of connection handler.
    
    Added a new finite state "AwaitingPortNumber" to explicitly wait for the
    worker to send the port, and close the connection. Stopping the actor
    prematurely sends a TCP RST to the worker, causing the worker to crash
    on AssertionError.
    
    * Added interface IRabitTracker so that user can switch implementations.
    
    * Default timeout duration changes.
    
    * Dependency for Akka tests.
    
    * Removed the main function of RabitTracker.
    
    * A skeleton for testing Akka-based Rabit tracker.
    
    * waitFor() in RabitTracker no longer throws exceptions.
    
    * Completed unit test for the 'start' command of Rabit tracker.
    
    * Preliminary support for Rabit Allreduce via JNI (no prepare function support yet.)
    
    * Fixed the default timeout duration.
    
    * Use Java container to avoid serialization issues due to intermediate wrappers.
    
    * Added tests for Allreduce/model training using Scala Rabit tracker.
    
    * Added spill-over unit test for the Scala Rabit tracker.
    
    * Fixed a typo.
    
    * Overhaul of RabitTracker interface per code review.
    
      - Removed methods start() waitFor() (no arguments) from IRabitTracker.
      - The timeout in start(timeout) is now worker connection timeout, as tcp
        socket binding timeout is less intuitive.
      - Dropped time unit from start(...) and waitFor(...) methods; the default
        time unit is millisecond.
      - Moved random port number generation into the RabitTrackerHandler.
      - Moved all Rabit-related classes to package ml.dmlc.xgboost4j.scala.rabit.
    
    * More code refactoring and comments.
    
    * Unified timeout constants. Readable tracker status code.
    
    * Add comments to indicate that allReduce is for tests only. Removed all other variants.
    
    * Removed unused imports.
    
    * Simplified signatures of training methods.
    
     - Moved TrackerConf into parameter map.
     - Changed GeneralParams so that TrackerConf becomes a standalone parameter.
     - Updated test cases accordingly.
    
    * Changed monitoring strategies.
    
    * Reverted monitoring changes.
    
    * Update test case for Rabit AllReduce.
    
    * Mix in UncaughtExceptionHandler into IRabitTracker to prevent tracker from hanging due to exceptions thrown by workers.
    
    * More comprehensive test cases for exception handling and worker connection timeout.
    
    * Handle executor loss due to unknown cause: the newly spawned executor will attempt to connect to the tracker. Interrupt tracker in such case.
    
    * Per code-review, removed training timeout from TrackerConf. Timeout logic must be implemented explicitly and externally in the driver code.
    
    * Reverted scalastyle-config changes.
    
    * Visibility scope change. Interface tweaks.
    
    * Use match pattern to handle tracker_conf parameter.
    
    * Minor clarification in JNI code.
    
    * Clearer intent in match pattern to suppress warnings.
    
    * Removed Future from constructor. Block in start() and waitFor() instead.
    
    * Revert inadvertent comment changes.
    
    * Removed debugging information.
    
    * Updated test cases that are a bit finicky.
    
    * Added comments on the reasoning behind the unit tests for testing Rabit tracker robustness.
    [FLINK] remove nWorker from API
    force the user to set number of workers
    adjust the API signature as well as the docs
    fix examples
    more updates for Flink
    
    more fix
    [DOC-JVM] Refactor JVM docs
    [JVM] Refactor, add filesys API
    [FLINK] Make runnable flink
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Use configure_file() to configure version only (#4974)
    
    * Avoid writing build_config.h
    
    * Remove build_config.h all together.
    
    * Lint.
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Use configure_file() to configure version only (#4974)
    
    * Avoid writing build_config.h
    
    * Remove build_config.h all together.
    
    * Lint.
    [R] Remove dependency on gendef for Visual Studio builds (fixes #5608) (#5764)
    
    * [R-package] Remove dependency on gendef for Visual Studio builds (fixes #5608)
    
    * clarify docs
    
    * removed debugging print statement
    
    * Make R CMake install more robust
    
    * Fix doc format; add ToC
    
    * Update build.rst
    
    * Fix AppVeyor
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add CMake option to run Undefined Behavior Sanitizer (UBSan) (#5211)
    
    * Fix related errors.
    Enable building with sanitizers. (#3525)
    [CI] Test C API demo (#6159)
    
    * Fix CMake install config to use dependencies
    
    * [CI] Test C API demo
    
    * Explicitly cast num_feature, to avoid warning in Linux
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    [R] Remove dependency on gendef for Visual Studio builds (fixes #5608) (#5764)
    
    * [R-package] Remove dependency on gendef for Visual Studio builds (fixes #5608)
    
    * clarify docs
    
    * removed debugging print statement
    
    * Make R CMake install more robust
    
    * Fix doc format; add ToC
    
    * Update build.rst
    
    * Fix AppVeyor
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Option for generating device debug info. (#6168)
    
    * Supply `-G;-src-in-ptx` when `USE_DEVICE_DEBUG` is set and debug mode is selected.
    * Refactor CMake script to gather all CUDA configuration.
    * Use CMAKE_CUDA_ARCHITECTURES.  Close #6029.
    * Add compute 80.  Close #5999
    [R] Remove dependency on gendef for Visual Studio builds (fixes #5608) (#5764)
    
    * [R-package] Remove dependency on gendef for Visual Studio builds (fixes #5608)
    
    * clarify docs
    
    * removed debugging print statement
    
    * Make R CMake install more robust
    
    * Fix doc format; add ToC
    
    * Update build.rst
    
    * Fix AppVeyor
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Enhance nvtx support. (#5636)
    Adding static library option (#5397)
    Fix visual studio output library directories (#5119)
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    [CI] Add Windows GPU to Jenkins CI pipeline (#4463)
    
    * Fix #4462: Use /MT flag consistently for MSVC target
    
    * First attempt at Windows CI
    
    * Distinguish stages in Linux and Windows pipelines
    
    * Try running CMake in Windows pipeline
    
    * Add build step
    Correctly determine cuda version (#4453)
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    support cuda 10.1 (#4223)
    
    * support cuda 10.1
    
    * add cuda 10.1 to jenkins build matrix
    Add cuda forwards compatibility (#3316)
    [R] AppVeyor CI for R package (#2954)
    
    * [R] fix finding R.exe with cmake on WIN when it is in PATH
    
    * [R] appveyor config for R package
    
    * [R] wrap the lines to make R check happier
    
    * [R] install only binary dep-packages in appveyor
    
    * [R] for MSVC appveyor, also build a binary for R package and keep as an artifact
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    [GPU-Plugin] Add load balancing search to gpu_hist. Add compressed iterator. (#2504)
    Cmake improvements (#2487)
    
    * Cmake improvements
    * Add google test to cmake
    [GPU-Plugin] Change GPU plugin to use tree_method parameter, bump cmake version to 3.5 for GPU plugin, add compute architecture 3.5, remove unused cmake files (#2455)
    [GPU-Plugin] Multi-GPU for grow_gpu_hist histogram method using NVIDIA NCCL. (#2395)
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Add pkgconfig to cmake (#5744)
    
    * Add pkgconfig to cmake
    
    * Move xgboost.pc.in to cmake/
    
    Co-authored-by: Peter Jung <peter.jung@heureka.cz>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Enable building with sanitizers. (#3525)
    Run training with empty DMatrix. (#4990)
    
    This makes GPU Hist robust in distributed environment as some workers might not
    be associated with any data in either training or evaluation.
    
    * Disable rabit mock test for now: See #5012 .
    
    * Disable dask-cudf test at prediction for now: See #5003
    
    * Launch dask job for all workers despite they might not have any data.
    * Check 0 rows in elementwise evaluation metrics.
    
       Using AUC and AUC-PR still throws an error.  See #4663 for a robust fix.
    
    * Add tests for edge cases.
    * Add `LaunchKernel` wrapper handling zero sized grid.
    * Move some parts of allreducer into a cu file.
    * Don't validate feature names when the booster is empty.
    
    * Sync number of columns in DMatrix.
    
      As num_feature is required to be the same across all workers in data split
      mode.
    
    * Filtering in dask interface now by default syncs all booster that's not
    empty, instead of using rank 0.
    
    * Fix Jenkins' GPU tests.
    
    * Install dask-cuda from source in Jenkins' test.
    
      Now all tests are actually running.
    
    * Restore GPU Hist tree synchronization test.
    
    * Check UUID of running devices.
    
      The check is only performed on CUDA version >= 10.x, as 9.x doesn't have UUID field.
    
    * Fix CMake policy and project variables.
    
      Use xgboost_SOURCE_DIR uniformly, add policy for CMake >= 3.13.
    
    * Fix copying data to CPU
    
    * Fix race condition in cpu predictor.
    
    * Fix duplicated DMatrix construction.
    
    * Don't download extra nccl in CI script.
    Add CMake option to run Undefined Behavior Sanitizer (UBSan) (#5211)
    
    * Fix related errors.
    Enhance nvtx support. (#5636)
    Fix mingw build with R. (#5918)
    [R] Remove dependency on gendef for Visual Studio builds (fixes #5608) (#5764)
    
    * [R-package] Remove dependency on gendef for Visual Studio builds (fixes #5608)
    
    * clarify docs
    
    * removed debugging print statement
    
    * Make R CMake install more robust
    
    * Fix doc format; add ToC
    
    * Update build.rst
    
    * Fix AppVeyor
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [R-package] fixed inconsistency in R -e calls in FindLibR.cmake (#5438)
    [R-package] changed FindLibR to take advantage of CMake cache (#5427)
    [core] fix slow predict-caching with many classes (#3109)
    
    * fix prediction caching inefficiency for multiclass
    
    * silence some warnings
    
    * redundant if
    
    * workaround for R v3.4.3 bug; fixes #3081
    [R] AppVeyor CI for R package (#2954)
    
    * [R] fix finding R.exe with cmake on WIN when it is in PATH
    
    * [R] appveyor config for R package
    
    * [R] wrap the lines to make R check happier
    
    * [R] install only binary dep-packages in appveyor
    
    * [R] for MSVC appveyor, also build a binary for R package and keep as an artifact
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    Enable building with shared NCCL. (#4447)
    
    * Add `BUILD_WITH_SHARED_NCCL` to CMake.
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Upgrading to NCCL2 (#3404)
    
    * Upgrading to NCCL2
    
    * Part - II of NCCL2 upgradation
    
     - Doc updates to build with nccl2
     - Dockerfile.gpu update for a correct CI build with nccl2
     - Updated FindNccl package to have env-var NCCL_ROOT to take precedence
    
    * Upgrading to v9.2 for CI workflow, since it has the nccl2 binaries available
    
    * Added NCCL2 license + copy the nccl binaries into /usr location for the FindNccl module to find
    
    * Set LD_LIBRARY_PATH variable to pick nccl2 binary at runtime
    
    * Need the nccl2 library download instructions inside Dockerfile.release as well
    
    * Use NCCL2 as a static library
    Add asan.so.5 to cmake script. (#4999)
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Enable building with sanitizers. (#3525)
    Add travis sanitizers tests. (#3557)
    
    * Add travis sanitizers tests.
    
    * Add gcc-7 in Travis.
    * Add SANITIZER_PATH for CMake.
    * Enable sanitizer tests in Travis.
    
    * Fix memory leaks in tests.
    
    * Fix all memory leaks reported by Address Sanitizer.
    * tests/cpp/helpers.h/CreateDMatrix now returns raw pointer.
    Enable building with sanitizers. (#3525)
    Bump header version. (#6056)
    Bump version in header. (#5742)
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    Use configure_file() to configure version only (#4974)
    
    * Avoid writing build_config.h
    
    * Remove build_config.h all together.
    
    * Lint.
    Feature weights (#5962)
    Add XGBoosterGetNumFeature (#5856)
    
    - add GetNumFeature to Learner
    - add XGBoosterGetNumFeature to C API
    - update c-api-demo accordingly
    Move feature names and types of DMatrix from Python to C++. (#5858)
    
    
    * Add thread local return entry for DMatrix.
    * Save feature name and feature type in binary file.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Implement iterative DMatrix. (#5837)
    Clarify meaning of `training` parameter in XGBoosterPredict() (#5604)
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Add number of columns to native data iterator. (#5202)
    
    * Change native data iter into an adapter.
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Tests and documents for new JSON routines. (#5120)
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Model IO in JSON. (#5110)
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Deprecate set group (#4864)
    
    
    
    * Convert jvm package and R package.
    
    * Restore for compatibility.
    Add support for cross-validation using query ID (#4474)
    
    * adding support for matrix slicing with query ID for cross-validation
    
    * hail mary test of unrar installation for windows tests
    
    * trying to modify tests to run in Github CI
    
    * Remove dependency on wget and unrar
    
    * Save error log from R test
    
    * Relax assertion in test_training
    
    * Use int instead of bool in C function interface
    
    * Revise R interface
    
    * Add XGDMatrixSliceDMatrixEx and keep old XGDMatrixSliceDMatrix for API compatibility
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    Remove deprecated C APIs. (#4266)
    Fix #4163: always copy sliced data (#4165)
    
    * Revert "Accept numpy array view. (#4147)"
    
    This reverts commit a985a99cf0dacb26a5d734835473d492d3c2a0df.
    
    * Fix #4163: always copy sliced data
    
    * Remove print() from the test; check shape equality
    
    * Check if 'base' attribute exists
    
    * Fix lint
    
    * Address reviewer comment
    
    * Fix lint
    Accept numpy array view. (#4147)
    
    * Accept array view (slice) in metainfo.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Add callback interface to re-direct console output (#3438)
    
    * Add callback interface to re-direct console output
    
    * Exempt TrackerLogger from custom logging
    
    * Fix lint
    Allow import via python datatable. (#3272)
    
    * Allow import via python datatable.
    
    * Write unit tests
    
    * Refactor dt API functions
    
    * Refactor python code
    
    * Lint fixes
    
    * Address review comments
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Allow compilation with -Werror=strict-prototypes (#3183)
    Fix XGDMatrixFree argument type (#2898)
    Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation (#2530)
    
    * Multi-threaded XGDMatrixCreateFromMat for faster DMatrix creation from numpy arrays for python interface.
    Fix C API header compatibility with C compilers (#2369)
    adding feature contributions to R and gblinear (#2295)
    
    * [gblinear] add features contribution prediction; fix DumpModel bug
    
    * [gbtree] minor changes to PredContrib
    
    * [R] add feature contribution prediction to R
    
    * [R] bump up version; update NEWS
    
    * [gblinear] fix the base_margin issue; fixes #1969
    
    * [R] list of matrices as output of multiclass feature contributions
    
    * [gblinear] make order of DumpModel coefficients consistent: group index changes the fastest
    [R] native routines registration (#2290)
    
    * [R] add native routines registration
    
    * c_api.h needs to include <cstdint> since it uses fixed width integer types
    
    * [R] use registered native routines from R code
    
    * [R] bump version; add info on native routine registration to the contributors guide
    
    * make lint happy
    Add prediction of feature contributions (#2003)
    
    * Add prediction of feature contributions
    
    This implements the idea described at http://blog.datadive.net/interpreting-random-forests/
    which tries to give insight in how a prediction is composed of its feature contributions
    and a bias.
    
    * Support multi-class models
    
    * Calculate learning_rate per-tree instead of using the one from the first tree
    
    * Do not rely on node.base_weight * learning_rate having the same value as the node mean value (aka leaf value, if it were a leaf); instead calculate them (lazily) on-the-fly
    
    * Add simple test for contributions feature
    
    * Check against param.num_nodes instead of checking for non-zero length
    
    * Loop over all roots instead of only the first
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    [jvm-packages] XGBoost4j Windows fixes (#1639)
    
    * Changes for Mingw64 compilation to ensure long is a consistent size.
    
    Mainly impacts the Java API which would not compile, but there may be
    silent errors on Windows with large datasets before this patch (as long
    is 32-bits when compiled with mingw64 even in 64-bit mode).
    
    * Adding ifdefs to ensure it still compiles on MacOS
    
    * Makefile and create_jni.bat changes for Windows.
    
    * Switching XGDMatrixCreateFromCSREx JNI call to use size_t cast
    
    * Fixing lint error, adding profile switching to jvm-packages build to make create-jni.bat get called, adding myself to Contributors.Md
    More robust DMatrix creation from a sparse matrix (#1606)
    
    * [CORE] DMatrix from sparse w/ explicit #col #row; safer arg types
    
    * [python-package] c-api change for _init_from_csr _init_from_csc
    
    * fix spaces
    
    * [R-package] adopt the new XGDMatrixCreateFromCSCEx interface
    
    * [CORE] redirect old sparse creators to new ones
    cmake build system (#1314)
    
    * Changed c api to compile under MSVC
    
    * Include functional.h header for MSVC
    
    * Add cmake build
    doxygen suggested fix
    methods to delete an attribute and get names of available attributes
    XGBoosterCreate api unified to use const DMatrix[] argument
    [JVM] Add Iterator loading API
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [TRAVIS] cleanup travis script
    [LIBXGBOOST] pass demo running.
    [CLI] initial refactor of CLI
    [GBM] Finish migrate all gbms
    [REFACTOR] cleanup structure
    Use ctypes
    ENH: allow python to handle feature names
    API refactor to make fault handling easy
    fix all cpp lint
    fix pkl problem
    fix python windows installation problem, enable mingw compile, but seems mingw dll was not fast in loading
    OK
    some initial try of cachefiles
    remove sync from wrapper.h
    add saveload to raw
    add dump statistics
    make wrapper ok
    add predict leaf indices
    remove cstdio
    add ifdef __cplusplus to wrapper header file
    add create from csc
    add ntree limit
    fix
    FIX: If you are using Windows, __declspec(dllexport) is necessary
    change uint64_t to ulong, to make mac happy, this is final change
    change uint64_t to depend on utils
    change omp loop var to bst_omp_uint, add XGB_DLL to wrapper
    chg size_t to uint64_t
    chg size_t to uint64_t unsigned long in wrapper
    move ncol, row to booster, add set/get uint info
    seems ok
    complete R example
    chg
    finish dump
    workable R wrapper
    add base_margin
    remake the wrapper
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Config for linear updaters. (#5222)
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Implement intrusive ptr (#6129)
    
    
    * Use intrusive ptr for JSON.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Remove remaining reg:linear. (#4544)
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Fix warnings from g++5 or higher (#1510)
    [LIBXGBOOST] pass demo running.
    [LEARNER] refactor learner
    [METRIC] all metric move finished
    [OBJ] Add basic objective function and registry
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Use ellpack for prediction only when sparsepage doesn't exist. (#5504)
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Model IO in JSON. (#5110)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    [Breaking] Remove num roots. (#5059)
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Restrict access to `cfg_` in gbm. (#4801)
    
    * Restrict access to `cfg_` in gbm.
    
    * Verify having correct updaters.
    
    * Remove `grow_global_histmaker`
    
    This updater is the same as `grow_histmaker`.  The former is not in our
    document so we just remove it.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Offload some configurations into GBM. (#4553)
    
    
    This is part 1 of refactoring configuration.
    
    * Move tree heuristic configurations.
    * Split up declarations and definitions for GBTree.
    * Implement UseGPU in gbm.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Revert "Fix #3485, #3540: Don't use dropout for predicting test sets" (#3563)
    
    * Revert "Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)"
    
    This reverts commit 44811f233071c5805d70c287abd22b155b732727.
    
    * Document behavior of predict() for DART booster
    
    * Add notice to parameter.rst
    Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)
    
    * Fix #3485, #3540: Don't use dropout for predicting test sets
    
    Dropout (for DART) should only be used at training time.
    
    * Add regression test
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    [WIP] Extract prediction into separate interface (#2531)
    
    * [WIP] Extract prediction into separate interface
    
    * Add copyright, fix linter errors
    
    * Add predictor to amalgamation
    
    * Fix documentation
    
    * Move prediction cache into predictor, add GBTreeModel
    
    * Updated predictor doc comments
    adding feature contributions to R and gblinear (#2295)
    
    * [gblinear] add features contribution prediction; fix DumpModel bug
    
    * [gbtree] minor changes to PredContrib
    
    * [R] add feature contribution prediction to R
    
    * [R] bump up version; update NEWS
    
    * [gblinear] fix the base_margin issue; fixes #1969
    
    * [R] list of matrices as output of multiclass feature contributions
    
    * [gblinear] make order of DumpModel coefficients consistent: group index changes the fastest
    Add prediction of feature contributions (#2003)
    
    * Add prediction of feature contributions
    
    This implements the idea described at http://blog.datadive.net/interpreting-random-forests/
    which tries to give insight in how a prediction is composed of its feature contributions
    and a bias.
    
    * Support multi-class models
    
    * Calculate learning_rate per-tree instead of using the one from the first tree
    
    * Do not rely on node.base_weight * learning_rate having the same value as the node mean value (aka leaf value, if it were a leaf); instead calculate them (lazily) on-the-fly
    
    * Add simple test for contributions feature
    
    * Check against param.num_nodes instead of checking for non-zero length
    
    * Loop over all roots instead of only the first
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    [CORE] Refactor cache mechanism (#1540)
    Fix warnings from g++5 or higher (#1510)
    [TRAVIS] cleanup travis script
    [LIBXGBOOST] pass demo running.
    [LEARNER] refactor learner
    [GBM] remove need to explicit InitModel, rename save/load
    [LEARNER] Init learner interface
    [GBM] Finish migrate all gbms
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [Update] remove rabit subtree, use submodule, move code
    [REFACTOR] cleanup structure
    spelling, wording, and doc fixes in c++ code
    
    I was reading through the code and fixing some things in the comments.
    Only a few trivial actual code changes were made to make things more
    readable.
    lint half way
    add single instance prediction
    change makefile to lazy checkpt, fix col splt code
    add rabit checkpoint to xgb
    add predict leaf indices
    checkin continue training
    new change for mpi
    add ntree limit
    complete refactor data.h, now replies on iterator to access column
    check in linear model
    chg root index to booster info, need review
    first version that reproduce binary classification demo
    pass fmatrix as const
    mv code into src
    start unity refactor
    CPU predict performance improvement (#6127)
    
    Co-authored-by: ShvetsKS <kirill.shvets@intel.com>
    Improve JSON format for categorical features. (#6128)
    
    * Gather categories for all nodes.
    Updates to GPUTreeShap (#6087)
    
    * Extract paths on device
    
    * Update GPUTreeShap
    Expand categorical node. (#6028)
    
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Swap byte-order in binary serializer to support big-endian arch (#5813)
    
    * fixed some endian issues
    
    * Use dmlc::ByteSwap() to simplify code
    
    * Fix lint check
    
    * [CI] Add test for s390x
    
    * Download latest CMake on s390x
    
    * Fix a bug in my code
    
    * Save magic number in dmatrix with byteswap on big-endian machine
    
    * Save version in binary with byteswap on big-endian machine
    
    * Load scalar with byteswap in MetaInfo
    
    * Add a debugging message
    
    * Handle arrays correctly when byteswapping
    
    * EOF can also be 255
    
    * Handle magic number in MetaInfo carefully
    
    * Skip Tree.Load test for big-endian, since the test manually builds little-endian binary model
    
    * Handle missing packages in Python tests
    
    * Don't use boto3 in model compatibility tests
    
    * Add s390 Docker file for local testing
    
    * Add model compatibility tests
    
    * Add R compatibility test
    
    * Revert "Add R compatibility test"
    
    This reverts commit c2d2bdcb7dbae133cbb927fcd20f7e83ee2b18a8.
    
    Co-authored-by: Qi Zhang <q.zhang@ibm.com>
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Requires setting leaf stat when expanding tree. (#5501)
    
    * Fix GPU Hist feature importance.
    Check whether current updater can modify a tree. (#5406)
    
    * Check whether current updater can modify a tree.
    
    * Fix tree model JSON IO for pruned trees.
    Fix pruner. (#5335)
    
    
    * Honor the tree depth.
    * Prevent pruning pruned node.
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Fix num_roots to be 1. (#5165)
    Model IO in JSON. (#5110)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    [Breaking] Remove num roots. (#5059)
    Don't use 0 for "fresh leaf". (#5084)
    
    
    * Allow using right child as marker for Exact tree_method.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
     Combine thread launches into single launch per tree for gpu_hist (#4343)
    
    * Combine thread launches into single launch per tree for gpu_hist
    algorithm.
    
    * Address deprecation warning
    
    * Add manual column sampler constructor
    
    * Turn off omp dynamic to get a guaranteed number of threads
    
    * Enable openmp in cuda code
    Fix node reuse. (#4404)
    
    * Reinitialize `_sindex` when reallocating a deleted node.
    Further optimisations for gpu_hist. (#4283)
    
    - Fuse final update position functions into a single more efficient kernel
    
    - Refactor gpu_hist with a more explicit ellpack  matrix representation
    Fix gpu_hist apply_split test. (#4158)
     Require leaf statistics when expanding tree (#4015)
    
    * Cache left and right gradient sums
    
    * Require leaf statistics when expanding tree
    Reduce tree expand boilerplate code (#4008)
    Combine TreeModel and RegTree (#3995)
    Remove leaf vector, add tree serialisation test, fix Windows tests (#3989)
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    [R] maintenance Nov 2017; SHAP plots (#2888)
    
    * [R] fix predict contributions for data with no colnames
    
    * [R] add a render parameter for xgb.plot.multi.trees; fixes #2628
    
    * [R] update Rd's
    
    * [R] remove unnecessary dep-package from R cmake install
    
    * silence type warnings; readability
    
    * [R] silence complaint about incomplete line at the end
    
    * [R] initial version of xgb.plot.shap()
    
    * [R] more work on xgb.plot.shap
    
    * [R] enforce black font in xgb.plot.tree; fixes #2640
    
    * [R] if feature names are available, check in predict that they are the same; fixes #2857
    
    * [R] cran check and lint fixes
    
    * remove tabs
    
    * [R] add references; a test for plot.shap
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    Fix broken make on windows (#2499)
    
    * fix Makefile for make on windows
    
    * clean up compilation warnings
    
    * fix for `no file name for include` make warning
    Add prediction of feature contributions (#2003)
    
    * Add prediction of feature contributions
    
    This implements the idea described at http://blog.datadive.net/interpreting-random-forests/
    which tries to give insight in how a prediction is composed of its feature contributions
    and a bias.
    
    * Support multi-class models
    
    * Calculate learning_rate per-tree instead of using the one from the first tree
    
    * Do not rely on node.base_weight * learning_rate having the same value as the node mean value (aka leaf value, if it were a leaf); instead calculate them (lazily) on-the-fly
    
    * Add simple test for contributions feature
    
    * Check against param.num_nodes instead of checking for non-zero length
    
    * Loop over all roots instead of only the first
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    [TRAVIS] cleanup travis script
    [LIBXGBOOST] pass demo running.
    [GBM] remove need to explicit InitModel, rename save/load
    [TREE] Enable updater registry
    [TREE] Finalize regression tree refactor
    [TREE] Refactor to new logging
    [TREE] move tree model
    [REFACTOR] cleanup structure
    spelling, wording, and doc fixes in c++ code
    
    I was reading through the code and fixing some things in the comments.
    Only a few trivial actual code changes were made to make things more
    readable.
    lint half way
    fix
    some potential fix
    try fix memleak when test data have more features than training
    change dump stats
    fix a bug in node sindex set
    windows strange
    everything is ready, except for propose
    fix load bug
    middle version
    remove using std from cpp
    rename SparseBatch to RowBatch
    rename findex->index
    change things back
    fix by giulio
    fix by giulio
    a fixed version
    tstats now depend on param
    add tree refresher, need review
    change dense fvec logic to tree
    mv code into src
    start unity refactor
    Cleanup warnings. (#5247)
    
    From clang-tidy-9 and gcc-7: Invalid case style, narrowing definition, wrong
    initialization order, unused variables.
    Model IO in JSON. (#5110)
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Implement tree model dump with code generator. (#4602)
    
    * Implement tree model dump with a code generator.
    
    * Split up generators.
    * Implement graphviz generator.
    * Use pattern matching.
    
    * [Breaking] Return a Source in `to_graphviz` instead of Digraph in Python package.
    
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    [GPU-Plugin] Integration of a faster version of grow_gpu plugin into mainstream (#2360)
    
    * Integrating a faster version of grow_gpu plugin
    1. Removed the older files to reduce duplication
    2. Moved all of the grow_gpu files under 'exact' folder
    3. All of them are inside 'exact' namespace to avoid any conflicts
    4. Fixed a bug in benchmark.py while running only 'grow_gpu' plugin
    5. Added cub and googletest submodules to ease integration and unit-testing
    6. Updates to CMakeLists.txt to directly build cuda objects into libxgboost
    
    * Added support for building gpu plugins through make flow
    1. updated makefile and config.mk to add right targets
    2. added unit-tests for gpu exact plugin code
    
    * 1. Added support for building gpu plugin using 'make' flow as well
    2. Updated instructions for building and testing gpu plugin
    
    * Fix travis-ci errors for PR#2360
    1. lint errors on unit-tests
    2. removed googletest, instead depended upon dmlc-core provide gtest cache
    
    * Some more fixes to travis-ci lint failures PR#2360
    
    * Added Rory's copyrights to the files containing code from both.
    
    * updated copyright statement as per Rory's request
    
    * moved the static datasets into a script to generate them at runtime
    
    * 1. memory usage print when silent=0
    2. tests/ and test/ folder organization
    3. removal of the dependency of googletest for just building xgboost
    4. coding style updates for .cuh as well
    
    * Fixes for compilation warnings
    
    * add cuda object files as well when JVM_BINDINGS=ON
    [TREE] Finalize regression tree refactor
    Update GPUTreeshap (#6163)
    
    * Reduce shap test duration
    
    * Test interoperability with shap package
    
    * Add feature interactions
    
    * Update GPUTreeShap
    Thread-safe prediction by making the prediction cache thread-local. (#5853)
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Move prediction cache to Learner. (#5220)
    
    * Move prediction cache into Learner.
    
    * Clean-ups
    
    - Remove duplicated cache in Learner and GBM.
    - Remove ad-hoc fix of invalid cache.
    - Remove `PredictFromCache` in predictors.
    - Remove prediction cache for linear altogether, as it's only moving the
      prediction into training process but doesn't provide any actual overall speed
      gain.
    - The cache is now unique to Learner, which means the ownership is no longer
      shared by any other components.
    
    * Changes
    
    - Add version to prediction cache.
    - Use weak ptr to check expired DMatrix.
    - Pass shared pointer instead of raw pointer.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    [Breaking] Remove num roots. (#5059)
    Use DART tree weights when computing SHAPs (#5050)
    
    This PR fixes tree weights in dart being ignored when computing contributions.
    
    * Fix ellpack page source link.
    * Add tree weights to compute contribution.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    [GPU-Plugin] Add GPU accelerated prediction (#2593)
    
    * [GPU-Plugin] Add GPU accelerated prediction
    
    * Improve allocation message
    
    * Update documentation
    
    * Resolve linker error for predictor
    
    * Add unit tests
    [WIP] Extract prediction into separate interface (#2531)
    
    * [WIP] Extract prediction into separate interface
    
    * Add copyright, fix linter errors
    
    * Add predictor to amalgamation
    
    * Fix documentation
    
    * Move prediction cache into predictor, add GBTreeModel
    
    * Updated predictor doc comments
    Unify evaluation functions. (#6037)
    Expand categorical node. (#6028)
    
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Implement GK sketching on GPU. (#5846)
    
    * Implement GK sketching on GPU.
    * Strong tests on quantile building.
    * Handle sparse dataset by binary searching the column index.
    * Hypothesis test on dask.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    [R] Address warnings to comply with CRAN submission policy (#5600)
    
    * [R] Address warnings to comply with CRAN submission policy
    
    * Include <xgboost/logging.h>
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Reduce span check overhead. (#5464)
    Fix span constructor. (#5166)
    
    * Make sure copy constructor is used.
    Span: use `size_t' for index_type,  add `front' and `back'. (#4935)
    
    * Use `size_t' for index_type.  Add `front' and `back'.
    
    * Remove a batch of `static_cast'.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Mimic cuda assert output in span check. (#4762)
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Feature interaction for GPU Hist. (#4534)
    
    * GPU hist Interaction Constraints.
    * Duplicate related parameters.
    * Add tests for CPU interaction constraint.
    * Add better error reporting.
    * Thorough tests.
    Fix clang-tidy warnings. (#4149)
    
    
    * Upgrade gtest for clang-tidy.
    * Use CMake to install GTest instead of mv.
    * Don't enforce clang-tidy to return 0 due to errors in thrust.
    * Add a small test for tidy itself.
    
    * Reformat.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Fix empty subspan. (#4151)
    
    * Silent the death tests.
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
    Use Span in gpu coordinate. (#4029)
    
    * Use Span in gpu coordinate.
    
    * Use Span in device code.
    * Fix shard size calculation.
      - Use lower_bound instead of upper_bound.
    * Check empty devices.
    Multi-GPU support in GPUPredictor. (#3738)
    
    * Multi-GPU support in GPUPredictor.
    
    - GPUPredictor is multi-GPU
    - removed DeviceMatrix, as it has been made obsolete by using HostDeviceVector in DMatrix
    
    * Replaced pointers with spans in GPUPredictor.
    
    * Added a multi-GPU predictor test.
    
    * Fix multi-gpu test.
    
    * Fix n_rows < n_gpus.
    
    * Reinitialize shards when GPUSet is changed.
    * Tests range of data.
    
    * Remove commented code.
    
    * Remove commented code.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Fix ptrdiff_t namespace in Span. (#3588)
    
    Fix #3587.
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    [Blocking] Fix #3840: Clean up logic for parsing tree_method parameter (#3849)
    
    * Clean up logic for converting tree_method to updater sequence
    
    * Use C++11 enum class for extra safety
    
    Compiler will give warnings if switch statements don't handle all
    possible values of C++11 enum class.
    
    Also allow enum class to be used as DMLC parameter.
    
    * Fix compiler error + lint
    
    * Address reviewer comment
    
    * Better docstring for DECLARE_FIELD_ENUM_CLASS
    
    * Fix lint
    
    * Add C++ test to see if tree_method is recognized
    
    * Fix clang-tidy error
    
    * Add test_learner.h to R package
    
    * Update comments
    
    * Fix lint error
    Implement extend method for meta info. (#5800)
    
    * Implement extend for host device vector.
    Implement `Empty` method for host device vector. (#5781)
    
    * Fix accessing nullptr.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Implement host span. (#5459)
    Extensible binary serialization format for DMatrix::MetaInfo (#5187)
    
    * Turn xgboost::DataType into C++11 enum class
    
    * New binary serialization format for DMatrix::MetaInfo
    
    * Fix clang-tidy
    
    * Fix c++ test
    
    * Implement new format proposal
    
    * Move helper functions to anonymous namespace; remove unneeded field
    
    * Fix lint
    
    * Add shape.
    
    * Keep only roundtrip test.
    
    * Fix test.
    
    * various fixes
    
    * Update data.cc
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Lazy initialization of device vector. (#5173)
    
    * Lazy initialization of device vector.
    
    * Fix #5162.
    
    * Disable copy constructor of HostDeviceVector.  Prevents implicit copying.
    
    * Fix CPU build.
    
    * Bring back move assignment operator.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    remove device shards (#4867)
    further cleanup of single process multi-GPU code (#4810)
    
    
    * use subspan in gpu predictor instead of copying
    * Revise `HostDeviceVector`
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    mgpu predictor using explicit offsets (#4438)
    
    * mgpu prediction using explicit sharding
    More explict sharding methods for device memory (#4396)
    
    * Rename the Reshard method to Shard
    
    * Add a new Reshard method for sharding a vector that's already sharded
    Improve HostDeviceVector exception safety (#4301)
    
    
    * make the assignments of HostDeviceVector exception safe.
    * storing a dummy GPUDistribution instance in HDV for CPU based code.
    * change testxgboost binary location to build directory.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Clean up training code. (#3825)
    
    * Remove GHistRow, GHistEntry, GHistIndexRow.
    * Remove kSimpleStats.
    * Remove CheckInfo, SetLeafVec in GradStats and in SKStats.
    * Clean up the GradStats.
    * Cleanup calcgain.
    * Move LossChangeMissing out of common.
    * Remove [] operator from GHistIndexBlock.
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Implement transform to reduce CPU/GPU code duplication. (#3643)
    
    * Implement Transform class.
    * Add tests for softmax.
    * Use Transform in regression, softmax and hinge objectives, except for Cox.
    * Mark old gpu objective functions deprecated.
    * static_assert for softmax.
    * Split up multi-gpu tests.
    Implement devices to devices reshard. (#3721)
    
    * Force clearing device memory before Reshard.
    * Remove calculating row_segments for gpu_hist and gpu_sketch.
    * Guard against changing device.
    Fix gpu devices. (#3693)
    
    * Fix gpu_set normalized and unnormalized.
    * Fix DeviceSpan.
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Merge generic device helper functions into gpu set. (#3626)
    
    * Remove the use of old NDevices* functions.
    * Use GPUSet in timer.h.
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Multi-GPU HostDeviceVector. (#3287)
    
    * Multi-GPU HostDeviceVector.
    
    - HostDeviceVector instances can now span multiple devices, defined by GPUSet struct
    - the interface of HostDeviceVector has been modified accordingly
    - GPU objective functions are now multi-GPU
    - GPU predicting from cache is now multi-GPU
    - avoiding omp_set_num_threads() calls
    - other minor changes
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Ensure that LoadSequentialFile() actually read the whole file (#5831)
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Model IO in JSON. (#5110)
    Add Json integer, remove specialization. (#4739)
    A simple Json implementation for future use. (#4708)
    
    * A simple Json implementation for future use.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Porting elementwise metrics to GPU. (#3952)
    
    * Port elementwise metrics to GPU.
    
    * All elementwise metrics are converted to static polymorphic.
    * Create a reducer for metrics reduction.
    * Remove const of Metric::Eval to accommodate CubMemory.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [TRAVIS] cleanup travis script
    [LIBXGBOOST] pass demo running.
    [LEARNER] refactor learner
    [METRIC] all metric move finished
    Reduce compile warnings (#6198)
    
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Check whether current updater can modify a tree. (#5406)
    
    * Check whether current updater can modify a tree.
    
    * Fix tree model JSON IO for pruned trees.
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Implement JSON IO for updaters (#5094)
    
    * Implement JSON IO for updaters.
    
    * Remove parameters in split evaluator.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Objective function evaluation on GPU with minimal PCIe transfers (#2935)
    
    * Added GPU objective function and no-copy interface.
    
    - xgboost::HostDeviceVector<T> syncs automatically between host and device
    - no-copy interfaces have been added
    - default implementations just sync the data to host
      and call the implementations with std::vector
    - GPU objective function, predictor, histogram updater process data
      directly on GPU
    Fix compilation on OS X with GCC 7 (#2256)
    
    * Fix compilation on OS X with GCC 7
    
    Compilation failed with
    
    In file included from src/tree/tree_updater.cc:6:0:
    include/xgboost/tree_updater.h:75:46: error: 'function' is not a member of 'std'
                                             std::function<TreeUpdater* ()> > {
    
    caused by a missing <functional> include.
    
    * Fixed another occurence of that issue spotted by @ClimberPG
    [GPU Plugin] Fast histogram speed improvements. Updated benchmarks. (#2258)
    Improve multi-threaded performance (#2104)
    
    * Add UpdatePredictionCache() option to updaters
    
    Some updaters (e.g. fast_hist) has enough information to quickly compute
    prediction cache for the training data. Each updater may override
    UpdaterPredictionCache() method to update the prediction cache. Note: this
    trick does not apply to validation data.
    
    * Respond to code review
    
    * Disable some debug messages by default
    * Document UpdatePredictionCache() interface
    * Remove base_margin logic from UpdatePredictionCache() implementation
    * Do not take pointer to cfg, as reference may get stale
    
    * Improve multi-threaded performance
    
    * Use columnwise accessor to accelerate ApplySplit() step,
      with support for a compressed representation
    * Parallel sort for evaluation step
    * Inline BuildHist() function
    * Cache gradient pairs when building histograms in BuildHist()
    
    * Add missing #if macro
    
    * Respond to code review
    
    * Use wrapper to enable parallel sort on Linux
    
    * Fix C++ compatibility issues
    
    * MSVC doesn't support unsigned in OpenMP loops
    * gcc 4.6 doesn't support using keyword
    
    * Fix lint issues
    
    * Respond to code review
    
    * Fix bug in ApplySplitSparseData()
    
    * Attempting to read beyond the end of a sparse column
    * Mishandling the case where an entire range of rows have missing values
    
    * Fix training continuation bug
    
    Disable UpdatePredictionCache() in the first iteration. This way, we can
    accomodate the scenario where we build off of an existing (nonempty) ensemble.
    
    * Add regression test for fast_hist
    
    * Respond to code review
    
    * Add back old version of ApplySplitSparseData
    Fix warnings from g++5 or higher (#1510)
    cmake build system (#1314)
    
    * Changed c api to compile under MSVC
    
    * Include functional.h header for MSVC
    
    * Add cmake build
    [TRAVIS] cleanup travis script
    [LIBXGBOOST] pass demo running.
    [LEARNER] refactor learner
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [TREE] finish move of updater
    [TREE] Enable updater registry
    Implement intrusive ptr (#6129)
    
    
    * Use intrusive ptr for JSON.
    Implement fast number serialization routines. (#5772)
    
    * Implement ryu algorithm.
    * Implement integer printing.
    * Full coverage roundtrip test.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Convenient methods for JSON integer. (#5089)
    
    * Fix parsing empty object.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    monitor for distributed envorinment. (#4829)
    
    * Collect statistics from other ranks in monitor.
    
    * Workaround old GCC bug.
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Add Json integer, remove specialization. (#4739)
    A simple Json implementation for future use. (#4708)
    
    * A simple Json implementation for future use.
    Unify evaluation functions. (#6037)
    Expand categorical node. (#6028)
    
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Feature weights (#5962)
    Remove rabit dependency on public headers. (#6005)
    Unify CPU hist sketching (#5880)
    Move feature names and types of DMatrix from Python to C++. (#5858)
    
    
    * Add thread local return entry for DMatrix.
    * Save feature name and feature type in binary file.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Implement iterative DMatrix. (#5837)
    Implement a DMatrix Proxy. (#5803)
    Implement extend method for meta info. (#5800)
    
    * Implement extend for host device vector.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Move device dmatrix construction code into ellpack. (#5623)
    Remove dead code. (#5635)
    Fix build on big endian CPUs (#5617)
    
    * Fix build on big endian CPUs
    
    * Clang-tidy
    Set device in device dmatrix. (#5596)
    Group aware GPU sketching. (#5551)
    
    * Group aware GPU weighted sketching.
    
    * Distribute group weights to each data point.
    * Relax the test.
    * Validate input meta info.
    * Fix metainfo copy ctor.
    Fix slice and get info. (#5552)
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    Fix memory usage of device sketching (#5407)
    Remove unnecessary DMatrix methods (#5324)
    Predict on Ellpack. (#5327)
    
    * Unify GPU prediction node.
    * Add `PageExists`.
    * Dispatch prediction on input data for GPU Predictor.
    Remove SimpleCSRSource (#5315)
    Extensible binary serialization format for DMatrix::MetaInfo (#5187)
    
    * Turn xgboost::DataType into C++11 enum class
    
    * New binary serialization format for DMatrix::MetaInfo
    
    * Fix clang-tidy
    
    * Fix c++ test
    
    * Implement new format proposal
    
    * Move helper functions to anonymous namespace; remove unneeded field
    
    * Fix lint
    
    * Add shape.
    
    * Keep only roundtrip test.
    
    * Fix test.
    
    * various fixes
    
    * Update data.cc
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Implement cudf construction with adapters. (#5189)
    Lazy initialization of device vector. (#5173)
    
    * Lazy initialization of device vector.
    
    * Fix #5162.
    
    * Disable copy constructor of HostDeviceVector.  Prevents implicit copying.
    
    * Fix CPU build.
    
    * Bring back move assignment operator.
    Use adapters for SparsePageDMatrix (#5092)
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    [Breaking] Remove num roots. (#5059)
    External data adapters (#5044)
    
    * Use external data adapters as lightweight intermediate layer between external data and DMatrix
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Write ELLPACK pages to disk (#4879)
    
    * add ellpack source
    * add batch param
    * extract function to parse cache info
    * construct ellpack info separately
    * push batch to ellpack page
    * write ellpack page.
    * make sparse page source reusable
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Support feature names/types for cudf. (#4902)
    
    * Implement most of the pandas procedure for cudf except for type conversion.
    * Requires an array of interfaces in metainfo.
    Move ellpack page construction into DMatrix (#4833)
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Enable natural copies of the batch iterators without the need of the clone method (#4748)
    
    - the synthesized copy constructor should do the appropriate job
    remove the qids_ field in MetaInfo (#4744)
    Refactor DMatrix to return batches of different page types (#4686)
    
    
    * Use explicit template parameter for specifying page type.
    remove RowSet which is no longer being used (#4697)
    Fix CPU hist init for sparse dataset. (#4625)
    
    * Fix CPU hist init for sparse dataset.
    
    * Implement sparse histogram cut.
    * Allow empty features.
    
    * Fix windows build, don't use sparse in distributed environment.
    
    * Comments.
    
    * Smaller threshold.
    
    * Fix windows omp.
    
    * Fix msvc lambda capture.
    
    * Fix MSVC macro.
    
    * Fix MSVC initialization list.
    
    * Fix MSVC initialization list x2.
    
    * Preserve categorical feature behavior.
    
    * Rename matrix to sparse cuts.
    * Reuse UseGroup.
    * Check for categorical data when adding cut.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    
    * Sanity check.
    
    * Fix comments.
    
    * Fix comment.
    Fix external memory for get column batches. (#4622)
    
    * Fix external memory for get column batches.
    
    This fixes two bugs:
    
    * Use PushCSC for get column batches.
    * Don't remove the created temporary directory before finishing test.
    
    * Check all pages.
    Initial support for external memory in gpu_predictor (#4284)
    add a test for cpu predictor using external memory (#4308)
    
    * add a test for cpu predictor using external memory
    
    * allow different page size for testing
    Add PushCSC for SparsePage. (#4193)
    
    * Add PushCSC for SparsePage.
    
    * Move Push* definitions into cc file.
    * Add std:: prefix to `size_t` make clang++ happy.
    * Address monitor count == 0.
    Fix #4163: always copy sliced data (#4165)
    
    * Revert "Accept numpy array view. (#4147)"
    
    This reverts commit a985a99cf0dacb26a5d734835473d492d3c2a0df.
    
    * Fix #4163: always copy sliced data
    
    * Remove print() from the test; check shape equality
    
    * Check if 'base' attribute exists
    
    * Fix lint
    
    * Address reviewer comment
    
    * Fix lint
    Accept numpy array view. (#4147)
    
    * Accept array view (slice) in metainfo.
    [BLOCKING] fix the issue with infrequent feature (#4045)
    
    * fix the issue with infrequent feature
    
    * handle exception
    
    * use only 2 workers
    
    * address the comments
    Add basic unittests for gpu-hist method. (#3785)
    
    * Split building histogram into separated class.
    * Extract `InitCompressedRow` definition.
    * Basic tests for gpu-hist.
    * Document the code more verbosely.
    * Removed `HistCutUnit`.
    * Removed some duplicated copies in `GPUHistMaker`.
    * Implement LCG and use it in tests.
    Dmatrix refactor stage 2 (#3395)
    
    * DMatrix refactor 2
    
    * Remove buffered rowset usage where possible
    
    * Transition to c++11 style iterators for row access
    
    * Transition column iterators to C++ 11
    Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage. (#3446)
    
    * Replaced std::vector with HostDeviceVector in MetaInfo and SparsePage.
    
    - added distributions to HostDeviceVector
    - using HostDeviceVector for labels, weights and base margings in MetaInfo
    - using HostDeviceVector for offset and data in SparsePage
    - other necessary refactoring
    
    * Added const version of HostDeviceVector API calls.
    
    - const versions added to calls that can trigger data transfers, e.g. DevicePointer()
    - updated the code that uses HostDeviceVector
    - objective functions now accept const HostDeviceVector<bst_float>& for predictions
    
    * Updated src/linear/updater_gpu_coordinate.cu.
    
    * Added read-only state for HostDeviceVector sync.
    
    - this means no copies are performed if both host and devices access
      the HostDeviceVector read-only
    
    * Fixed linter and test errors.
    
    - updated the lz4 plugin
    - added ConstDeviceSpan to HostDeviceVector
    - using device % dh::NVisibleDevices() for the physical device number,
      e.g. in calls to cudaSetDevice()
    
    * Fixed explicit template instantiation errors for HostDeviceVector.
    
    - replaced HostDeviceVector<unsigned int> with HostDeviceVector<int>
    
    * Fixed HostDeviceVector tests that require multiple GPUs.
    
    - added a mock set device handler; when set, it is called instead of cudaSetDevice()
    Span class. (#3548)
    
    * Add basic Span class based on ISO++20.
    
    * Use Span<Entry const> instead of Inst in SparsePage.
    
    * Add DeviceSpan in HostDeviceVector, use it in regression obj.
    Add qid like ranklib format (#2749)
    
    * add qid for https://github.com/dmlc/xgboost/issues/2748
    
    * change names
    
    * change spaces
    
    * change qid to bst_uint type
    
    * change qid type to size_t
    
    * change qid first to SIZE_MAX
    
    * change qid type from size_t to uint64_t
    
    * update dmlc-core
    
    * fix qids name error
    
    * fix group_ptr_ error
    
    * Style fix
    
    * Add qid handling logic to SparsePage
    
    * New MetaInfo format + backward compatibility fix
    
    Old MetaInfo format (1.0) doesn't contain qid field. We still want to be able
    to read from MetaInfo files saved in old format. Also, define a new format
    (2.0) that contains the qid field. This way, we can distinguish files that
    contain qid and those that do not.
    
    * Update MetaInfo test
    
    * Simply group assignment logic
    
    * Explicitly set qid=nullptr in NativeDataIter
    
    NativeDataIter's callback does not support qid field. Users of NativeDataIter
    will need to call setGroup() function separately to set group information.
    
    * Save qids_ in SaveBinary()
    
    * Upgrade dmlc-core submodule
    
    * Add a test for reading qid
    
    * Add contributor
    
    * Check the size of qids_
    
    * Document qid format
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    GPU binning and compression. (#3319)
    
    * GPU binning and compression.
    
    - binning and index compression are done inside the DeviceShard constructor
    - in case of a DMatrix with multiple row batches, it is first converted into a single row batch
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Refactor linear modelling and add new coordinate descent updater (#3103)
    
    * Refactor linear modelling and add new coordinate descent updater
    
    * Allow unsorted column iterator
    
    * Add prediction cacheing to gblinear
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    Histogram Optimized Tree Grower (#1940)
    
    * Support histogram-based algorithm + multiple tree growing strategy
    
    * Add a brand new updater to support histogram-based algorithm, which buckets
      continuous features into discrete bins to speed up training. To use it, set
      `tree_method = fast_hist` to configuration.
    * Support multiple tree growing strategies. For now, two policies are supported:
      * `grow_policy=depthwise` (default):  favor splitting at nodes closest to the
        root, i.e. grow depth-wise.
      * `grow_policy=lossguide`: favor splitting at nodes with highest loss change
    * Improve single-threaded performance
      * Unroll critical loops
      * Introduce specialized code for dense data (i.e. no missing values)
    * Additional training parameters: `max_leaves`, `max_bin`, `grow_policy`, `verbose`
    
    * Adding a small test for hist method
    
    * Fix memory error in row_set.h
    
    When std::vector is resized, a reference to one of its element may become
    stale. Any such reference must be updated as well.
    
    * Resolve cross-platform compilation issues
    
    * Versions of g++ older than 4.8 lacks support for a few C++11 features, e.g.
      alignas(*) and new initializer syntax. To support g++ 4.6, use pre-C++11
      initializer and remove alignas(*).
    * Versions of MSVC older than 2015 does not support alignas(*). To support
      MSVC 2012, remove alignas(*).
    * For g++ 4.8 and newer, alignas(*) is enabled for performance benefits.
    * Some old compilers (MSVC 2012, g++ 4.6) do not support template aliases
      (which uses `using` to declate type aliases). So always use `typedef`.
    
    * Fix a host of CI issues
    
    * Remove dependency for libz on osx
    * Fix heading for hist_util
    * Fix minor style issues
    * Add missing #include
    * Remove extraneous logging
    
    * Enable tree_method=hist in R
    
    * Renaming HistMaker to GHistBuilder to avoid confusion
    
    * Fix R integration
    
    * Respond to style comments
    
    * Consistent tie-breaking for priority queue using timestamps
    
    * Last-minute style fixes
    
    * Fix issuecomment-271977647
    
    The way we quantize data is broken. The agaricus data consists of all
    categorical values. When NAs are converted into 0's,
    `HistCutMatrix::Init` assign both 0's and 1's to the same single bin.
    
    Why? gmat only the smallest value (0) and an upper bound (2), which is twice
    the maximum value (1). Add the maximum value itself to gmat to fix the issue.
    
    * Fix issuecomment-272266358
    
    * Remove padding from cut values for the continuous case
    * For categorical/ordinal values, use midpoints as bin boundaries to be safe
    
    * Fix CI issue -- do not use xrange(*)
    
    * Fix corner case in quantile sketch
    
    Signed-off-by: Philip Cho <chohyu01@cs.washington.edu>
    
    * Adding a test for an edge case in quantile sketcher
    
    max_bin=2 used to cause an exception.
    
    * Fix fast_hist test
    
    The test used to require a strictly increasing Test AUC for all examples.
    One of them exhibits a small blip in Test AUC before achieving a Test AUC
    of 1. (See bottom.)
    
    Solution: do not require monotonic increase for this particular example.
    
    [0] train-auc:0.99989 test-auc:0.999497
    [1] train-auc:1 test-auc:0.999749
    [2] train-auc:1 test-auc:0.999749
    [3] train-auc:1 test-auc:0.999749
    [4] train-auc:1 test-auc:0.999749
    [5] train-auc:1 test-auc:0.999497
    [6] train-auc:1 test-auc:1
    [7] train-auc:1 test-auc:1
    [8] train-auc:1 test-auc:1
    [9] train-auc:1 test-auc:1
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    [MEM] Add rowset struct to save memory with billion level rows
    [TRAVIS] cleanup travis script
    [LIBXGBOOST] pass demo running.
    [CLI] initial refactor of CLI
    [LEARNER] refactor learner
    [TREE] Move colmaker
    [OBJ] Add basic objective function and registry
    [DATA] basic data refactor done, basic version of csr source.
    Data interface ready
    [REFACTOR] cleanup structure
    Remove rabit dependency on public headers. (#6005)
    Add XGBoosterGetNumFeature (#5856)
    
    - add GetNumFeature to Learner
    - add XGBoosterGetNumFeature to C API
    - update c-api-demo accordingly
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Fix dump model. (#5485)
    Thread safe, inplace prediction. (#5389)
    
    Normal prediction with DMatrix is now thread safe with locks.  Added inplace prediction is lock free thread safe.
    
    When data is on device (cupy, cudf), the returned data is also on device.
    
    * Implementation for numpy, csr, cudf and cupy.
    
    * Implementation for dask.
    
    * Remove sync in simple dmatrix.
    Move thread local entry into Learner. (#5396)
    
    * Move thread local entry into Learner.
    
    This is an attempt to workaround CUDA context issue in static variable, where
    the CUDA context can be released before device vector.
    
    * Add PredictionEntry to thread local entry.
    
    This eliminates one copy of prediction vector.
    
    * Don't define CUDA C API in a namespace.
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Pass shared pointer instead of raw pointer to Learner. (#5302)
    
    Extracted from https://github.com/dmlc/xgboost/pull/5220 .
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    JSON configuration IO. (#5111)
    
    * Add saving/loading JSON configuration.
    * Implement Python pickle interface with new IO routines.
    * Basic tests for training continuation.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Feature interaction for GPU Hist. (#4534)
    
    * GPU hist Interaction Constraints.
    * Duplicate related parameters.
    * Add tests for CPU interaction constraint.
    * Add better error reporting.
    * Thorough tests.
    Fix prediction from loaded pickle. (#4516)
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Fix #3342 and h2oai/h2o4gpu#625: Save predictor parameters in model file (#3856)
    
    * Fix #3342 and h2oai/h2o4gpu#625: Save predictor parameters in model file
    
    This allows pickled models to retain predictor attributes, such as
    'predictor' (whether to use CPU or GPU) and 'n_gpu' (number of GPUs
    to use). Related: h2oai/h2o4gpu#625
    
    Closes #3342.
    
    TODO. Write a test.
    
    * Fix lint
    
    * Do not load GPU predictor into CPU-only XGBoost
    
    * Add a test for pickling GPU predictors
    
    * Make sample data big enough to pass multi GPU test
    
    * Update test_gpu_predictor.cu
    Dmatrix refactor stage 1 (#3301)
    
    * Use sparse page as singular CSR matrix representation
    
    * Simplify dmatrix methods
    
    * Reduce statefullness of batch iterators
    
    * BREAKING CHANGE: Remove prob_buffer_row parameter. Users are instead recommended to sample their dataset as a preprocessing step before using XGBoost.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Replaced std::vector-based interfaces with HostDeviceVector-based interfaces. (#3116)
    
    * Replaced std::vector-based interfaces with HostDeviceVector-based interfaces.
    
    - replacement was performed in the learner, boosters, predictors,
      updaters, and objective functions
    - only interfaces used in training were replaced;
      interfaces like PredictInstance() still use std::vector
    - refactoring necessary for replacement of interfaces was also performed,
      such as using HostDeviceVector in prediction cache
    
    * HostDeviceVector-based interfaces for custom objective function example plugin.
    Add SHAP interaction effects, fix minor bug, and add cox loss (#3043)
    
    * Add interaction effects and cox loss
    
    * Minimize whitespace changes
    
    * Cox loss now no longer needs a pre-sorted dataset.
    
    * Address code review comments
    
    * Remove mem check, rename to pred_interactions, include bias
    
    * Make lint happy
    
    * More lint fixes
    
    * Fix cox loss indexing
    
    * Fix main effects and tests
    
    * Fix lint
    
    * Use half interaction values on the off-diagonals
    
    * Fix lint again
    SHAP values for feature contributions (#2438)
    
    * SHAP values for feature contributions
    
    * Fix commenting error
    
    * New polynomial time SHAP value estimation algorithm
    
    * Update API to support SHAP values
    
    * Fix merge conflicts with updates in master
    
    * Correct submodule hashes
    
    * Fix variable sized stack allocation
    
    * Make lint happy
    
    * Add docs
    
    * Fix typo
    
    * Adjust tolerances
    
    * Remove unneeded def
    
    * Fixed cpp test setup
    
    * Updated R API and cleaned up
    
    * Fixed test typo
    [WIP] Extract prediction into separate interface (#2531)
    
    * [WIP] Extract prediction into separate interface
    
    * Add copyright, fix linter errors
    
    * Add predictor to amalgamation
    
    * Fix documentation
    
    * Move prediction cache into predictor, add GBTreeModel
    
    * Updated predictor doc comments
    adding feature contributions to R and gblinear (#2295)
    
    * [gblinear] add features contribution prediction; fix DumpModel bug
    
    * [gbtree] minor changes to PredContrib
    
    * [R] add feature contribution prediction to R
    
    * [R] bump up version; update NEWS
    
    * [gblinear] fix the base_margin issue; fixes #1969
    
    * [R] list of matrices as output of multiclass feature contributions
    
    * [gblinear] make order of DumpModel coefficients consistent: group index changes the fastest
    Add prediction of feature contributions (#2003)
    
    * Add prediction of feature contributions
    
    This implements the idea described at http://blog.datadive.net/interpreting-random-forests/
    which tries to give insight in how a prediction is composed of its feature contributions
    and a bias.
    
    * Support multi-class models
    
    * Calculate learning_rate per-tree instead of using the one from the first tree
    
    * Do not rely on node.base_weight * learning_rate having the same value as the node mean value (aka leaf value, if it were a leaf); instead calculate them (lazily) on-the-fly
    
    * Add simple test for contributions feature
    
    * Check against param.num_nodes instead of checking for non-zero length
    
    * Loop over all roots instead of only the first
    fix online prediction function in learner.h (#2010)
    
    I use the online prediction function(`inline void Predict(const SparseBatch::Inst &inst, ... ) const;`), the results obtained are different from the results of the batch prediction function(`  virtual void Predict(DMatrix* data, ...) const = 0`). After the investigation found that the online prediction function using the `base_score_` parameters, and the batch prediction function is not used in this parameter. It is found that the `base_score_` values are different when the same model file is loaded many times.
    
    ```
    1st times：base_score_: 6.69023e-21
    2nd times：base_score_: -3.7668e+19
    3rd times：base_score_: 5.40507e+07
    ```
     Online prediction results are affected by `base_score_` parameters. After deleting the if condition(`if (out_preds->size() == 1)`) , the online prediction is consistent with the batch prediction results, and the xgboost prediction results are consistent with python version.  Therefore, it is likely that the online prediction function is bug
    fix typo in comment. (#1850)
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    Add dump_format=json option (#1726)
    
    * Add format to the params accepted by DumpModel
    
    Currently, only the test format is supported when trying to dump
    a model. The plan is to add more such formats like JSON which are
    easy to read and/or parse by machines. And to make the interface
    for this even more generic to allow other formats to be added.
    
    Hence, we make some modifications to make these function generic
    and accept a new parameter "format" which signifies the format of
    the dump to be created.
    
    * Fix typos and errors in docs
    
    * plugin: Mention all the register macros available
    
    Document the register macros currently available to the plugin
    writers so they know what exactly can be extended using hooks.
    
    * sparce_page_source: Use same arg name in .h and .cc
    
    * gbm: Add JSON dump
    
    The dump_format argument can be used to specify what type
    of dump file should be created. Add functionality to dump
    gblinear and gbtree into a JSON file.
    
    The JSON file has an array, each item is a JSON object for the tree.
    For gblinear:
     - The item is the bias and weights vectors
    For gbtree:
     - The item is the root node. The root node has a attribute "children"
       which holds the children nodes. This happens recursively.
    
    * core.py: Add arg dump_format for get_dump()
    Fix typos and messages in docs (#1723)
    [CORE] Refactor cache mechanism (#1540)
    methods to delete an attribute and get names of available attributes
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LIBXGBOOST] pass demo running.
    [LEARNER] refactor learner
    [LEARNER] Init learner interface
    Disable JSON full serialization for now. (#6248)
    
    * Disable JSON serialization for now.
    
    * Multi-class classification is checkpointing for each iteration.
    This brings significant overhead.
    
    Revert: 90355b4f007ae
    
    * Set R tests to use binary.
    Make JSON the default full serialization format. (#6027)
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Support 64bit seed. (#5643)
    [jvm-packages]add feature size for LabelPoint and DataBatch (#5303)
    
    * fix type error
    
    * Validate number of features.
    
    * resolve comments
    
    * add feature size for LabelPoint and DataBatch
    
    * pass the feature size to native
    
    * move feature size validating tests into a separate suite
    
    * resolve comments
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Remove update prediction cache from predictors. (#5312)
    
    
    Move this function into gbtree, and uses only updater for doing so. As now the predictor knows exactly how many trees to predict, there's no need for it to update the prediction cache.
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    Disable parameter validation for Scikit-Learn interface. (#5167)
    
    * Disable parameter validation for now.
    
    Scikit-Learn passes all parameters down to XGBoost, whether they are used or
    not.
    
    * Add option `validate_parameters`.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    Support multiple batches in gpu_hist (#5014)
    
    
    * Initial external memory training support for GPU Hist tree method.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    [Breaking] Update sklearn interface. (#4929)
    
    
    * Remove nthread, seed, silent. Add tree_method, gpu_id, num_parallel_tree. Fix #4909.
    * Check data shape. Fix #4896.
    * Check element of eval_set is tuple. Fix #4875
    *  Add doc for random_state with hogwild. Fixes #4919
    Check deprecated `n_gpus`. (#4908)
    Add `n_jobs` as an alias of `nthread`. (#4842)
    Make HostDeviceVector single gpu only (#4773)
    
    * Make HostDeviceVector single gpu only
    [BREAKING] prevent multi-gpu usage (#4749)
    
    * prevent multi-gpu usage
    
    * fix distributed test
    
    * combine gpu predictor tests
    
    * set upper bound on n_gpus
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Deprecate single node multi-gpu mode (#4579)
    
    * deprecate multi-gpu training
    
    * add single node
    
    * add warning
    Offload some configurations into GBM. (#4553)
    
    
    This is part 1 of refactoring configuration.
    
    * Move tree heuristic configurations.
    * Split up declarations and definitions for GBTree.
    * Implement UseGPU in gbm.
    De-duplicate GPU parameters. (#4454)
    
    
    * Only define `gpu_id` and `n_gpus` in `LearnerTrainParam`
    * Pass LearnerTrainParam through XGBoost vid factory method.
    * Disable all GPU usage when GPU related parameters are not specified (fixes XGBoost choosing GPU over aggressively).
    * Test learner train param io.
    * Fix gpu pickling.
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Remove silent parameter. (#5476)
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Add Model and Configurable interface. (#4945)
    
    
    * Apply Configurable to objective functions.
    * Apply Model to Learner and Regtree, gbm.
    * Add Load/SaveConfig to objs.
    * Refactor obj tests to use smart pointer.
    * Dummy methods for Save/Load Model.
    Cudf support. (#4745)
    
    * Initial support for cudf integration.
    
    * Add two C APIs for consuming data and metainfo.
    
    * Add CopyFrom for SimpleCSRSource as a generic function to consume the data.
    
    * Add FromDeviceColumnar for consuming device data.
    
    * Add new MetaInfo::SetInfo for consuming label, weight etc.
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Fix and optimize logger (#4002)
    
    * Fix logging switch statement.
    
    * Remove debug_verbose_ in AllReducer.
    
    * Don't construct the stream when not needed.
    
    * Make default constructor deleted.
    
    * Remove redundant IsVerbose.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Fix CRAN check by removing reference to std::cerr (#3660)
    
    * Fix CRAN check by removing reference to std::cerr
    
    * Mask tests that fail on 32-bit Windows R
    Add callback interface to re-direct console output (#3438)
    
    * Add callback interface to re-direct console output
    
    * Exempt TrackerLogger from custom logging
    
    * Fix lint
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    [LIBXGBOOST] pass demo running.
    Unify evaluation functions. (#6037)
    Make binary bin search reusable. (#6058)
    
    * Move binary search row to hist util.
    * Remove dead code.
    Expand categorical node. (#6028)
    
    
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Revert "Reorder includes. (#5749)" (#5771)
    
    This reverts commit d3a0efbf162f3dceaaf684109e1178c150b32de3.
    Reorder includes. (#5749)
    
    * Reorder includes.
    
    * R.
    Add float32 histogram (#5624)
    
    * new single_precision_histogram param was added.
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Co-authored-by: fis <jm.yuan@outlook.com>
    Upgrade clang-tidy on CI. (#5469)
    
    * Correct all clang-tidy errors.
    * Upgrade clang-tidy to 10 on CI.
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    Gradient based sampling for GPU Hist (#5093)
    
    
    * Implement gradient based sampling for GPU Hist tree method.
    * Add samplers and handle compacted page in GPU Hist.
    Pass pointer to model parameters. (#5101)
    
    * Pass pointer to model parameters.
    
    This PR de-duplicates most of the model parameters except the one in
    `tree_model.h`.  One difficulty is `base_score` is a model property but can be
    changed at runtime by objective function.  Hence when performing model IO, we
    need to save the one provided by users, instead of the one transformed by
    objective.  Here we created an immutable version of `LearnerModelParam` that
    represents the value of model parameter after configuration.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Use `UpdateAllowUnknown' for non-model related parameter. (#4961)
    
    * Use `UpdateAllowUnknown' for non-model related parameter.
    
    Model parameter can not pack an additional boolean value due to binary IO
    format.  This commit deals only with non-model related parameter configuration.
    
    * Add tidy command line arg for use-dmlc-gtest.
    Use configure_file() to configure version only (#4974)
    
    * Avoid writing build_config.h
    
    * Remove build_config.h all together.
    
    * Lint.
    [Breaking] Add global versioning. (#4936)
    
    * Use CMake config file for representing version.
    
    * Generate c and Python version file with CMake.
    
    The generated file is written into source tree.  But unless XGBoost upgrades
    its version, there will be no actual modification.  This retains compatibility
    with Makefiles for R.
    
    * Add XGBoost version the DMatrix binaries.
    * Simplify prefetch detection in CMakeLists.txt
    Add Json integer, remove specialization. (#4739)
    Refactor configuration [Part II]. (#4577)
    
    * Refactor configuration [Part II].
    
    * General changes:
    ** Remove `Init` methods to avoid ambiguity.
    ** Remove `Configure(std::map<>)` to avoid redundant copying and prepare for
       parameter validation. (`std::vector` is returned from `InitAllowUnknown`).
    ** Add name to tree updaters for easier debugging.
    
    * Learner changes:
    ** Make `LearnerImpl` the only source of configuration.
    
        All configurations are stored and carried out by `LearnerImpl::Configure()`.
    
    ** Remove booster in C API.
    
        Originally kept for "compatibility reason", but did not state why.  So here
        we just remove it.
    
    ** Add a `metric_names_` field in `LearnerImpl`.
    ** Remove `LazyInit`.  Configuration will always be lazy.
    ** Run `Configure` before every iteration.
    
    * Predictor changes:
    ** Allocate both cpu and gpu predictor.
    ** Remove cpu_predictor from gpu_predictor.
    
        `GBTree` is now used to dispatch the predictor.
    
    ** Remove some GPU Predictor tests.
    
    * IO
    
    No IO changes.  The binary model format stability is tested by comparing
    hashing value of save models between two commits
    Fix cpplint. (#4157)
    
    * Add comment after #endif.
    * Add missing headers.
    Performance optimizations for Intel CPUs (#3957)
    
    * Initial performance optimizations for xgboost
    
    * remove includes
    
    * revert float->double
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * fix for CI
    
    * Check existence of _mm_prefetch and __builtin_prefetch
    
    * Fix lint
    Use __CUDA__ macro with __NVCC__. (#3539)
    
    * __CUDA__ is defined in clang. Making the change won't make clang
    compile xgboost, but syntax checking from clang is at least partially
    working.
    Clang-tidy static analysis (#3222)
    
    * Clang-tidy static analysis
    
    * Modernise checks
    
    * Google coding standard checks
    
    * Identifier renaming according to Google style
    Fix Google test warnings and error (#2957)
    Add warnings for large labels when using GPU histogram algorithms (#2834)
    Various bug fixes (#2825)
    
    * Fatal error if GPU algorithm selected without GPU support compiled
    
    * Resolve type conversion warnings
    
    * Fix gpu unit test failure
    
    * Fix compressed iterator edge case
    
    * Fix python unit test failures due to flake8 update on pip
    -Add experimental GPU algorithm for lossguided mode (#2755)
    
    -Improved GPU algorithm unit tests
    -Removed some thrust code to improve compile times
    Integer gradient summation for GPU histogram algorithm. (#2681)
    Add parallel sort for MSVC (#2609)
    [GPU-Plugin] Various fixes (#2579)
    
    * Fix test large
    
    * Add check for max_depth 0
    
    * Update readme
    
    * Add LBS specialisation for dense data
    
    * Add bst_gpair_precise
    
    * Temporarily disable accuracy tests on test_large.py
    
    * Solve unused variable compiler warning
    
    * Fix max_bin > 1024 error
    MinGW: shared library prefix and appveyor CI (#2539)
    
    * for MinGW, drop the 'lib' prefix from shared library name
    
    * fix defines for 'g++ 4.8 or higher' to include g++ >= 5
    
    * fix compile warnings
    
    * [Appveyor] add MinGW with python; remove redundant jobs
    
    * [Appveyor] also do python build for one of msvc jobs
    [GPU-Plugin] Unify gpu_gpair/bst_gpair. Refactor. (#2477)
    [GPU-Plugin] (#2227)
    
    * Add fast histogram algorithm
    * Fix Linux build
    * Add 'gpu_id' parameter
    Fix bugs in multithreaded ApplySplitSparseData() (#2161)
    
    * Bugfix 1: Fix segfault in multithreaded ApplySplitSparseData()
    
    When there are more threads than rows in rowset, some threads end up
    with empty ranges, causing them to crash. (iend - 1 needs to be
    accessible as part of algorithm)
    
    Fix: run only those threads with nonempty ranges.
    
    * Add regression test for Bugfix 1
    
    * Moving python_omp_test to existing python test group
    
    Turns out you don't need to set "OMP_NUM_THREADS" to enable
    multithreading. Just add nthread parameter.
    
    * Bugfix 2: Fix corner case of ApplySplitSparseData() for categorical feature
    
    When split value is less than all cut points, split_cond is set
    incorrectly.
    
    Fix: set split_cond = -1 to indicate this scenario
    
    * Bugfix 3: Initialize data layout indicator before using it
    
    data_layout_ is accessed before being set; this variable determines
    whether feature 0 is included in feat_set.
    
    Fix: re-order code in InitData() to initialize data_layout_ first
    
    * Adding regression test for Bugfix 2
    
    Unfortunately, no regression test for Bugfix 3, as there is no
    way to deterministically assign value to an uninitialized variable.
    Improve multi-threaded performance (#2104)
    
    * Add UpdatePredictionCache() option to updaters
    
    Some updaters (e.g. fast_hist) has enough information to quickly compute
    prediction cache for the training data. Each updater may override
    UpdaterPredictionCache() method to update the prediction cache. Note: this
    trick does not apply to validation data.
    
    * Respond to code review
    
    * Disable some debug messages by default
    * Document UpdatePredictionCache() interface
    * Remove base_margin logic from UpdatePredictionCache() implementation
    * Do not take pointer to cfg, as reference may get stale
    
    * Improve multi-threaded performance
    
    * Use columnwise accessor to accelerate ApplySplit() step,
      with support for a compressed representation
    * Parallel sort for evaluation step
    * Inline BuildHist() function
    * Cache gradient pairs when building histograms in BuildHist()
    
    * Add missing #if macro
    
    * Respond to code review
    
    * Use wrapper to enable parallel sort on Linux
    
    * Fix C++ compatibility issues
    
    * MSVC doesn't support unsigned in OpenMP loops
    * gcc 4.6 doesn't support using keyword
    
    * Fix lint issues
    
    * Respond to code review
    
    * Fix bug in ApplySplitSparseData()
    
    * Attempting to read beyond the end of a sparse column
    * Mishandling the case where an entire range of rows have missing values
    
    * Fix training continuation bug
    
    Disable UpdatePredictionCache() in the first iteration. This way, we can
    accomodate the scenario where we build off of an existing (nonempty) ensemble.
    
    * Add regression test for fast_hist
    
    * Respond to code review
    
    * Add back old version of ApplySplitSparseData
    Histogram Optimized Tree Grower (#1940)
    
    * Support histogram-based algorithm + multiple tree growing strategy
    
    * Add a brand new updater to support histogram-based algorithm, which buckets
      continuous features into discrete bins to speed up training. To use it, set
      `tree_method = fast_hist` to configuration.
    * Support multiple tree growing strategies. For now, two policies are supported:
      * `grow_policy=depthwise` (default):  favor splitting at nodes closest to the
        root, i.e. grow depth-wise.
      * `grow_policy=lossguide`: favor splitting at nodes with highest loss change
    * Improve single-threaded performance
      * Unroll critical loops
      * Introduce specialized code for dense data (i.e. no missing values)
    * Additional training parameters: `max_leaves`, `max_bin`, `grow_policy`, `verbose`
    
    * Adding a small test for hist method
    
    * Fix memory error in row_set.h
    
    When std::vector is resized, a reference to one of its element may become
    stale. Any such reference must be updated as well.
    
    * Resolve cross-platform compilation issues
    
    * Versions of g++ older than 4.8 lacks support for a few C++11 features, e.g.
      alignas(*) and new initializer syntax. To support g++ 4.6, use pre-C++11
      initializer and remove alignas(*).
    * Versions of MSVC older than 2015 does not support alignas(*). To support
      MSVC 2012, remove alignas(*).
    * For g++ 4.8 and newer, alignas(*) is enabled for performance benefits.
    * Some old compilers (MSVC 2012, g++ 4.6) do not support template aliases
      (which uses `using` to declate type aliases). So always use `typedef`.
    
    * Fix a host of CI issues
    
    * Remove dependency for libz on osx
    * Fix heading for hist_util
    * Fix minor style issues
    * Add missing #include
    * Remove extraneous logging
    
    * Enable tree_method=hist in R
    
    * Renaming HistMaker to GHistBuilder to avoid confusion
    
    * Fix R integration
    
    * Respond to style comments
    
    * Consistent tie-breaking for priority queue using timestamps
    
    * Last-minute style fixes
    
    * Fix issuecomment-271977647
    
    The way we quantize data is broken. The agaricus data consists of all
    categorical values. When NAs are converted into 0's,
    `HistCutMatrix::Init` assign both 0's and 1's to the same single bin.
    
    Why? gmat only the smallest value (0) and an upper bound (2), which is twice
    the maximum value (1). Add the maximum value itself to gmat to fix the issue.
    
    * Fix issuecomment-272266358
    
    * Remove padding from cut values for the continuous case
    * For categorical/ordinal values, use midpoints as bin boundaries to be safe
    
    * Fix CI issue -- do not use xrange(*)
    
    * Fix corner case in quantile sketch
    
    Signed-off-by: Philip Cho <chohyu01@cs.washington.edu>
    
    * Adding a test for an edge case in quantile sketcher
    
    max_bin=2 used to cause an exception.
    
    * Fix fast_hist test
    
    The test used to require a strictly increasing Test AUC for all examples.
    One of them exhibits a small blip in Test AUC before achieving a Test AUC
    of 1. (See bottom.)
    
    Solution: do not require monotonic increase for this particular example.
    
    [0] train-auc:0.99989 test-auc:0.999497
    [1] train-auc:1 test-auc:0.999749
    [2] train-auc:1 test-auc:0.999749
    [3] train-auc:1 test-auc:0.999749
    [4] train-auc:1 test-auc:0.999749
    [5] train-auc:1 test-auc:0.999497
    [6] train-auc:1 test-auc:1
    [7] train-auc:1 test-auc:1
    [8] train-auc:1 test-auc:1
    [9] train-auc:1 test-auc:1
    Use bst_float consistently throughout (#1824)
    
    * Fix various typos
    
    * Add override to functions that are overridden
    
    gcc gives warnings about functions that are being overridden by not
    being marked as oveirridden. This fixes it.
    
    * Use bst_float consistently
    
    Use bst_float for all the variables that involve weight,
    leaf value, gradient, hessian, gain, loss_chg, predictions,
    base_margin, feature values.
    
    In some cases, when due to additions and so on the value can
    take a larger value, double is used.
    
    This ensures that type conversions are minimal and reduces loss of
    precision.
    [jvm-packages] XGBoost4j Windows fixes (#1639)
    
    * Changes for Mingw64 compilation to ensure long is a consistent size.
    
    Mainly impacts the Java API which would not compile, but there may be
    silent errors on Windows with large datasets before this patch (as long
    is 32-bits when compiled with mingw64 even in 64-bit mode).
    
    * Adding ifdefs to ensure it still compiles on MacOS
    
    * Makefile and create_jni.bat changes for Windows.
    
    * Switching XGDMatrixCreateFromCSREx JNI call to use size_t cast
    
    * Fixing lint error, adding profile switching to jvm-packages build to make create-jni.bat get called, adding myself to Contributors.Md
    [TREE] Remove gap constraint, make tree construction more robust
    [R] make all customizations to meet strict standard of cran
    [IO] Enable external memory
    [LIBXGBOOST] pass demo running.
    [REFACTOR] Add alias, allow missing variables, init gbm interface
    [OBJ] Add basic objective function and registry
    [DATA] basic data refactor done, basic version of csr source.
    Data interface ready
    [REFACTOR] cleanup structure
    Add release note for 1.2.0 in NEWS.md (#6063)
    
    * Update query_contributors.py to account for pagination
    
    * Add the release note for 1.2.0
    
    * Add release note for patch releases
    
    * Apply suggestions from code review
    
    * Fix typo
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Co-authored-by: John Zedlewski <904524+JohnZed@users.noreply.github.com>
    Add release note for 1.0.0 in NEWS.md (#5329)
    
    * Add release note for 1.0.0
    
    * Fix a small bug in the Python script that compiles the list of contributors
    
    * Clarify governance of CI infrastructure; now PMC is formally in charge
    
    * Address reviewer comment
    
    * Fix typo
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    Refactor CMake scripts. (#4323)
    
    * Refactor CMake scripts.
    
    * Remove CMake CUDA wrapper.
    * Bump CMake version for CUDA.
    * Use CMake to handle Doxygen.
    * Split up CMakeList.
    * Export install target.
    * Use modern CMake.
    * Remove build.sh
    * Workaround for gpu_hist test.
    * Use cmake 3.12.
    
    * Revert machine.conf.
    
    * Move CLI test to gpu.
    
    * Small cleanup.
    
    * Support using XGBoost as submodule.
    
    * Fix windows
    
    * Fix cpp tests on Windows
    
    * Remove duplicated find_package.
    disable travis model_recover tests, fix doc generate failure (#71)
    
    * add missing packackges used in dmlc submit
    
    * disable local_recovery tests til we have code fix
    
    * fix doc gen failure
    only doc rabit
    refactor: librabit
    [TRAVIS] cleanup travis script
    Squashed 'subtree/rabit/' changes from fa99857..e81a11d
    
    e81a11d Merge pull request #25 from daiyl0320/master
    35c3b37 add retry mechanism to ConnectTracker and modify Listen backlog to 128 in rabit_traker.py
    c71ed6f try deply doxygen
    62e5647 try deply doxygen
    732f1c6 try
    2fa6e02 ok
    0537665 minor
    7b59dcb minor
    5934950 new doc
    f538187 ok
    44b6049 new doc
    387339b add more
    9d4397a chg
    2879a48 chg
    30e3110 ok
    9ff0301 add link translation
    6b629c2 k
    32e1955 ok
    8f4839d fix
    93137b2 ok
    7eeeb79 reload recommonmark
    a8f00cc minor
    19b0f01 ok
    dd01184 minor
    c1cdc19 minor
    fcf0f43 try rst
    cbc21ae try
    62ddfa7 tiny
    aefc05c final change
    2aee9b4 minor
    fe4e7c2 ok
    8001983 change to subtitle
    5ca33e4 ok
    88f7d24 update guide
    29d43ab add code
    fe8bb3b minor hack for readthedocs
    229c71d Merge branch 'master' of ssh://github.com/dmlc/rabit
    7424218 ok
    d1d45bb Update README.md
    1e8813f Update README.md
    1ccc990 Update README.md
    0323e06 remove readme
    679a835 remove theme
    7ea5b7c remove numpydoc to napoleon
    b73e2be Merge branch 'master' of ssh://github.com/dmlc/rabit
    1742283 ok
    1838e25 Update python-requirements.txt
    bc4e957 ok
    fba6fc2 ok
    0251101 ok
    d50b905 ok
    d4f2509 ok
    cdf401a ok
    fef0ef2 new doc
    cef360d ok
    c125d2a ok
    270a49e add requirments
    744f901 get the basic doc
    1cb5cad Merge branch 'master' of ssh://github.com/dmlc/rabit
    8cc07ba minor
    d74f126 Update .travis.yml
    52b3dcd Update .travis.yml
    099581b Update .travis.yml
    1258046 Update .travis.yml
    7addac9 Update Makefile
    0ea7adf Update .travis.yml
    f858856 Update travis_script.sh
    d8eac4a Update README.md
    3cc49ad lint and travis
    ceedf4e fix
    fd8920c fix win32
    8bbed35 modify
    9520b90 Merge pull request #14 from dmlc/hjk41
    df14bb1 fix type
    f441dc7 replace tab with blankspace
    2467942 remove unnecessary include
    181ef47 defined long long and ulonglong
    1582180 use int32_t to define int and int64_t to define long. in VC long is 32bit
    e0b7da0 fix
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: e81a11dd7ee3cff87a38a42901315821df018bae
    get the basic doc
    lint and travis
    Squashed 'subtree/rabit/' changes from b15f6cd..e08542c
    
    e08542c fix doc
    e95c962 remove I prefix from interface, serializable now takes in pointer
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: e08542c6357bc044199e4876d9bea949d05f614c
    fix doc
    Squashed 'subtree/rabit/' content from commit c7282ac
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: c7282acb2a92e1d6a32614889ff466fec58937f3
    checkin API doc
    [Doc] Add info on GPU compiler (#6204)
    
    * Add note about the required compiler version for CUDA.
    * Also added a link that gives a short explanation on compute capability version
    Fix doc for CMake requirement. (#6123)
    Fix nightly build doc. [skip ci] (#6004)
    
    * Fix nightly build doc. [skip ci]
    
    * Fix title too short. [skip ci]
    [R] Remove dependency on gendef for Visual Studio builds (fixes #5608) (#5764)
    
    * [R-package] Remove dependency on gendef for Visual Studio builds (fixes #5608)
    
    * clarify docs
    
    * removed debugging print statement
    
    * Make R CMake install more robust
    
    * Fix doc format; add ToC
    
    * Update build.rst
    
    * Fix AppVeyor
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Changed build.rst (binary wheels are supported for macOS also) (#5711)
    Remove makefiles. (#5513)
    Updated Windows build docs (#5283)
    Rewrite setup.py. (#5271)
    
    The setup.py is rewritten.  This new script uses only Python code and provide customized
    implementation of setuptools commands.  This way users can run most of setuptools commands
    just like any other Python libraries.
    
    * Remove setup_pip.py
    * Remove soft links.
    * Define customized commands.
    * Remove shell script.
    * Remove makefile script.
    * Update the doc for building from source.
    Make `pip install xgboost*.tar.gz` work by fixing build-python.sh (#5241)
    
    * Make pip install xgboost*.tar.gz work by fixing build-python.sh
    
    * Simplify install doc
    
    * Add test
    
    * Install Miniconda for Linux target too
    
    * Build XGBoost only once in sdist
    
    * Try importing xgboost after installation
    
    * Don't set PYTHONPATH env var for sdist test
    [R] Enable OpenMP with AppleClang in XGBoost R package (#5240)
    
    * [R] Enable OpenMP with AppleClang in XGBoost R package
    
    * Dramatically simplify install doc
    Fix MacOS build error. (#5080)
    
    * Update build script.
    * Update build doc.
    Update doc for building on OSX (#5074)
    
    
    Co-Authored-By: Jiaming Yuan <jm.yuan@outlook.com>
    [CI] Upload nightly builds to S3 (#4976)
    
    * Do not store built artifacts in the Jenkins master
    
    * Add wheel renaming script
    
    * Upload wheels to S3 bucket
    
    * Use env.GIT_COMMIT
    
    * Capture git hash correctly
    
    * Add missing import in Jenkinsfile
    
    * Address reviewer's comments
    
    * Put artifacts for pull requests in separate directory
    
    * No wildcard expansion in Windows CMD
    [CI] Update lint configuration to support latest pylint convention (#4971)
    
    * Update lint configuration
    
    * Use gcc 8 consistently in build instruction
    Copy CMake parameter from dmlc-core. (#4948)
    Remove plugin, cuda related code in automake & autoconf files (#4789)
    
    * Build plugin example with CMake.
    
    * Remove plugin, cuda related code in automake & autoconf files.
    
    * Fix typo in GPU doc.
    Remove gpu_exact tree method (#4742)
    Re-organize contributor's guide (#4659)
    
    * Reorganize contributor's doc
    
    * Address comments from @trivialfis
    
    * Address @sriramch's comment: include ABI compatibility guarantee
    
    * Address @rongou's comment
    
    * Postpone ABI compatibility guarantee for now
    Address some sphinx warnings and errors, add doc for building doc. (#4589)
    Deprecate single node multi-gpu mode (#4579)
    
    * deprecate multi-gpu training
    
    * add single node
    
    * add warning
    Remove doc about not supporting cuda 10.1 (#4578)
    Mark CUDA 10.1 as unsupported. (#4265)
    Update build doc: PyPI wheel now support multi-GPU (#4219)
    Correct typo
    Move MinGW-w64 + Python section to the end, since it's 'advanced' (#3863)
    Added some instructions on using MinGW-built XGBoost with python. (#3774)
    
    * Added some instructions on using MinGW-built XGBoost with python.
    
    * Changes according to the discussion and some additions
    
    * Fixed wording and removed redundancy.
    
    * Even more fixes
    
    * Fixed links. Removed redundancy.
    
    * Some fixes according to the discussion
    
    * fixes
    
    * Some fixes
    
    * fixes
    Produce xgboost.so for XGBoost-R on Mac OSX, so that `make install` works (#3767)
    
    * Produce xgboost.so for XGBoost-R on Mac OSX, so that `make install` works
    
    * Modernize R build instructions
    
    * Fix crossref
    Clarify multi-GPU training, binary wheels, Pandas integration (#3581)
    
    * Clarify multi-GPU training, binary wheels, Pandas integration
    
    * Add a note about multi-GPU on gpu/index.rst
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    [python-package] remove unused imports (#5776)
    Fix simple typo: utilty -> utility (#5182)
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    [DOC] Update R doc
    Squashed 'subtree/rabit/' changes from fa99857..e81a11d
    
    e81a11d Merge pull request #25 from daiyl0320/master
    35c3b37 add retry mechanism to ConnectTracker and modify Listen backlog to 128 in rabit_traker.py
    c71ed6f try deply doxygen
    62e5647 try deply doxygen
    732f1c6 try
    2fa6e02 ok
    0537665 minor
    7b59dcb minor
    5934950 new doc
    f538187 ok
    44b6049 new doc
    387339b add more
    9d4397a chg
    2879a48 chg
    30e3110 ok
    9ff0301 add link translation
    6b629c2 k
    32e1955 ok
    8f4839d fix
    93137b2 ok
    7eeeb79 reload recommonmark
    a8f00cc minor
    19b0f01 ok
    dd01184 minor
    c1cdc19 minor
    fcf0f43 try rst
    cbc21ae try
    62ddfa7 tiny
    aefc05c final change
    2aee9b4 minor
    fe4e7c2 ok
    8001983 change to subtitle
    5ca33e4 ok
    88f7d24 update guide
    29d43ab add code
    fe8bb3b minor hack for readthedocs
    229c71d Merge branch 'master' of ssh://github.com/dmlc/rabit
    7424218 ok
    d1d45bb Update README.md
    1e8813f Update README.md
    1ccc990 Update README.md
    0323e06 remove readme
    679a835 remove theme
    7ea5b7c remove numpydoc to napoleon
    b73e2be Merge branch 'master' of ssh://github.com/dmlc/rabit
    1742283 ok
    1838e25 Update python-requirements.txt
    bc4e957 ok
    fba6fc2 ok
    0251101 ok
    d50b905 ok
    d4f2509 ok
    cdf401a ok
    fef0ef2 new doc
    cef360d ok
    c125d2a ok
    270a49e add requirments
    744f901 get the basic doc
    1cb5cad Merge branch 'master' of ssh://github.com/dmlc/rabit
    8cc07ba minor
    d74f126 Update .travis.yml
    52b3dcd Update .travis.yml
    099581b Update .travis.yml
    1258046 Update .travis.yml
    7addac9 Update Makefile
    0ea7adf Update .travis.yml
    f858856 Update travis_script.sh
    d8eac4a Update README.md
    3cc49ad lint and travis
    ceedf4e fix
    fd8920c fix win32
    8bbed35 modify
    9520b90 Merge pull request #14 from dmlc/hjk41
    df14bb1 fix type
    f441dc7 replace tab with blankspace
    2467942 remove unnecessary include
    181ef47 defined long long and ulonglong
    1582180 use int32_t to define int and int64_t to define long. in VC long is 32bit
    e0b7da0 fix
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: e81a11dd7ee3cff87a38a42901315821df018bae
    update
    try
    Document refactor
    
    change badge
    new doc
    ok
    new doc
    enable basic sphinx doc
    add more
    chg
    chg
    add link translation
    [Breaking] Change default evaluation metric for classification to logloss / mlogloss (#6183)
    
    * Change DefaultEvalMetric of classification from error to logloss
    
    * Change default binary metric in plugin/example/custom_obj.cc
    
    * Set old error metric in python tests
    
    * Set old error metric in R tests
    
    * Fix missed eval metrics and typos in R tests
    
    * Fix setting eval_metric twice in R tests
    
    * Add warning for empty eval_metric for classification
    
    * Fix Dask tests
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    Add MAPE metric (#6119)
    Feature weights (#5962)
    Fix nightly build doc. [skip ci] (#6004)
    
    * Fix nightly build doc. [skip ci]
    
    * Fix title too short. [skip ci]
    Remove skmaker. (#5971)
    [Breaking] Fix custom metric for multi output. (#5954)
    
    
    * Set output margin to true for custom metric.  This fixes only R and Python.
    [Doc] Document new objectives and metrics available on GPUs (#5909)
    Add float32 histogram (#5624)
    
    * new single_precision_histogram param was added.
    
    Co-authored-by: SHVETS, KIRILL <kirill.shvets@intel.com>
    Co-authored-by: fis <jm.yuan@outlook.com>
    Pseudo-huber loss metric added (#5647)
    
    
    - Add pseudo huber loss objective.
    - Add pseudo huber loss metric.
    
    Co-authored-by: Reetz <s02reetz@iavgroup.local>
    Refactor the CLI. (#5574)
    
    
    * Enable parameter validation.
    * Enable JSON.
    * Catch `dmlc::Error`.
    * Show help message.
    Enable parameter validation for R. (#5569)
    
    * Enable parameter validation for R.
    
    * Add test.
    Add missing aft parameters. [skip ci] (#5553)
    Update doc for parameter validation. (#5508)
    
    * Update doc for parameter validation.
    
    * Fix github rebase.
    Remove distcol updater. (#5507)
    
    Closes #5498.
    Remove silent parameter. (#5476)
    Deterministic GPU histogram. (#5361)
    
    * Use pre-rounding based method to obtain reproducible floating point
      summation.
    * GPU Hist for regression and classification are bit-by-bit reproducible.
    * Add doc.
    * Switch to thrust reduce for `node_sum_gradient`.
    update docs for gpu external memory (#5332)
    
    * update docs for gpu external memory
    
    * add hist limitation
    Add constraint parameters to Scikit-Learn interface. (#5227)
    
    * Add document for constraints.
    
    * Fix a format error in doc for objective function.
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    Disable parameter validation for Scikit-Learn interface. (#5167)
    
    * Disable parameter validation for now.
    
    Scikit-Learn passes all parameters down to XGBoost, whether they are used or
    not.
    
    * Add option `validate_parameters`.
    Check against R seed. (#5125)
    
    
    
    * Handle it in R instead.
    Update document for tree_method. [skip ci] (#5106)
    Fix GPU ID and prediction cache from pickle (#5086)
    
    * Hack for saving GPU ID.
    
    * Declare prediction cache on GBTree.
    
    * Add a simple test.
    
    * Add `auto` option for GPU Predictor.
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Remove plugin, cuda related code in automake & autoconf files (#4789)
    
    * Build plugin example with CMake.
    
    * Remove plugin, cuda related code in automake & autoconf files.
    
    * Fix typo in GPU doc.
    Remove gpu_exact tree method (#4742)
    Address some sphinx warnings and errors, add doc for building doc. (#4589)
    Add `rmsle` metric and `reg:squaredlogerror` objective (#4541)
    Deprecate gpu_exact, bump required cuda version in docs (#4527)
    Fix document about colsample_by* parameter (#4340)
    
    Correct the calculation mistake in colsample_by* example.
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Fix docs for `num_parallel_tree` (#4221)
    
    Minor formatting correction for `num_parallel_tree`.
    Distributed Fast Histogram Algorithm (#4011)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * init
    
    * allow hist algo
    
    * more changes
    
    * temp
    
    * update
    
    * remove hist sync
    
    * udpate rabit
    
    * change hist size
    
    * change the histogram
    
    * update kfactor
    
    * sync per node stats
    
    * temp
    
    * update
    
    * final
    
    * code clean
    
    * update rabit
    
    * more cleanup
    
    * fix errors
    
    * fix failed tests
    
    * enforce c++11
    
    * fix lint issue
    
    * broadcast subsampled feature correctly
    
    * revert some changes
    
    * fix lint issue
    
    * enable monotone and interaction constraints
    
    * don't specify default for monotone and interactions
    
    * update docs
    fix doc about max_depth (#4078)
    
    * fix doc
    
    * Update doc/parameter.rst
    
    Co-Authored-By: CodingCat <CodingCat@users.noreply.github.com>
    Document num_parallel_tree. (#4022)
    Column sampling at individual nodes (splits). (#3971)
    
    * Column sampling at individual nodes (splits).
    
    * Documented colsample_bynode parameter.
    
    - also updated documentation for colsample_by* parameters
    
    * Updated documentation.
    
    * GetFeatureSet() returns shared pointer to std::vector.
    
    * Sync sampled columns across multiple processes.
    Unify logging facilities. (#3982)
    
    * Unify logging facilities.
    
    * Enhance `ConsoleLogger` to handle different verbosity.
    * Override macros from `dmlc`.
    * Don't use specialized gamma when building with GPU.
    * Remove verbosity cache in monitor.
    * Test monitor.
    * Deprecate `silent`.
    * Fix doc and messages.
    * Fix python test.
    * Fix silent tests.
    Enable running objectives with 0 GPU. (#3878)
    
    * Enable running objectives with 0 GPU.
    
    * Enable 0 GPU for objectives.
    * Add doc for GPU objectives.
    * Fix some objectives defaulted to running on all GPUs.
    Update doc: colsample_bylevel now works for tree_method=hist (#3862)
    
    This feature was introduced by #3635
    Fix typo in docs (#3852)
    
    Fix typo in docs
    Update parameter.rst (#3843)
    Document gblinear parameters: feature_selector and top_k (#3780)
    update eval_metric doc (#3687)
    Document LambdaMART objectives: pairwise, listwise (#3672)
    
    * Document LambdaMART objectives
    
    * Distinguish between pairwise and listwise objectives
    Link fixed. (#3640)
    Fix #3609: Removed unused parameter 'use_buffer' (#3610)
    Add option to disable default metric (#3606)
    Add JSON dump functionality documentation (#3600)
    Revert "Fix #3485, #3540: Don't use dropout for predicting test sets" (#3563)
    
    * Revert "Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)"
    
    This reverts commit 44811f233071c5805d70c287abd22b155b732727.
    
    * Document behavior of predict() for DART booster
    
    * Add notice to parameter.rst
    Update broken links (#3565)
    
    Fix #3559
    Fix #3562
    Implementation of hinge loss for binary classification (#3477)
    Fix typo in parameter.rst, gblinear section (#3518)
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Add JSON schema for categorical splits. (#6194)
    [Breaking] Don't save leaf child count in JSON. (#6094)
    
    The field is deprecated and not used anywhere in XGBoost.
    Update JSON schema. (#5982)
    
    
    * Update JSON schema for pseudo huber.
    * Update JSON model schema.
    Port patches from 1.0.0 branch (#5336)
    
    * Remove f-string, since it's not supported by Python 3.5 (#5330)
    
    * Remove f-string, since it's not supported by Python 3.5
    
    * Add Python 3.5 to CI, to ensure compatibility
    
    * Remove duplicated matplotlib
    
    * Show deprecation notice for Python 3.5
    
    * Fix lint
    
    * Fix lint
    
    * Fix a unit test that mistook MINOR ver for PATCH ver
    
    * Enforce only major version in JSON model schema
    
    * Bump version to 1.1.0-SNAPSHOT
    Add dart to JSON schema. (#5218)
    
    * Add dart to JSON schema.
    
    * Use spaces instead of tab.
    Example JSON model parser and Schema. (#5137)
    [DOC] Update R doc
    Squashed 'subtree/rabit/' changes from fa99857..e81a11d
    
    e81a11d Merge pull request #25 from daiyl0320/master
    35c3b37 add retry mechanism to ConnectTracker and modify Listen backlog to 128 in rabit_traker.py
    c71ed6f try deply doxygen
    62e5647 try deply doxygen
    732f1c6 try
    2fa6e02 ok
    0537665 minor
    7b59dcb minor
    5934950 new doc
    f538187 ok
    44b6049 new doc
    387339b add more
    9d4397a chg
    2879a48 chg
    30e3110 ok
    9ff0301 add link translation
    6b629c2 k
    32e1955 ok
    8f4839d fix
    93137b2 ok
    7eeeb79 reload recommonmark
    a8f00cc minor
    19b0f01 ok
    dd01184 minor
    c1cdc19 minor
    fcf0f43 try rst
    cbc21ae try
    62ddfa7 tiny
    aefc05c final change
    2aee9b4 minor
    fe4e7c2 ok
    8001983 change to subtitle
    5ca33e4 ok
    88f7d24 update guide
    29d43ab add code
    fe8bb3b minor hack for readthedocs
    229c71d Merge branch 'master' of ssh://github.com/dmlc/rabit
    7424218 ok
    d1d45bb Update README.md
    1e8813f Update README.md
    1ccc990 Update README.md
    0323e06 remove readme
    679a835 remove theme
    7ea5b7c remove numpydoc to napoleon
    b73e2be Merge branch 'master' of ssh://github.com/dmlc/rabit
    1742283 ok
    1838e25 Update python-requirements.txt
    bc4e957 ok
    fba6fc2 ok
    0251101 ok
    d50b905 ok
    d4f2509 ok
    cdf401a ok
    fef0ef2 new doc
    cef360d ok
    c125d2a ok
    270a49e add requirments
    744f901 get the basic doc
    1cb5cad Merge branch 'master' of ssh://github.com/dmlc/rabit
    8cc07ba minor
    d74f126 Update .travis.yml
    52b3dcd Update .travis.yml
    099581b Update .travis.yml
    1258046 Update .travis.yml
    7addac9 Update Makefile
    0ea7adf Update .travis.yml
    f858856 Update travis_script.sh
    d8eac4a Update README.md
    3cc49ad lint and travis
    ceedf4e fix
    fd8920c fix win32
    8bbed35 modify
    9520b90 Merge pull request #14 from dmlc/hjk41
    df14bb1 fix type
    f441dc7 replace tab with blankspace
    2467942 remove unnecessary include
    181ef47 defined long long and ulonglong
    1582180 use int32_t to define int and int64_t to define long. in VC long is 32bit
    e0b7da0 fix
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: e81a11dd7ee3cff87a38a42901315821df018bae
    enable basic sphinx doc
    get the basic doc
    Squashed 'subtree/rabit/' content from commit c7282ac
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: c7282acb2a92e1d6a32614889ff466fec58937f3
    update doc
    remove doc from main repo
    changing report folder to doc
    adding simple image
    adding some initial skeleton of the report.
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add JSON schema to model dump. (#5660)
    [CI] Migrate linters to GitHub Actions (#6035)
    
    * [CI] Move lint to GitHub Actions
    
    * [CI] Move Doxygen to GitHub Actions
    
    * [CI] Move Sphinx build test to GitHub Actions
    
    * [CI] Reduce workload for Windows R tests
    
    * [CI] Move clang-tidy to Build stage
    [Doc] Fix rendering of Markdown docs, e.g. R doc (#5821)
    Rewrite setup.py. (#5271)
    
    The setup.py is rewritten.  This new script uses only Python code and provide customized
    implementation of setuptools commands.  This way users can run most of setuptools commands
    just like any other Python libraries.
    
    * Remove setup_pip.py
    * Remove soft links.
    * Define customized commands.
    * Remove shell script.
    * Remove makefile script.
    * Update the doc for building from source.
    fixed year to 2019 in conf.py, helpers.h and LICENSE (#4661)
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    [CI] Refactor Jenkins CI pipeline + migrate all Linux tests to Jenkins (#4401)
    
    * All Linux tests are now in Jenkins CI
    * Tests are now de-coupled from builds. We can now build XGBoost with one version of CUDA/JDK and test it with another version of CUDA/JDK
    * Builds (compilation) are significantly faster because 1) They use C5 instances with faster CPU cores; and 2) build environment setup is cached using Docker containers
    Fix broken doc build due to Matplotlib 3.0 release (#3764)
    Revert #3677 and #3674 (#3678)
    
    * Revert "Add scikit-learn as dependency for doc build (#3677)"
    
    This reverts commit 308f664ade0547242608e21f6198c895415f03da.
    
    * Revert "Add scikit-learn tests (#3674)"
    
    This reverts commit d176a0fbc8165e3afe3e42ff464ab7b253211555.
    Add scikit-learn as dependency for doc build (#3677)
    [TREE] add interaction constraints (#3466)
    
    * add interaction constraints
    
    * enable both interaction and monotonic constraints at the same time
    
    * fix lint
    
    * add R test, fix lint, update demo
    
    * Use dmlc::JSONReader to express interaction constraints as nested lists; Use sparse arrays for bookkeeping
    
    * Add Python test for interaction constraints
    
    * make R interaction constraints parameter based on feature index instead of column names, fix R coding style
    
    * Fix lint
    
    * Add BlueTea88 to CONTRIBUTORS.md
    
    * Short circuit when no constraint is specified; address review comments
    
    * Add tutorial for feature interaction constraints
    
    * allow interaction constraints to be passed as string, remove redundant column_names argument
    
    * Fix typo
    
    * Address review comments
    
    * Add comments to Python test
    Update Python API doc (#3619)
    
    * Add XGBRanker to Python API doc
    
    * Show inherited members of XGBRegressor in API doc, since XGBRegressor uses default methods from XGBModel
    
    * Add table of contents to Python API doc
    
    * Skip JVM doc download if not available
    
    * Show inherited members for XGBRegressor and XGBRanker
    
    * Expose XGBRanker to Python XGBoost module directory
    
    * Add docstring to XGBRegressor.predict() and XGBRanker.predict()
    
    * Fix rendering errors in Python docstrings
    
    * Fix lint
    Refined logic for locating git branch inside ReadTheDocs (#3573)
    [BLOCKING] Adding JVM doc build to Jenkins CI (#3567)
    
    * Adding Java/Scala doc build to Jenkins CI
    
    * Deploy built doc to S3 bucket
    
    * Build doc only for branches
    
    * Build doc first, to get doc faster for branch updates
    
    * Have ReadTheDocs download doc tarball from S3
    
    * Update JVM doc links
    
    * Put doc build commands in a script
    
    * Specify Spark 2.3+ requirement for XGBoost4J-Spark
    
    * Build GPU wheel without NCCL, to reduce binary size
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Documenting CSV loading into DMatrix (#3137)
    
    * Support CSV file in DMatrix
    
    We'd just need to expose the CSV parser in dmlc-core to the Python wrapper
    
    * Revert extra code; document existing CSV support
    
    CSV support is already there but undocumented
    
    * Add notice about categorical features
    Fix doc build (#3126)
    
    * Fix doc build
    
    ReadTheDocs build has been broken for a while due to incompatibilities between
    commonmark, recommonmark, and sphinx. See:
    * "Recommonmark not working with Sphinx 1.6"
      https://github.com/rtfd/recommonmark/issues/73
    * "CommonMark 0.6.0 breaks compatibility"
      https://github.com/rtfd/recommonmark/issues/24
    For now, we fix the versions to get the build working again
    
    * Fix search bar
    [GPU-Plugin] Major refactor 2 (#2664)
    
    * Change cmake option
    
    * Move source files
    
    * Move google tests
    
    * Move python tests
    
    * Move benchmarks
    
    * Move documentation
    
    * Remove makefile support
    
    * Fix test run
    
    * Move GPU tests
    Use jQuery 2.2.4 (#2581)
    [DOC] refactor doc
    update year in LICENSE, conf.py and README.md files
    
    I found that year in files is not up-to-date
    [DOC] Update R doc
    Squashed 'subtree/rabit/' changes from fa99857..e81a11d
    
    e81a11d Merge pull request #25 from daiyl0320/master
    35c3b37 add retry mechanism to ConnectTracker and modify Listen backlog to 128 in rabit_traker.py
    c71ed6f try deply doxygen
    62e5647 try deply doxygen
    732f1c6 try
    2fa6e02 ok
    0537665 minor
    7b59dcb minor
    5934950 new doc
    f538187 ok
    44b6049 new doc
    387339b add more
    9d4397a chg
    2879a48 chg
    30e3110 ok
    9ff0301 add link translation
    6b629c2 k
    32e1955 ok
    8f4839d fix
    93137b2 ok
    7eeeb79 reload recommonmark
    a8f00cc minor
    19b0f01 ok
    dd01184 minor
    c1cdc19 minor
    fcf0f43 try rst
    cbc21ae try
    62ddfa7 tiny
    aefc05c final change
    2aee9b4 minor
    fe4e7c2 ok
    8001983 change to subtitle
    5ca33e4 ok
    88f7d24 update guide
    29d43ab add code
    fe8bb3b minor hack for readthedocs
    229c71d Merge branch 'master' of ssh://github.com/dmlc/rabit
    7424218 ok
    d1d45bb Update README.md
    1e8813f Update README.md
    1ccc990 Update README.md
    0323e06 remove readme
    679a835 remove theme
    7ea5b7c remove numpydoc to napoleon
    b73e2be Merge branch 'master' of ssh://github.com/dmlc/rabit
    1742283 ok
    1838e25 Update python-requirements.txt
    bc4e957 ok
    fba6fc2 ok
    0251101 ok
    d50b905 ok
    d4f2509 ok
    cdf401a ok
    fef0ef2 new doc
    cef360d ok
    c125d2a ok
    270a49e add requirments
    744f901 get the basic doc
    1cb5cad Merge branch 'master' of ssh://github.com/dmlc/rabit
    8cc07ba minor
    d74f126 Update .travis.yml
    52b3dcd Update .travis.yml
    099581b Update .travis.yml
    1258046 Update .travis.yml
    7addac9 Update Makefile
    0ea7adf Update .travis.yml
    f858856 Update travis_script.sh
    d8eac4a Update README.md
    3cc49ad lint and travis
    ceedf4e fix
    fd8920c fix win32
    8bbed35 modify
    9520b90 Merge pull request #14 from dmlc/hjk41
    df14bb1 fix type
    f441dc7 replace tab with blankspace
    2467942 remove unnecessary include
    181ef47 defined long long and ulonglong
    1582180 use int32_t to define int and int64_t to define long. in VC long is 32bit
    e0b7da0 fix
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: e81a11dd7ee3cff87a38a42901315821df018bae
    try deply doxygen
    try deply doxygen
    ok
    minor
    minor
    Document refactor
    
    change badge
    new doc
    enable basic sphinx doc
    ok
    add link translation
    k
    ok
    fix
    ok
    reload recommonmark
    minor
    ok
    minor
    minor
    try
    tiny
    update guide
    remove theme
    remove numpydoc to napoleon
    ok
    ok
    new doc
    ok
    get the basic doc
    Fix doc build (#3126)
    
    * Fix doc build
    
    ReadTheDocs build has been broken for a while due to incompatibilities between
    commonmark, recommonmark, and sphinx. See:
    * "Recommonmark not working with Sphinx 1.6"
      https://github.com/rtfd/recommonmark/issues/73
    * "CommonMark 0.6.0 breaks compatibility"
      https://github.com/rtfd/recommonmark/issues/24
    For now, we fix the versions to get the build working again
    
    * Fix search bar
    Fixing a few typos (#1771)
    
    * Fixing a few typos
    
    * Fixing a few typos
    update
    Document refactor
    
    change badge
    try
    minor
    ok
    Remove silent from R demos. (#5675)
    
    * Remove silent from R demos.
    
    * Vignettes.
    Remove `silent` in doc. [skip ci] (#4689)
    replace nround with nrounds to match actual parameter (#3592)
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Add swift package reference (#5728)
    
    Co-authored-by: Peter Jung <peter.jung@heureka.cz>
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Add link to Ruby XGBoost gem (#4856)
    Re-organize contributor's guide (#4659)
    
    * Reorganize contributor's doc
    
    * Address comments from @trivialfis
    
    * Address @sriramch's comment: include ABI compatibility guarantee
    
    * Address @rongou's comment
    
    * Postpone ABI compatibility guarantee for now
    contribute to community doc (#4646)
    
    * add community doc
    
    * update
    
    * update
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    try
    update guide
    minor hack for readthedocs
    ok
    get the basic doc
    Added conda environment file for building docs (#5773)
    Use Sphinx 2.1+ to compile documentation [skip ci] (#4609)
    Revert #3677 and #3674 (#3678)
    
    * Revert "Add scikit-learn as dependency for doc build (#3677)"
    
    This reverts commit 308f664ade0547242608e21f6198c895415f03da.
    
    * Revert "Add scikit-learn tests (#3674)"
    
    This reverts commit d176a0fbc8165e3afe3e42ff464ab7b253211555.
    Add scikit-learn as dependency for doc build (#3677)
    Add numpy and matplotlib as requirements for doc build (#3669)
    [BLOCKING] Adding JVM doc build to Jenkins CI (#3567)
    
    * Adding Java/Scala doc build to Jenkins CI
    
    * Deploy built doc to S3 bucket
    
    * Build doc only for branches
    
    * Build doc first, to get doc faster for branch updates
    
    * Have ReadTheDocs download doc tarball from S3
    
    * Update JVM doc links
    
    * Put doc build commands in a script
    
    * Specify Spark 2.3+ requirement for XGBoost4J-Spark
    
    * Build GPU wheel without NCCL, to reduce binary size
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Fix doc build (#3126)
    
    * Fix doc build
    
    ReadTheDocs build has been broken for a while due to incompatibilities between
    commonmark, recommonmark, and sphinx. See:
    * "Recommonmark not working with Sphinx 1.6"
      https://github.com/rtfd/recommonmark/issues/73
    * "CommonMark 0.6.0 breaks compatibility"
      https://github.com/rtfd/recommonmark/issues/24
    For now, we fix the versions to get the build working again
    
    * Fix search bar
    Added conda environment file for building docs (#5773)
    Cosmetic fixes in faq.rst (#6161)
    Update faq.rst (#3521)
    
    Just fixing a minor typo
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    [R package] GPU support (#2732)
    
    * [R] MSVC compatibility
    
    * [GPU] allow seed in BernoulliRng up to size_t and scale to uint32_t
    
    * R package build with cmake and CUDA
    
    * R package CUDA build fixes and cleanups
    
    * always export the R package native initialization routine on windows
    
    * update the install instructions doc
    
    * fix lint
    
    * use static_cast directly to set BernoulliRng seed
    
    * [R] demo for GPU accelerated algorithm
    
    * tidy up the R package cmake stuff
    
    * R pack cmake: installs main dependency packages if needed
    
    * [R] version bump in DESCRIPTION
    
    * update NEWS
    
    * added short missing/sparse values explanations to FAQ
    Update faq.md (#2727)
    
    Changed dead link to actual one
    Fixed links in faq.md (#2726)
    fixed some typos (#1814)
    Fixing a few typos (#1771)
    
    * Fixing a few typos
    
    * Fixing a few typos
    spelling and grammar changes
    minor
    Update faq.md
    Document refactor
    
    change badge
    Export c++ headers in CMake installation. (#4897)
    
    * Move get transpose into cc.
    
    * Clean up headers in host device vector, remove thrust dependency.
    
    * Move span and host device vector into public.
    
    * Install c++ headers.
    
    * Short notes for c and c++.
    
    Co-Authored-By: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Squashed 'subtree/rabit/' changes from fa99857..e81a11d
    
    e81a11d Merge pull request #25 from daiyl0320/master
    35c3b37 add retry mechanism to ConnectTracker and modify Listen backlog to 128 in rabit_traker.py
    c71ed6f try deply doxygen
    62e5647 try deply doxygen
    732f1c6 try
    2fa6e02 ok
    0537665 minor
    7b59dcb minor
    5934950 new doc
    f538187 ok
    44b6049 new doc
    387339b add more
    9d4397a chg
    2879a48 chg
    30e3110 ok
    9ff0301 add link translation
    6b629c2 k
    32e1955 ok
    8f4839d fix
    93137b2 ok
    7eeeb79 reload recommonmark
    a8f00cc minor
    19b0f01 ok
    dd01184 minor
    c1cdc19 minor
    fcf0f43 try rst
    cbc21ae try
    62ddfa7 tiny
    aefc05c final change
    2aee9b4 minor
    fe4e7c2 ok
    8001983 change to subtitle
    5ca33e4 ok
    88f7d24 update guide
    29d43ab add code
    fe8bb3b minor hack for readthedocs
    229c71d Merge branch 'master' of ssh://github.com/dmlc/rabit
    7424218 ok
    d1d45bb Update README.md
    1e8813f Update README.md
    1ccc990 Update README.md
    0323e06 remove readme
    679a835 remove theme
    7ea5b7c remove numpydoc to napoleon
    b73e2be Merge branch 'master' of ssh://github.com/dmlc/rabit
    1742283 ok
    1838e25 Update python-requirements.txt
    bc4e957 ok
    fba6fc2 ok
    0251101 ok
    d50b905 ok
    d4f2509 ok
    cdf401a ok
    fef0ef2 new doc
    cef360d ok
    c125d2a ok
    270a49e add requirments
    744f901 get the basic doc
    1cb5cad Merge branch 'master' of ssh://github.com/dmlc/rabit
    8cc07ba minor
    d74f126 Update .travis.yml
    52b3dcd Update .travis.yml
    099581b Update .travis.yml
    1258046 Update .travis.yml
    7addac9 Update Makefile
    0ea7adf Update .travis.yml
    f858856 Update travis_script.sh
    d8eac4a Update README.md
    3cc49ad lint and travis
    ceedf4e fix
    fd8920c fix win32
    8bbed35 modify
    9520b90 Merge pull request #14 from dmlc/hjk41
    df14bb1 fix type
    f441dc7 replace tab with blankspace
    2467942 remove unnecessary include
    181ef47 defined long long and ulonglong
    1582180 use int32_t to define int and int64_t to define long. in VC long is 32bit
    e0b7da0 fix
    
    git-subtree-dir: subtree/rabit
    git-subtree-split: e81a11dd7ee3cff87a38a42901315821df018bae
    enable basic sphinx doc
    get the basic doc
    Export DaskDeviceQuantileDMatrix in doc. [skip ci] (#5975)
    Add support for dlpack, expose python docs for DeviceQuantileDMatrix (#5465)
    Note for `DaskDMatrix`. (#5144)
    
    * Brief introduction to `DaskDMatrix`.
    
    * Add xgboost.dask.train to API doc
    Rewrite Dask interface. (#4819)
    Fix dask API sphinx docstrings (#4507)
    
    * Fix dask API sphinx docstrings
    
    * Update GPU docs page
    Add native support for Dask (#4473)
    
    * Add native support for Dask
    
    * Add multi-GPU demo
    
    * Add sklearn example
    Add python RF documentation (#4500)
    Fix #3663: Allow sklearn API to use callbacks (#3682)
    
    * Fix #3663: Allow sklearn API to use callbacks
    
    * Fix lint
    
    * Add Callback API to Python API doc
    Update Python API doc (#3619)
    
    * Add XGBRanker to Python API doc
    
    * Show inherited members of XGBRegressor in API doc, since XGBRegressor uses default methods from XGBModel
    
    * Add table of contents to Python API doc
    
    * Skip JVM doc download if not available
    
    * Show inherited members for XGBRegressor and XGBRanker
    
    * Expose XGBRanker to Python XGBoost module directory
    
    * Add docstring to XGBRegressor.predict() and XGBRanker.predict()
    
    * Fix rendering errors in Python docstrings
    
    * Fix lint
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    ENH: Add visualization to python package
    Document refactor
    
    change badge
    enable basic sphinx doc
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    [doc] Some notes for external memory. (#5065)
    Add cuDF DataFrame to doc. (#5053)
    Fix data loading (#4862)
    
    * Fix loading text data.
    * Fix config regex.
    * Try to explain the error better in exception.
    * Update doc.
    Remove `silent` in doc. [skip ci] (#4689)
    Make `HistCutMatrix::Init' be aware of groups. (#4115)
    
    
    * Add checks for group size.
    * Simple docs.
    * Search group index during hist cut matrix initialization.
    
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Prevent training without setting up caches. (#4066)
    
    * Prevent training without setting up caches.
    
    * Add warning for internal functions.
    * Check number of features.
    
    * Address reviewer's comment.
    Address #3933: document limitation of DMLC CSV parser + recommend Pandas (#3934)
    Clarify multi-GPU training, binary wheels, Pandas integration (#3581)
    
    * Clarify multi-GPU training, binary wheels, Pandas integration
    
    * Add a note about multi-GPU on gpu/index.rst
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Rework Python callback functions. (#6199)
    
    
    * Define a new callback interface for Python.
    * Deprecate the old callbacks.
    * Enable early stopping on dask.
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    [Doc] Fix typos in AFT tutorial (#5716)
    Add R code to AFT tutorial [skip ci] (#5486)
    Implement robust regularization in 'survival:aft' objective (#5473)
    
    * Robust regularization of AFT gradient and hessian
    
    * Fix AFT doc; expose it to tutorial TOC
    
    * Apply robust regularization to uncensored case too
    
    * Revise unit test slightly
    
    * Fix lint
    
    * Update test_survival.py
    
    * Use GradientPairPrecise
    
    * Remove unused variables
    Add Accelerated Failure Time loss for survival analysis task (#4763)
    
    * [WIP] Add lower and upper bounds on the label for survival analysis
    
    * Update test MetaInfo.SaveLoadBinary to account for extra two fields
    
    * Don't clear qids_ for version 2 of MetaInfo
    
    * Add SetInfo() and GetInfo() method for lower and upper bounds
    
    * changes to aft
    
    * Add parameter class for AFT; use enum's to represent distribution and event type
    
    * Add AFT metric
    
    * changes to neg grad to grad
    
    * changes to binomial loss
    
    * changes to overflow
    
    * changes to eps
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * changes to code refactoring
    
    * Re-factor survival analysis
    
    * Remove aft namespace
    
    * Move function bodies out of AFTNormal and AFTLogistic, to reduce clutter
    
    * Move function bodies out of AFTLoss, to reduce clutter
    
    * Use smart pointer to store AFTDistribution and AFTLoss
    
    * Rename AFTNoiseDistribution enum to AFTDistributionType for clarity
    
    The enum class was not a distribution itself but a distribution type
    
    * Add AFTDistribution::Create() method for convenience
    
    * changes to extreme distribution
    
    * changes to extreme distribution
    
    * changes to extreme
    
    * changes to extreme distribution
    
    * changes to left censored
    
    * deleted cout
    
    * changes to x,mu and sd and code refactoring
    
    * changes to print
    
    * changes to hessian formula in censored and uncensored
    
    * changes to variable names and pow
    
    * changes to Logistic Pdf
    
    * changes to parameter
    
    * Expose lower and upper bound labels to R package
    
    * Use example weights; normalize log likelihood metric
    
    * changes to CHECK
    
    * changes to logistic hessian to standard formula
    
    * changes to logistic formula
    
    * Comply with coding style guideline
    
    * Revert back Rabit submodule
    
    * Revert dmlc-core submodule
    
    * Comply with coding style guideline (clang-tidy)
    
    * Fix an error in AFTLoss::Gradient()
    
    * Add missing files to amalgamation
    
    * Address @RAMitchell's comment: minimize future change in MetaInfo interface
    
    * Fix lint
    
    * Fix compilation error on 32-bit target, when size_t == bst_uint
    
    * Allocate sufficient memory to hold extra label info
    
    * Use OpenMP to speed up
    
    * Fix compilation on Windows
    
    * Address reviewer's feedback
    
    * Add unit tests for probability distributions
    
    * Make Metric subclass of Configurable
    
    * Address reviewer's feedback: Configure() AFT metric
    
    * Add a dummy test for AFT metric configuration
    
    * Complete AFT configuration test; remove debugging print
    
    * Rename AFT parameters
    
    * Clarify test comment
    
    * Add a dummy test for AFT loss for uncensored case
    
    * Fix a bug in AFT loss for uncensored labels
    
    * Complete unit test for AFT loss metric
    
    * Simplify unit tests for AFT metric
    
    * Add unit test to verify aggregate output from AFT metric
    
    * Use EXPECT_* instead of ASSERT_*, so that we run all unit tests
    
    * Use aft_loss_param when serializing AFTObj
    
    This is to be consistent with AFT metric
    
    * Add unit tests for AFT Objective
    
    * Fix OpenMP bug; clarify semantics for shared variables used in OpenMP loops
    
    * Add comments
    
    * Remove AFT prefix from probability distribution; put probability distribution in separate source file
    
    * Add comments
    
    * Define kPI and kEulerMascheroni in probability_distribution.h
    
    * Add probability_distribution.cc to amalgamation
    
    * Remove unnecessary diff
    
    * Address reviewer's feedback: define variables where they're used
    
    * Eliminate all INFs and NANs from AFT loss and gradient
    
    * Add demo
    
    * Add tutorial
    
    * Fix lint
    
    * Use 'survival:aft' to be consistent with 'survival:cox'
    
    * Move sample data to demo/data
    
    * Add visual demo with 1D toy data
    
    * Add Python tests
    
    Co-authored-by: Philip Cho <chohyu01@cs.washington.edu>
    fix doc
    Fix typo in model.rst (#4393)
    Document limitation of one-split-at-a-time Greedy tree learning heuristic (#4233)
    Remove errant $ (#3618)
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Loop over `thrust::reduce`. (#6229)
    
    * Check input chunk size of dqdm.
    * Add doc for current limitation.
    [R] Provide better guidance for persisting XGBoost model (#5964)
    
    * [R] Provide better guidance for persisting XGBoost model
    
    * Update saving_model.rst
    
    * Add a paragraph about xgb.serialize()
    Update document for model dump. (#5818)
    
    * Clarify the relationship between dump and save.
    * Mention the schema.
    Fix changing locale. (#5314)
    
    * Fix changing locale.
    
    * Don't use locale guard.
    
    As number parsing is implemented in house, we don't need locale.
    
    * Update doc.
    Add configuration to R interface. (#5217)
    
    * Save and load internal parameter configuration as JSON.
    Merge model compatibility fixes from 1.0rc branch. (#5305)
    
    * Port test model compatibility.
    * Port logit model fix.
    
    https://github.com/dmlc/xgboost/pull/5248
    https://github.com/dmlc/xgboost/pull/5281
    Save Scikit-Learn attributes into learner attributes. (#5245)
    
    * Remove the recommendation for pickle.
    
    * Save skl attributes in booster.attr
    
    * Test loading scikit-learn model with native booster.
    Add dart to JSON schema. (#5218)
    
    * Add dart to JSON schema.
    
    * Use spaces instead of tab.
    Example JSON model parser and Schema. (#5137)
    Tests and documents for new JSON routines. (#5120)
    add pointers to the gpu external memory paper (#5684)
    update docs for gpu external memory (#5332)
    
    * update docs for gpu external memory
    
    * add hist limitation
    [doc] Some notes for external memory. (#5065)
    [CI] Fix Travis tests. (#5062)
    
    - Install wget explicitly to match openssl.
    - Install CMake explicitly.
    - Use newer miniconda link.
    - Reenable unittests.
    - gcc@9 + xcode@10 for osx due to missing <_stdio.h>.  Other versions of gcc should also work.  But as homebrew pour gcc@9 after update by default, so I just stick with latest version.
    - Disabled one external memory test for OSX.  Not sure about the thread implementation in there and fixing external memory is beyond the scope of this PR.
    - Use Python3 with conda in jvm package.
    Fix external memory documentation [skip ci] (#4747)
    
    * - fix external memory documentation [skip ci]
       - to state that it is supported now on gpu algorithms
    Remove gpu_exact tree method (#4742)
    Document CUDA requirement, lack of external memory on GPU (#3624)
    
    * Document fact that GPU doesn't support external memory
    
    * Document CUDA requirement
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Enforce correct data shape. (#5191)
    
    * Fix syncing DMatrix columns.
    * notes for tree method.
    * Enable feature validation for all interfaces except for jvm.
    * Better tests for boosting from predictions.
    * Disable validation on JVM.
    grammar fixes and typos (#3568)
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Update param_tuning.md
    [DOC] refactor doc
    Document refactor
    
    change badge
    fixed typos and sentence structure
    add parameter tunning
    Address some sphinx warnings and errors, add doc for building doc. (#4589)
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Mark Scikit-Learn RF interface as experimental in doc. (#4258)
    
    * Mark Scikit-Learn RF interface as experimental in doc.
    Added SKLearn-like random forest Python API. (#4148)
    
    
    * Added SKLearn-like random forest Python API.
    
    - added XGBRFClassifier and XGBRFRegressor classes to SKL-like xgboost API
    - also added n_gpus and gpu_id parameters to SKL classes
    - added documentation describing how to use xgboost for random forests,
      as well as existing caveats
    [doc] Some notes for external memory. (#5065)
    Fix data loading (#4862)
    
    * Fix loading text data.
    * Fix config regex.
    * Try to explain the error better in exception.
    * Update doc.
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Fix dask doc. [skip ci] (#6108)
    Update XGBoost + Dask overview documentation (#5961)
    
    * Add imports to code snippet
    
    * Better writing.
    [Dask] Asyncio support. (#5862)
    [DOC] Mention dask blog post in doc. [skip ci] (#5789)
    [dask] Fix missing value for scikit-learn interface. (#5435)
    [dask] Honor `nthreads` from dask worker. (#5414)
    Update dask.rst to correct a spelling mistake (#5371)
    
    Change `signle-node` to `single-node`
    Note for `DaskDMatrix`. (#5144)
    
    * Brief introduction to `DaskDMatrix`.
    
    * Add xgboost.dask.train to API doc
    Rewrite Dask interface. (#4819)
    Fix #3857: take down AWS YARN tutorial, as it is outdated (#3885)
    Add link to XGBoost4J-Spark tutorial on AWS Yarn tutorial (#3582)
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Edits on tutorial for XGBoost job on Kubernetes (#5487)
    Add tutorial for distributed training and batch prediction with Kubernetes (#4621)
    
    * provide the readme
    
    * update for format
    
    * reformat
    
    * reformat -2
    
    * update again
    
    * update format
    
    * update w.r.t yinlou's comments
    
    * Add kubernetes tutorial to Table of Contents
    
    * Style edit
    [Breaking] Don't drop trees during DART prediction by default (#5115)
    
    * Simplify DropTrees calling logic
    
    * Add `training` parameter for prediction method.
    
    * [Breaking]: Add `training` to C API.
    
    * Change for R and Python custom objective.
    
    * Correct comment.
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Co-authored-by: Jiaming Yuan <jm.yuan@outlook.com>
    Remove `silent` in doc. [skip ci] (#4689)
    Revert "Fix #3485, #3540: Don't use dropout for predicting test sets" (#3563)
    
    * Revert "Fix #3485, #3540: Don't use dropout for predicting test sets (#3556)"
    
    This reverts commit 44811f233071c5805d70c287abd22b155b732727.
    
    * Document behavior of predict() for DART booster
    
    * Add notice to parameter.rst
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Move dask tutorial closer other distributed tutorials (#5613)
    Implement robust regularization in 'survival:aft' objective (#5473)
    
    * Robust regularization of AFT gradient and hessian
    
    * Fix AFT doc; expose it to tutorial TOC
    
    * Apply robust regularization to uncensored case too
    
    * Revise unit test slightly
    
    * Fix lint
    
    * Update test_survival.py
    
    * Use GradientPairPrecise
    
    * Remove unused variables
    Tests and documents for new JSON routines. (#5120)
    Rewrite Dask interface. (#4819)
    Add tutorial for distributed training and batch prediction with Kubernetes (#4621)
    
    * provide the readme
    
    * update for format
    
    * reformat
    
    * reformat -2
    
    * update again
    
    * update format
    
    * update w.r.t yinlou's comments
    
    * Add kubernetes tutorial to Table of Contents
    
    * Style edit
    Doc and demo for customized metric and obj. (#4598)
    
    Co-Authored-By: Theodore Vasiloudis <theodoros.vasiloudis@gmail.com>
    [RFC] Version 0.90 release candidate (#4475)
    
    * Release 0.90
    
    * Add script to automatically generate acknowledgment
    
    * Update NEWS.md
    [TREE] add interaction constraints (#3466)
    
    * add interaction constraints
    
    * enable both interaction and monotonic constraints at the same time
    
    * fix lint
    
    * add R test, fix lint, update demo
    
    * Use dmlc::JSONReader to express interaction constraints as nested lists; Use sparse arrays for bookkeeping
    
    * Add Python test for interaction constraints
    
    * make R interaction constraints parameter based on feature index instead of column names, fix R coding style
    
    * Fix lint
    
    * Add BlueTea88 to CONTRIBUTORS.md
    
    * Short circuit when no constraint is specified; address review comments
    
    * Add tutorial for feature interaction constraints
    
    * allow interaction constraints to be passed as string, remove redundant column_names argument
    
    * Fix typo
    
    * Address review comments
    
    * Add comments to Python test
    [jvm-packages] Tutorial of XGBoost4J-Spark (#3534)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add new
    
    * update doc
    
    * finish Gang Scheduling
    
    * more
    
    * intro
    
    * Add sections: Prediction, Model persistence and ML pipeline.
    
    * Add XGBoost4j-Spark MLlib pipeline example
    
    * partial finished version
    
    * finish the doc
    
    * adjust code
    
    * fix the doc
    
    * use rst
    
    * Convert XGBoost4J-Spark tutorial to reST
    
    * Bring XGBoost4J up to date
    
    * add note about using hdfs
    
    * remove duplicate file
    
    * fix descriptions
    
    * update doc
    
    * Wrap HDFS/S3 export support as a note
    
    * update
    
    * wrap indexing_mode example in code block
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    [TREE] add interaction constraints (#3466)
    
    * add interaction constraints
    
    * enable both interaction and monotonic constraints at the same time
    
    * fix lint
    
    * add R test, fix lint, update demo
    
    * Use dmlc::JSONReader to express interaction constraints as nested lists; Use sparse arrays for bookkeeping
    
    * Add Python test for interaction constraints
    
    * make R interaction constraints parameter based on feature index instead of column names, fix R coding style
    
    * Fix lint
    
    * Add BlueTea88 to CONTRIBUTORS.md
    
    * Short circuit when no constraint is specified; address review comments
    
    * Add tutorial for feature interaction constraints
    
    * allow interaction constraints to be passed as string, remove redundant column_names argument
    
    * Fix typo
    
    * Address review comments
    
    * Add comments to Python test
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Extract interaction constraint from split evaluator. (#5034)
    
    
    *  Extract interaction constraints from split evaluator.
    
    The reason for doing so is mostly for model IO, where num_feature and interaction_constraints are copied in split evaluator. Also interaction constraint by itself is a feature selector, acting like column sampler and it's inefficient to bury it deep in the evaluator chain. Lastly removing one another copied parameter is a win.
    
    *  Enable inc for approx tree method.
    
    As now the implementation is spited up from evaluator class, it's also enabled for approx method.
    
    *  Removing obsoleted code in colmaker.
    
    They are never documented nor actually used in real world. Also there isn't a single test for those code blocks.
    
    *  Unifying the types used for row and column.
    
    As the size of input dataset is marching to billion, incorrect use of int is subject to overflow, also singed integer overflow is undefined behaviour. This PR starts the procedure for unifying used index type to unsigned integers. There's optimization that can utilize this undefined behaviour, but after some testings I don't see the optimization is beneficial to XGBoost.
    Update doc for feature constraints and `n_gpus`. (#4596)
    
    * Update doc for feature constraints.
    
    * Fix some warnings.
    
    * Clean up doc for `n_gpus`.
    Fix typo in Feature Interaction Constraints tutorial (#3975)
    [TREE] add interaction constraints (#3466)
    
    * add interaction constraints
    
    * enable both interaction and monotonic constraints at the same time
    
    * fix lint
    
    * add R test, fix lint, update demo
    
    * Use dmlc::JSONReader to express interaction constraints as nested lists; Use sparse arrays for bookkeeping
    
    * Add Python test for interaction constraints
    
    * make R interaction constraints parameter based on feature index instead of column names, fix R coding style
    
    * Fix lint
    
    * Add BlueTea88 to CONTRIBUTORS.md
    
    * Short circuit when no constraint is specified; address review comments
    
    * Add tutorial for feature interaction constraints
    
    * allow interaction constraints to be passed as string, remove redundant column_names argument
    
    * Fix typo
    
    * Address review comments
    
    * Add comments to Python test
    Fix custom obj link. [skip ci] (#6100)
    [Breaking] Set output margin to True for custom objective. (#5564)
    
    * Set output margin to True for custom objective in Python and R.
    
    * Add a demo for writing multi-class custom objective function.
    
    * Run tests on selected demos.
    Fix doc for customized objective/metric [skip ci] (#4608)
    Doc and demo for customized metric and obj. (#4598)
    
    Co-Authored-By: Theodore Vasiloudis <theodoros.vasiloudis@gmail.com>
    Add MAPE metric (#6119)
    [Doc] Document new objectives and metrics available on GPUs (#5909)
    [Doc] Document that CUDA 10.0 is required [skip ci] (#5872)
    add pointers to the gpu external memory paper (#5684)
    Update document. (#5572)
    add reference to gpu external memory (#5490)
    Small updates to GPU documentation (#5483)
    Some guidelines on device memory usage (#5038)
    
    * Add memory usage demo
    
    * Update documentation
    Update GPU doc. (#4953)
    Remove gpu_exact tree method (#4742)
    Doc and demo for customized metric and obj. (#4598)
    
    Co-Authored-By: Theodore Vasiloudis <theodoros.vasiloudis@gmail.com>
    Update doc for feature constraints and `n_gpus`. (#4596)
    
    * Update doc for feature constraints.
    
    * Fix some warnings.
    
    * Clean up doc for `n_gpus`.
    Address some sphinx warnings and errors, add doc for building doc. (#4589)
    Deprecate single node multi-gpu mode (#4579)
    
    * deprecate multi-gpu training
    
    * add single node
    
    * add warning
    Deprecate gpu_exact, bump required cuda version in docs (#4527)
    Fix dask API sphinx docstrings (#4507)
    
    * Fix dask API sphinx docstrings
    
    * Update GPU docs page
     Deprecate `reg:linear' in favor of `reg:squarederror'. (#4267)
    
    * Deprecate `reg:linear' in favor of `reg:squarederror'.
    * Replace the use of `reg:linear'.
    * Replace the use of `silent`.
    Remove various synchronisations from cuda API calls, instrument monitor (#4205)
    
    * Remove various synchronisations from cuda API calls, instrument monitor
    with nvtx profiler ranges.
    [REVIEW] Enable Multi-Node Multi-GPU functionality (#4095)
    
    * Initial commit to support multi-node multi-gpu xgboost using dask
    
    * Fixed NCCL initialization by not ignoring the opg parameter.
    
    - it now crashes on NCCL initialization, but at least we're attempting it properly
    
    * At the root node, perform a rabit::Allreduce to get initial sum_gradient across workers
    
    * Synchronizing in a couple of more places.
    
    - now the workers don't go down, but just hang
    - no more "wild" values of gradients
    - probably needs syncing in more places
    
    * Added another missing max-allreduce operation inside BuildHistLeftRight
    
    * Removed unnecessary collective operations.
    
    * Simplified rabit::Allreduce() sync of gradient sums.
    
    * Removed unnecessary rabit syncs around ncclAllReduce.
    
    - this improves performance _significantly_ (7x faster for overall training,
      20x faster for xgboost proper)
    
    * pulling in latest xgboost
    
    * removing changes to updater_quantile_hist.cc
    
    * changing use_nccl_opg initialization, removing unnecessary if statements
    
    * added definition for opaque ncclUniqueId struct to properly encapsulate GetUniqueId
    
    * placing struct defintion in guard to avoid duplicate code errors
    
    * addressing linting errors
    
    * removing
    
    * removing additional arguments to AllReduer initialization
    
    * removing distributed flag
    
    * making comm init symmetric
    
    * removing distributed flag
    
    * changing ncclCommInit to support multiple modalities
    
    * fix indenting
    
    * updating ncclCommInitRank block with necessary group calls
    
    * fix indenting
    
    * adding print statement, and updating accessor in vector
    
    * improving print statement to end-line
    
    * generalizing nccl_rank construction using rabit
    
    * assume device_ordinals is the same for every node
    
    * test, assume device_ordinals is identical for all nodes
    
    * test, assume device_ordinals is unique for all nodes
    
    * changing names of offset variable to be more descriptive, editing indenting
    
    * wrapping ncclUniqueId GetUniqueId() and aesthetic changes
    
    * adding synchronization, and tests for distributed
    
    * adding  to tests
    
    * fixing broken #endif
    
    * fixing initialization of gpu histograms, correcting errors in tests
    
    * adding to contributors list
    
    * adding distributed tests to jenkins
    
    * fixing bad path in distributed test
    
    * debugging
    
    * adding kubernetes for distributed tests
    
    * adding proper import for OrderedDict
    
    * adding urllib3==1.22 to address ordered_dict import error
    
    * added sleep to allow workers to save their models for comparison
    
    * adding name to GPU contributors under docs
    Single precision histograms on GPU (#3965)
    
    * Allow single precision histogram summation in gpu_hist
    
    * Add python test, reduce run-time of gpu_hist tests
    
    * Update documentation
    Enable running objectives with 0 GPU. (#3878)
    
    * Enable running objectives with 0 GPU.
    
    * Enable 0 GPU for objectives.
    * Add doc for GPU objectives.
    * Fix some objectives defaulted to running on all GPUs.
    Fix specifying gpu_id, add tests. (#3851)
    
    * Rewrite gpu_id related code.
    
    * Remove normalised/unnormalised operatios.
    * Address difference between `Index' and `Device ID'.
    * Modify doc for `gpu_id'.
    * Better LOG for GPUSet.
    * Check specified n_gpus.
    * Remove inappropriate `device_idx' term.
    * Clarify GpuIdType and size_t.
    Document CUDA requirement, lack of external memory on GPU (#3624)
    
    * Document fact that GPU doesn't support external memory
    
    * Document CUDA requirement
    Clarify multi-GPU training, binary wheels, Pandas integration (#3581)
    
    * Clarify multi-GPU training, binary wheels, Pandas integration
    
    * Add a note about multi-GPU on gpu/index.rst
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    add style
    add style
    add style
    [DOC] Update R doc
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    replace nround with nrounds to match actual parameter (#3592)
    minor typo (#2751)
    
    * minor typo
    
    * typo
    
    * Update discoverYourData.md
    Fixing a few typos (#1771)
    
    * Fixing a few typos
    
    * Fixing a few typos
    Fixed a typo (#1172)
    
    panda -> Pandas
    [Doc] documents update:
    
    (1) install_github is not support due to the usage of submodule
    
    (2) remove part of the markdown which is not displayed correctly, see
    https://xgboost.readthedocs.org/en/latest/R-package/discoverYourData.html
    [DOC] Update R doc
    Cleaning in documentation
    Cleaning in documentation
    Frequence to Frequency
    Update vignette
    Add Random Forest parameter (num_parallel_tree) in function doc + example in Vignette.
    update links dmlc
    code simplification
    Vignette text
    Vignette text
    df spell
    trademark RF
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    Update discoverYourData.Rmd
    Vignette text
    Vignette text
    Vignette text
    Vignette text
    Vignette text
    Fix Vignette bug!
    Vignette text
    text
    splell
    vignette text
    refix
    vignette text
    text refactor
    vignette text
    text vignette
    Vignette text
    refactor vignette
    Vignettes
    Vignette text
    Vignettes improvement
    text change
    improved vignette text
    modif CSS
    improve text of the Vignette
    Vignette, 1st version
    [R] replace uses of T and F with TRUE and FALSE (#5778)
    
    * [R-package] replace uses of T and F with TRUE and FALSE
    
    * enable linting
    
    * Remove skip
    
    Co-authored-by: Philip Hyunsu Cho <chohyu01@cs.washington.edu>
    Remove `max.depth` in R gblinear example. (#5753)
    minor updates to links and grammar (#4673)
    
    updated links to caret data splitting, xgb.dump(with_stats), and some grammar
    fix typos (#4027)
    Fix a typo in the R-package documentation: max.deph -> max.depth (#3890)
    
    Signed-off-by: Jiacheng Xu <xjcmaxwellcjx@gmail.com>
    replace nround with nrounds to match actual parameter (#3592)
    Fixed typo in doc (#2799)
    [Doc] documents update:
    
    (1) install_github is not support due to the usage of submodule
    
    (2) remove part of the markdown which is not displayed correctly, see
    https://xgboost.readthedocs.org/en/latest/R-package/discoverYourData.html
    [R] update doc; add drat repo
    [DOC] Update R doc
    Fix typo
    
    "Until Know" to "Until Now"
    Cleaning in documentation
    Cleaning in documentation
    spelling changes
    Document refactor
    
    change badge
    Adding examples on xgb.importance, xgb.plot.importance and xgb.plot tree
    Update xgboostPresentation.Rmd
    
    Edited to note unavailability of stable version of this package on CRAN.
    
    http://cran.r-project.org/web/packages/xgboost/index.html
    update links dmlc
    To submit to CRAN we cannot use more than 2 threads in our examples/vignettes
    Add ref.
    possible polishments
    Update xgboostPresentation.Rmd
    Vignette text
    text vignette
    Vignette text
    text vignette
    Vignette txt
    CSS improvement
    add linear boosting part
    clean temp
    vignette text
    Vignette text
    vignette text
    text vignette
    add bibliography
    Vignette text
    add introduction paragraph from PDF file
    refactor vignette
    Vignettes
    Vignette text
    Vignette text
    text vignette
    Vignettes improvement
    text change
    add saveload to raw
    add saveload to raw
    Update wlkthrough R demo code to include variable importance.
    ok
    add basic walkthrough
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    [R] update doc; add drat repo
    [DOC] Update R doc
    [Doc] add doc for kill_spark_context_on_worker_failure parameter (#6097)
    
    * [Doc] add doc for kill_spark_context_on_worker_failure parameter
    
    * resolve comments
    [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)
    Add new lines for Spark XGBoost missing values section (#5180)
    [jvm-packages] Allow for bypassing spark missing value check (#4805)
    
    * Allow for bypassing spark missing value check
    
    * Update documentation for dealing with missing values in spark xgboost
    Copy CMake parameter from dmlc-core. (#4948)
    Update xgboost-spark doc (#4804)
    Address some sphinx warnings and errors, add doc for building doc. (#4589)
    Update xgboost4j_spark_tutorial.rst (#4476)
    [jvm-packages] Automatically set maximize_evaluation_metrics if not explicitly given in XGBoost4J-Spark (#4446)
    
    * Automatically set maximize_evaluation_metrics if not explicitly given.
    
    * When custom_eval is set, require maximize_evaluation_metrics.
    
    * Update documents on early stop in XGBoost4J-Spark.
    
    * Fix code error.
    Fix list formatting in missing value tutorial in XGBoost4J-Spark
    Fix list formatting in missing value tutorial in XGBoost4J-Spark
    [jvm-packages] Tutorial on handling missing values (#4425)
    
    Add tutorial on missing values and how to handle those within XGBoost.
    [jvm-packages] support spark 2.4 and compatibility test with previous xgboost version (#4377)
    
    * bump spark version
    
    * keep float.nan
    
    * handle brokenly changed name/value
    
    * add test
    
    * add model files
    
    * add model files
    
    * update doc
    Fix snapshot artifact name in docs. (#4196)
    [jvm-packages] Fix early stop with xgboost4j-spark (#4176)
    
    * Fix early stop with xgboost4j-spark
    
    * Update XGBoost.java
    
    * Update XGBoost.java
    
    * Update XGBoost.java
    
    To use -Float.MAX_VALUE as the lower bound, in case there is positive metric.
    
    * Only update best score if the current score is better (no update when equal)
    
    * Update xgboost-spark tutorial to fix early stopping docs.
    [jvm-packages]support multiple validation datasets in Spark (#3910)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * wrap iterators
    
    * enable copartition training and validationset
    
    * add parameters
    
    * converge code path and have init unit test
    
    * enable multi evals for ranking
    
    * unit test and doc
    
    * update example
    
    * fix early stopping
    
    * address the offline comments
    
    * udpate doc
    
    * test eval metrics
    
    * fix compilation issue
    
    * fix example
    [jvm-packages] Require vanilla Apache Spark (#3854)
    [jvm-packages] documenting tracker  (#3831)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * documenting tracker
    
    * Make it a separate note
    [Blocking][jvm-packages] fix the early stopping feature (#3808)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * temp
    
    * add method for classifier and regressor
    
    * update tutorial
    
    * address the comments
    
    * update
    Update Python API doc (#3619)
    
    * Add XGBRanker to Python API doc
    
    * Show inherited members of XGBRegressor in API doc, since XGBRegressor uses default methods from XGBModel
    
    * Add table of contents to Python API doc
    
    * Skip JVM doc download if not available
    
    * Show inherited members for XGBRegressor and XGBRanker
    
    * Expose XGBRanker to Python XGBoost module directory
    
    * Add docstring to XGBRegressor.predict() and XGBRanker.predict()
    
    * Fix rendering errors in Python docstrings
    
    * Fix lint
    [BLOCKING] Adding JVM doc build to Jenkins CI (#3567)
    
    * Adding Java/Scala doc build to Jenkins CI
    
    * Deploy built doc to S3 bucket
    
    * Build doc only for branches
    
    * Build doc first, to get doc faster for branch updates
    
    * Have ReadTheDocs download doc tarball from S3
    
    * Update JVM doc links
    
    * Put doc build commands in a script
    
    * Specify Spark 2.3+ requirement for XGBoost4J-Spark
    
    * Build GPU wheel without NCCL, to reduce binary size
    [jvm-packages] Tutorial of XGBoost4J-Spark (#3534)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add new
    
    * update doc
    
    * finish Gang Scheduling
    
    * more
    
    * intro
    
    * Add sections: Prediction, Model persistence and ML pipeline.
    
    * Add XGBoost4j-Spark MLlib pipeline example
    
    * partial finished version
    
    * finish the doc
    
    * adjust code
    
    * fix the doc
    
    * use rst
    
    * Convert XGBoost4J-Spark tutorial to reST
    
    * Bring XGBoost4J up to date
    
    * add note about using hdfs
    
    * remove duplicate file
    
    * fix descriptions
    
    * update doc
    
    * Wrap HDFS/S3 export support as a note
    
    * update
    
    * wrap indexing_mode example in code block
    Remove `silent` in doc. [skip ci] (#4689)
    Doc and demo for customized metric and obj. (#4598)
    
    Co-Authored-By: Theodore Vasiloudis <theodoros.vasiloudis@gmail.com>
    Bring XGBoost4J Intro up-to-date (#3574)
    [jvm-packages] Tutorial of XGBoost4J-Spark (#3534)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add new
    
    * update doc
    
    * finish Gang Scheduling
    
    * more
    
    * intro
    
    * Add sections: Prediction, Model persistence and ML pipeline.
    
    * Add XGBoost4j-Spark MLlib pipeline example
    
    * partial finished version
    
    * finish the doc
    
    * adjust code
    
    * fix the doc
    
    * use rst
    
    * Convert XGBoost4J-Spark tutorial to reST
    
    * Bring XGBoost4J up to date
    
    * add note about using hdfs
    
    * remove duplicate file
    
    * fix descriptions
    
    * update doc
    
    * Wrap HDFS/S3 export support as a note
    
    * update
    
    * wrap indexing_mode example in code block
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    Fix doc for CMake requirement. (#6123)
    [jvm-packages] [doc] Update install doc for JVM packages (#6051)
    [BLOCKING] [jvm-packages] add gpu_hist and enable gpu scheduling (#5171)
    
    * [jvm-packages] add gpu_hist tree method
    
    * change updater hist to grow_quantile_histmaker
    
    * add gpu scheduling
    
    * pass correct parameters to xgboost library
    
    * remove debug info
    
    * add use.cuda for pom
    
    * add CI for gpu_hist for jvm
    
    * add gpu unit tests
    
    * use gpu node to build jvm
    
    * use nvidia-docker
    
    * Add CLI interface to create_jni.py using argparse
    
    Co-authored-by: Hyunsu Cho <chohyu01@cs.washington.edu>
    [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)
    Copy CMake parameter from dmlc-core. (#4948)
    [jvm-packages] Require vanilla Apache Spark (#3854)
    [BLOCKING] Adding JVM doc build to Jenkins CI (#3567)
    
    * Adding Java/Scala doc build to Jenkins CI
    
    * Deploy built doc to S3 bucket
    
    * Build doc only for branches
    
    * Build doc first, to get doc faster for branch updates
    
    * Have ReadTheDocs download doc tarball from S3
    
    * Update JVM doc links
    
    * Put doc build commands in a script
    
    * Specify Spark 2.3+ requirement for XGBoost4J-Spark
    
    * Build GPU wheel without NCCL, to reduce binary size
    [jvm-packages] Tutorial of XGBoost4J-Spark (#3534)
    
    * add back train method but mark as deprecated
    
    * add back train method but mark as deprecated
    
    * fix scalastyle error
    
    * fix scalastyle error
    
    * add new
    
    * update doc
    
    * finish Gang Scheduling
    
    * more
    
    * intro
    
    * Add sections: Prediction, Model persistence and ML pipeline.
    
    * Add XGBoost4j-Spark MLlib pipeline example
    
    * partial finished version
    
    * finish the doc
    
    * adjust code
    
    * fix the doc
    
    * use rst
    
    * Convert XGBoost4J-Spark tutorial to reST
    
    * Bring XGBoost4J up to date
    
    * add note about using hdfs
    
    * remove duplicate file
    
    * fix descriptions
    
    * update doc
    
    * Wrap HDFS/S3 export support as a note
    
    * update
    
    * wrap indexing_mode example in code block
    Clarify supported OSes for XGBoost4J published JARs (#3547)
    Doc modernization (#3474)
    
    * Change doc build to reST exclusively
    
    * Rewrite Intro doc in reST; create toctree
    
    * Update parameter and contribute
    
    * Convert tutorials to reST
    
    * Convert Python tutorials to reST
    
    * Convert CLI and Julia docs to reST
    
    * Enable markdown for R vignettes
    
    * Done migrating to reST
    
    * Add guzzle_sphinx_theme to requirements
    
    * Add breathe to requirements
    
    * Fix search bar
    
    * Add link to user forum
    [BLOCKING] Adding JVM doc build to Jenkins CI (#3567)
    
    * Adding Java/Scala doc build to Jenkins CI
    
    * Deploy built doc to S3 bucket
    
    * Build doc only for branches
    
    * Build doc first, to get doc faster for branch updates
    
    * Have ReadTheDocs download doc tarball from S3
    
    * Update JVM doc links
    
    * Put doc build commands in a script
    
    * Specify Spark 2.3+ requirement for XGBoost4J-Spark
    
    * Build GPU wheel without NCCL, to reduce binary size
    [BLOCKING] Adding JVM doc build to Jenkins CI (#3567)
    
    * Adding Java/Scala doc build to Jenkins CI
    
    * Deploy built doc to S3 bucket
    
    * Build doc only for branches
    
    * Build doc first, to get doc faster for branch updates
    
    * Have ReadTheDocs download doc tarball from S3
    
    * Update JVM doc links
    
    * Put doc build commands in a script
    
    * Specify Spark 2.3+ requirement for XGBoost4J-Spark
    
    * Build GPU wheel without NCCL, to reduce binary size
    [BLOCKING] Adding JVM doc build to Jenkins CI (#3567)
    
    * Adding Java/Scala doc build to Jenkins CI
    
    * Deploy built doc to S3 bucket
    
    * Build doc only for branches
    
    * Build doc first, to get doc faster for branch updates
    
    * Have ReadTheDocs download doc tarball from S3
    
    * Update JVM doc links
    
    * Put doc build commands in a script
    
    * Specify Spark 2.3+ requirement for XGBoost4J-Spark
    
    * Build GPU wheel without NCCL, to reduce binary size
    [BLOCKING] Adding JVM doc build to Jenkins CI (#3567)
    
    * Adding Java/Scala doc build to Jenkins CI
    
    * Deploy built doc to S3 bucket
    
    * Build doc only for branches
    
    * Build doc first, to get doc faster for branch updates
    
    * Have ReadTheDocs download doc tarball from S3
    
    * Update JVM doc links
    
    * Put doc build commands in a script
    
    * Specify Spark 2.3+ requirement for XGBoost4J-Spark
    
    * Build GPU wheel without NCCL, to reduce binary size
    Use bundled gtest (#4900)
    
    * Suggest to use gtest bundled with dmlc
    
    * Use dmlc bundled gtest in all CI scripts
    
    * Make clang-tidy to use dmlc embedded gtest
    Re-organize contributor's guide (#4659)
    
    * Reorganize contributor's doc
    
    * Address comments from @trivialfis
    
    * Address @sriramch's comment: include ABI compatibility guarantee
    
    * Address @rongou's comment
    
    * Postpone ABI compatibility guarantee for now
    Add release note for 1.0.0 in NEWS.md (#5329)
    
    * Add release note for 1.0.0
    
    * Fix a small bug in the Python script that compiles the list of contributors
    
    * Clarify governance of CI infrastructure; now PMC is formally in charge
    
    * Address reviewer comment
    
    * Fix typo
    Re-organize contributor's guide (#4659)
    
    * Reorganize contributor's doc
    
    * Address comments from @trivialfis
    
    * Address @sriramch's comment: include ABI compatibility guarantee
    
    * Address @rongou's comment
    
    * Postpone ABI compatibility guarantee for now
    contribute to community doc (#4646)
    
    * add community doc
    
    * update
    
    * update
    Re-organize contributor's guide (#4659)
    
    * Reorganize contributor's doc
    
    * Address comments from @trivialfis
    
    * Address @sriramch's comment: include ABI compatibility guarantee
    
    * Address @rongou's comment
    
    * Postpone ABI compatibility guarantee for now
    Re-organize contributor's guide (#4659)
    
    * Reorganize contributor's doc
    
    * Address comments from @trivialfis
    
    * Address @sriramch's comment: include ABI compatibility guarantee
    
    * Address @rongou's comment
    
    * Postpone ABI compatibility guarantee for now
    contribute to community doc (#4646)
    
    * add community doc
    
    * update
    
    * update
    Implement intrusive ptr (#6129)
    
    
    * Use intrusive ptr for JSON.
    Document minimum version required for gtest [skip ci] (#5001)
    Use bundled gtest (#4900)
    
    * Suggest to use gtest bundled with dmlc
    
    * Use dmlc bundled gtest in all CI scripts
    
    * Make clang-tidy to use dmlc embedded gtest
    Re-organize contributor's guide (#4659)
    
    * Reorganize contributor's doc
    
    * Address comments from @trivialfis
    
    * Address @sriramch's comment: include ABI compatibility guarantee
    
    * Address @rongou's comment
    
    * Postpone ABI compatibility guarantee for now
    Add release note for 1.0.0 in NEWS.md (#5329)
    
    * Add release note for 1.0.0
    
    * Fix a small bug in the Python script that compiles the list of contributors
    
    * Clarify governance of CI infrastructure; now PMC is formally in charge
    
    * Address reviewer comment
    
    * Fix typo
    Re-organize contributor's guide (#4659)
    
    * Reorganize contributor's doc
    
    * Address comments from @trivialfis
    
    * Address @sriramch's comment: include ABI compatibility guarantee
    
    * Address @rongou's comment
    
    * Postpone ABI compatibility guarantee for now
    contribute to community doc (#4646)
    
    * add community doc
    
    * update
    
    * update
    Add release note for 1.0.0 in NEWS.md (#5329)
    
    * Add release note for 1.0.0
    
    * Fix a small bug in the Python script that compiles the list of contributors
    
    * Clarify governance of CI infrastructure; now PMC is formally in charge
    
    * Address reviewer comment
    
    * Fix typo
    Fix data loading (#4862)
    
    * Fix loading text data.
    * Fix config regex.
    * Try to explain the error better in exception.
    * Update doc.
    Re-organize contributor's guide (#4659)
    
    * Reorganize contributor's doc
    
    * Address comments from @trivialfis
    
    * Address @sriramch's comment: include ABI compatibility guarantee
    
    * Address @rongou's comment
    
    * Postpone ABI compatibility guarantee for now
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove unused RABIT targets. (#6110)
    
    
    * Remove rabit mock.
    * Remove rabit base.
    Enable building rabit on Windows (#6105)
    Refactor rabit tests (#6096)
    
    
    * Merge rabit tests into XGBoost.
    * Run them On CI.
    * Simplification for CMake scripts.
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Enable building rabit on Windows (#6105)
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Enable building rabit on Windows (#6105)
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Enable building rabit on Windows (#6105)
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Enable building rabit on Windows (#6105)
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Enable building rabit on Windows (#6105)
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Correct style warnings from clang-tidy for rabit. (#6095)
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
    Remove rabit.
    Rabit update. (#5978)
    
    * Remove parameter on JVM Packages.
    Upgrade Rabit (#5876)
    Update rabit. (#5680)
    Hide C++ symbols in libxgboost.so when building Python wheel (#5590)
    
    * Hide C++ symbols in libxgboost.so when building Python wheel
    
    * Update Jenkinsfile
    
    * Add test
    
    * Upgrade rabit
    
    * Add setup.py option.
    
    Co-authored-by: fis <jm.yuan@outlook.com>
    Update Rabit (#5237)
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4966)
    
    * [phase 1] expose sets of rabit configurations to spark layer
    
    * add back mutable import
    
    * disable ring_mincount till https://github.com/dmlc/rabit/pull/106d
    
    * Revert "disable ring_mincount till https://github.com/dmlc/rabit/pull/106d"
    
    This reverts commit 65e95a98e24f5eb53c6ba9ef9b2379524258984d.
    
    * apply latest rabit
    
    * fix build error
    
    * apply https://github.com/dmlc/xgboost/pull/4880
    
    * downgrade cmake in rabit
    
    * point to rabit with DMLC_ROOT fix
    
    * relative path of rabit install prefix
    
    * split rabit parameters to another trait
    
    * misc
    
    * misc
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Delete .classpath
    
    * Update XGBoostClassifier.scala
    
    * Update XGBoostRegressor.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Update GeneralParams.scala
    
    * Delete .classpath
    
    * Update RabitParams.scala
    
    * Update .gitignore
    
    * Update .gitignore
    
    * apply rabitParams to training
    
    * use string as rabit parameter value type
    
    * cleanup
    
    * add rabitEnv check
    
    * point to dmlc/rabit
    
    * per feedback
    
    * update private scope
    
    * misc
    
    * update rabit
    
    * add rabit_timtout, fix failing test.
    
    * split tests
    
    * allow build jvm with rabit mock
    
    * pass mock failures to rabit with test
    
    * add mock error and graceful handle rabit assertion error test
    
    * split mvn test
    
    * remove sign for test
    
    * update rabit
    
    * build jvm_packages with rabit mock
    
    * point back to dmlc/rabit
    
    * per feedback, update scala header
    
    * cleanup pom
    
    * per feedback
    
    * try fix lint
    
    * fix lint
    
    * per feedback, remove bootstrap_cache
    
    * per feedback 2
    
    * try replace dev profile with passing mvn property
    
    * fix build error
    
    * remove mvn property and replace with env setting to build test jar
    
    * per feedback
    
    * revert copyright headlines, point to dmlc/rabit
    
    * revert python lint
    
    * remove multiple failure test case as retry is not enabled in spark
    
    * Update core.py
    
    * Update core.py
    
    * per feedback, style fix
    Revert "[jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)" (#4965)
    
    This reverts commit 86ed01c4bbecef66e1bc4d02fb13116bd6130fae.
    [jvm-packages] update rabit, surface new changes to spark, add parity and failure tests (#4876)
    
    * Expose sets of rabit configurations to spark layer
    [rabit_bootstrap_cache ] failed xgb worker recover from other workers (#4808)
    
    
    * Better recovery support.  Restarting only the failed workers.
    updating rabit commit hash (#4718)
    [jvm-packages] allowing chaining prediction (#4667)
    
    * add test for chaining prediction
    
    * update rabit
    
    * Update XGBoostGeneralSuite.scala
    [jvm-packages] delete all constraints from spark layer about obj and eval metrics and handle error in jvm layer (#4560)
    
    * temp
    
    * prediction part
    
    * remove supported*
    
    * add for test
    
    * fix param name
    
    * add rabit
    
    * update rabit
    
    * return value of rabit init
    
    * eliminate compilation warnings
    
    * update rabit
    
    * shutdown
    
    * update rabit again
    
    * check sparkcontext shutdown
    
    * fix logic
    
    * sleep
    
    * fix tests
    
    * test with relaxed threshold
    
    * create new thread each time
    
    * stop for job quitting
    
    * udpate rabit
    
    * update rabit
    
    * update rabit
    
    * update git modules
    [jvm-packages] allow partial evaluation of dataframe before prediction (#4407)
    
    * allow partial evaluation of dataframe before prediction
    
    * resume spark test
    
    * comments
    
    * Run unit tests after building JVM packages
    Upgrade rabit. (#4159)
    update rabit (#3835)
    Fix CRAN check for lintr (#3372)
    
    * fix CRAN check
    
    * Update submodules dmlc-core and rabit
    
    * Add kintr to rmingw test
    Point rabit submodule at latest commit from master. (#3330)
    Implement GPU accelerated coordinate descent algorithm (#3178)
    
    * Implement GPU accelerated coordinate descent algorithm.
    
    * Exclude external memory tests for GPU
    Update rabit submodule to latest version. (#3246)
    [UPDATE] Update rabit and threadlocal (#2114)
    
    * [UPDATE] Update rabit and threadlocal
    
    * minor fix to make build system happy
    
    * upgrade requirement to g++4.8
    
    * upgrade dmlc-core
    
    * update travis
    Fix warnings from g++5 or higher (#1510)
    Update rabit repository (#1409)
    [PYTHON] Refactor trainnig API to use callback
    Update rabit to latest
    allow common python output in single node
    [FLINK] Make runnable flink
    framework of xgboost-spark
    
    iterator
    
    return java iterator and recover test
    [JVM] Add Iterator loading API
    revise the RabitTracker Impl
    
    delete FileUtil class
    
    fix bugs
    [DIST] Enable multiple thread and tracker, make rabit and xgboost more thread-safe by using thread local variables.
    apply  google-java-style indentation and impose import orders....
    Update rabit
    [PYTHON-DIST] Distributed xgboost python training API.
    [PYTHON] Simplify training logic, update rabit lib
    [LZ] Improve lz4 format
    [LIBXGBOOST] pass demo running.
    [REFACTOR] cleanup structure
